QuestionKenntnis:
Why is processing a sorted array faster than an unsorted array?
qn_description:
Here is a piece of C++ code that seems very peculiar. For some strange reason, sorting the data miraculously makes the code almost six times faster:

#include <algorithm>
#include <ctime>
#include <iostream>

int main()
{
    // Generate data
    const unsigned arraySize = 32768;
    int data[arraySize];

    for (unsigned c = 0; c < arraySize; ++c)
        data[c] = std::rand() % 256;

    // !!! With this, the next loop runs faster
    std::sort(data, data + arraySize);

    // Test
    clock_t start = clock();
    long long sum = 0;

    for (unsigned i = 0; i < 100000; ++i)
    {
        // Primary loop
        for (unsigned c = 0; c < arraySize; ++c)
        {
            if (data[c] >= 128)
                sum += data[c];
        }
    }

    double elapsedTime = static_cast<double>(clock() - start) / CLOCKS_PER_SEC;

    std::cout << elapsedTime << std::endl;
    std::cout << "sum = " << sum << std::endl;
}



Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds.
With the sorted data, the code runs in 1.93 seconds.




Initially, I thought this might be just a language or compiler anomaly. So I tried it in Java:

import java.util.Arrays;
import java.util.Random;

public class Main
{
    public static void main(String[] args)
    {
        // Generate data
        int arraySize = 32768;
        int data[] = new int[arraySize];

        Random rnd = new Random(0);
        for (int c = 0; c < arraySize; ++c)
            data[c] = rnd.nextInt() % 256;

        // !!! With this, the next loop runs faster
        Arrays.sort(data);

        // Test
        long start = System.nanoTime();
        long sum = 0;

        for (int i = 0; i < 100000; ++i)
        {
            // Primary loop
            for (int c = 0; c < arraySize; ++c)
            {
                if (data[c] >= 128)
                    sum += data[c];
            }
        }

        System.out.println((System.nanoTime() - start) / 1000000000.0);
        System.out.println("sum = " + sum);
    }
}


With a somewhat similar, but less extreme result.



My first thought was that sorting brings the data into the cache, but my next thought was how silly that is, because the array was just generated.


What is going on? 
Why is a sorted array faster than an unsorted array? 
The code is summing up some independent terms, and the order should not matter.
AnswersKenntnis
AnswerKenntnis:
You are the victim of branch prediction fail.



What is Branch Prediction?

Consider a railroad junction:


Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license.

Now for the sake of argument, suppose this is back in the 1800s - before long distance or radio communication.

You are the operator of a junction and you hear a train coming. You have no idea which way it will go. You stop the train to ask the captain which direction he wants. And then you set the switch appropriately.

Trains are heavy and have a lot of inertia. So they take forever to start up and slow down.

Is there a better way? You guess which direction the train will go!


If you guessed right, it continues on.
If you guessed wrong, the captain will stop, back up, and yell at you to flip the switch. Then it can restart down the other path.


If you guess right every time, the train will never have to stop.
If you guess wrong too often, the train will spend a lot of time stopping, backing up, and restarting.



Consider an if-statement: At the processor level, it is a branch instruction:



You are a processor and you see a branch. You have no idea which way it will go. What do you do? You halt execution and wait until the previous instructions are complete. Then you continue down the correct path.

Modern processors are complicated and have long pipelines. So they take forever to "warm up" and "slow down".

Is there a better way? You guess which direction the branch will go!


If you guessed right, you continue executing.
If you guessed wrong, you need to flush the pipeline and roll back to the branch. Then you can restart down the other path.


If you guess right every time, the execution will never have to stop.
If you guess wrong too often, you spend a lot of time stalling, rolling back, and restarting.



This is branch prediction. I admit it's not the best analogy since the train could just signal the direction with a flag. But in computers, the processor doesn't know which direction a branch will go until the last moment.

So how would you strategically guess to minimize the number of times that the train must back up and go down the other path? You look at the past history! If the train goes left 99% of the time, then you guess left. If it alternates, then you alternate your guesses. If it goes one way every 3 times, you guess the same...

In other words, you try to identify a pattern and follow it. This is more or less how branch predictors work.

Most applications have well-behaved branches. So modern branch predictors will typically achieve >90% hit rates. But when faced with unpredictable branches with no recognizable patterns, branch predictors are virtually useless.

Further reading: "Branch predictor" article on Wikipedia.



As hinted from above, the culprit is this if-statement:

if (data[c] >= 128)
    sum += data[c];


Notice that the data is evenly distributed between 0 and 255. 
When the data is sorted, roughly the first half of the iterations will not enter the if-statement. After that, they will all enter the if-statement.

This is very friendly to the branch predictor since the branch consecutively goes the same direction many times.
Even a simple saturating counter will correctly predict the branch except for the few iterations after it switches direction.

Quick visualization:

T = branch taken
N = branch not taken

data[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ...
branch = N  N  N  N  N  ...   N    N    T    T    T  ...   T    T    T  ...

       = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT  (easy to predict)


However, when the data is completely random, the branch predictor is rendered useless because it can't predict random data.
Thus there will probably be around 50% misprediction. (no better than random guessing)

data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118,  14, 150, 177, 182, 133, ...
branch =   T,   T,   N,   T,   T,   T,   T,  N,   T,   N,   N,   T,   T,   T,   N  ...

       = TTNTTTTNTNNTTTN ...   (completely random - hard to predict)




So what can be done?

If the compiler isn't able to optimize the branch into a conditional move, you can try some hacks if you are willing to sacrifice readability for performance.

Replace:

if (data[c] >= 128)
    sum += data[c];


with:

int t = (data[c] - 128) >> 31;
sum += ~t & data[c];


This eliminates the branch and replaces it with some bitwise operations.

(Note that this hack is not strictly equivalent to the original if-statement. But in this case, it's valid for all the input values of data[].)

Benchmarks: Core i7 920 @ 3.5 GHz

C++ - Visual Studio 2010 - x64 Release

//  Branch - Random
seconds = 11.777

//  Branch - Sorted
seconds = 2.352

//  Branchless - Random
seconds = 2.564

//  Branchless - Sorted
seconds = 2.587


Java - Netbeans 7.1.1 JDK 7 - x64

//  Branch - Random
seconds = 10.93293813

//  Branch - Sorted
seconds = 5.643797077

//  Branchless - Random
seconds = 3.113581453

//  Branchless - Sorted
seconds = 3.186068823


Observations:


With the Branch: There is a huge difference between the sorted and unsorted data.
With the Hack: There is no difference between sorted and unsorted data.
In the C++ case, the hack is actually a tad slower than with the branch when the data is sorted.


A general rule of thumb is to avoid data-dependent branching in critical loops. (such as in this example)



Update :


GCC 4.6.1 with -O3 or -ftree-vectorize on x64 is able to generate a conditional move. So there is no difference between the sorted and unsorted data - both are fast.
VC++ 2010 is unable to generate conditional moves for this branch even under /Ox.
Intel Compiler 11 does something miraculous. It interchanges the two loops, thereby hoisting the unpredictable branch to the outer loop. So not only is it immune the mispredictions, it is also twice as fast as whatever VC++ and GCC can generate! In other words, ICC took advantage of the test-loop to defeat the benchmark...
If you give the Intel Compiler the branchless code, it just out-right vectorizes it... and is just as fast as with the branch (with the loop interchange).


This goes to show that even mature modern compilers can vary wildly in their ability to optimize code...
AnswerKenntnis:
Branch prediction. With a sorted array, the condition data[c] >= 128 is first false for a streak of values, then becomes true for all later values. That's easy to predict. With an unsorted array, you pay the branching cost.
AnswerKenntnis:
The reason why the performance improves drastically when the data are sorted is that the branch prediction penalty is removed, as explained beautifully in Mysticial's answer.

Now, if we look at the code

if (data[c] >= 128)
    sum += data[c];


we can find that the meaning of this particular if... else... branch is to add something when a condition is satisfied. This type of branch can be easily transformed into a conditional move statement, which would be compiled into a conditional move instruction, cmovl, in an x86 system. The branch and thus the potential branch prediction penalty is removed. 

In C, thus C++, the statment, which would compile directly (without any optimization) into the conditional move instruction in x86, is the ternary operator ... ? ... : .... So we rewrite the above statement into an equivalent one:

sum += data[c] >=128 ? data[c] : 0;


While maintaining readability, we can check the speedup factor.

On an Intel Core i7-2600K @ 3.4 GHz and Visual Studio 2010 Release Mode,
the benchmark is (format copied from Mysticial):

x86

//  Branch - Random
seconds = 8.885

//  Branch - Sorted
seconds = 1.528

//  Branchless - Random
seconds = 3.716

//  Branchless - Sorted
seconds = 3.71


x64

//  Branch - Random
seconds = 11.302

//  Branch - Sorted
 seconds = 1.830

//  Branchless - Random
seconds = 2.736

//  Branchless - Sorted
seconds = 2.737


The result is robust in multiple tests. We get great speedup when the branch result is unpredictable, but we suffer a little bit when it is predictable. In fact, when using a conditional move, the performance is the same regardless of the data pattern.

Now let's look more closely by investigating at the x86 assembly they generate. For simplicity, we use two functions max1 and max2.

max1 uses the conditional branch if... else ...:

int max1(int a, int b) {
    if (a > b)
        return a;
    else
        return b;
}


max2 uses the ternary operator ... ? ... : ...:

int max2(int a, int b) {
    return a > b ? a : b;
}


On a x86-64 machine, GCC -S generates the assembly below.

:max1
    movl    %edi, -4(%rbp)
    movl    %esi, -8(%rbp)
    movl    -4(%rbp), %eax
    cmpl    -8(%rbp), %eax
    jle     .L2
    movl    -4(%rbp), %eax
    movl    %eax, -12(%rbp)
    jmp     .L4
.L2:
    movl    -8(%rbp), %eax
    movl    %eax, -12(%rbp)
.L4:
    movl    -12(%rbp), %eax
    leave
    ret

:max2
    movl    %edi, -4(%rbp)
    movl    %esi, -8(%rbp)
    movl    -4(%rbp), %eax
    cmpl    %eax, -8(%rbp)
    cmovge  -8(%rbp), %eax
    leave
    ret


max2 uses much less code due to the usage of instruction cmovge. But the real gain is that max2 does not involve branch jumps, jmp, which would have a significant performance penalty if the predicted result is not right.

So why can a conditional move perform better?

In a typical x86 processor, the execution of an instruction is divided to several stages. Roughly, we have different hardware to deal with different stages. So we do not have to wait for one instruction to finish to start a new one. This is called pipelining.

In a branch case, the following instruction is determined by the preceding one, so we can not do pipelining. We have to either wait or predict.

In a conditional move case, the execution conditional move instruction is divided into several stages, but the earlier stages like Fetch, Decode, does not depend on the result of previous instruction, only latter stages need the result. So we wait a fraction of one instruction's execution time. This is why the conditional move version is slower than the branch when prediction is easy.

The book Computer Systems: A Programmer's Perspective, second edition explains this in detail. You can check Section 3.6.6 for Conditional Move Instructions, entire Chapter 4 for Processor Architecture, and Section 5.11.2 for a special treatment for Branch Prediction and Misprediction Penalties.

Sometimes, some modern compilers can optimize our code to assembly with better performance, sometimes some compilers can't (the code in question is using Visual Studio's native compiler). Knowing the performance difference between branch and conditional move when unpredictable can help us write code with better performance when the scenario gets so complex that the compiler can not optimize them automatically.
AnswerKenntnis:
If you are curious about even more optimizations that can be done to this code, consider this... Starting with the original loop:

for (unsigned i = 0; i < 100000; ++i)
{
    for (unsigned j = 0; j < arraySize; ++j)
    {
        if (data[j] >= 128)
            sum += data[j];
    }
}


With loop interchange, we can safely change this loop to:

for (unsigned j = 0; j < arraySize; ++j)
{
    for (unsigned i = 0; i < 100000; ++i)
    {
        if (data[j] >= 128)
            sum += data[j];
    }
}


Then, you can see that the "if" conditional is constant throughout the execution of the "i" loop, so you can hoist the "if" out:

for (unsigned j = 0; j < arraySize; ++j)
{
    if (data[j] >= 128)
    {
        for (unsigned i = 0; i < 100000; ++i)
        {
            sum += data[j];
        }
    }
}


Then, you see that the inner loop can be collapsed into one single expression, assuming the floating point model allows it (/fp:fast is thrown, for example)

for (unsigned j = 0; j < arraySize; ++j)
{
    if (data[j] >= 128)
    {
        sum += data[j] * 100000;
    }
}


That one is 100,000x faster than before (-8
AnswerKenntnis:
No doubt some of us would be interested in ways of identifying code that is problematic for the CPU's branch-predictor. The Valgrind tool cachegrind has a branch-predictor simulator, enabled by using the --branch-sim=yes flag. Running it over the examples in this question, with the number of outer loops reduced to 10000 and compiled with g++, gives these results:

Sorted:

==32551== Branches:        656,645,130  (  656,609,208 cond +    35,922 ind)
==32551== Mispredicts:         169,556  (      169,095 cond +       461 ind)
==32551== Mispred rate:            0.0% (          0.0%     +       1.2%   )


Unsorted:

==32555== Branches:        655,996,082  (  655,960,160 cond +  35,922 ind)
==32555== Mispredicts:     164,073,152  (  164,072,692 cond +     460 ind)
==32555== Mispred rate:           25.0% (         25.0%     +     1.2%   )


Drilling down into the line-by-line output produced by cg_annotate we see for the loop in question:

Sorted:

          Bc    Bcm Bi Bim
      10,001      4  0   0      for (unsigned i = 0; i < 10000; ++i)
           .      .  .   .      {
           .      .  .   .          // primary loop
 327,690,000 10,016  0   0          for (unsigned c = 0; c < arraySize; ++c)
           .      .  .   .          {
 327,680,000 10,006  0   0              if (data[c] >= 128)
           0      0  0   0                  sum += data[c];
           .      .  .   .          }
           .      .  .   .      }


Unsorted:

          Bc         Bcm Bi Bim
      10,001           4  0   0      for (unsigned i = 0; i < 10000; ++i)
           .           .  .   .      {
           .           .  .   .          // primary loop
 327,690,000      10,038  0   0          for (unsigned c = 0; c < arraySize; ++c)
           .           .  .   .          {
 327,680,000 164,050,007  0   0              if (data[c] >= 128)
           0           0  0   0                  sum += data[c];
           .           .  .   .          }
           .           .  .   .      }


This lets you easily identify the problematic line - in the unsorted version the if (data[c] >= 128) line is causing 164,050,007 mispredicted conditional branches (Bcm) under cachegrind's branch-predictor model, whereas it's only causing 10,006 in the sorted version.



Alternatively, on Linux you can use the performance counters subsystem to accomplish the same task, but with native performance using CPU counters.

perf stat ./sumtest_sorted


Sorted:

 Performance counter stats for './sumtest_sorted':

  11808.095776 task-clock                #    0.998 CPUs utilized          
         1,062 context-switches          #    0.090 K/sec                  
            14 CPU-migrations            #    0.001 K/sec                  
           337 page-faults               #    0.029 K/sec                  
26,487,882,764 cycles                    #    2.243 GHz                    
41,025,654,322 instructions              #    1.55  insns per cycle        
 6,558,871,379 branches                  #  555.455 M/sec                  
       567,204 branch-misses             #    0.01% of all branches        

  11.827228330 seconds time elapsed


Unsorted:

 Performance counter stats for './sumtest_unsorted':

  28877.954344 task-clock                #    0.998 CPUs utilized          
         2,584 context-switches          #    0.089 K/sec                  
            18 CPU-migrations            #    0.001 K/sec                  
           335 page-faults               #    0.012 K/sec                  
65,076,127,595 cycles                    #    2.253 GHz                    
41,032,528,741 instructions              #    0.63  insns per cycle        
 6,560,579,013 branches                  #  227.183 M/sec                  
 1,646,394,749 branch-misses             #   25.10% of all branches        

  28.935500947 seconds time elapsed


It can also do source code annotation with dissassembly.

perf record -e branch-misses ./sumtest_unsorted
perf annotate -d sumtest_unsorted




 Percent |      Source code & Disassembly of sumtest_unsorted
------------------------------------------------
...
         :                      sum += data[c];
    0.00 :        400a1a:       mov    -0x14(%rbp),%eax
   39.97 :        400a1d:       mov    %eax,%eax
    5.31 :        400a1f:       mov    -0x20040(%rbp,%rax,4),%eax
    4.60 :        400a26:       cltq   
    0.00 :        400a28:       add    %rax,-0x30(%rbp)
...


See the performance tutorial for more details.
AnswerKenntnis:
As data is distributed between 0 and 255 when array is sorted, around first half of the iterations will not enter the if-statement (if statement shared below).

if (data[c] >= 128)
    sum += data[c];


Question is what make the above statement not execute in certain case as in case of sorted data? Here comes the "Branch predictor" a branch predictor is a digital circuit that tries to guess which way a branch (e.g. an if-then-else structure) will go before this is known for sure. The purpose of the branch predictor is to improve the flow in the instruction pipeline. Branch predictors play a critical role in achieving high effective performance!

Lets do some bench marking to understand it better

The performance of an if-statement depends on whether its condition has a predictable pattern. If the condition is always true or always false, the branch prediction logic in the processor will pick up the pattern. On the other hand, if the pattern is unpredictable, the if-statement will be much more expensive.

LetΓÇÖs measure the performance of this loop with different conditions:

for (int i = 0; i < max; i++) if (condition) sum++;


Here are the timings of the loop with different True-False patterns:

Condition           Pattern              Time (ms)

(i & 0├ù80000000) == 0   T repeated        322

(i & 0xffffffff) == 0   F repeated        276

(i & 1) == 0            TF alternating    760

(i & 3) == 0            TFFFTFFFΓÇª         513

(i & 2) == 0            TTFFTTFFΓÇª         1675

(i & 4) == 0            TTTTFFFFTTTTFFFFΓÇª 1275

(i & 8) == 0            8T 8F 8T 8F ΓÇª     752

(i & 16) == 0           16T 16F 16T 16F ΓÇª 490


A ΓÇ£badΓÇ¥ true-false pattern can make an if-statement up to six times slower than a ΓÇ£goodΓÇ¥ pattern! Of course, which pattern is good and which is bad depends on the exact instructions generated by the compiler and on the specific processor.

So there is no doubt about impact of branch prediction on performance!
AnswerKenntnis:
Just read up on the thread and I feel an answer is missing. A common way to eliminate branch prediction that I've found to work particularly good in managed languages is a table lookup instead of using a branch. (although I haven't tested it in this case)

This approach works in general if:


It's a small table and is likely to be cached in the processor
You are running things in a quite tight loop and/or the processor can pre-load the data


Background and why

Pfew, so what the hell is that supposed to mean?

From a processor perspective, your memory is slow. To compensate for the difference in speed, they build in a couple of caches in your processor (L1/L2 cache) that compensate for that. So imagine that you're doing your nice calculations and figure out that you need a piece of memory. The processor will get his 'load' operation and loads the piece of memory into cache - and then uses the cache to do the rest of the calculations. Because memory is relatively slow, this 'load' will slow down your program. 

Like branch prediction, this was optimized in the Pentium processors: the processor predicts that it needs to load a piece of data and attempts to load that into the cache before the operation actually hits the cache. As we've already seen, branch prediction sometimes goes horribly wrong -- in the worst case scenario you need to go back and actually wait for a memory load, which will take forever (in other words: failing branch prediction is bad, a memory load after a branch prediction fail is just horrible!).

Fortunately for us, if the memory access pattern is predictable, the processor will load it in its fast cache and all is well.

First thing we need to know is what is small? While smaller is generally better, a rule of thumb is to stick to lookup tables that are <=4096 bytes in size. As an upper limit: if your lookup table is larger than 64K it's probably worth reconsidering.

Constructing a table

So we've figured out that we can create a small table. Next thing to do is get a lookup function in place. Lookup functions are usually small functions that use a couple of basic integer operations (and, or, xor, shift, add, remove and perhaps a multiply). What you want is to have your input translated by the lookup function to some kind of 'unique key' in your table, which then simply gives you the answer of all the work you wanted it to do.

In this case: >=128 means we can keep the value, <128 means we get rid of it. The easiest way to do that is by using an 'AND': if we keep it, we AND it with 7FFFFFFF ; if we want to get rid of it, we AND it with 0. Notice also that 128 is a power of 2 -- so we can go ahead and make a table of 32768/128 integers and fill it with one zero and a lot of 7FFFFFFFF's.

Managed languages

You might wonder why this works well in managed languages. After all, managed languages check the boundaries of the arrays with a branch to ensure you don't mess up...

Well, not exactly... :-)

There has been quite some work on eliminating this branch for managed languages. For example:

for (int i=0; i<array.Length; ++i)
   // use array[i]


in this case it's obvious to the compiler that the boundary condition will never hit. At least the Microsoft JIT compiler (but I expect Java does similar things) will notice this and remove the check all together. WOW - that means no branch. Similarly, it will deal with other obvious cases.

If you run into trouble with lookups on managed languages - the key is to add a & 0x[something]FFF to your lookup function to make the boundary check predictable - and watch it going faster.

The result for this case

// generate data
int arraySize = 32768;
int[] data = new int[arraySize];

Random rnd = new Random(0);
for (int c = 0; c < arraySize; ++c)
    data[c] = rnd.Next(256);


// Too keep the spirit of the code in-tact I'll make a separate lookup table
// (I assume we cannot modify 'data' or the number of loops)
int[] lookup = new int[256];

for (int c = 0; c < 256; ++c)
    lookup[c] = (c >= 128) ? c : 0;

// test
DateTime startTime = System.DateTime.Now;
long sum = 0;

for (int i = 0; i < 100000; ++i)
{
    // primary loop
    for (int j = 0; j < arraySize; ++j)
    {
        // here you basically want to use simple operations - so no 
        // random branches, but things like &, |, *, -, +, etc are fine.
        sum += lookup[data[j]];
    }
}

DateTime endTime = System.DateTime.Now;
Console.WriteLine(endTime - startTime);
Console.WriteLine("sum = " + sum);

Console.ReadLine();
AnswerKenntnis:
In the sorted case, you can do better than relying on successful branch prediction or any branchless comparison trick: completely remove the branch.

Indeed, the array is partitioned in a contiguous zone with data < 128 and another with data >= 128. So you should find the partition point with a dichtomic search (using Lg(arraySize) = 15 comparisons), then do a straight accumulation from that point.

Something like (unchecked)

int i= 0, j, k= arraySize;
while (i < k)
{
  j= (i + k) >> 1;
  if (data[j] >= 128)
    k= j;
  else
    i= j;
}
sum= 0;
for (; i < arraySize; i++)
  sum+= data[i];


or, slightly more obfuscated

int i, k, j= (i + k) >> 1;
for (i= 0, k= arraySize; i < k; (data[j] >= 128 ? k : i)= j)
  j= (i + k) >> 1;
for (sum= 0; i < arraySize; i++)
  sum+= data[i];


A yet faster approach, that gives an approximate solution for both sorted or unsorted is: sum= 3137536; (assuming a truly uniform distribution, 16384 samples with expected value 191.5) :-)
AnswerKenntnis:
One way to avoid branch prediction errors is to build a lookup table, and index it using the data.  Stefan de Bruijn discussed that in his answer.

But in this case, we know values are in the range [0, 255] and we only care about values >= 128.  That means we can easily extract a single bit that will tell us whether we want a value or not: by shifting the data to the right 7 bits, we are left with a 0 bit or a 1 bit, and we only want to add the value when we have a 1 bit.  Let's call this bit the "decision bit".

By using the 0/1 value of the decision bit as an index into an array, we can make code that will be equally fast whether the data is sorted or not sorted.  Our code will always add a value, but when the decision bit is 0, we will add the value somewhere we don't care about.  Here's the code:

// Test
clock_t start = clock();
long long a[] = {0, 0};
long long sum;

for (unsigned i = 0; i < 100000; ++i)
{
    // Primary loop
    for (unsigned c = 0; c < arraySize; ++c)
    {
        int j = (data[c] >> 7);
        a[j] += data[c];
    }
}

double elapsedTime = static_cast<double>(clock() - start) / CLOCKS_PER_SEC;
sum = a[1];


This code wastes half of the adds, but never has a branch prediction failure.  It's tremendously faster on random data than the version with an actual if statement.

But in my testing, an explicit lookup table was slightly faster than this, probably because indexing into a lookup table was slightly faster than bit shifting.  This shows how my code sets up and uses the lookup table (unimaginatively called lut for "LookUp Table" in the code).  Here's the C++ code:

// declare and then fill in the lookup table
int lut[256];
for (unsigned c = 0; c < 256; ++c)
    lut[c] = (c >= 128) ? c : 0;

// use the lookup table after it is built
for (unsigned i = 0; i < 100000; ++i)
{
    // Primary loop
    for (unsigned c = 0; c < arraySize; ++c)
    {
        sum += lut[data[c]];
    }
}


In this case the lookup table was only 256 bytes, so it fit nicely in cache and all was fast.  This technique wouldn't work well if the data was 24-bit values and we only wanted half of them... the lookup table would be far too big to be practical.  On the other hand, we can combine the two techniques shown above: first shift the bits over, then index a lookup table.  For a 24-bit value that we only want the top half value, we could potentially shift the data right by 12 bits, and be left with a 12-bit value for a table index.  A 12-bit table index implies a table of 4096 values, which might be practical.

EDIT: One thing I forgot to put in.

The technique of indexing into an array, instead of using an if statement, can be used for deciding which pointer to use.  I saw a library that implemented binary trees, and instead of having two named pointers (pLeft and pRight or whatever) had a length-2 array of pointers, and used the "decision bit" technique to decide which one to follow.  For example, instead of:

if (x < node->value)
    node = node->pLeft;
else
    node = node->pRight;


this library would do something like:

i = (x < node->value);
node = node->link[i];


Here's a link to this code: http://www.eternallyconfuzzled.com/tuts/datastructures/jsw_tut_rbtree.aspx
QuestionKenntnis:
Edit an incorrect commit message in Git
qn_description:
I wrote the wrong thing in a commit message.

How do I change the message? I have not yet pushed the commit to anyone.
AnswersKenntnis
AnswerKenntnis:
git commit --amend


Will open your editor, allowing you to change the commit message of the most recent commit. As the Git official documentation states,


  Used to amend the tip of the current branch. Prepare the tree object you would want to replace the latest commit as usual (this includes the usual -i/-o and explicit paths), and the commit log editor is seeded with the commit message from the tip of the current branch. The commit you create replaces the current tip ΓÇö if it was a merge, it will have the parents of the current tip as parents ΓÇö so the current top commit is discarded.
  
  It is a rough equivalent for
  
  git reset --soft HEAD^
# do something else to come up with the right tree ...
git commit -c ORIG_HEAD

  
  but can be used to amend a merge commit.
  
  You should understand the implications of rewriting history if you amend a commit that has already been published. (See the "RECOVERING FROM UPSTREAM REBASE" section in git-rebase(1).)


Additionally, you can set the commit message directly in the command line with:

git commit --amend -m "New commit message"


ΓÇªhowever, this can make multi-line commit messages or small corrections more cumbersome to enter.

Source: Git official documentation. For more details, refer to:

git help commit
AnswerKenntnis:
git commit --amend -m "your new message"
AnswerKenntnis:
If the commit you want to fix isnΓÇÖt the most recent one:


git rebase --interactive $parent_of_flawed_commit

If you want to fix several flawed commits, pass the parent of the oldest one of them.
An editor will come up, with a list of all commits since the one you gave.


Change pick to reword (or on old versions of Git, to edit) in front of any commits you want to fix.
Once you save, Git will replay the listed commits.  

For each commit you want to reword, Git will drop you back into your editor. For each commit you want to edit, Git drops you into the shell. If youΓÇÖre in the shell:


Change the commit in any way you like.
git commit --amend
git rebase --continue



Most of this sequence will be explained to you by the output of the various commands as you go. ItΓÇÖs very easy, you donΓÇÖt need to memorise it ΓÇô just remember that git rebase --interactive lets you correct commits no matter how long ago they were.



Note that you will not want to change commits that you have already pushed. Or maybe you do, but in that case you will have to take great care to communicate with everyone who may have pulled your commits and done work on top of them. How do I recover/resynchronise after someone pushes a rebase or a reset to a published branch?
AnswerKenntnis:
To amend the previous commit, make the changes you want and stage those changes, and then run



git commit --amend


This will open a file in your text editor representing your new commit message. It starts out populated with the text from your old commit message. Change the commit message as you want, then save the file and quit your editor to finish.

To amend the previous commit and keep the same log message, run

git commit --amend -C HEAD


To fix the previous commit by removing it entirely, run

git reset --hard HEAD^


If you want to edit more than one commit message, run

git rebase -i HEAD~commit_count

(Replace commit_count with number of commits that you want to edit.) This command launches your editor. Mark the first commit (the one that you want to change) as ΓÇ£editΓÇ¥ instead of ΓÇ£pickΓÇ¥, then save and exit your editor. Make the change you want to commit and then run

git commit --amend
git rebase --continue
AnswerKenntnis:
A plain

git commit --amend


will run your editor and load the previous commit message. All you have to do is edit it and save.
AnswerKenntnis:
As already mentioned, git commit --amend is the way to overwrite the last commit. One note: if you would like to also overwrite the files, the command would be 

git commit -a --amend -m "My new commit message"
AnswerKenntnis:
You also can use git filter-branch for that.

git filter-branch -f --msg-filter "sed 's/errror/error/'" $flawed_commit..HEAD


It's not as easy as a trivial git commit --amend, but it's especially useful, if you already have some merges after your erroneous commit message.

Note that this will try to rewrite EVERY commit between HEAD and the flawed commit, so you should choose your msg-filter command very wise ;-)
AnswerKenntnis:
I prefer this way. 

git commit --amend -c <commit ID>


Otherwise, there will be a new commit with a new commit ID
AnswerKenntnis:
You can use Git rebasing. For example, if you want to modify back to commit bbc643cd, run

$ git rebase bbc643cd^ --interactive


In the default editor, modify 'pick' to 'edit' in the line whose commit you want to modify. Make your changes and then stage them with

$ git add <filepattern>


Now you can use

$ git commit --amend


to modify the commit, and after that

$ git rebase --continue


to return back to the previous head commit.
AnswerKenntnis:
If you only want to modify your last commit message, then do:

git commit --amend


That will drop you into your text exitor and let you change the last commit message.
If you want to change the last 3 commit messages, or any of the commit messages up to that point, supply HEAD~3 to the git rebase -i command:

git rebase -i HEAD~3
AnswerKenntnis:
If you have to change an old commit message over multiple branches (i.e., the  commit with the erroneous message is present in multiple branches) you might want to use:

git filter-branch -f --msg-filter \
'sed "s/<old message>/<new message>/g"' -- --all


Git will create a temporary directory for rewriting and additionally backup old references in refs/original/.


-f will enforce the execution of the operation. This is necessary if the the temporary directory is already present or if there are already references stored under refs/original. If that is not the case, you can drop this flag.
-- separates filter-branch options from revision options.
--all will make sure, that all branches and tags are rewritten.


Due to the backup of your old references, you can easily go back to the state before executing the command.

Say, you want to recover your master and access it in branch old_master:

git checkout -b old_master refs/original/refs/heads/master
AnswerKenntnis:
Use

git commit --amend


To understand it in detail, an excellent post is 4. Rewriting Git History. It also talks about when not to use git commit --amend.
AnswerKenntnis:
If you are using the
Git
GUI tool, there is a button named amend last commit. Click
on that button and then it will display your last commit
files and message. Just edit that message and you can
commit it with new commit message.

Or use this command from a console/terminal:

git commit -a --amend -m "My new commit message"
AnswerKenntnis:
If you are using the Git GUI, you can amend the last commit which hasn't been pushed with:  

Commit/Amend Last Commit
AnswerKenntnis:
You have a couple of options here. You can do git commit --amend as long as it's your last commit. Otherwise if it's not your last commit you can do an interactive rebase. git rebase -i [branched_from] [hash before commit]. Then inside the interactive rebase you simply add edit to that commit. When it comes up do a git commit --amend and modify the commit message. If you want to roll back before that commit point you could also use git reflog and just delete that commit. Then you just do a git commit again.
AnswerKenntnis:
I use the Git GUI as much as I can, and that gives you the option to amend the last commit:



Also, git rebase -i origin/masteris a nice mantra that will always present you with the commits you have done on top of master, and give you the option to amend, delete, reorder or squash. No need to get hold of that hash first.
AnswerKenntnis:
You can use Git rebasing. For example, if you want to modify back to commit bbc643cd, run:

$ git rebase bbc643cd^ --interactive


In the default editor, modify 'pick' to 'edit' in the line whose commit you want to modify. Make your changes and then stage them with:

$ git add <filepattern>


Now you can use:

$ git commit --amend


to modify the commit, and after that:

$ git rebase --continue


to return back to the previous head commit.
AnswerKenntnis:
Wow, so there are a lot of ways to do this.

Yet another way to do this is to delete the last commit, but keep its changes so that you won't lose your work. You can then do another commit with the corrected message. This would look something like this:

git reset --soft HEAD~1
git commit -m 'New and corrected commit message'


I always do this if I forget to add a file or do a change.

Remember to specify --soft instead of --hard, otherwise you lose that commit entirely.
AnswerKenntnis:
If you just want to edit the latest commit use:

git commit --amend


or

git commit --amend -m 'one line message'


But if you want to edit several commits in a row you should use rebasing instead:

git rebase -i <hash of one commit before the wrong commit>




In a file like the one above write edit/e or one of the other option and hit save and exit.

Now you'll be at the first wrong commit. Make changes in the files, and they'll be automatically staged for you. Type

git commit --amend


save and exit that and type

git rebase --continue 


to move to next selection until finished with all your selections.

Note that these things change all your SHA hashes after that particular commit.
AnswerKenntnis:
I have added the alias of reci, recm for recommit (amend) it, now I can do it with git recm or git recm -m .

$ vim ~/.gitconfig

[alias]

    ......
    cm = commit
    reci = commit --amend
    recm = commit --amend
    ......
AnswerKenntnis:
Update your last wrong commit message with new commit message in one line:

git commit --amend -m "your new commit message"


Or, try git reset like below:

# You can reset your head to n number of commit
git reset --soft HEAD^

# it will reset you last commit. Now, you
# can re-commit it with new commit message.


git reset helps you to break one commit to multiple commits too

# reset your head. I am resetting to last commits:
git reset --soft HEAD^
# (you can reset multiple commit by doing HEAD~2(no. of commits)

# Now, reset your head for splitting it to multiple commits
git reset HEAD

# add and commit your files seperately to make multiple commits: e.g
git add app/
git commit -m "add all files in app directory"

git add config/
git commit -m "add all files in config directory"


Here you have successfully broke your last commit into two commits.
AnswerKenntnis:
If you only want to change your last message you should use the --only flag or it's shortcut -o with commit --amend:

git commit --amend -o -m "New commit message"


This ensures that you don't accidentally enhance your commit with staged stuff. Of course it's best to have a proper $EDITOR configuration. Then you can leave the -m option out, and git will pre-fill the commit message with the old one. In this way it can be easily edited.
QuestionKenntnis:
Undo the last Git commit?
qn_description:
I accidentally added the wrong directory containing my files in Git. Instead of adding a .java file, I added the directory containing the .class file.

How can I undo this action?
AnswersKenntnis
AnswerKenntnis:
From the docs for git-reset:


  Undo a commit and redo

$ git commit ...              (1)
$ git reset --soft 'HEAD^'    (2)
$ edit                        (3)
$ git add ....                (4)
$ git commit -c ORIG_HEAD     (5)

  
  
  This is what you want to undo
  This is most often done when you remembered what you just committed is incomplete, or you misspelled your commit message, or both. Leaves working tree as it was before "reset". (The quotes may or may not be required in your shell)
  Make corrections to working tree files.
  Stage changes for commit.
  "reset" copies the old head to .git/ORIG_HEAD; redo the commit by starting with its log message. If you do not need to edit the message further, you can give -C option instead.
AnswerKenntnis:
Undoing a commit is a little scary if you don't know how it works.  But it's actually amazingly easy if you do understand.

Say you have this, where C is your HEAD and (F) is the state of your files.

   (F)
A-B-C
    Γåæ
  master


You want to nuke commit C and never see it again.  You do this:

git reset --hard HEAD~1


The result is:

 (F)
A-B
  Γåæ
master


Now B is the HEAD.  Because you used --hard, your files are reset to their state at commit B.

Ah, but suppose commit C wasn't a disaster, but just a bit off.  You want to undo the commit but keep your changes for a bit of editing before you do a better commit.  Starting again from here, with C as your HEAD:

   (F)
A-B-C
    Γåæ
  master


You can do this, leaving off the --hard:

git reset HEAD~1


In this case the result is:

   (F)
A-B-C
  Γåæ
master


In both cases, HEAD is just a pointer to the latest commit.  When you do a git reset HEAD~1, you tell Git to move the HEAD pointer back one commit.  But (unless you use --hard) you leave your files as they were.  So now git status shows the changes you had checked into C.  You haven't lost a thing!

For the lightest touch, you can even undo your commit but leave your files and your index:

git reset --soft HEAD~1


This not only leaves your files alone, it even leaves your index alone.  When you do git status, you'll see that the same files are in the index as before.  In fact, right after this command, you could do git commit and you'd be redoing the same commit you just had.

One more thing: Suppose you destroy a commit as in the first example, but then discover you needed it after all?  Tough luck, right?

Nope, there's still a way to get it back.  Type git reflog and you'll see a list of (partial) commit shas that you've moved around in.  Find the commit you destroyed, and do this:

git checkout -b someNewBranchName shaYouDestroyed


You've now resurrected that commit.  Commits don't actually get destroyed in Git for some 90 days, so you can usually go back and rescue one you didn't mean to get rid of.
AnswerKenntnis:
Add/remove files to get things the way you want:

git rm classdir
git add sourcedir


Then amend the commit:

git commit --amend


The previous, erroneous commit will be edited to reflect the new index state - in other words, it'll be like you never made the mistake in the first place :)

Note that you should only do this if you haven't pushed yet. If you have pushed, then you'll just have to commit a fix normally.
AnswerKenntnis:
This took me a while to figure out, so maybe this will help someone...

There are two ways to "undo" your last commit, depending on whether or not you have already made your commit public (pushed to your remote repository):

How to undo a local commit

Lets say I committed locally, but now want to remove that commit.

git log
    commit 101: bad commit    # latest commit, this would be called 'HEAD'
    commit 100: good commit   # second to last commit, this is the one we want


To restore everything back to the way it was prior to the last commit, we need to reset to the commit before HEAD:

git reset --soft HEAD^     # use --soft if you want to keep your changes
git reset --hard HEAD^     # use --hard if you don't care about keeping the changes you made


Now git log will show that our last commit has been removed.

How to undo a public commit

If you have already made your commits public, you will want to create a new commit which will "revert" the changes you made in your previous commit (current HEAD).

git revert HEAD


Your changes will now be reverted and ready for you to commit:

git commit -m 'restoring the file I removed on accident'
git log
    commit 102: restoring the file I removed on accident
    commit 101: removing a file we dont need
    commit 100: adding a file that we need


For more info, check out Git Book - Reset, Checkout and Revert
AnswerKenntnis:
git rm yourfiles/*.class
git commit -a -m "deleted all class files in folder 'yourfiles'"


or

git reset --hard HEAD~1


The hard reset to HEAD-1 will set your working copy to the state of the commit before your wrong commit.
AnswerKenntnis:
Replace the files in the index:

git rm --cached file.class
git add file.java


Then, if it's a private branch, amend the commit:

git commit --amend


Or, if it's a shared branch, make a new commit:

git commit -m 'Replace .class files with .java files'




You may also want to add *.class to a gitignore to stop this happening again.
AnswerKenntnis:
Use git revert commit-id

To get the commit ID, just use git log
AnswerKenntnis:
If you have Git Extras installed, you can run git undo to undo the latest commit. git undo 3 will undo the last 3 commits.
AnswerKenntnis:
I wanted to undo the lastest 5 commits in our shared repository. I looked up the revision id that I wanted to rollback to. Then I typed in the following.

prompt> git reset --hard 5a7404742c85
HEAD is now at 5a74047 Added one more page to catalogue
prompt> git push origin master --force
Total 0 (delta 0), reused 0 (delta 0)
remote: bb/acl: neoneye is allowed. accepted payload.
To git@bitbucket.org:thecompany/prometheus.git
 + 09a6480...5a74047 master -> master (forced update)
prompt>
AnswerKenntnis:
If you are planning undoing a local commit entirely, whatever you changes you did on the commit, and if you don't worry anything about that, just do the following command.

git reset --hard HEAD^1


(This command will ignore your entire commit and your changes will be lost completely from your local working tree). If you want undo your commit, but you want changes you did on the commit into the staging area (before commit just like after git add called the staging area) then do the following command.

git reset --soft HEAD^1


Now your committed files comes into the staging area. Suppose if you want to unstage the files, because you need to edit some wrong conent, then do the following command

git reset HEAD


Now committed files come from the staged area into the unstaged area. Now files are ready to edit, so whatever you changes, you want go edit and added it and make a fresh/new commit.

More
AnswerKenntnis:
I prefer to use git rebase for this job, because a nice list pops up where I can choose the commits to get rid of. It might not be as direct as some other answers here, but it just feels "right".

Choose how many commits you want to list, then invoke like this

git rebase -i HEAD~3


Sample list

pick aa28ba7 Sanity check for RtmpSrv port
pick c26c541 RtmpSrv version option
pick 58d6909 Better URL decoding support


Then git will remove commits for any line that you remove.
AnswerKenntnis:
how to fix the previous local commit

Use git-gui (or similar) to perform a git commit --amend.  From the GUI you can add or remove individual files from the commit.  You can also modify the commit message.

how to undo the previous local commit

Just reset your branch to the previous location (e.g. using gitk or git rebase), then reapply your changes from a saved copy.  After garbage collection in your local repo, it will be like the unwanted commit never happened.  To do all of that in a single command, use git reset HEAD~1.

Word of warning: Careless use of git reset is a good way to get your working copy into a confusing state.  I recommend that Git novices avoid this if they can.

how to undo a public commit

Perform a reverse cherry pick (git-revert) to undo the changes.

If you haven't yet pulled other changes onto your branch, you can simply do...

git revert --no-edit HEAD


Then push your updated branch to the shared repo.
AnswerKenntnis:
A Single command:

 git reset --soft 'HEAD^' 


Works great to undo last local commit!

Cheers!
AnswerKenntnis:
How to undo the last Git commit?

To restore everything back to the way it was prior to the last commit, we need to reset to the commit before HEAD.


If you don't want to keep your changes that you made:

git reset --hard HEAD^

If you want to keep your changes:

git reset --soft HEAD^



Now check your git log, it will show that our last commit has been removed.
AnswerKenntnis:
If you want to permanently undo it and you have cloned some repository 

The commit id can be seen by 

git log 


Then you can do -

git reset --hard <commit_id>

git push origin <branch_name> -f
AnswerKenntnis:
first run: 

git reflog


It will show you all the possible actions you have performed on your repo e.g. commit, merge, pull etc

then:

git reset --hard ActionIdFromRefLog
AnswerKenntnis:
"Reset the working tree to the last commit"

git reset --hard HEAD^ 


"Clean unknown files from the working tree"

git clean    


see - Git Quick Reference

NOTE: This command will delete your previous commit, so use with caution! git reset --hard is safer ΓÇô
AnswerKenntnis:
Use reflog to find a correct state

git reflog



REFLOG BEFORE RESET

Select the correct reflog (f3cb6e2 in my case) and type 

git reset --hard f3cb6e2


After that the repo HEAD will be reset to that HEADid

LOG AFTER RESET

Finally the reflog looks like the picture below


REFLOG FINAL
AnswerKenntnis:
On SourceTree (GUI for GitHub), you may Right-Click the commit and do a 'Reverse Commit'. This should undo your changes.

On terminal: 
You may alternatively use  :    

git revert


or

git reset --soft HEAD^     # use --soft if you want to keep your changes
git reset --hard HEAD^     # use --hard if you don't care about keeping the changes you made
AnswerKenntnis:
Another way:

Checkout the branch you want to revert, then reset your local working copy back to the commit that you want to be the latest one on the remote server (everything after it will go bye-bye). To do this, in SourceTree I right-clicked on the and selected "Reset BRANCHNAME to this commit".

Then navigate to your repository's local directory and run this command:

git -c diff.mnemonicprefix=false -c core.quotepath=false push -v -f --tags REPOSITORY_NAME BRANCHNAME:BRANCHNAME


This will erase all commits after the current one in your local repository but only for that one branch.
AnswerKenntnis:
In my case I accidentally committed some files I did not want to. So I did the following and it worked:

git reset --soft HEAD^
git rm --cached [files you do not need]
git add [files you need]
git commit -c ORIG_HEAD


Verify the results with gitk or git log --stat
AnswerKenntnis:
This article has an excellent explanation as to how to go about various scenarios (where a commit has been done as well as the push OR just a commit, before the push):

http://christoph.ruegg.name/blog/git-howto-revert-a-commit-already-pushed-to-a-remote-reposit.html

From the article, the easiest command I saw to revert a previous commit by its commit id, was:

git revert dd61ab32
AnswerKenntnis:
Use Source tree (graphical tool for git) to see your comments and Tree and manually you can reset it directly by right clicking it.
AnswerKenntnis:
type git log and find the last commit hash code and then give
git reset
AnswerKenntnis:
For a local commit:

by git reset --soft HEAD~1 or if you not remmender exacly in wich commit is, you might use git rm --cached <file>

For a pushed commit:

the proper way of remove files of repo history is using git filter-branch.

i.e. git filter-branch --index-filter 'git rm --cached <file>' HEAD

but I recomnend you try this command with care, read more here.

hope that hepls
QuestionKenntnis:
What is the correct JSON content type?
qn_description:
I've been messing around with JSON for some time, just pushing it out as text and it hasn't hurt anybody (that I know of), but I'd like to start doing things properly.

I have seen so many purported "standards" for the JSON content type:

application/json
application/x-javascript
text/javascript
text/x-javascript
text/x-json


But which is correct, or best? I gather that there are security and browser support issues varying between them.

I know there's a similar question, What MIME type if JSON is being returned by a REST API?, but I'd like a slightly more targeted answer.
AnswersKenntnis
AnswerKenntnis:
For JSON text:


  The MIME media type for JSON text is application/json. The default encoding is UTF-8. (Source: RFC 4627).


For JSONP with callback:

text/javascript


Here are some blog posts that were mentioned in the comments that are relevant.


Why you shouldn't use text/html for JSON
IE sometimes has issues with application/json
A rather complete list of Mimetypes and what to use them for
AnswerKenntnis:
IANA has registered the official mimetype for JSON as application/json.

When asked about why not text/json, Crockford seems to have said JSON is not really JavaScript nor text and also IANA was more likely to hand out application/* than text/*.

More resources:


Media Types
rfc4627.txt
Douglas Crockford pointed to this document here: Yahoo Groups
bluesmoon: JSON has a type
AnswerKenntnis:
Of course, the correct MIME media type for JSON is application/json, but it's necessary to realize what type of data is expected in your application.

For example, I use Ext GWT and the server response must go as text/html but contains JSON data.

Client side, Ext GWT form listener

uploadForm.getForm().addListener(new FormListenerAdapter(){
    @Override
    public void onActionFailed(Form form, int httpStatus,
                               String responseText) {
        MessageBox.alert("Error");
    }

    @Override
    public void onActionComplete(Form form, int httpStatus,
                                 String responseText) {
        MessageBox.alert("Success");
    }
});


In case of using application/json response type, the browser suggests me to save the file.

Server side source code snippet using Spring MVC

return new AbstractUrlBasedView() {
    @SuppressWarnings("unchecked")
    @Override
    protected void renderMergedOutputModel(Map model, HttpServletRequest request,
                                           HttpServletResponse response) throws Exception {
        response.setContentType("text/html");
        response.getWriter().write(json);
    }
};
AnswerKenntnis:
For JSON:



Content-Type: application/json


For JSON-P:

Content-Type: application/javascript
AnswerKenntnis:
If you are using Ubuntu or Debian and you serve .json files through Apache, you might want to serve the files with the correct content type. I am doing this primarily because I want to use the Firefox extension JSONView

The Apache module mod_mime will help to do this easily. However, with Ubuntu you need to edit the file /etc/mime.types and add the line

application/json json


Then restart Apache:

sudo service apache2 restart
AnswerKenntnis:
JSON:

Response is dynamically generated data, according to the query parameters passed in the URL.

Example:

{ "Name": "Foo", "Id": 1234, "Rank": 7 }


Content-Type: application/json



JSON-P:

JSON with padding.
Response is JSON data, with a function call wrapped around it.

Example:

functionCall({"Name": "Foo", "Id": 1234, "Rank": 7});


Content-Type: application/javascript
AnswerKenntnis:
If you're calling ASP.NET Web Services from the client-side you have to use application/json for it to work. I believe this is the same for the jQuery and Ext frameworks.
AnswerKenntnis:
The right content type for JSON is application/json UNLESS you're using JSONP, also known as JSON with Padding, which is actually JavaScript and so the right content type would be application/javascript.
AnswerKenntnis:
There is no doubt that application/json is the best MIME type for a JSON response.

But I had some experience where I had to use application/x-javascript because of some compression issues. My hosting environment is shared hosting with GoDaddy. They do not allow me to change server configurations. I had added the following code to my web.config file for compressing responses.

<httpCompression>
    <scheme name="gzip" dll="%Windir%\system32\inetsrv\gzip.dll"/>
    <dynamicTypes>
        <add mimeType="text/*" enabled="true"/>
        <add mimeType="message/*" enabled="true"/>
        <add mimeType="application/javascript" enabled="true"/>
        <add mimeType="*/*" enabled="false"/>
    </dynamicTypes>
    <staticTypes>
        <add mimeType="text/*" enabled="true"/>
        <add mimeType="message/*" enabled="true"/>
        <add mimeType="application/javascript" enabled="true"/>
        <add mimeType="*/*" enabled="false"/>
    </staticTypes>
</httpCompression>
<urlCompression doStaticCompression="true" doDynamicCompression="true"/>


By using this, the .aspx pages was compressed with g-zip but JSON responses were not. I added

<add mimeType="application/json" enabled="true"/>


in the static and dynamic types section. But this does not compressed JSON responses at all.

After that I removed this newly added type and added

<add mimeType="application/x-javascript" enabled="true"/>


in both the static and dynamic types section, and changed the response type in

.ashx (asynchronous handler) to

application/x-javascript


And now I found that my JSON responses were compressed with g-zip. So I personally recommending to use

application/x-javascript


only if you want to compress your JSON responses on a shared hosting environment. Because in shared hosting, they do not allow you to change IIS configurations.
AnswerKenntnis:
Only when using application/json as the MIME type I have the following (as of November 2011 with the most recent versions of Chrome, Firefox with Firebug):


No more warnings from Chrome when the JSON is loaded from the server.
Firebug will add a tab to the response showing you the JSON data
formatted. If the MIME type is different, it will just show up as
'Response content'.
AnswerKenntnis:
Not everything works for content type application/json.

If you are using Ext JS form submit to upload file, be aware that the server response is parsed by the browser to create the document for the <iframe>.

If the server is using JSON to send the return object, then the Content-Type header must be set to text/html in order to tell the browser to insert the text unchanged into the document body.

See the Ext JS 3.4.0 API documentation.
AnswerKenntnis:
If you're in a client-side environment, investigating about the cross-browser support is mandatory for a well supported web application.

The right HTTP Content-Type would be application/json, as others already highlighted too, but some clients do not handle it very well, that's why jQuery recommends the default text/html.
AnswerKenntnis:
JSON is a DSL and a data format independent of JavaScript, and as such has its own mimetype application/json. Respect for mime types is of course client driven, so text/plain may do for transfer of bytes, but then you would be pushing up interpretation to the vendor application domain unnecessarily - application/json goddammit! would you transfer XML via text/plain, ya big troll ya?!

But honestly, assuming its an honest question, your choice of mime type is advice to the client as to how to interpret the data- text/plain or text/HTML (when it's not HTML) is like type erasure- its as uninformative as making sll your objects of type Object in a typed language. 

No browser runtime I know of will take a JSON document and automatically make it available to the runtime as a JavaScript accessible object without intervention, but if you are working with a crippled client, that's an entirely different matter. But that's not the whole story- Restful JSON services often don't have JavaScript runtimes, but doesn't stop them using JSON as a viable data interchange format. If clients are that crippled.... then I would consider perhaps HTML injection via an AJAX templating service instead.

Application/JSON!
AnswerKenntnis:
Correct Answer:

Content-Type: application/json
AnswerKenntnis:
In JSP, you can use this in page directive:

<%@ page language="java" contentType="application/json; charset=UTF-8"
    pageEncoding="UTF-8"%>


The correct MIME media type for JSON is application/json.  JSP will use it for sending a response to the client
AnswerKenntnis:
As many others have mentioned, application/json is the correct answer.

But what haven't been explained yet is what the other options you proposed mean.


application/x-javascript: Experimental MIME type for javascript before application/javascript was made standard.
text/javascript: Now obsolete. You should use application/javascript when using javascript.
text/x-javascript: Experimental MIME type for the above situation.
text/x-json: Experimental MIME type for json before application/json got officially registered.


All in all, whenever you have any doubts about content types, you should check this link
AnswerKenntnis:
ΓÇ£application/jsonΓÇ¥ is the correct JSON content type.

def ajaxFindSystems = {
  def result = Systems.list()
  render(contentType:'application/json') {
    results {
      result.each{sys->
        system(id:sys.id, name:sys.name)
      }
    }
    resultset (rows:result.size())
  }
}
AnswerKenntnis:
The right MIME type is application/json

BUT

I experienced many situations where the browser type or the framework user needed:

text/html

application/javascript
AnswerKenntnis:
I use the below

contentType: 'application/json',
data: JSON.stringify(SendData),
AnswerKenntnis:
In Spring you have a defined type: MediaType.APPLICATION_JSON_VALUE which is equivalent to application/json.
AnswerKenntnis:
If the JSON is with padding then it will be application/jsonp. If the JSON is without padding then it will be application/json.

To deal with both, it is a good practice to use: 'application/javascript' without bothering whether it is with padding or without padding.
AnswerKenntnis:
The Content-Type header should be set to 'application/json' when posting. Server listening for the request should include "Accept=application/json".
In Spring MVC you can do it like this:

@RequestMapping(value="location", method = RequestMethod.POST, headers = "Accept=application/json")


Add headers to the response:

HttpHeaders headers = new HttpHeaders();
headers.add("Content-Type", "application/json");
AnswerKenntnis:
The IANA registration for application/json says


  Applications that use this media type:  JSON has been used to
     exchange data between applications written in all of these
     programming languages: ActionScript, C, C#, Clojure, ColdFusion,
     Common Lisp, E, Erlang, Go, Java, JavaScript, Lua, Objective CAML,
     Perl, PHP, Python, Rebol, Ruby, Scala, and Scheme.


You'll notice that IANA.org doesn't list any of these other media types, in fact even application/javascript is now obsolete. So application/json is really the only possible correct answer. 

Browser support is another thing. 

The most widely supported non-standard media types text/json or text/javascript. But some big names even use text/plain.
Examples are the Google Search API and the flickr API.

Examples:

curl -I http://ajax.googleapis.com:80/ajax/services/search/web\?q\=json => Content-Type: text/javascript

curl -I http://api.flickr.com/services/rest/\?method\=flickr.photos.search\&api_key\=2b5fc2179356354594cab80edc3533bb\&user_id\=1%1\&format\=json\&nojsoncallback\=1 => Content-Type: text/plain
AnswerKenntnis:
PHP developers use this:

<?php

header("Content-type: application/json");

// Do somthing here...

?>
QuestionKenntnis:
The Definitive C++ Book Guide and List
qn_description:
This question attempts to collect the few pearls among the dozens of bad C++ books that are published every year.

Unlike many other programming languages, which are often picked up on the go from tutorials found on the Internet, few are able to quickly pick up C++ without studying a well-written C++ book. It is way too big and complex for doing this. In fact, it is so big and complex, that there are very many very bad C++ books out there. And we are not talking about bad style, but things like sporting glaringly obvious factual errors and promoting abysmally bad programming styles.

Please edit the accepted answer to provide quality books and an approximate skill level ΓÇö preferably after discussing your addition in the C++ chat room. (The regulars might mercilessly undo your work if they disagree with a recommendation.) Add a short blurb/description about each book that you have personally read/benefited from. Feel free to debate quality, headings, etc. Books that meet the criteria will be added to the list.  Books that have reviews by the Association of C and C++ Users (ACCU) have links to the review. 

Note: FAQs and other resources can be found in the C++ tag info and under c++-faq. There is also a similar post for C: The Definitive C Book Guide and List
AnswersKenntnis
AnswerKenntnis:
Reference Style - All Levels


A Tour of C++ (Bjarne Stroustrup) The "tour" is a quick (about 180 pages and 14 chapters) tutorial overview of all of standard C++ (language and standard library, and using C++11) at a moderately high level for people who already know C++ or at least are experienced programmers. This book is an extended version of the material that constitutes Chapters 2-5 of The C++ Programming Language, 4th edition.
The C++ Programming Language (Bjarne Stroustrup) (updated for C++11) The classic introduction to C++ by its creator. Written to parallel the classic K&R, this indeed reads very much alike it and covers just about everything from the core language to the standard library, to programming paradigms to the language's philosophy. (Thereby making the latest editions break the 1k page barrier.)  [Review] The fourth edition (released on May 19, 2013) covers C++11.
C++ Standard Library Tutorial and Reference (Nicolai Josuttis) (updated for C++11) The introduction and reference for the C++ Standard Library. The second edition (released on April 9, 2012) covers C++11. [Review]
The C++ IO Streams and Locales (Angelika Langer and Klaus Kreft)  There's very little to say about this book except that, if you want to know anything about streams and locales, then this is the one place to find definitive answers. [Review]


C++11 References:


The C++ Standard (INCITS/ISO/IEC 14882-2011) This, of course, is the final arbiter of all that is or isn't C++. Be aware, however, that it is intended purely as a reference for experienced users willing to devote considerable time and effort to its understanding. As usual, the first release was quite expensive ($300+ US), but it has now been released in electronic form for $60US
Overview of the New C++ (C++11/14) (PDF only) (Scott Meyers) (updated for C++1y/C++14) These are the presentation materials (slides and some lecture notes) of a three-day training course offered by Scott Meyers, who's a highly respected author on C++. Even though the list of items is short, the quality is high.




Beginner

Introductory

If you are new to programming or if you have experience in other languages and are new to C++, these books are highly recommended.


C++ Primer * (Stanley Lippman, Jos├⌐e Lajoie, and Barbara E. Moo)  (updated for C++11) Coming at 1k pages, this is a very thorough introduction into C++ that covers just about everything in the language in a very accessible format and in great detail. The fifth edition (released August 16, 2012) covers C++11. [Review] 
Accelerated C++ (Andrew Koenig and Barbara Moo)  This basically covers the same ground as the C++ Primer, but does so on a fourth of its space. This is largely because it does not attempt to be an introduction to programming, but an introduction to C++ for people who've previously programmed in some other language. It has a steeper learning curve, but, for those who can cope with this, it is a very compact introduction into the language. (Historically, it broke new ground by being the first beginner's book using a modern approach at teaching the language.) [Review]
Thinking in C++ (Bruce Eckel)  Two volumes; second is more about standard library, but still very good
Programming: Principles and Practice Using C++ (Bjarne Stroustrup) (updated for C++11/C++14) An introduction to programming using C++ by the creator of the language. A good read, that assumes no previous programming experience, but is not only for beginners.



* Not to be confused with C++ Primer Plus (Stephen Prata), with a significantly less favorable review.


Best practices


Effective C++ (Scott Meyers)  This was written with the aim of being the best second book C++ programmers should read, and it succeeded. Earlier editions were aimed at programmers coming from C, the third edition changes this and targets programmers coming from languages like Java. It presents ~50 easy-to-remember rules of thumb along with their rationale in a very accessible (and enjoyable) style. [Review]
Effective STL (Scott Meyers)  This aims to do the same to the part of the standard library coming from the STL what Effective C++ did to the language as a whole: It presents rules of thumb along with their rationale. [Review]




Intermediate


More Effective C++ (Scott Meyers) Even more rules of thumb than Effective C++. Not as important as the ones in the first book, but still good to know.
Exceptional C++ (Herb Sutter)  Presented as a set of puzzles, this has one of the best and thorough discussions of the proper resource management and exception safety in C++ through Resource Acquisition is Initialization (RAII) in addition to in-depth coverage of a variety of other topics including the pimpl idiom, name lookup, good class design, and the C++ memory model. [Review]
More Exceptional C++ (Herb Sutter)  Covers additional exception safety topics not covered in Exceptional C++, in addition to discussion of effective object oriented programming in C++ and correct use of the STL. [Review]
Exceptional C++ Style (Herb Sutter)  Discusses generic programming, optimization, and resource management; this book also has an excellent exposition of how to write modular code in C++ by using nonmember functions and the single responsibility principle. [Review]
C++ Coding Standards (Herb Sutter and Andrei Alexandrescu) "Coding standards" here doesn't mean "how many spaces should I indent my code?"  This book contains 101 best practices, idioms, and common pitfalls that can help you to write correct, understandable, and efficient C++ code. [Review]
C++ Templates: The Complete Guide (David Vandevoorde and Nicolai M. Josuttis) This is the book about templates as they existed before C++11.  It covers everything from the very basics to some of the most advanced template metaprogramming and explains every detail of how templates work (both conceptually and at how they are implemented) and discusses many common pitfalls.  Has excellent summaries of the One Definition Rule (ODR) and overload resolution in the appendices. A second edition is scheduled for 2015. [Review]




Advanced


Modern C++ Design (Andrei Alexandrescu)  A groundbreaking book on advanced generic programming techniques.  Introduces policy-based design, type lists, and fundamental generic programming idioms then explains how many useful design patterns (including small object allocators, functors, factories, visitors, and multimethods) can be implemented efficiently, modularly, and cleanly using generic programming. [Review]
C++ Template Metaprogramming (David Abrahams and Aleksey Gurtovoy)
C++ Concurrency In Action (Anthony Williams) A book covering C++11 concurrency support including the thread library, the atomics library, the C++ memory model, locks and mutexes, as well as issues of designing and debugging multithreaded applications.
Advanced C++ Metaprogramming (Davide Di Gennaro) A pre-C++11 manual of TMP techniques, focused more on practice than theory.  There are a ton of snippets in this book, some of which are made obsolete by typetraits, but the techniques, are nonetheless, useful to know.  If you can put up with the quirky formatting/editing, it is easier to read than Alexandrescu, and arguably, more rewarding.  For more experienced developers, there is a good chance that you may pick up something about a dark corner of C++ (a quirk) that usually only comes about through extensive experience.




Classics / Older

Note: Some information contained within these books may not be up-to-date or no longer considered best practice.


The Design and Evolution of C++ (Bjarne Stroustrup)  If you want to know why the language is the way it is, this book is where you find answers. This covers everything before the standardization of C++.
Ruminations on C++ - (Andrew Koenig and Barbara Moo) [Review]
Advanced C++ Programming Styles and Idioms (James Coplien)  A predecessor of the pattern movement, it describes many C++-specific "idioms". It's certainly a very good book and still worth a read if you can spare the time, but quite old and not up-to-date with current C++. 
Large Scale C++ Software Design (John Lakos)  Lakos explains techniques to manage very big C++ software projects. Certainly a good read, if it only was up to date. It was written long before C++98, and misses on many features (e.g. namespaces) important for large scale projects. If you need to work in a big C++ software project, you might want to read it, although you need to take more than a grain of salt with it. There's been the rumor that Lakos is writing an up-to-date edition of the book for years. 
Inside the C++ Object Model (Stanley Lippman)  If you want to know how virtual member functions are commonly implemented and how base objects are commonly laid out in memory in a multi-inheritance scenario, and how all this affects performance, this is where you will find thorough discussions of such topics.
QuestionKenntnis:
How to delete a Git branch both locally and remotely?
qn_description:
I want to delete a branch both locally and on my remote project fork on GitHub.

Successfully Deleted Local Branch

$ git branch -D bugfix
Deleted branch bugfix (was 2a14ef7).


Attempts to Delete Remote Branch

$ git branch -d remotes/origin/bugfix
error: branch 'remotes/origin/bugfix' not found.

$ git branch -d origin/bugfix
error: branch 'origin/bugfix' not found.

$ git branch -rd origin/bugfix
Deleted remote branch origin/bugfix (was 2a14ef7).

$ git push
Everything up-to-date

$ git pull
From github.com:gituser/gitproject
* [new branch] bugfix -> origin/bugfix
Already up-to-date.


What do I need to do differently to successfully delete the
remotes/origin/bugfix branch both locally and on GitHub?
AnswersKenntnis
AnswerKenntnis:
Updated Answer on 1-Feb-2012

As of Git v1.7.0, you can delete a remote branch using

git push origin --delete <branchName>


which is easier to remember than

git push origin :<branchName>


which was added in Git v1.5.0 "to delete a remote branch or a tag."

Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.

Original Answer from 5-Jan-2010

From Chapter 3 of Pro Git by Scott Chacon:


  Deleting Remote Branches
  
  Suppose youΓÇÖre done with a remote branch ΓÇö say, you and your collaborators are finished with a feature and have merged it into your remoteΓÇÖs master branch (or whatever branch your stable codeline is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your serverfix branch from the server, you run the following:

$ git push origin :serverfix
To git@github.com:schacon/simplegit.git
 - [deleted]         serverfix

  
  Boom. No more branch on your server. You may want to dog-ear this page, because youΓÇÖll need that command, and youΓÇÖll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then youΓÇÖre basically saying, ΓÇ£Take nothing on my side and make it be [remotebranch].ΓÇ¥


I issued git push origin :bugfix and it worked beautifully. Scott Chacon was rightΓÇöI will want to dog ear that page (or virtually dog ear by answering this on Stack Overflow).
AnswerKenntnis:
Matthew's answer is great for removing remote branches and I also appreciate the explanation, but to make a simple distinction between the two commands:

To remove a local branch from your machine:

git branch -d the_local_branch

To remove a remote branch:

git push origin :the_remote_branch

*Taken from here: https://makandracards.com/makandra/621-git-delete-a-branch-local-or-remote
AnswerKenntnis:
You can also use the following to delete the remote branch

git push --delete origin serverfix


which does the same thing as

git push origin :serverfix


but may be easier to remember.
AnswerKenntnis:
Tip: When you delete branches using

git branch -d <branchname>    # deletes local branch


or

git push origin :<branchname> # deletes remote branch


only the references are deleted. Even though the branch is actually removed on the remote the references to it still exists in the local repositories of your team members. This means that for other team members the deleted branches are still visible when they do a git branch -a.

To solve this your team members can prune the deleted branches with

git remote prune <repository>


this is typically git remote prune origin.
AnswerKenntnis:
Another approach is

git push --prune origin


This will delete all remote branches that do not exist locally. Or more comprehensively,

git push --mirror


will effectively make the remote look like the local copy of the repo (local heads, remotes and tags are mirrored on remote).
AnswerKenntnis:
I use the following in my bash settings:

alias git-shoot="git push origin --delete"


Then you can call:

git-shoot branchname
AnswerKenntnis:
Since January 2013, GitHub included a Delete branch button next to each branch in your "Branches" page.

Relevant blog post: Create and delete branches
AnswerKenntnis:
If you want to complete both these steps with a single command, you can make an alias for it by adding the below to your ~/.gitconfig:

[alias]
    rmbranch = "!f(){ git branch -d ${1} && git push origin --delete ${1}; };f"


Alternatively, you can add this to your global config from the command line using

git config --global alias.rmbranch \
'!f(){ git branch -d ${1} && git push origin --delete ${1}; };f'


NOTE: If using -d (lowercase d), the branch will only be deleted if it has been merged. To force the delete to happen, you will need to use -D (uppercase D).
AnswerKenntnis:
You can also do this git remote prune origin

$ git remote prune origin
Pruning origin
URL: git@example.com/yourrepo.git
 * [pruned] origin/some-branchs


prune delete remote-tracking branches from git branch -r listing
AnswerKenntnis:
In addition to the other answers, I often use the git_remote_branch tool. It's an extra install, but it gets you a convenient way to interact with remote branches. In this case, to delete:

grb delete branch


I find that I also use the publish and track commands quite often.
AnswerKenntnis:
Intro (and summary, tl;dr)

When you're dealing with deleting branches both locally and remotely, keep in mind that there are 3 different branches involved:


The local branch X.
The remote origin branch X.
The local remote-tracking branch origin/X that tracks the remote X.


The original poster used

git branch -rd origin/bugfix


which only deleted his local remote-tracking branch origin/bugfix, and not the actual remote branch bugfix on origin. To delete that actual remote branch, you need

git push origin --delete bugfix


The following sections are the detailed steps needed to actually delete these various branches.

Deleting the local branch X

git branch --delete X

# Or shorter
git branch -d X

# If X hasn't been merged yet, you'll need to force delete
git branch -D X


Deleting the remote branch X

As Matthew Rankin points out in his answer, you can use either of the following to delete the remote branch (depending on what version of Git you're using):

# Use an empty refspec to delete the remote branch.
# This will work in versions of Git older than 1.7.0.
git push origin :X

# If you're using Git version 1.7.0 or newer
# you can use the new --delete flag instead.
git push origin --delete X


Note that deleting the remote branch X from the command line this way will also delete the local remote-tracking branch origin/X, so it is not necessary to prune the obsolete remote-tracking branch with git fetch --prune or git fetch -p, though it wouldn't hurt if you did it anyway.

You can verify that the remote-tracking branch origin/X was also deleted by running the following:

# View just remote-tracking branches
git branch --remotes
git branch -r

# View both strictly local as well as remote-tracking branches
git branch --all
git branch -a


Pruning the obsolete local remote-tracking branch origin/X

If you didn't delete your remote branch X from the command line (like above), then your local repo will still contain (a now obsolete) remote-tracking branch origin/X. This can happen if you deleted a remote branch directly through GitHub's web interface, for example.

A typical way to remove these obsolete remote-tracking branches (since Git version 1.6.6) is to simply run git fetch with the --prune or shorter -p. Note that this removes all obsolete local remote-tracking branches for any remote branches that no longer exist on the remote:

git fetch origin --prune

# Or shorter
git fetch origin -p


Here is the relevant quote from the 1.6.6 release notes (emphasis mine):


  "git fetch" learned --all and --multipleoptions, to run fetch from
  many repositories, and --prune option to remove remote tracking
  branches that went stale.  These make "git remote update" and "git
  remote prune" less necessary (there is no plan to remove "remote
  update" nor "remote prune", though).


Alternative to above automatic pruning for obsolete remote-tracking branches

Alternatively, instead of pruning your obsolete local remote-tracking branches through git fetch -p, you can avoid making the extra network operation by just manually removing the branch(es) with the --remote or -r flags:

git branch --delete --remotes origin/X

# Or shorter
git branch -dr origin/X


See Also


Official Linux Kernel git branch documentation.
Official Linux Kernel git fetch documentation.
Pro Git § 3.5 Git Branching - Remote Branches.
AnswerKenntnis:
Many of the other answers will lead to errors/warnings. This approach is relatively fool proof although you may still need git branch -D branch_to_delete if it's not fully merged into some_other_branch, for example.

git checkout some_other_branch
git push origin :branch_to_delete
git branch -d branch_to_delete


Remote pruning isn't needed if you deleted the remote branch. It's only used to get the most up to date remotes available on a repo you're tracking. I've observed git fetch will add remotes, not remove them. Here's an example of when git remote prune origin will actually do something:

User A does the steps above. User B would run the following commands to see the most up to date remote branches

git fetch
git remote prune origin
git branch -r
AnswerKenntnis:
Mashup of all the other answers. Requires Ruby 1.9.3+, tested only on OS X.

Call this file git-remove, make it executable, and put it in your path. Then use, for example, git remove temp. 

#!/usr/bin/env ruby
require 'io/console'

if __FILE__ == $0
      branch_name = ARGV[0] if (ARGV[0])
      print "Press Y to force delete local and remote branch #{branch_name}..."
    response = STDIN.getch
    if ['Y', 'y', 'yes'].include?(response)
      puts "\nContinuing."
      `git branch -D #{branch_name}`
      `git branch -D -r origin/#{branch_name}`
      `git push origin --delete #{branch_name}` 
    else
      puts "\nQuitting."
    end
end
QuestionKenntnis:
What is the name of this operator: "-->"?
qn_description:
After reading Hidden Features and Dark Corners of C++/STL on comp.lang.c++.moderated, I was completely surprised that it compiled and worked in both Visual Studio 2008 and G++ 4.4.

The code:

#include <stdio.h>
int main()
{
     int x = 10;
     while( x --> 0 ) // x goes to 0
     {
       printf("%d ", x);
     }
}


I'd assume this is C, since it works in GCC as well. 
Where is this defined in the standard, and where has it come from?
AnswersKenntnis
AnswerKenntnis:
That's not an operator -->. That's two separate operators, -- and >.

Your condition code is decrementing x, while returning xs original (not decremented) value, and then comparing the original value with 0 using the > operator.

To better understand, the statement could be as follows:

while( (x--) > 0 )
AnswerKenntnis:
That's a very complicated operator, so even the C++ Standard committee placed its description in two different parts of the Standard.

Joking aside, they are two different operators: -- and > described respectively in §5.2.6/2 and §5.9 of the C++03 Standard.
AnswerKenntnis:
It's equivalent to

while (x-- > 0)
AnswerKenntnis:
It's

#include <stdio.h>
int main(void)
{
     int x = 10;
     while( x-- > 0 ) // x goes to 0
     {
       printf("%d ", x);
     }
     return 0;
}


Just the space make the things look funny, -- decrements and > compares.
AnswerKenntnis:
while( x-- > 0 )


is how that's parsed.
AnswerKenntnis:
The usage of --> has historical relevance. Decrementing was (and still is in some cases), faster than incrementing on the x86 architecture. Using --> suggests that x is going to 0, and appeals to those with mathematical backgrounds.
AnswerKenntnis:
This is exactly the same as

while (x--){
   printf("%d ", x);
}


for non-negative numbers
AnswerKenntnis:
In one book I read they said (I don't remember correctly what is book): Compiler try to parse expression to bigest token by using left right rule:
In this case is:

x-->0


Parse to biggest token: 

token 1: x
token 2: --
token 3: >
token 4: 0
conclude: x-- > 0


Same as:

a-----b


After parse

token 1: a
token 2: --
token 3: --
token 4: -
token 5: b
conclude: (a--)-- - b


I hope it helps to understand the complicated expression ^^
AnswerKenntnis:
Anyway, we have a "goes to" operator now. "-->" is easy to be remembered as a direction, and "while x goes to zero" is meaning-straight.

Furthermore, it is a little more efficient than "for (x = 10; x > 0; x --)" on some platforms.
AnswerKenntnis:
This code first compares x and 0 and then decrement x. (Also said in the first answer: You're post-decrementing x and then comparing x and 0 with the > operator.) See the output of this code:

9 8 7 6 5 4 3 2 1 0


We now first compare and then decrement by see 0 in the output.

If we want to first decrement and then compare, use this code:

#include <stdio.h>
int main(void)
{
    int x = 10;
    while( --x> 0 ) // x goes to 0
    {
        printf("%d ", x);
    }
    return 0;
}


That output is:

9 8 7 6 5 4 3 2 1
AnswerKenntnis:
There is a space missing between -- and >. x is post decremented, that is, decremented after checking the condition x>0 ?.
AnswerKenntnis:
-- is the decrement operator and > is the greater-than operator.

The two operators are applied as a single one like -->.
AnswerKenntnis:
My compiler will print out 9876543210 when I run this code.

#include <iostream>
int main()
{
    int x = 10;
    while( x --> 0 ) // x goes to 0
    {
        std::cout << x;
    }
}


As expected. The while( x-- > 0 ) actually means while( x > 0). The x-- post decrements x.

while( x > 0 ) {
    x--;
    std::cout << x;
}


is a different way of writing the same thing.

It is nice that the original looks like "while x goes to 0" though.
AnswerKenntnis:
Actually, x is post-decrementing and with that condition is being checked. It's not -->, it's (x--) > 0

Note: value of x is changed after the condition is checked, because it post-decrementing. Some similar cases can also occur, for example:

-->    x-->0
++>    x++>0
-->=   x-->=0
++>=   x++>=0
AnswerKenntnis:
It is to prevent confusion (obfuscation, perceived cleverness, etc.) such as this, that static analysis checkers (for example) MISRA seeks to prevent assignment operations (including ++ and -- as well as the obvious =) in conditional statements.
AnswerKenntnis:
It's a combination of two operators. First -- is for decrementing the value, and > is for checking whether the value is greater than the right-hand operand.

#include<stdio.h>
int main()
{
    int x = 10;
    while (x-- > 0)
        printf("%d ",x);
    return 0;
}


The output will be:

9 8 7 6 5 4 3 2 1 0
AnswerKenntnis:
C and C++ obey the "maximum munch" rule. The same way a---b is translated to (a--) - b, in your case  x-->0 translates to (x--)>0.

What the rule says essentially is that going left to right, expressions are formed by taking the maximum of characters which will form an valid expression.
QuestionKenntnis:
Difference between 'git pull' and 'git fetch'?
qn_description:
What's the difference between git pull and git fetch in Git?
AnswersKenntnis
AnswerKenntnis:
In the simplest terms, git pull does a git fetch followed by a git merge.

You can do a git fetch at any time to update your remote-tracking branches under refs/remotes/<remote>/. This operation never changes any of your own local branches under refs/heads, and is safe to do without changing your working copy. I have even heard of people running git fetch periodically in a cron job in the background (although I wouldn't recommend doing this).

A git pull is what you would do to bring a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.
AnswerKenntnis:
When you use pull, Git tries to automatically do your work for you. It is context sensitive, so Git will merge any pulled commits into the branch you are currently working in.  pull automatically merges the commits without letting you review them first. If you donΓÇÖt closely manage your branches you may run into frequent conflicts.
When you fetch, Git gathers any commits from the target branch that do not exist in your current branch and stores them in your local repository. However, it does not merge them with your current branch. This is particularly useful if you need to keep your repository up to date, but are working on something that might break if you update your files. 
To integrate the commits into your master branch, you use merge.
AnswerKenntnis:
It is important to contrast the design philosophy of git with the philosophy of a more traditional source control tool like svn.

Subversion was designed and built with a client/server model. There is a single repository that is the server, and several clients can fetch code from the server, work on it, then commit it back to the server. The assumption is that the client can always contact the server when it needs to perform an operation.

Git was designed to support a more distributed model with no need for a central repository (though you can certainly use one if you like.) Also git was designed so that the client and the "server" don't need to be online at the same time. Git was designed so that people on an unreliable link could exchange code via email, even. It is possible to work completely disconnected and burn a CD to exchange code via git.

In order to support this model git maintains a local repository with your code and also an additional local repository that mirrors the state of the remote repository. By keeping a copy of the remote repository locally, git can figure out the changes needed even when the remote repository is not reachable.  Later when you need to send the changes to someone else, git can transfer them as a set of changes from a point in time known to the remote repository.


git fetch is the command that says "bring my local copy of the remote repository up to date." 
git pull says "bring the changes in the remote repository where I keep my own code."


Normally "git pull" does this by doing a "git fetch" to bring the local copy of the remote repository up to date, and then merging the changes into your own code repository and possibly your working copy.

The take away is to keep in mind that there are often at least three copies of a project on your workstation. One copy is your own repository with your own commit history. The second copy is your working copy where you are editing and building. The third copy is your local "cached" copy of a remote repository.
AnswerKenntnis:
One use case of git fetch is that the following will tell you any changes in the remote branch since your last pull... so you can check before doing an actual pull, which could change files in your current branch and working copy.

git fetch
git diff ...origin
AnswerKenntnis:
It cost me a little bit to understand what was the difference is, but for me this is a simple explanation. Just think that "master" in your localhost is a branch.

When you clone a repository you fetch the entire repository to you local host. This means that at that time you have an origin/master pointer to HEAD and master pointing to the same HEAD.

when you start working and do commits you advance the master pointer to HEAD + your commits. But the origin/master pointer is still pointing to what it was when you cloned.

So the difference will be:


If you do a "git fetch" it will just fetch all the changes in the remote repository (GitHub) and move the origin/master pointer to HEAD. Meanwhile your local branch master will keep pointing to where it has.
If you do a "git pull", it will do basically the same thing, but it will try to merge whatever you it pulled into your master branch.
AnswerKenntnis:
git-pull - Fetch from and merge with another repository or a local branch
SYNOPSIS

git pull   ΓÇª
DESCRIPTION

Runs git-fetch with the given parameters, and calls git-merge to merge the 
retrieved head(s) into the current branch. With --rebase, calls git-rebase 
instead of git-merge.

Note that you can use . (current directory) as the <repository> to pull 
from the local repository ΓÇö this is useful when merging local branches 
into the current branch.

Also note that options meant for git-pull itself and underlying git-merge 
must be given before the options meant for git-fetch.


You would pull if you want the histories merged, you'd fetch if you just 'want the codez' as some person has been tagging some articles around here.
AnswerKenntnis:
Be careful with git pull --rebase. This warning is from the git-pull man page:

This is a potentially dangerous mode of operation. It rewrites history, which does not bode well when you published that history already. Do not use this option unless you have read git-rebase(1) carefully.
AnswerKenntnis:
You can fetch from a remote repository, see the differences and then pull or merge.

This is an example for a remote repository called origin and a branch called master tracking the remote branch origin/master:

git checkout master                                                  
git fetch                                        
git diff origin/master
git pull --rebase origin master
AnswerKenntnis:
I found this blog post useful:

The difference between git pull, git fetch and git clone (and git rebase) - Mike Pearce

It covers git pull, git fetch, git clone and git rebase.

git pull pulls down from a remote and instantly merges.

git fetch is similar to pull but doesn't merge. i.e. it fetches info about remote changes but no changes are made locally.

git clone clones a repo.

git rebase saves stuff from your current branch that isn't in the upstream branch to a temporary area. Your branch is now the same as before you started your changes. So, git pull -rebase will pull down the remote changes, rewind your local branch, replay your changes over the top of your current branch one by one until you're up-to-date.

Also, git branch -a will show you exactly whatΓÇÖs going on with all your branches - local and remote.

====

UPDATE

I thought I'd update this to show how you'd actually use this in practice. 


Update your local repo from the remote (but don't merge):

git fetch 
After downloading the updates, let's see the differences:

git diff master origin/master 
If you're happy with those updates, then merge:

git pull


Notes:

On step 2: For more on diffs between local and remotes, see: compare local git branch with remote branch?

On step 3: It's probably more accurate (e.g. on a fast changing repo) to do a git rebase origin here. See @Justin Ohms comment in another answer.
AnswerKenntnis:
I have struggled with this as well.  In fact I got here with a google search of exactly the same question.  Reading all these answers finally painted a picture in my head and I decided to try to get this down looking at the state of the 2 repositories and 1 sandbox and actions preformed over time while watching the version of them.  So here is what I came up with.  Please correct me if I messed up anywhere.

The three repos with a fetch

---------------------     -----------------------     -----------------------
- Remote Repo       -     - Remote Repo         -     - Remote Repo         -
-                   -     - gets pushed         -     -                     -
- @ R01             -     - @ R02               -     - @ R02               -
---------------------     -----------------------     -----------------------

---------------------     -----------------------     -----------------------
- Local Repo        -     - Local Repo          -     - Local Repo          -
- pull              -     -                     -     - fetch               -
- @ R01             -     - @ R01               -     - @ R02               -
---------------------     -----------------------     -----------------------

---------------------     -----------------------     -----------------------
- Local Sandbox     -     - Local Sandbox       -     - Local Sandbox       -
- Checkout          -     - new work done       -     -                     -
- @ R01             -     - @ R01+              -     - @R01+               -
---------------------     -----------------------     -----------------------


The three repos with a pull

---------------------     -----------------------     -----------------------
- Remote Repo       -     - Remote Repo         -     - Remote Repo         -
-                   -     - gets pushed         -     -                     -
- @ R01             -     - @ R02               -     - @ R02               -
---------------------     -----------------------     -----------------------

---------------------     -----------------------     -----------------------
- Local Repo        -     - Local Repo          -     - Local Repo          -
- pull              -     -                     -     - pull                -
- @ R01             -     - @ R01               -     - @ R02               -
---------------------     -----------------------     -----------------------

---------------------     -----------------------     -----------------------
- Local Sandbox     -     - Local Sandbox       -     - Local Sandbox       -
- Checkout          -     - new work done       -     - merged with R02     -
- @ R01             -     - @ R01+              -     - @R02+               -
---------------------     -----------------------     -----------------------


This helped me understand why a fetch is pretty important.
AnswerKenntnis:
I like to have some visual representation of the situation to grasp these things. Maybe other developers would like to see it too, so here's my addition. I'm not totally sure that it all is correct, so please correct me in the comments if I have made a mistake.

                                         LOCAL SYSTEM
                  . =====================================================    
================= . =================  ===================  =============
REMOTE REPOSITORY . REMOTE REPOSITORY  LOCAL REPOSITORY     WORKING COPY
(ORIGIN)          . (CACHED)           
for example,      . mirror of the      
a github repo.    . remote repo
Can also be       .
multiple repo's   .
                  .
                  .
FETCH  *------------------>*
Your local cache of the remote is updated with the origin (or multiple
external sources, that is git's distributed nature)
                  .
PULL   *-------------------------------------------------------->*
changes are merged directly into your local copy. when conflicts occur, 
you are asked for decisions.
                  .
COMMIT            .                             *<---------------*
When coming from, for example, subversion, you might think that a commit
will update the origin. In git, a commit is only done to your local repo.
                  .
PUSH   *<---------------------------------------*
Synchronizes your changes back into the origin.


Some major advantages for having a fetched mirror of the remote are:


Performance (scroll through all commits and messages without trying to squeeze it through the network)
Feedback about the state of your local repo (for example, I use Atlassian's SourceTree, which will give me a bulb indicating if I'm commits ahead or behind compared to the origin. This information can be updated with a GIT FETCH).
AnswerKenntnis:
git fetch will retrieve remote branches so that you can git diff or git merge them with the current branch. git pull will run fetch on the remote brach tracked by the current branch and then merge the result. You can use git fetch to see if there are any updates to the remote branch without necessary merging them with your local branch.
AnswerKenntnis:
We simply say:

git pull == git fetch + git merge


If you run git pull, you do not need to merge the data to local. If you run git fetch, it means you must run git merge for getting the latest code to your local machine. Otherwise, the local machine code would not be changed without merge. 

So in the Git Gui, when you do fetch, you have to merge the data. Fetch itself won't make the code changes at your local. You can check that when you update the code by fetching
once fetch and see; the code it won't change. Then you merge... You will see the changed code.
AnswerKenntnis:
The only difference between git pull and git fetch is that :

git pull pulls from a remote branch and merges it.

git fetch only fetches from the remote branch but it does not merge

i.e. git pull = git fetch + git merge ...
AnswerKenntnis:
Git allows chronologically older commits to be applied after newer commits.
Because of this, the act of transferring commits between repositories is split into two steps:


Copying new commits from remote branch to copy of this remote branch inside local repo.  

(repo to repo operation) master@remote >> remote/origin/master@local
Integrating new commits to local branch

(inside-repo operation) remote/origin/master@local >> master@local


There are two ways of doing step 2. You can:


Fork local branch after last common ancestor and add new commits parallel to commits which are unique to local repository, finalized by merging commit, closing the fork. 
Insert new commits after last common ancestor and reapply commits unique to local repository.


In git terminology, step 1 is git fetch, step 2 is git merge or git rebase

git pull is git fetch and git merge
AnswerKenntnis:
git fetch pulls down the code from the remote server to your tracking branches.  These are not your current branches, they are local copies of the branches from the server.

git pull does a git fetch but then also merges the code from the tracking branch into your current local branch.  If you're not in the 'right' branch at the time this can be disastrous, so that's one reason why many of us git fetch first.
AnswerKenntnis:
git pull == ( git fetch + git merge) 

git fetch does not changes to local branches.

If you already have a local repository with a remote set up for the desired project, you can grab all branches and tags for the existing remote using git fetch . ... Fetch does not make any changes to local branches, so you will need to merge a remote branch with a paired local branch to incorporate newly fetch changes. from github
QuestionKenntnis:
What and where are the stack and heap?
qn_description:
Programming language books usually explain that value types are created on the stack, and reference types are created on the heap, without really explaining what these two things are.  With my only programming experience being in high level languages, I haven't read a clear explanation of this.  I mean I understand what a stack is, but where and what are they (relative to the physical memory of a real computer)?  


To what extent are they controlled by the OS or language runtime?
What is their scope?
What determines the size of each of them?
What makes one faster?
AnswersKenntnis
AnswerKenntnis:
The stack is the memory set aside as scratch space for a thread of execution.  When a function is called, a block is reserved on the top of the stack for local variables and some bookkeeping data.  When that function returns, the block becomes unused and can be used the next time a function is called.  The stack is always reserved in a LIFO order; the most recently reserved block is always the next block to be freed.  This makes it really simple to keep track of the stack; freeing a block from the stack is nothing more than adjusting one pointer.

The heap is memory set aside for dynamic allocation.  Unlike the stack, there's no enforced pattern to the allocation and deallocation of blocks from the heap; you can allocate a block at any time and free it at any time.  This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time; there are many custom heap allocators available to tune heap performance for different usage patterns.

Each thread gets a stack, while there's typically only one heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation).

To answer your questions directly:  


  To what extent are they controlled by the OS or language runtime?


The OS allocates the stack for each system-level thread when the thread is created.  Typically the OS is called by the language runtime to allocate the heap for the application.


  What is their scope?


The stack is attached to a thread, so when the thread exits the stack is reclaimed.  The heap is typically allocated at application startup by the runtime, and is reclaimed when the application (technically process) exits.


  What determines the size of each of them?  


The size of the stack is set when a thread is created.  The size of the heap is set on application startup, but can grow as space is needed (the allocator requests more memory from the operating system).


  What makes one faster?


The stack is faster because the access pattern makes it trivial to allocate and deallocate memory from it (a pointer/integer is simply incremented or decremented), while the heap has much more complex bookkeeping involved in an allocation or free.  Also, each byte in the stack tends to be reused very frequently which means it tends to be mapped to the processor's cache, making it very fast. Another performance hit for the heap is that the heap, being mostly a global resource, typically has to be multi-threading safe, i.e. each allocation and deallocation needs to be - typically - synchronized with "all" other heap accesses in the program.

A clear demonstration:


  
  Image source: vikashazrati.wordpress.com
AnswerKenntnis:
Stack:


Stored in computer RAM just like the heap.
Variables created on the stack will go out of scope and automatically deallocate.
Much faster to allocate in comparison to variables on the heap.
Implemented with an actual stack data structure.
Stores local data, return addresses, used for parameter passing
Can have a stack overflow when too much of the stack is used. (mostly from infinite (or too much) recursion, very large allocations)
Data created on the stack can be used without pointers.
You would use the stack if you know exactly how much data you need to allocate before compile time and it is not too big.
Usually has a maximum size already determined when your program starts


Heap:


Stored in computer RAM just like the stack.
Variables on the heap must be destroyed manually and never fall out of scope.  The data is freed with delete, delete[] or free
Slower to allocate in comparison to variables on the stack.
Used on demand to allocate a block of data for use by the program.
Can have fragmentation when there are a lot of allocations and deallocations
In C++ data created on the heap will be pointed to by pointers and allocated with new or malloc
Can have allocation failures if too big of a buffer is requested to be allocated.
You would use the heap if you don't know exactly how much data you will need at runtime or if you need to allocate a lot of data.
Responsible for memory leaks


Example:

int foo()
{
  char *pBuffer; //<--nothing allocated yet (excluding the pointer itself, which is allocated here on the stack).
  bool b = true; // Allocated on the stack.
  if(b)
  {
    //Create 500 bytes on the stack
    char buffer[500];

    //Create 500 bytes on the heap
    pBuffer = new char[500];

   }//<-- buffer is deallocated here, pBuffer is not
}//<--- oops there's a memory leak, I should have called delete[] pBuffer;
AnswerKenntnis:
The most important point is that heap and stack are generic terms for ways in which memory can be allocated.  They can be implemented in many different ways, and the terms apply to the basic concepts.


In a stack of items, items sit one on top of the other in the order they were placed there, and you can only remove the top one (without toppling the whole thing over).


In a heap, there is no particular order to the way items are placed.  You can reach in and remove items in any order because there is no clear 'top' item.




It does a fairly good job of describing the two ways of allocating and freeing memory in a stack and a heap.  Yum!


To what extent are they controlled by the OS or language runtime?

As mentioned, heap and stack are general terms, and can be implemented in many ways.  Computer programs typically have a stack called a call stack which stores information relevant to the current function such as a pointer to whichever function it was called from, and any local variables.  Because functions call other functions and then return, the stack grows and shrinks to hold information from the functions further down the call stack.  A program doesn't really have runtime control over it; it's determined by the programming language, OS and even the system architecture.

A heap is a general term used for any memory that is allocated dynamically and randomly; i.e. out of order.  The memory is typically allocated by the OS, with the application calling API functions to do this allocation.  There is a fair bit of overhead required in managing dynamically allocated memory, which is usually handled by the OS.
What is their scope?

The call stack is such a low level concept that it doesn't relate to 'scope' in the sense of programming.  If you disassemble some code you'll see relative pointer style references to portions of the stack, but as far as a higher level language is concerned, the language imposes its own rules of scope.  One important aspect of a stack, however, is that once a function returns, anything local to that function is immediately freed from the stack.  That works the way you'd expect it to work given how your programming languages work.  In a heap, it's also difficult to define.  The scope is whatever is exposed by the OS, but your programming language probably adds its rules about what a "scope" is in your application.  The processor architecture and the OS use virtual addressing, which the processor translates to physical addresses and there are page faults, etc.  They keep track of what pages belong to which applications.  You never really need to worry about this, though, because you just use whatever method your programming language uses to allocate and free memory, and check for errors (if the allocation/freeing fails for any reason).
What determines the size of each of them?

Again, it depends on the language, compiler, operating system and architecture.  A stack is usually pre-allocated, because by definition it must be contiguous memory (more on that in the last paragraph).  The language compiler or the OS determine its size.  You don't store huge chunks of data on the stack, so it'll be big enough that it should never be fully used, except in cases of unwanted endless recursion (hence, "stack overflow") or other unusual programming decisions.

A heap is a general term for anything that can be dynamically allocated.  Depending on which way you look at it, it is constantly changing size.  In modern processors and operating systems the exact way it works is very abstracted anyway, so you don't normally need to worry much about how it works deep down, except that (in languages where it lets you) you mustn't use memory that you haven't allocated yet or memory that you have freed.
What makes one faster?

The stack is faster because all free memory is always contiguous.  No list needs to be maintained of all the segments of free memory, just a single pointer to the current top of the stack.  Compilers usually store this pointer in a special, fast register for this purpose.  What's more, subsequent operations on a stack are usually concentrated within very nearby areas of memory, which at a very low level is good for optimization by the processor on-die caches.
AnswerKenntnis:
(I have moved this answer from another question that was more or less a dupe of this one.)

The answer to your question is implementation specific and may vary across compilers and processor architectures. However, here is a simplified explanation.


Both the stack and the heap are memory areas allocated from the underlying operating system (often virtual memory that is mapped to physical memory on demand).
In a multi-threaded environment each thread will have its own completely independent stack but they will share the heap. Concurrent access has to be controlled on the heap and is not possible on the stack.


The heap


The heap contains a linked list of used and free blocks. New allocations on the heap (by new or malloc) are satisfied by creating a suitable block from one of the free blocks. This requires updating list of blocks on the heap. This meta information about the blocks on the heap is also stored on the heap often in a small area just in front of every block.
As the heap grows new blocks are often allocated from lower addresses towards higher addresses. Thus you can think of the heap as a heap of memory blocks that grows in size as memory is allocated. If the heap is too small for an allocation the size can often be increased by acquiring more memory from the underlying operating system.
Allocating and deallocating many small blocks may leave the heap in a state where there are a lot of small free blocks interspersed between the used blocks. A request to allocate a large block may fail because none of the free blocks are large enough to satisfy the allocation request even though the combined size of the free blocks may be large enough. This is called heap fragmentation.
When a used block that is adjacent to a free block is deallocated the new free block may be merged with the adjacent free block to create a larger free block effectively reducing the fragmentation of the heap.




The stack


The stack often works in close tandem with a special register on the CPU named the stack pointer. Initially the stack pointer points to the top of the stack (the highest address on the stack).
The CPU has special instructions for pushing values onto the stack and popping them back from the stack. Each push stores the value at the current location of the stack pointer and decreases the stack pointer.  A pop retrieves the value pointed to by the stack pointer and then increases the stack pointer (don't be confused by the fact that adding a value to the stack decreases the stack pointer and removing a value increases it. Remember that the stack grows to the bottom). The values stored and retrieved are the values of the CPU registers.
When a function is called the CPU uses special instructions that push the current instruction pointer, i.e. the address of the code executing on the stack. The CPU then jumps to the function by setting the 
instruction pointer to the address of the function called. Later, when the function returns, the old instruction pointer is popped from the stack and execution resumes at the code just after the call to the function.
When a function is entered, the stack pointer is decreased to allocate more space on the stack for local (automatic) variables. If the function has one local 32 bit variable four bytes are set aside on the stack. When the function returns, the stack pointer is moved back to free the allocated area.
If a function has parameters, these are pushed onto the stack before the call to the function. The code in the function is then able to navigate up the stack from the current stack pointer to locate these values.
Nesting function calls work like a charm. Each new call will allocate function parameters, the return address and space for local variables and these activation records can be stacked for nested calls and will unwind in the correct way when the functions return.
As the stack is a limited block of memory, you can cause a stack overflow by calling too many nested functions and/or allocating too much space for local variables. Often the memory area used for the stack is set up in such a way that writing below the bottom (the lowest address) of the stack will trigger a trap or exception in the CPU. This exceptional condition can then be caught by the runtime and converted into some kind of stack overflow exception.





  Can a function be allocated on the heap instead of a stack?


No, activation records for functions (i.e. local or automatic variables) are allocated on the stack that is used not only to store these variables, but also to keep track of nested function calls.

How the heap is managed is really up to the runtime environment. C uses malloc and C++ uses new, but many other languages have garbage collection.

However, the stack is a more low-level feature closely tied to the processor architecture. Growing the heap when there is not enough space isn't too hard since it can be implemented in the library call that handles the heap. However, growing the stack is often impossible as the stack overflow only is discovered when it is too late; and shutting down the thread of execution is the only viable option.
AnswerKenntnis:
This helps:



Fixed size items (like an int and cls1 (the object pointer reference)) go on the stack 'cos we know their size.

[Note, I had used the phrase static items which was misleading 'cos some people where interpreting this to mean static variables. I have now changed the phrase to fixed size to be clearer.]

Objects (which vary in size as we update them) go on the heap 'cos we don't know their size.

To summarise, if you know the size of the variable (e.g. an int) it goes on the stack. If not, it goes on the heap. And objects always, always, always go on the heap.

More information can be found here:

The difference between stack and heap memory allocation «  timmurphy.org

and here: 

Creating Objects on the Stack and Heap

This article is the source of picture above: Six important .NET concepts: Stack, heap, value types, reference types, boxing, and unboxing - CodeProject

but be aware it may contain some inaccuracies.
AnswerKenntnis:
The Stack
When you call a function the arguments to that function plus some other overhead is put on the stack. Some info (such as where to go on return) is also stored there.
When you declare a variable inside your function, that variable is also allocated on the stack. 

Deallocating the stack is pretty simple because you always deallocate in the reverse order in which you allocate. Stack stuff is added as you enter functions, the corresponding data is removed as you exit them. This means that you tend to stay within a small region of the stack unless you call lots of functions that call lots of other functions (or create a recursive solution).

The Heap
The heap is a generic name for where you put the data that you create on the fly. If you don't know how many spaceships your program is going to create, you are likely to use the new (or malloc or equivalent) operator to create each spaceship. This allocation is going to stick around for a while, so it is likely we will free things in a different order than we created them. 

Thus, the heap is far more complex, because there end up being regions of memory that are unused interleaved with chunks that are - memory gets fragmented. Finding free memory of the size you need is a difficult problem. This is why the heap should be avoided (though it is still often used).

Implementation
Implementation of both the stack and heap is usually down to the runtime / OS. Often games and other applications that are performance critical create their own memory solutions that grab a large chunk of memory from the heap and then dish it out internally to avoid relying on the OS for memory. 

This is only practical if your memory usage is quite different from the norm - i.e for games where you load a level in one huge operation and can chuck the whole lot away in another huge operation.

Physical location in memory
This is less relevant than you think because of a technology called Virtual Memory which makes your program think that you have access to a certain address where the physical data is somewhere else (even on the hard disc!). The addresses you get for the stack are in increasing order as your call tree gets deeper. The addresses for the heap are un-predictable (i.e implimentation specific) and frankly not important.
AnswerKenntnis:
Others have answered the broad strokes pretty well, so I'll throw in a few details.


Stack and heap need not be singular.  A common situation in which you have more than one stack is if you have more than one thread in a process.  In this case each thread has its own stack.  You can also have more than one heap, for example some DLL configurations can result in different DLLs allocating from different heaps, which is why it's generally a bad idea to release memory allocated by a different library.
In C you can get the benefit of variable length allocation through the use of alloca, which allocates on the stack, as opposed to alloc, which allocates on the heap.  This memory won't survive your return statement, but it's useful for a scratch buffer.
On windows making a huge temporary buffer that you don't use much of is not free. This is because the compiler will generate a stack probe loop that is called every time your function is entered to make sure the stack exists (because windows uses a single guard page at the end of your stack to detect when it needs to grow the stack, if you access memory more than one page off the end of the stack you will crash).  Example:


void myfunction()
{
   char big[10000000];
   // do something that only uses for first 1K of big 99% of the time.
}
AnswerKenntnis:
To clarify, this answer has incorrect information (thomas fixed his answer after comments, cool :) ). Other answers just avoid explaining what static allocation means. So I will explain the three main forms of allocation and how they usually relate to the heap, stack, and data segment below. I also will show some examples in both C/CPP and Python to help people understand.

"Static" (aka statically allocated) variables are not allocated on the stack. Do not assume so - many people do only because "static" sounds a lot like "stack". They actually exist in neither the stack nor the heap. The are part of what's called the data segment.

However it is generally better to consider "scope" and "lifetime" rather than "stack" and "heap". 

Scope refers to what parts of the code can access a variable. Generally we think of local scope (can only be accessed by the current function) versus global scope (can be accessed anywhere) although scope can get much more complex.

Lifetime refers to when a variable is allocated and deallocated during program execution. Usually we think of static allocation (variable will persist through the entire duration of the program, making it useful for storing the same information across several function calls) versus automatic allocation (variable only persists during a single call to a function, making it useful for storing information that is only used during your function and can be discarded once you are done) versus dynamic allocation (variables whose duration is defined at runtime, instead of compile time like static or automatic).

Although most compilers and interpreters implement this behavior similarly in terms of using stacks, heaps, etc, a compiler may sometimes break these conventions if it wants as long as behavior is correct. For instance, due to optimization a local variable may only exist in a register or be removed entirely, even though most local variables exist in the stack. As has been pointed out in a few comments, you are free to implement a compiler that doesn't even use a stack or a heap, but instead some other storage mechanisms (rarely done, since stacks and heaps are great for this).

I will provide some simple annotated C code to illustrate all of this. The best way to learn is to run a program under a debugger and watch the behavior. If you prefer to read python, skip to the end of the answer :)

//statically allocated in the data segment when the program/DLL is first loaded 
//deallocated when the program/DLL exits
//scope - can be accessed from anywhere in the code
int someGlobalVariable;

//statically allocated in the data segment when the program is first loaded
//deallocated when the program/DLL exits
//scope - can be accessed from anywhere in this particular code file
static int someStaticVariable;

//"someArgument" is allocated on the stack each time MyFunction is called
//"someArgument" is deallocated when MyFunction returns
//scope - can be accessed only within MyFunction()
void MyFunction(int someArgument) {

    //statically allocated in the data segment when the program is first loaded
    //deallocated when the program/DLL exits
    //scope - can be accessed only within MyFunction()
    static int someLocalStaticVariable;

    //allocated on the stack each time MyFunction is called
    //deallocated when MyFunction returns
    //scope - can be accessed only within MyFunction()
    int someLocalVariable;

    //a *pointer* is allocated on the stack each time MyFunction is called
    //this pointer is deallocated when MyFunction returns
    //scope - the pointer can be accessed only within MyFunction()
    int* someDynamicVariable; 

    //this line causes space for an integer to be allocated in the heap
    //when this line is executed. note this is not at the begining of 
    //the call to MyFunction(), like the automatic variables
    //scope - only code within MyFunction() can access this space
    //*through this particular variable*
    //however, if you pass the address somewhere else, that code
    //can access it too
    someDynamicVariable = new int; 


    //this line deallocates the space for the integer in the heap
    //if we did not write it, the memory would be "leaked"
    //note a fundamental difference b/w the stack and heap
    //the heap must be managed. the stack is managed for us
    delete someDynamicVariable;

    //in other cases, instead of deallocating this heap space you 
    //might store the address somewhere more permanent to use later
    //some languages even take care of deallocation for you... but
    //always it needs to be taken care of at runtime by some mechanism

    //when the function returns, someArgument, someLocalVariable
    //and the pointer someDynamicVariable are deallocated.
    //the space pointed to by someDynamicVariable was already
    //deallocated prior to returning
    return;
}

//note that someGlobalVariable, someStaticVariable and 
//someLocalStaticVariable continue to exist, and are not 
//deallocated until the program exits


A particularly poignant example of why it's important to distinguish between lifetime and scope is that a variable can have local scope but static lifetime - for instance, "someLocalStaticVariable" in the code sample above. Such variables can make our common but informal naming habits very confusing. For instance when we say "local" we usually mean "locally scoped automatically allocated variable" and when we say global we usually mean "globally scoped statically allocated variable". Unfortunately when it comes to things like "file scoped statically allocated variables" many people just say... "huh???". 

Some of the syntax choices in c/cpp exacerbate this problem - for instance many people think global variables are not "static" because of the syntax shown below.

int var1; //has global scope and static allocation
static int var2; //has file scope and static allocation

int main() {return 0;}


Note that putting the keyword "static" in the declaration above prevents var2 from having global scope. Nevertheless, the global var1 has static allocation. This is not intuitive! For this reason, I try to never use the word "static" when describing scope, and instead say something like "file" or "file limited" scope. However many people use the phrase "static" or "static scope" to describe a variable that can only be accessed from one code file. In the context of lifetime, "static" always means the variable is allocated at program start and deallocated when program exits.

Some people think of these concepts as C/CPP specific. They are not. For instance, the Python sample below illustrates all three types of allocation (there are some subtle differences possible in interpreted languages that I won't get into here).

from datetime import datetime

class Animal:
    _FavoriteFood = 'Undefined' #_FavoriteFood is statically allocated

    def PetAnimal(self):
        curTime = datetime.time(datetime.now()) #curTime is automatically allocatedion
        print("Thank you for petting me. But it's " + str(curTime) + ", you should feed me. My favorite food is " + self._FavoriteFood)

class Cat(Animal):
    _FavoriteFood = 'tuna' #note since we override, Cat class has its own statically allocated _FavoriteFood variable, different from Animal's

class Dog(Animal):
    _FavoriteFood = 'steak' #likewise the Dog class gets its own static variable. Important to note - this one static variable is shared among all instances of Dog, hence it is not dynamic!



if __name__ == "__main__":
    whiskers = Cat() #dynamically allocated
    fido = Dog() #dynamically allocated
    rinTinTin = Dog() #dynamically allocated

    whiskers.PetAnimal()
    fido.PetAnimal()
    rinTinTin.PetAnimal()

    Dog._FavoriteFood = 'milkbones'
    whiskers.PetAnimal()
    fido.PetAnimal()
    rinTinTin.PetAnimal()


# output is
# Thank you for petting me. But it's 13:05:02.255000, you should feed me. My favorite food is tuna
# Thank you for petting me. But it's 13:05:02.255000, you should feed me. My favorite food is steak
# Thank you for petting me. But it's 13:05:02.255000, you should feed me. My favorite food is steak
# Thank you for petting me. But it's 13:05:02.255000, you should feed me. My favorite food is tuna
# Thank you for petting me. But it's 13:05:02.255000, you should feed me. My favorite food is milkbones
# Thank you for petting me. But it's 13:05:02.256000, you should feed me. My favorite food is milkbones
AnswerKenntnis:
Others have directly answered your question, but when trying to understand the stack and the heap, I think it is helpful to consider the memory layout of a traditional UNIX process (without threads and mmap()-based allocators). The Memory Management Glossary web page has a diagram of this memory layout.

The stack and heap are traditionally located at opposite ends of the process's virtual address space. The stack grows automatically when accessed, up to a size set by the kernel (which can be adjusted with setrlimit(RLIMIT_STACK, ...)). The heap grows when the memory allocator invokes the brk() or sbrk() system call, mapping more pages of physical memory into the process's virtual address space. 

In systems without virtual memory, such as some embedded systems, the same basic layout often applies, except the stack and heap are fixed in size. However, in other embedded systems (such as those based on Microchip PIC microcontrollers), the program stack is a separate block of memory that is not addressable by data movement instructions, and can only be modified or read indirectly through program flow instructions (call, return, etc.). Other architectures, such as Intel Itanium processors, have multiple stacks. In this sense, the stack is an element of the CPU architecture.
AnswerKenntnis:
The stack is a portion of memory that can be manipulated via several key assembly language instructions, such as 'pop' (remove and return a value from the stack) and 'push' (push a value to the stack), but also call (call a subroutine - this pushes the address to return to the stack) and return (return from a subroutine - this pops the address off of the stack and jumps to it).  It's the region of memory below the stack pointer register, which can be set as needed.  The stack is also used for passing arguments to subroutines, and also for preserving the values in registers before calling subroutines.

The heap is a portion of memory that is given to an application by the operating system, typically through a syscall like malloc.  On modern OSes this memory is a set of pages that only the calling process has access to.

The size of the stack is determined at runtime, and generally does not grow after the program launches.  In a C program, the stack needs to be large enough to hold every variable declared within each function.  The heap will grow dynamically as needed, but the OS is ultimately making the call (it will often grow the heap by more than the value requested by malloc, so that at least some future mallocs won't need to go back to the kernel to get more memory.  This behavior is often customizable)

Because you've allocated the stack before launching the program, you never need to malloc before you can use the stack, so that's a slight advantage there.  In practice, it's very hard to predict what will be fast and what will be slow in modern operating systems that have virtual memory subsystems, because how the pages are implemented and where they are stored is an implementation detail.
AnswerKenntnis:
I think many other people have given you mostly correct answers on this matter.

One detail that has been missed, however, is that the "heap" should in fact probably be called the "free store".  The reason for this distinction is that the original free store was implemented with a data structure known as a "binomial heap."  For that reason, allocating from early implementations of malloc()/free() was allocation from a heap.  However, in this modern day, most free stores are implemented with very elaborate data structures that are not binomial heaps.
AnswerKenntnis:
You can do some interesting things with the stack.  For instance, you have functions like alloca (assuming you can get past the copious warnings concerning its use), which is a form of malloc that specifically uses the stack, not the heap, for memory.

That said, stack-based memory errors are some of the worst I've experienced.  If you use heap memory, and you overstep the bounds of your allocated block, you have a decent chance of triggering a segment fault.  (Not 100%: your block may be incidentally contiguous with another that you have previously allocated.)  But since variables created on the stack are always contiguous with each other, writing out of bounds can change the value of another variable.  I have learned that whenever I feel that my program has stopped obeying the laws of logic, it is probably buffer overflow.
AnswerKenntnis:
Simply, the stack is where local variables get created. Also, every time you call a subroutine the program counter (pointer to the next machine instruction) and any important registers, and sometimes the parameters get pushed on the stack. Then any local variables inside the subroutine are pushed onto the stack (and used from there). When the subroutine finishes, that stuff all gets popped back off the stack. The PC and register data gets and put back where it was as it is popped, so your program can go on its merry way.

The heap is the area of memory dynamic memory allocations are made out of (explicit "new" or "allocate" calls). It is a special data structure that can keep track of blocks of memory of varying sizes and their allocation status.

In "classic" systems RAM was laid out such that the stack pointer started out at the bottom of memory, the heap pointer started out at the top, and they grew towards each other. If they overlap, you are out of RAM. That doesn't work with modern multi-threaded OSes though. Every thread has to have its own stack, and those can get created dynamicly.
AnswerKenntnis:
From WikiAnwser.

Stack

When a function or a method calls another function which in turns calls another function etc., the execution of all those functions remains suspended until the very last function returns its value.

This chain of suspended function calls is the stack, because elements in the stack (function calls) depend on each other.

The stack is important to consider in exception handling and thread executions.

Heap

The heap is simply the memory used by programs to store variables.
Element of the heap (variables) have no dependencies with each other and can always be accessed randomly at any time.

I like the accepted answer better since it's even more low level.
AnswerKenntnis:
Stack


very fast access
don't have to explicitly de-allocate variables
space is managed efficiently by CPU, memory will not become fragmented
local variables only
limit on stack size (OS-dependent)
variables cannot be resized


Heap


variables can be accessed globally
no limit on memory size
(relatively) slower access
no guaranteed efficient use of space, memory may become fragmented over time as blocks of memory are allocated, then freed
you must manage memory (you're in charge of allocating and freeing variables)
variables can be resized using realloc()
AnswerKenntnis:
A very simple explanation in that guide: http://gribblelab.org/CBootcamp/7_Memory_Stack_vs_Heap.html

While Paul Gribble web site did not respond again, here's the google cached page: http://webcache.googleusercontent.com/search?q=cache:http://gribblelab.org/CBootcamp/7_Memory_Stack_vs_Heap.html
AnswerKenntnis:
My little contribution:

What is Stack?


  
    Stacks in computing architectures are regions of memory where data is added or removed in a last-in-first-out manner.In a multi-threaded application, each thread will have its own stack.
  


What is heap?


  
    The heap is an area of dynamically-allocated memory that is managed automatically by the operating system or the memory manager library. Memory on the heap is allocated, deallocated, and resized regularly during program execution, and this can lead to a problem called fragmentation. Fragmentation occurs when memory objects are allocated with small spaces in between that are too small to hold additional memory objects. The net result is a percentage of the heap space that is not usable for further memory allocations.
  


Both together


  
    In a multi-threaded application, each thread will have its own stack. But, all the different threads will share the heap. Because the different threads share the heap in a multi-threaded application, this also means that there has to be some coordination between the threads so that they donΓÇÖt try to access and manipulate the same piece(s) of memory in the heap at the same time.
  


Which is faster ΓÇô the stack or the heap? And why?


  
    The stack is much faster than the heap. This is because of the way that memory is allocated on the stack. Allocating memory on the stack is as simple as moving the stack pointer up.
  


For people new to programming, itΓÇÖs probably a good idea to use the stack since itΓÇÖs easier. Because the stack is small, you would want to use it when you know exactly how much memory you will need for your data, or if you know the size of your data is very small. ItΓÇÖs better to use the heap when you know that you will need a lot of memory for your data, or you just are not sure how much memory you will need (like with a dynamic array).
QuestionKenntnis:
How do I "think in AngularJS" if I have a jQuery background?
qn_description:
How do I ΓÇ£think in AngularJSΓÇ¥ if I have a jQuery background?

Suppose I'm familiar with developing client-side applications in jQuery, but now I'd like to start using AngularJS. Can you describe the paradigm shift that is necessary ? Here are a few questions that might help you frame an answer:


How do I architect and design client-side web applications differently? What is the biggest difference?
What should I stop doing/using; what should I start doing/using instead?
Are there any server-side considerations/restrictions?


I'm not looking for a detailed comparison between jQuery and AngularJS.
AnswersKenntnis
AnswerKenntnis:
1. Don't design your page, and then change it with DOM manipulations

In jQuery, you design a page, and then you make it dynamic. This is because jQuery was designed for augmentation and has grown incredibly from that simple premise.

But in AngularJS, you must start from the ground up with your architecture in mind. Instead of starting by thinking "I have this piece of the DOM and I want to make it do X", you have to start with what you want to accomplish, then go about designing your application, and then finally go about designing your view.

2. Don't augment jQuery with AngularJS

Similarly, don't start with the idea that jQuery does X, Y, and Z, so I'll just add AngularJS on top of that for models and controllers. This is really tempting when you're just starting out, which is why I always recommend that new AngularJS developers don't use jQuery at all, at least until they get used to doing things the "Angular Way".

I've seen many developers here and on the mailing list create these elaborate solutions with jQuery plugins of 150 or 200 lines of code that they then glue into AngularJS with a collection of callbacks and $applys that are confusing and convoluted; but they eventually get it working! The problem is that in most cases that jQuery plugin could be rewritten in AngularJS in a fraction of the code, where suddenly everything becomes comprehensible and straightforward.

The bottom line is this: when solutioning, first "think in AngularJS"; if you can't think of a solution, ask the community; if after all of that there is no easy solution, then feel free to reach for the jQuery. But don't let jQuery become a crutch or you'll never master AngularJS.

3. Always think in terms of architecture

First know that single-page applications are applications. They're not webpages. So we need to think like a server-side developer in addition to thinking like a client-side developer. We have to think about how to divide our application into individual, extensible, testable components.

So then how do you do that? How do you "think in AngularJS"? Here are some general principles, contrasted with jQuery.

The view is the "official record"

In jQuery, we programmatically change the view. We could have a dropdown menu defined as a ul like so:

<ul class="main-menu">
    <li class="active">
        <a href="#/home">Home</a>
    </li>
    <li>
        <a href="#/menu1">Menu 1</a>
        <ul>
            <li><a href="#/sm1">Submenu 1</a></li>
            <li><a href="#/sm2">Submenu 2</a></li>
            <li><a href="#/sm3">Submenu 3</a></li>
        </ul>
    </li>
    <li>
        <a href="#/home">Menu 2</a>
    </li>
</ul>


In jQuery, in our application logic, we would activate it with something like:

$('.main-menu').dropdownMenu();


When we just look at the view, it's not immediately obvious that there is any functionality here. For small applications, that's fine. But for non-trivial applications, things quickly get confusing and hard to maintain.

In AngularJS, though, the view is the official record of view-based functionality. Our ul declaration would look like this instead:

<ul class="main-menu" dropdown-menu>
    ...
</ul>


These two do the same thing, but in the AngularJS version anyone looking at the template knows what's supposed to happen. Whenever a new member of the development team comes on board, she can look at this and then know that there is a directive called dropdownMenu operating on it; she doesn't need to intuit the right answer or sift through any code. The view told us what was supposed to happen. Much cleaner.

Developers new to AngularJS often ask a question like: how do I find all links of a specific kind and add a directive onto them. The developer is always flabbergasted when we reply: you don't. But the reason you don't do that is that this is like half-jQuery, half-AngularJS, and no good. The problem here is that the developer is trying to "do jQuery" in the context of AngularJS. That's never going to work well. The view is the official record. Outside of a directive (more on this below), you never, ever, never change the DOM. And directives are applied in the view, so intent is clear.

Remember: don't design, and then mark up. You must architect, and then design.

Data binding

This is by far one of the most awesome features of AngularJS and cuts out a lot of the need to do the kinds of DOM manipulations I mentioned in the previous section. AngularJS will automatically update your view so you don't have to! In jQuery, we respond to events and then update content. Something like:

$.ajax({
  url: '/myEndpoint.json',
  success: function ( data, status ) {
    $('ul#log').append('<li>Data Received!</li>');
  }
});


For a view that looks like this:

<ul class="messages" id="log">
</ul>


Apart from mixing concerns, we also have the same problems of signifying intent that I mentioned before. But more importantly, we had to manually reference and update a DOM node. And if we want to delete a log entry, we have to code against the DOM for that too. How do we test the logic apart from the DOM? And what if we want to change the presentation?

This a little messy and a trifle frail. But in AngularJS, we can do this:

$http( '/myEndpoint.json' ).then( function ( response ) {
    $scope.log.push( { msg: 'Data Received!' } );
});


And our view can look like this:

<ul class="messages">
    <li ng-repeat="entry in log">{{ entry.msg }}</li>
</ul>


But for that matter, our view could look like this:

<div class="messages">
    <div class="alert" ng-repeat="entry in log">
        {{ entry.msg }}
    </div>
</div>


And now instead of using an unordered list, we're using Bootstrap alert boxes. And we never had to change the controller code! But more importantly, no matter where or how the log gets updated, the view will change too. Automatically. Neat!

Though I didn't show it here, the data binding is two-way. So those log messages could also be editable in the view just by doing this: <input ng-model="entry.msg" />. And there was much rejoicing.

Distinct model layer

In jQuery, the DOM is kind of like the model. But in AngularJS, we have a separate model layer that we can manage in any way we want, completely independently from the view. This helps for the above data binding, maintains separation of concerns, and introduces far greater testability. Other answers mentioned this point, so I'll just leave it at that.

Separation of concerns

And all of the above tie into this over-arching theme: keep your concerns separate. Your view acts as the official record of what is supposed to happen (for the most part); your model represents your data; you have a service layer to perform reusable tasks; you do DOM manipulation and augment your view with directives; and you glue it all together with controllers. This was also mentioned in other answers, and the only thing I would add pertains to testability, which I discuss in another section below.

Dependency injection

To help us out with separation of concerns is dependency injection (DI). If you come from a server-side language (from Java to PHP) you're probably familiar with this concept already, but if you're a client-side guy coming from jQuery, this concept can seem anything from silly to superfluous to hipster. But it's not. :-)

From a broad perspective, DI means that you can declare components very freely and then from any other component, just ask for an instance of it and it will be granted. You don't have to know about loading order, or file locations, or anything like that. The power may not immediately be visible, but I'll provide just one (common) example: testing.

Let's say in our application, we require a service that implements server-side storage through a REST API and, depending on application state, local storage as well. When running tests on our controllers, we don't want to have to communicate with the server - we're testing the controller, after all. We can just add a mock service of the same name as our original component, and the injector will ensure that our controller gets the fake one automatically - our controller doesn't and needn't know the difference.

Speaking of testing...

4. Test-driven development - always

This is really part of section 3 on architecture, but it's so important that I'm putting it as its own top-level section.

Out of all of the many jQuery plugins you've seen, used, or written, how many of them had an accompanying test suite? Not very many because jQuery isn't very amenable to that. But AngularJS is.

In jQuery, the only way to test is often to create the component independently with a sample/demo page against which our tests can perform DOM manipulation. So then we have to develop a component separately and then integrate it into our application. How inconvenient! So much of the time, when developing with jQuery, we opt for iterative instead of test-driven development. And who could blame us?

But because we have separation of concerns, we can do test-driven development iteratively in AngularJS! For example, let's say we want a super-simple directive to indicate in our menu what our current route is. We can declare what we want in our view:

<a href="/hello" when-active>Hello</a>


Okay, now we can write a test:

it( 'should add "active" when the route changes', inject(function() {
    var elm = $compile( '<a href="/hello" when-active>Hello</a>' )( $scope );

    $location.path('/not-matching');
    expect( elm.hasClass('active') ).toBeFalsey();

    $location.path( '/hello' );
    expect( elm.hasClass('active') ).toBeTruthy();
}));


We run our test and confirm that it fails. So now we can write our directive:

.directive( 'whenActive', function ( $location ) {
    return {
        scope: true,
        link: function ( scope, element, attrs ) {
            scope.$on( '$routeChangeSuccess', function () {
                if ( $location.path() == element.attr( 'href' ) ) {
                    element.addClass( 'active' );
                }
                else {
                    element.removeClass( 'active' );
                }
            });
        }
    };
});


Our test now passes and our menu performs as requested. Our development is both iterative and test-driven. Wicked-cool.

5. Conceptually, directives are not packaged jQuery

You'll often hear "only do DOM manipulation in a directive". This is a necessity. Treat it with due deference!

But let's dive a little deeper...

Some directives just decorate what's already in the view (think ngClass) and therefore sometimes do DOM manipulation straight away and then are basically done. But if a directive is like a "widget" and has a template, it should also respect separation of concerns. That is, the template too should remain largely independent from its implementation in the link and controller functions.

AngularJS comes with an entire set of tools to make this very easy; with ngClass we can dynamically update the class; ngBind allows two-way data binding; ngShow and ngHide programmatically show or hide an element; and many more - including the ones we write ourselves. In other words, we can do all kinds of awesomeness without DOM manipulation. The less DOM manipulation, the easier directives are to test, the easier they are to style, the easier they are to change in the future, and the more re-usable and distributable they are.

I see lots of developers new to AngularJS using directives as the place to throw a bunch of jQuery. In other words, they think "since I can't do DOM manipulation in the controller, I'll take that code put it in a directive". While that certainly is much better, it's often still wrong.

Think of the logger we programmed in section 3. Even if we put that in a directive, we still want to do it the "Angular Way". It still doesn't take any DOM manipulation! There are lots of times when DOM manipulation is necessary, but it's a lot rarer than you think! Before doing DOM manipulation anywhere in your application, ask yourself if you really need to. There might be a better way.

Here's a quick example that shows the pattern I see most frequently. We want a toggleable button. (Note: this example is a little contrived and a skosh verbose to represent more complicated cases that are solved in exactly the same way.)

.directive( 'myDirective', function () {
    return {
        template: '<a class="btn">Toggle me!</a>',
        link: function ( scope, element, attrs ) {
            var on = false;

            $(element).click( function () {
                if ( on ) {
                    $(element).removeClass( 'active' );
                }
                else {
                    $(element).addClass( 'active' );
                }

                on = !on;
            });
        }
    };
});


There are a few things wrong with this. First, jQuery was never necessary. There's nothing we did here that needed jQuery at all! Second, even if we already have jQuery on our page, there's no reason to use it here; we can simply use angular.element and our component will still work when dropped into a project that doesn't have jQuery. Third, even assuming jQuery was required for this directive to work, jqLite (angular.element) will always use jQuery if it was loaded! So we needn't use the $ - we can just use angular.element. Fourth, closely related to the third, is that jqLite elements needn't be wrapped in $ - the element that is passed to the link function would already be a jQuery element! And fifth, which we've mentioned in previous sections, why are we mixing template stuff into our logic?

This directive can be rewritten (even for very complicated cases!) much more simply like so:

.directive( 'myDirective', function () {
    return {
        scope: true,
        template: '<a class="btn" ng-class="{active: on}" ng-click="toggle()">Toggle me!</a>',
        link: function ( scope, element, attrs ) {
            scope.on = false;

            scope.toggle = function () {
                scope.on = !scope.on;
            };
        }
    };
});


Again, the template stuff is in the template, so you (or your users) can easily swap it out for one that meets any style necessary, and the logic never had to be touched. Reusability - boom!

And there are still all those other benefits, like testing - it's easy! No matter what's in the template, the directive's internal API is never touched, so refactoring is easy. You can change the template as much as you want without touching the directive. And no matter what you change, your tests still pass.

w00t!

So if directives aren't just collections of jQuery-like functions, what are they? Directives are actually extensions of HTML. If HTML doesn't do something you need it to do, you write a directive to do it for you, and then use it just as if it was part of HTML.

Put another way, if AngularJS doesn't do something out of the box, think how the team would accomplish it to fit right in with ngClick, ngClass, et al.

Summary

Don't even use jQuery. Don't even include it. It will hold you back. And when you come to a problem that you think you know how to solve in jQuery already, before you reach for the $, try to think about how to do it within the confines the AngularJS. If you don't know, ask! 19 times out of 20, the best way to do it doesn't need jQuery and to try to solve it with jQuery results in more work for you.
AnswerKenntnis:
Imperative ΓåÆ declarative

In jQuery, selectors are used to find DOM elements and then bind/register event handlers to them. When an event triggers, that (imperative) code executes to update/change the DOM.

In AngularJS, you want to think about views rather than DOM elements. Views are (declarative) HTML that contain AngularJS directives. Directives set up the event handlers behind the scenes for us and give us dynamic databinding. Selectors are rarely used, so the need for IDs (and some types of classes) is greatly diminished. Views are tied to models (via scopes). Views are a projection of the model. Events change models (that is, data, scope properties), and the views that project those models update "automatically."

In AngularJS, think about models, rather than jQuery-selected DOM elements that hold your data. Think about views as projections of those models, rather than registering callbacks to manipulate what the user sees.

Separation of concerns

jQuery employs unobtrusive JavaScript - behavior (JavaScript) is separated from the structure (HTML).

AngularJS uses controllers and directives (each of which can have their own controller, and/or compile and linking functions) to remove behavior from the view/structure (HTML).  Angular also has services and filters to help separate/organize your application.

See also http://stackoverflow.com/a/14346528/215945

Application design

One approach to designing an AngularJS application:


Think about your models. Create services or your own JavaScript objects for those models.
Think about how you want to present your models -- your views. Create HTML templates for each view, using the necessary directives to get dynamic databinding.
Attach a controller to each view (using ng-view and routing, or ng-controller). Have the controller find/get only whatever model data the view needs to do its job. Make controllers as thin as possible.


Prototypal inheritance

You can do a lot with jQuery without knowing about how JavaScript prototypal inheritance works. When developing AngularJS applications, you will avoid some common pitfalls if you have a good understanding of JavaScript inheritance. Recommended reading: What are the nuances of scope prototypal / prototypical inheritance in AngularJS?
AnswerKenntnis:
Can you describe the paradigm shift that is necessary?


Imperative vs Declarative

With jQuery you tell the DOM what needs to happen, step by step. With AngularJS you describe what results you want but not how to do it. More on this here. Also, check out Mark Rajcok's answer.


  How do I architect and design client-side web apps differently? 


AngularJS is an entire client-side framework that uses the MVC pattern (check out their graphical representation). It greatly focuses on separation of concerns.


  What is the biggest difference? What should I stop doing/using; what should I start doing/using instead?


jQuery is a library 

AngularJS is a beautiful client-side framework, highly testable, that combines tons of cool stuff such as MVC, dependency injection, data binding and much more. 

It focuses on separation of concerns and testing (unit testing and end-to-end testing), which facilitates test-driven development.

The best way to start is going through their awesome tutorial. You can go through the steps in a couple of hours; however, in case you want to master the concepts behind the scenes, they include a myriad of reference for further reading.


  Are there any server-side considerations/restrictions?


You may use it on existing applications where you are already using pure jQuery. However, if you want to fully take advantage of the AngularJS features you may consider coding the server side using a RESTful approach.

Doing so will allow you to leverage their resource factory, which creates an abstraction of your server side RESTful API and makes server-side calls (get, save, delete, etc.) incredibly easy.
AnswerKenntnis:
jQuery: you think a lot about 'QUERYing the DOM' for DOM elements and doing something.

AngularJS: THE model is the truth, and you always think from that ANGLE.

For example, when you get data from THE server which you intend to display in some format in the DOM, in jQuery, you need to '1. FIND' where in the DOM you want to place this data, the '2. UPDATE/APPEND' it there by creating a new node or just setting its innerHTML. Then when you want to update this view, you then '3. FIND' the location and '4. UPDATE'. This cycle of find and update all done within the same context of getting and formatting data from server is gone in AngularJS.

With AngularJS you have your model (JavaScript objects you are already used to) and the value of the model tells you about the model (obviously) and about the view, and an operation on the model automatically propagates to the view, so you don't have to think about it. You will find yourself in AngularJS no longer finding things in the DOM.

To put in another way, in jQuery, you need to think about CSS selectors, that is, where is the div or td that has a class or attribute, etc., so that I can get their HTML or color or value, but in AngularJS, you will find yourself thinking like this: what model am I dealing with, I will set the model's value to true. You are not bothering yourself of whether the view reflecting this value is a checked box or resides in a td element (details you would have often needed to think about in jQuery).

And with DOM manipulation in AngularJS, you find yourself adding directives and filters, which you can think of as valid HTML extensions.

One more thing you will experience in AngularJS: in jQuery you call the jQuery functions a lot, in AngularJS, AngularJS will call your functions, so AngularJS will 'tell you how to do things', but the benefits are worth it, so learning AngularJS usually means learning what AngularJS wants or the way AngularJS requires that you present your functions and it will call it accordingly. This is one of the things that makes AngularJS a framework rather than a library.
AnswerKenntnis:
jQuery is a DOM manipulation library.

AngularJS is an MV* framework.

In fact, AngularJS is one of the few JavaScript MV* frameworks (many JavaScript MVC tools still fall under the category library).

Being a framework, it hosts your code and takes ownership of decisions about what to call and when!

AngularJS itself includes a jQuery-lite edition within it. So for some basic DOM selection/manipulation, you really don't have to include the jQuery library (it saves many bytes to run on the network.)

AngularJS has the concept of "Directives" for DOM manipulation and designing reusable UI components, so you should use it whenever you feel the need of doing DOM manipulation related stuff (directives are only place where you should write jQuery code while using AngularJS).

AngularJS involves some learning curve (more than jQuery :-).

-->For any developer coming from jQuery background, my first advice would be to "learn JavaScript as a first class language before jumping onto a rich framework like AngularJS!"
I learned the above fact the hard way.

Good luck.
AnswerKenntnis:
I have a shorter, focused answer.  

AngularJS changes the way you find elements.

In jQuery, you spend a lot of time finding elements, and then you wire them up.
$('#id .class').click(doStuff)

In AngularJS, you don't need to find elements.  Instead, you use directives to wire them all up.
<a ng-click="doStuff()">

In fact, the primary difference between jqLite and jQuery is that jqLite does not support selectors.  This is because Angular doesn't need (or want you) to use selectors.  You should only use directives!  

So, when people say "don't include jQuery at all", it's mainly because they don't want you to use selectors to find elements. They want you to learn to use directives.
AnswerKenntnis:
jQuery

jQuery makes ridiculously long JavaScript commands like getElementByHerpDerp shorter and cross-browser.

AngularJS

AngularJS allows you to make your own HTML tags/attributes that do things which work well with dynamic web applications (since HTML was designed for static pages).

Edit:

Saying "I have a jQuery background how do I think in AngularJS?" is like saying "I have an HTML background how do I think in JavaScript?" The fact that you're asking the question shows you most likely don't understand the fundamental purposes of these two resources. This is why I chose to answer the question by simply pointing out the fundamental difference rather than going through the list saying "AngularJS makes use of directivies whereas jQuery uses CSS selectors to make a jQuery object which does this and that etc....". This question does not require a lengthy answer.

jQuery is a way to make programming JavaScript in the browser easier. Shorter, cross-browser commands, etc.

AngularJS extends HTML, so you don't have to put <div> all over the place just to make an application. It makes HTML actually work for applications rather than what it was designed for, which is static, educational web pages. It accomplishes this in a roundabout way using JavaScript, but fundamentally it is an extension of HTML, not JavaScript.
AnswerKenntnis:
They're apples and oranges. You don't want to compare them. They're two different things. AngularJs has already jQuery lite built in which allows you to perform basic DOM manipulation without even including the full blown jQuery version. 

jQuery is all about DOM manipulation. It solves all the cross browser pain otherwise you will have to deal with but it's not a framework that allows you to divide your app into components like AngularJS. 

A nice thing about AngularJs is that it allows you to separate/isolate the DOM manipulation in the directives.  There are built-in directives ready for you to use such as ng-click. You can create your own custom directives that will contain all your view logic or DOM manipulation so you don't end up mingle DOM manipulation code in the controllers or services that should take care of the business logic.

Angular breaks down your app into 
- Controllers
- Services
- Views
- etc.

and there is one more thing, that's the directive. It's an attribute  you can attach to any DOM element and you can go nuts with jQuery within it without worrying about your jQuery ever conflicts with AngularJs components or messes up with its architecture. 

I heard from a meetup I attended, one of the founders of Angular said they worked really hard to separate out the DOM manipulation so do not try to include them back in.
AnswerKenntnis:
Listen to the podcast JavaScript Jabber: Episode #32 that features the original creators of AngularJS: Misko Hevery & Igor Minar. They talk a lot about what it's like to come to AngularJS from other JavaScript backgrounds, especially jQuery.

One of the points made in the podcast made a lot of things click for me with respects to your question:


  MISKO: [...] one of the things we thought about very hardly in Angular is, how do we provide lots of escape hatches so that you can get out and basically figure out a way out of this. So to us, the answer is this thing called ΓÇ£DirectivesΓÇ¥. And with directives, you essentially become a regular little jQuery JavaScript, you can do whatever you want.
  
  IGOR: So think of directive as the instruction to the compiler that tells it whenever you come across this certain element or this CSS in the template, and you keep this kind of code and that code is in charge of the element and everything below that element in the DOM tree.


A transcript of the entire episode is available at the link provided above.

So, to directly answer your question: AngularJS is -very- opinionated and is a true MV* framework. However, you can still do all of the really cool stuff you know and love with jQuery inside of directives. It's not a matter of "How do I do what I used to in jQuery?" as much as it's a matter of "How do I supplement AngularJS with all of the stuff I used to do in jQuery?"

It's really two very different states of mind.
AnswerKenntnis:
I find this question interesting, because my first serious exposure to JavaScript programming was Node.js and AngularJS. I never learned jQuery, and I guess that's a good thing, because I don't have to unlearn anything. In fact, I actively avoid jQuery solutions to my problems, and instead, solely look for an "AngularJS way" to solve them. So, I guess my answer to this question would essentially boil down to, "think like someone who never learned jQuery" and avoid any temptation to incorporate jQuery directly (obviously AngularJS uses it to some extent behind the scenes).
AnswerKenntnis:
Those are some very nice but lengthy answers.

To sum up my experiences:


controllers and providers (services, factories, etc) are for modifying the data model NOT HTML.
HTML and directives define the layout and binding to the model.
If you need to share data between controllers, create a service or factory - they are singletons that are shared across the application.
If you need a html widget, create a directive.
If you have some data and are now trying to update HTML... STOP! update the model, and make sure your HTML is bound to the model.
AnswerKenntnis:
Angular vs. JQuery

Angular and jQuery adopt very different ideologies. If you're coming from jQuery you may find some of the differences surprising. Angular may make you angry. 

This is normal, you should push through. Angular is worth it.

First up, Angular doesn't replace JQuery

Angular and JQuery do different things. Angular gives you a set of tools to produce webapps. JQuery mainly gives you tools for modifying the DOM. If jQuery is present on your page, Angular will use it automatically. If it isn't, Angular ships with jQuery Lite, which is a cut down, but still perfectly usable version of jQuery.

Misko likes JQuery and encourages you to use it. You still need to manipulate the DOM and jQuery is your tool for this. However you can get a most of your work done using templates, and you should prefer templates where possible.

People encouraging you to drop jQuery should stop encouraging you to do that. You might like to lay off the jQuery for a while while you learn what Angular can do, but jQuery is not going away just yet.

Unobtrusive JavaScript vs. Declarative Templates

JQuery is typically applied unobtrusively. Your JavaScript is linked in the header, and this is the only place it is mentioned. The JavaScript wraps round the DOM like a snail on a twig, making changes as required. Onclick attributes are very bad practice.

One of the first things your will notice about Angular is that custom attributes are everywhere. Your HTML will be littered with ng attributes, which are essentially onClick attributes on steroids. These are directives, and are one of the main ways in which the template is hooked to the model. 

When you first see this you might be tempted to write Angular off as old school intrusive JavaScript (like I did at first). In fact, Angular does not play by those rules. In Angular, your HTML5 is a template. It is compiled by Angular to produce your web page.

This is the first big difference. To jQuery, your web page is a DOM to be manipulated. To Angular, your HTML is code to be compiled. Angular reads in your whole web page and literally compiles it into a new web page using it's built in compiler.

Your template should be declarative; it's meaning should be clear simply by reading it. We use custom attributes with meaningful names. We make up new HTML elements, again with meaningful names. A designer with minimal HTML knowledge and no coding skill can read your Angular template and understand what it is doing. He or she can make modifications. This is the Angular way.

The template is in the driving seat.

One of the first questions I asked myself when starting Angular and running through the tutorials is "Where is my code?". I've written no JavaScript, and yet I have all this behaviour. The answer is obvious. Because Angular compliles the DOM, Angular is treating your HTML as code. For simple cases it's often sufficient to just write a template and let Angular compile it into an app for you.

Your template drives your app. You write angular components, and Angular will take care of pulling them in and making them available at the right time based on the structure of your web page. This is very different to a standard MVC pattern, where the template is just the output. 

It's more similar to XSLT than Rails for example.

Semantic HTML vs. Semantic Models

With JQuery your HTML page should contain semantic meaningful content. If the JavaScript is turned off (by a user or search engine) your content remains accessible.

Because Angular treats your HTML page as a template. The template is not supposed to be semantic as your content is typically stored in your model. Angular compiles your DOM with the model to produce a semantic web page.

In Angular, meaning lives in the model, the HTML is for display only. 

At this point you likely have all sorts of questions concerning SEO and accessibility, and rightly so. There are open issues here. Most screen readers will now parse JavaScript. Search engines may also be able to index AJAXed content.

Separation of concerns vs. MVC

Separation of concerns is a pattern that has grown up over many years of web development for a variety of reasons including SEO, accessibility and browser incompatibility. It looks like this:


HTML - Semantic meaning. The HTML should stand alone.
CSS - Styling, without the CSS the page is still readable.
JavaScript - Behaviour, without the script the content remains.


Again, Angular does not play by their rules. Angular instead implements an MVC pattern.


Model - your models contains your semantic data. Models are usually JSON objects.
View - Your views are written in HTML. The view is usually not semantic because your data lives in the model.
Controller - Your controller is a JavaScript function which hooks the view to the model. Depending on your app, you may or may not need to create a controller. You can have many controllers on a page.


They are not on opposite ends of the same scale, they are on completely different axes.

Plugins vs. Directives

Plugins extend jQuery. Angular Directives extend the capabilities of your browser.

In jQuery we define plugins by adding functions to the jQuery.prototype. We then hook these into the DOM by selecting elements and calling the plugin on the result. The idea is to extend the capabilities of JQuery.

For example, if you want a carousel on your page, you might define an unordered list of figures, perhaps wrapped in a nav element. You might then write some jQuery to find the list on the page and restyle it as a gallery with timeouts to do the sliding animation.

In Angular, we define directives. A directive is a function which returns a JSON object. This object tells Angular what DOM elements to look for, and what changes to make to them. Directives are hooked in to the template using either attributes or elements, which you invent. The idea is to extend the capabilities of HTML with new attributes and elements.

The Angular way is to extend the capabilities of native looking HTML. You should write HTML that looks like HTML, extended with custom attributes and elements. 

If you want a carousel, just use a <carousel /> element, then define a directive to pull in a template, and make that sucker work.

Closure vs. $scope

JQuery plugins are created in a closure. Privacy is maintained within that closure. It's up to you to maintain your scope chain within that closure. You only really have access to the set of DOM nodes passed in to the plugin by jQuery, plus any local variables defined in the closure and any globals you have defined. This means that plugins are quite self contained. This is a good thing, but can get restrictive when creating a whole application. Trying to pass data between sections of a dynamic page becomes a chore.

Angular has $scope objects. These are special objects created and maintained by Angular in which you store your model. Certain directives will spawn a new $scope, which by default inherits from it's wrapping $scope using JavaScript prototypical inheritance. The $scope object is accessible in the controller and the view.

This is the clever part. Because the structure of $scope inheritance roughly follows the structure of the DOM, elements have access to their own scope, and any containing scopes seamlessly, all the way up to the global $scope (which is not the same as the global scope).

This makes it much easier to pass data around, and to store data at an appropriate level. If a dropdown is unfolded, only the dropdown $scope needs to know about it. If the user updates their preferences, you might want to update the global $scope, and any nested scopes listening to the user preferences would automatically be alerted.

When you apply a directive to a section of a page, you automatically (optionally) get a $scope object to act as the View-model for that section of the page.

Manual DOM changes vs. Data Binding

In jQuery you make all your DOM changes by hand. You construct new DOM elements programatically. If you have a JSON array and you want to put it to the DOM, you must write a function to generate the HTML and insert it.

In Angular you can do this too, but you are encouraged to make use of data binding. Change your model, and because the DOM is bound to it via a template your DOM will automatically update, no intervention required.

AJAX all of the time

In jQuery making an AJAX call is fairly simple, but it's still something you might think twice about. There's the added complexity to think about, and a fair chunk of script to maintain.

In Angular, AJAX is your default go-to solution and it happens all the time, almost without you noticing. You can include templates with ng-include. You can apply a template with the simplest custom directive. You can wrap an AJAX call in a service and create yourself a GitHub service, or a Flickr service, which you can access with astonishing ease.

Dependency injection (DI)

DI is a massive deal in Angular. It means that Angular will automatically instantiate your objects for you and ensure they are available for you where you need them. Until you start to use this, it's hard to explain just what a massive time saver this is. Nothing like Angular DI exists inside jQuery.

Modular service architecture

JQuery says very little about how you should organise your code. Angular has opinions. 

Angular gives you modules into which you can place your code. If you're writing a script that talks to Flickr for example, you might want to create a Flickr module to wrap all your Flickr related functions in. Modules can include other modules (DI). Your main application is usually a module, and this should include all the other modules your application will depend on.

You get simple code reuse, if you want to write another application based on Flickr, you can just include the Flickr module and voila, you have access to all your Flickr related functions in your new app.

Services are another way in which you can break down your code. services are singleton objects, meaning there is only one of each type of service. A service is an object that contains data and methods. Say you want your app to have a shopping cart. You can create a shopping cart service with functions to add items, remove items, an array of contents, and a function to calculate the total. 

You can inject that service into your application wherever you need it. If a controller needs to add items to the basket, just use DI to provide the basket to the controller. Angular will take care of the details, all you need to do is refer to the basket service by name when you create your controller. It's stupidly easy.

To sum up

Angular and jQuery are not enemies. It's possible to use jQuery within Angular very nicely. If you're using Angular well (templates, data-binding, $scope, directives, etc) you will find you need a lot less jQuery than you might otherwise require.

Think less about unobtrusive JavaScript, and instead think in terms of html extension.

My little book

I got so excited about Angular, I wrote a short book on it which you're very welcome to read online http://nicholasjohnson.com/angular-book/
QuestionKenntnis:
How do JavaScript closures work?
qn_description:
How would you explain closures to someone with a knowledge of the concepts which make up closures (e.g. functions, variables and the like), but does not understand closures themselves?

I have seen the Scheme example given in Stack Overflow, and it did not help.

Articles linked in post


Wikipedia Closures
Closures and its effects in loops
What is closures
MDN Closures
Understand Closures
Leak free Closures
AnswersKenntnis
AnswerKenntnis:
Whenever you see the function keyword within another function, the inner function has access to variables in the outer function.

function foo(x) {
  var tmp = 3;
  function bar(y) {
    alert(x + y + (++tmp)); // will alert 16
  }
  bar(10);
}
foo(2);


This will always alert 16, because bar can access the x which was defined as an argument to foo, and it can also access tmp from foo.

That is a closure. A function doesn't have to return in order to be called a closure. Simply accessing variables outside of your immediate lexical scope creates a closure.

function foo(x) {
  var tmp = 3;
  return function (y) {
    alert(x + y + (++tmp)); // will also alert 16
  }
}
var bar = foo(2); // bar is now a closure.
bar(10);


The above function will also alert 16, because bar can still refer to x and tmp, even though it is no longer directly inside the scope.

However, since tmp is still hanging around inside bar's closure, it is also being incremented. It will be incremented each time you call bar.

The simplest example of a closure is this:

var a = 10;
function test() {
  console.log(a); // will output 10
  console.log(b); // will output 6
}
var b = 6;
test();


When a JavaScript function is invoked, a new execution context is created. Together with the function arguments and the parent object, this execution context also receives all the variables declared outside of it (in the above example, both 'a' and 'b').

It is possible to create more than one closure function, either by returning a list of them or by setting them to global variables. All of these will refer to the same x and the same tmp, they don't make their own copies.

Here the number x is a literal number. As with other literals in JavaScript, when foo is called, the number x is copied into foo as its argument x.

On the other hand, JavaScript always uses references when dealing with Objects. If say, you called foo with an Object, the closure it returns will reference that original Object!

function foo(x) {
  var tmp = 3;
  return function (y) {
    alert(x + y + tmp);
    x.memb = x.memb ? x.memb + 1 : 1;
    alert(x.memb);
  }
}
var age = new Number(2);
var bar = foo(age); // bar is now a closure referencing age.
bar(10);


As expected, each call to bar(10) will increment x.memb. What might not be expected, is that x is simply referring to the same object as the age variable! After a couple of calls to bar, age.memb will be 2! This referencing is the basis for memory leaks with HTML objects.
AnswerKenntnis:
JavaScript Closures For Dummies (mirror) is the article that finally got me to understand closures. The explanation posted there is much better than anything I could write here.



For archiving purposes, I (flying sheep) will put the article from the link below. The article was created by Morris and put under the Creative Commons Attribution / Share alike license, so IΓÇÖll recreate it as close to the original as possible.

JavaScript Closures for Dummies

Submitted by Morris on Tue, 2006-02-21 10:19.

Closures Are Not Magic

This page explains closures so that a programmer can understand them — using working JavaScript code. It is not for gurus or functional programmers.

Closures are not hard to understand once the core concept is grokked. However, they are impossible to understand by reading any academic papers or academically oriented information about them!

This article is intended for programmers with some programming experience in a mainstream language, and who can read the following JavaScript function:

function sayHello(name) {
    var text = 'Hello ' + name;
    var sayAlert = function() { alert(text); }
    sayAlert();
}


An Example of a Closure

Two one sentence summaries:


a closure is the local variables for a function — kept alive after the function has returned, or
a closure is a stack-frame which is not deallocated when the function returns (as if a 'stack-frame' were malloc'ed instead of being on the stack!).


The following code returns a reference to a function:

function sayHello2(name) {
    var text = 'Hello ' + name; // Local variable
    var sayAlert = function() { alert(text); }
    return sayAlert;
}
say2 = sayHello2('Bob');
say2(); // alerts "Hello Bob"


Most JavaScript programmers will understand how a reference to a function is returned to a variable in the above code. If you don't, then you need to before you can learn closures. A C programmer would think of the function as returning a pointer to a function, and that the variables sayAlert and say2 were each a pointer to a function.

There is a critical difference between a C pointer to a function and a JavaScript reference to a function. In JavaScript, you can think of a function reference variable as having both a pointer to a function as well as a hidden pointer to a closure.

The above code has a closure because the anonymous function function() { alert(text); } is declared inside another function, sayHello2() in this example. In JavaScript, if you use the function keyword inside another function, you are creating a closure.

In C, and most other common languages after a function returns, all the local variables are no longer accessible because the stack-frame is destroyed.

In JavaScript, if you declare a function within another function, then the local variables can remain accessible after returning from the function you called. This is demonstrated above, because we call the function say2() after we have returned from sayHello2(). Notice that the code that we call references the variable text, which was a local variable of the function sayHello2().

function() { alert(text); } // Output of say2.toString();


Click the button above to get JavaScript to print out the code for the anonymous function. You can see that the code refers to the variable text. The anonymous function can reference text which holds the value 'Bob' because the local variables of sayHello2() are kept in a closure.

The magic is that in JavaScript a function reference also has a secret reference to the closure it was created in — similar to how delegates are a method pointer plus a secret reference to an object.

More examples

For some reason, closures seem really hard to understand when you read about them, but when you see some examples you can click to how they work (it took me a while).
I recommend working through the examples carefully until you understand how they work. If you start using closures without fully understanding how they work, you would soon create some very weird bugs!

Example 3

This example shows that the local variables are not copied — they are kept by reference. It is kind of like keeping a stack-frame in memory when the outer function exits!

function say667() {
    // Local variable that ends up within closure
    var num = 666;
    var sayAlert = function() { alert(num); }
    num++;
    return sayAlert;
}
var sayNumber = say667();
sayNumber(); // alerts 667


Example 4

All three global functions have a common reference to the same closure because they are all declared within a single call to setupSomeGlobals().

function setupSomeGlobals() {
    // Local variable that ends up within closure
    var num = 666;
    // Store some references to functions as global variables
    gAlertNumber = function() { alert(num); }
    gIncreaseNumber = function() { num++; }
    gSetNumber = function(x) { num = x; }
}

setupSomeGlobals();
gIncreaseNumber();
gAlertNumber(); // 667
gSetNumber(5);
gAlertNumber(); // 5

var oldAlert = gAlertNumber;

setupSomeGlobals();
gAlertNumber(); // 666

oldAlert() // 5


The three functions have shared access to the same closure — the local variables of setupSomeGlobals() when the three functions were defined.

Note that in the above example, if you call setupSomeGlobals() again, then a new closure (stack-frame!) is created. The old gAlertNumber, gIncreaseNumber, gSetNumber variables are overwritten with new functions that have the new closure. (In JavaScript, whenever you declare a function inside another function, the inside function(s) is/are recreated again each time the outside function is called.)

Example 5

This one is a real gotcha for many people, so you need to understand it. Be very careful if you are defining a function within a loop: the local variables from the closure do not act as you might first think.

function buildList(list) {
    var result = [];
    for (var i = 0; i < list.length; i++) {
        var item = 'item' + list[i];
        result.push( function() {alert(item + ' ' + list[i])} );
    }
    return result;
}

function testList() {
    var fnlist = buildList([1,2,3]);
    // Using j only to help prevent confusion -- could use i.
    for (var j = 0; j < fnlist.length; j++) {
        fnlist[j]();
    }
}


The line result.push( function() {alert(item + ' ' + list[i])} adds a reference to an anonymous function three times to the result array. If you are not so familiar with anonymous functions think of it like:

pointer = function() {alert(item + ' ' + list[i])};
result.push(pointer);


Note that when you run the example, "item3 undefined" is alerted three times! This is because just like previous examples, there is only one closure for the local variables for buildList. When the anonymous functions are called on the line fnlist[j](); they all use the same single closure, and they use the current value for i and item within that one closure (where i has a value of 3 because the loop had completed, and item has a value of 'item3').

Example 6

This example shows that the closure contains any local variables that were declared inside the outer function before it exited. Note that the variable alice is actually declared after the anonymous function. The anonymous function is declared first; and when that function is called it can access the alice variable because alice is in the closure.
Also sayAlice()() just directly calls the function reference returned from sayAlice() — it is exactly the same as what was done previously, but without the temporary variable.

function sayAlice() {
    var sayAlert = function() { alert(alice); }
    // Local variable that ends up within closure
    var alice = 'Hello Alice';
    return sayAlert;
}
sayAlice()();


Tricky: note also that the sayAlert variable is also inside the closure, and could be accessed by any other function that might be declared within sayAlice(), or it could be accessed recursively within the inside function.

Example 7

This final example shows that each call creates a separate closure for the local variables. There is not a single closure per function declaration. There is a closure for each call to a function.

function newClosure(someNum, someRef) {
    // Local variables that end up within closure
    var num = someNum;
    var anArray = [1,2,3];
    var ref = someRef;
    return function(x) {
        num += x;
        anArray.push(num);
        alert('num: ' + num +
            '\nanArray ' + anArray.toString() +
            '\nref.someVar ' + ref.someVar);
      }
}
obj = {someVar: 4};
fn1 = newClosure(4, obj);
fn2 = newClosure(5, obj);
fn1(1); // num: 5; anArray: 1,2,3,5; ref.someVar: 4;
fn2(1); // num: 6; anArray: 1,2,3,6; ref.someVar: 4;
obj.someVar++;
fn1(2); // num: 7; anArray: 1,2,3,5,7; ref.someVar: 5;
fn2(2); // num: 8; anArray: 1,2,3,6,8; ref.someVar: 5;


Summary

If everything seems completely unclear then the best thing to do is to play with the examples. Reading an explanation is much harder than understanding examples.
My explanations of closures and stack-frames, etc. are not technically correct — they are gross simplifications intended to help understanding. Once the basic idea is grokked, you can pick up the details later.

Final points:


Whenever you use function inside another function, a closure is used.
Whenever you use eval() inside a function, a closure is used. The text you eval can reference local variables of the function, and within eval you can even create new local variables by using eval('var foo = ΓÇª')
When you use new Function(ΓÇª) (the Function constructor) inside a function, it does not create a closure. (The new function cannot reference the local variables of the outer function.)
A closure in JavaScript is like keeping a copy of all the local variables, just as they were when a function exited.
It is probably best to think that a closure is always created just on entry to a function, and the local variables are added to that closure.
A new set of local variables is kept every time a function with a closure is called (given that the function contains a function declaration inside it, and a reference to that inside function is either returned or an external reference is kept for it in some way).
Two functions might look like they have the same source text, but have completely different behaviour because of their 'hidden' closure. I don't think JavaScript code can actually find out if a function reference has a closure or not.
If you are trying to do any dynamic source code modifications (for example: myFunction = Function(myFunction.toString().replace(/Hello/,'Hola'));), it won't work if myFunction is a closure (of course, you would never even think of doing source code string substitution at runtime, but...).
It is possible to get function declarations within function declarations within functions — and you can get closures at more than one level.
I think normally a closure is the term for both the function along with the variables that are captured. Note that I do not use that definition in this article!
I suspect that closures in JavaScript differ from those normally found in functional languages.


Links


TrimBreakpoint is a tricky use of closures to let you inspect local variables for a function from a popup breakpoint window.
Douglas Crockford's simulated private attributes and private methods for an object, using closures.
A great explanation of how closures can cause memory leaks in IE if you are not careful.


Thanks

If you have just learnt closures (here or elsewhere!), then I am interested in any feedback from you about any changes you might suggest that could make this article clearer. Send an email to morrisjohns.com (morris_closure @). Please note that I am not a guru on JavaScript — nor on closures.

Thanks for reading.
AnswerKenntnis:
I'm a big fan of analogy and metaphor when explaining difficult concepts, so let me try my hand with a story.

Once upon a time:

There was a princess...

function princess() {


She lived in a wonderful world full of adventures. She met her Prince Charming, rode around her world on a unicorn, battled dragons, encountered talking animals, and many other fantastical things.

    var adventures = [];

    function princeCharming() { /* ... */ }

    var unicorn = { /* ... */ },
        dragons = [ /* ... */ ],
        squirrel = "Hello!";


But she would always have to return back to her dull world of chores and grown-ups.

    return {


And she would often tell them of her latest amazing adventure as a princess.

        story: function() {
            return adventures[adventures.length - 1];
        }
    };
}


But all they would see is a little girl...

var littleGirl = princess();


...telling stories about magic and fantasy.

littleGirl.story();


And even though the grown-ups knew of real princesses, they would never believe in the unicorns or dragons because they could never see them. The grown-ups said that they only existed inside the little girl's imagination.

But we know the real truth; that the little girl with the princess inside...

...is really a princess with a little girl inside.
AnswerKenntnis:
Taking the question seriously, we should find out what a typical 6-year-old is capable of cognitively, though admittedly, one who is interested in JavaScript is not so typical.  

On  Childhood Development: 5 to 7 Years  it says:


  Your child will be able to follow two-step directions. For example, if you say to your child, "Go to the kitchen and get me a trash bag" they will be able to remember that direction.


We can use this example to explain closures, as follows:


  The kitchen is a closure that has a local variable, called trashBags.  There is a function inside the kitchen called getTrashBag that gets one trash bag and returns it.


We can code this in JavaScript like this:

function makeKitchen () {
  var trashBags = ['A', 'B', 'C']; // only 3 at first

  return {
    getTrashBag: function() {
      return trashBags.pop();
    }
  };
}

var kitchen = makeKitchen();

kitchen.getTrashBag(); // returns trash bag C
kitchen.getTrashBag(); // returns trash bag B
kitchen.getTrashBag(); // returns trash bag A


Further points that explain why closures are interesting:


Each time makeKitchen() is called, a new closure is created with its own separate trashBags.
The trashBags variable is local to the inside of each kitchen and is not accessible outside, but the inner function on the getTrashBag property does have access to it.  
Every function call creates a closure, but there would be no need to keep the closure around unless an inner function, which has access to the inside of the closure, can be called from outside the closure.  Returning the object with the getTrashBag function does that here.
AnswerKenntnis:
Closures are hard to explain because they are used to make some behaviour work that everybody intuitively expects to work anyway. I find the best way to explain them (and the way that I learned what they do) is to imagine the situation without them:

var bind = function(x) {
    return function(y) { return x + y; };
}

var plus5 = bind(5);
alert(plus5(3));


What would happen here if JavaScript didn't know closures? Just replace the call in the last line by its method body (which is basically what function calls do) and you get:

alert(x + 3);


Now, where's the definition of x? We didn't define it in the current scope. The only solution is to let plus5 carry its scope (or rather, its parent's scope) around. This way, x is well-defined and it is bound to the value 5.
AnswerKenntnis:
This is an attempt to clear up several (possible) misunderstandings about closures that appear in some of the other answers.


A closure is not only created when you return an inner function.  In fact, the enclosing function does not need to return at all in order for its closure to be created.  You might instead assign your inner function to a variable in an outer scope, or pass it as an argument to another function where it could be called immediately or any time later.  Therefore, the closure of the enclosing function is probably created as soon as the enclosing function is called since any inner function has access to that closure whenever the inner function is called, before or after the enclosing function returns. 
A closure does not reference a copy of the old values of variables in its scope.  The variables themselves are part of the closure, and so the value seen when accessing one of those variables is the latest value at the time it is accessed.  This is why inner functions created inside of loops can be tricky, since each one has access to the same outer variables rather than grabbing a copy of the variables at the time the function is created or called.
The "variables" in a closure include any named functions declared within the function.  They also include arguments of the function.  A closure also has access to its containing closure's variables, all the way up to the global scope.
Closures use memory, but they don't cause memory leaks since JavaScript by itself cleans up its own circular structures that are not referenced.  IE memory leaks involving closures are created when it fails to disconnect DOM attribute values that reference closures, thus maintaining references to possibly circular structures.
AnswerKenntnis:
A closure is much like an object. It gets instantiated whenever you call a function. 

The scope of a closure in JavaScript is lexical, which means that everything that is contained within the function the closure belongs to, has access to any variable that is in it.

A variable is contained in the closure if you


assign it with var foo=1; or
just write var foo;


If an inner function (a function contained inside another function) accesses such a variable without defining it in its own scope with var, it modifies the content of the variable in the outer closure.

A closure outlives the runtime of the function that spawned it. If other functions make it out of the closure/scope in which they are defined (for instance as return values), those will continue to reference that closure.

Example

function example(closure){
    // define somevariable to live in the closure of example
    var somevariable='unchanged';

    return {
        change_to:function(value){
            somevariable = value;
        },
        log:function(value){
            console.log('somevariable of closure %s is: %s',
                closure, somevariable);
        }
    }
}

closure_one = example('one');
closure_two = example('two');

closure_one.log();
closure_two.log();
closure_one.change_to('some new value');
closure_one.log();
closure_two.log();


Output

somevariable of closure one is: unchanged
somevariable of closure two is: unchanged
somevariable of closure one is: some new value
somevariable of closure two is: unchanged
AnswerKenntnis:
The Straw Man


    I need to know how many times a button has been clicked, and do something on every third click...


    Fairly Obvious Solution

// declare counter outside event handler's scope
var counter = 0;
var element = document.getElementById('button');

element.onclick = function() {
    // increment outside counter
    counter++;

    if (counter === 3) {
        // do something every third time
        alert("Third time's the charm!");
        // reset counter
        counter = 0;
    }
};



    Now this will work but it does encroach into the outer scope by adding a variable, whose sole purpose is to keep track of the count.  In some situations this would be preferable as your outer application might need access to this information.  But in this case we are only changing every third click's behavior, so it is preferable to enclose this functionality inside the event handler.


    Consider this option

var element = document.getElementById('button');

element.onclick = (function() {
    // init the count to 0
    var count = 0;

    return function(e) {  // <- This function becomes the onclick handler
        count++;          //    and will retain access to the above `count`

        if (count === 3) {
            // do something every third time
            alert("Third time's the charm!");
            //reset counter
            count = 0;
        }
    };
})();



    Notice a few things here


    In the above example I am using the closure behavior of JavaScript.  This behavior allows any function to have access to the scope in which it was created, indefinitely.  To practically apply this, I immediately invoke a function that returns another function, and because the function I'm returning has access to the internal count variable (because of the closure behavior explained above) this results in a private scope for usage by the resulting function... Not so simple?  Lets dilute it down...


    A simple one line closure

//         ____________Immediately executed (self invoked)___________________
//         |                                                                |
//         |      Scope Retained for use        __Returned as the______     |
//         |     only by returned function     |  value of func       |     |
//         |             |            |        |                      |     |
//         v             v            v        v                      v     v
var func = (function() { var a = 'val'; return function() { alert(a); }; })();



    all variables outside the returned function are available to the returned function, but are not directly available to the returned function object....

func();  // alerts "value"
func.a;  // undefined



    Get it? so in our primary example, the count variable is contianed within the closure and always available to the event handler, so it retains its state from click to click.

Also this private variable state is fully accessible, for both reading and assigning to it's private scoped variables.


    There you go, you're now fully encapsulating this behavior

Full Blog Post (including jQuery considerations)
AnswerKenntnis:
Ok, 6-year old closures fan. Do you want to hear the simplest example of closure?

Let's imagine the next situation: driver is sitting in a car. That car is inside a plane. Plane is in the airport. The ability of driver to access things outside his car, but inside the plane, even if that plane leaves an airport, is a closure. That's it. When you turn 27, look at the more detailed explanation or at the example below. Have a nice day!

UPDATE 

Here is how I can convert my plane story into the code.

    var plane = function (defaultAirport) {

    var lastAirportLeft = defaultAirport;

    var car = {
        driver: {
            startAccessPlaneInfo: function () {
                setInterval(function () {
                    console.log("Last airport was " + lastAirportLeft);
                }, 2000);
            }
        }
    };
    car.driver.startAccessPlaneInfo();

    return {
        leaveTheAirport: function (airPortName) {
            lastAirportLeft = airPortName;
        }
    }
}("Boryspil International Airport");

plane.leaveTheAirport("John F. Kennedy");
AnswerKenntnis:
I wrote a blog post a while back explaining closures. Here's what I said about closures in terms of why you'd want one.


  Closures are a way to let a function
  have persistent, private variables -
  that is, variables that only one
  function knows about, where it can
  keep track of info from previous times
  that it was run.


In that sense, they let a function act a bit like an object with private attributes.

Full post:

So what are these closure thingys?
AnswerKenntnis:
Closures are simple:

The following simple example covers all the main points of JavaScript closures.*
 

Here is a factory that produces calculators that can add and multiply:

function make_calculator() {
    var n = 0;  // this calculator stores a single number n
    return {
                  add : function (a) { n += a; return n; },
             multiply : function (a) { n *= a; return n; }
           };
}

first_calculator = make_calculator();
second_calculator = make_calculator();

first_calculator.add(3);                   // returns 3
second_calculator.add(400);                // returns 400

first_calculator.multiply(11);             // returns 33
second_calculator.multiply(10);            // returns 4000


The key point: Each call to make_calculator creates a new local variable n, which continues to be usable by that calculator's add and multiply functions long after make_calculator returns.

If you are familiar with stack frames, these calculators seem strange: How can they keep accessing n after make_calculator returns?  The answer is to imagine that JavaScript doesn't use "stack frames", but instead uses "heap frames", which can persist after the function call that made them returns.

Inner functions like add and multiply, which access variables declared in an outer function**, are called closures.

That is pretty much all there is to closures.





* For example, it covers all the points in the "Closures for Dummies" article given in another answer, except example 6, which simply shows that variables can be used before they are declared, a nice fact to know but completely unrelated to closures. It also covers all the points in the accepted answer, except for the points (1) that functions copy their arguments into local variables (the named function arguments), and (2) that copying numbers creates a new number, but copying an object reference gives you another reference to the same object. These are also good to know but again completely unrelated to closures. It is also very similar to the example in this answer but a bit shorter and less abstract. It does not cover the point of this answer or this comment, which is that JavaScript makes it difficult to plug the current value of a loop variable into your inner function: The "plugging in" step can only be done with a helper function that encloses your inner function and is invoked on each loop iteration. (Strictly speaking, the inner function accesses the helper function's copy of the variable, rather than having anything plugged in.) Again, very useful when creating closures, but not part of what a closure is or how it works. There is additional confusion due to closures working differently in functional languages like ML, where variables are bound to values rather than to storage space, providing a constant stream of people who understand closures in a way (namely the "plugging in" way) that is simply incorrect for JavaScript, where variables are always bound to storage space, and never to values. 

** Any outer function, if several are nested, or even in the global context, as this answer points out clearly.
AnswerKenntnis:
Can you explain closures to a 5 year old?*

I still think Google's explanation works very well and is concise:

/*
*    When a function is defined in another function and it
*    has access to the outer function's context even after
*    the outer function returns.
*
* An important concept to learn in JavaScript.
*/

function outerFunction(someNum) {
    var someString = 'Hey!';
    var content = document.getElementById('content');
    function innerFunction() {
        content.innerHTML = someNum + ': ' + someString;
        content = null; // Internet Explorer memory leak for DOM reference
    }
    innerFunction();
}

outerFunction(1);ΓÇï


*A C# question
AnswerKenntnis:
Wikipedia on closures:


  In computer science, a closure is a function together with a referencing environment for the nonlocal names (free variables) of that function.


Technically, in JavaScript, every function is a closure. It always has an access to variables defined in the surrounding scope.

Since scope-defining construction in JavaScript is a function, not a code block like in many other languages, what we usually mean by closure in JavaScript is a function working with nonlocal variables defined in already executed surrounding function.

Closures are often used for creating functions with some hidden private data (but it's not always the case).

var db = (function() {
    // Create a hidden object, which will hold the data
    // it's inaccessible from the outside.
    var data = {};

    // Make a function, which will provide some access to the data.
    return function(key, val) {
        if (val === undefined) { return data[key] } // Get
        else { return data[key] = val } // Set
    }
    // We are calling the anonymous surrounding function,
    // returning the above inner function, which is a closure.
})();

db('x')    // -> undefined
db('x', 1) // Set x to 1
db('x')    // -> 1
// It's impossible to access the data object itself.
// We are able to get or set individual it.


ems

The example above is using an anonymous function, which was executed once. But it does not have to be. It can be named (e.g. mkdb) and executed later, generating a database function each time it is invoked. Every generated function will have its own hidden database object. Another usage example of closures is when we don't return a function, but an object containing multiple functions for different purposes, each of those function having access to the same data.
AnswerKenntnis:
I put together an interactive JavaScript tutorial to explain how closures work.
What's a Closure?

Here's one of the examples:

var create = function (x) {
    var f = function () {
        return x; // we can refer to x here!
    };
    return f;
};
// create takes one argument, creates a function

var g = create(42);
// g is a function that takes no arguments now

var y = g();
// y is 42 here
AnswerKenntnis:
You're having a sleep over and you invite Dan.
You tell Dan to bring one XBox controller.

Dan invites Paul.
Dan asks Paul to bring one controller. How many controllers were brought to the party?

function sleepOver(howManyControllersToBring) {

    var numberOfDansControllers = howManyControllersToBring;

    return function danInvitedPaul(numberOfPaulsControllers) {
        var totalControllers = numberOfDansControllers + numberOfPaulsControllers;
        return totalControllers;
    }
}

var howManyControllersToBring = 1;

var inviteDan = sleepOver(howManyControllersToBring);

// The only reason Paul was invited is because Dan was invited. 
// So we set Paul's invitation = Dan's invitation.

var danInvitedPaul = inviteDan(howManyControllersToBring);

alert("There were " + danInvitedPaul + " controllers brought to the party.");
AnswerKenntnis:
Example for the first point by dlaliberte:


  A closure is not only created when you
  return an inner function. In fact,
  the enclosing function does not need
  to return at all. You might instead
  assign your inner function to a
  variable in an outer scope, or pass it
  as an argument to another function
  where it could be used immediately.
  Therefore, the closure of the
  enclosing function probably already
  exists at the time that enclosing
  function was called since any inner
  function has access to it as soon as
  it is called.


var i;
function foo(x) {
    var tmp = 3;
    i = function (y) {
        alert(x + y + (++tmp));
    }
}
foo(2);
i(3);
AnswerKenntnis:
How I'd explain it to a six-year old:

You know how grown-ups can own a house, and they call it home? When a mom has a child, the child doesn't really own anything, right? But it's parents own a house, so whenever someone asks the child "Where's your home?", he/she can answer "that house!", and point to the house of it's parents. A "Closure" is the ability of the child to always (even if abroad) be able to say it has a home, even though it's really the parent's who own the house. :-)
AnswerKenntnis:
A closure is where an inner function has access to variables in its outer function. That's probably the simplest one-line explanation you can get for closures.
AnswerKenntnis:
I tend to learn better by GOOD/BAD comparisons. I like to see working code followed by non-working code that someone is likely to encounter. I put together a jsFiddle that does a comparison and tries to boil down the differences to the simplest explanations I could come up with.

Closures done right:

console.log('CLOSURES DONE RIGHT');
var arr = [];
function createClosure(n) {
    return function () {
        return 'n = ' + n;
    }
}
for (var index = 0; index < 10; index++) {
    arr[index] = createClosure(index);
}
for (var index in arr) {
    console.log(arr[index]());
}



In the above code createClosure(n) is invoked in every iteration of the loop.
This creates a new scope and n is bound to that scope; this means we have 10 separate scopes, one for each iteration.
createClosure(n) returns a function that returns the n within that scope.
Within each scope n is bound to whatever value it had when createClosure(n) was invoked so the nested function that gets returned will always return the value of n that it had when createClosure(n) was invoked.


Closures done wrong:

console.log('CLOSURES DONE WRONG');
function createClosureArray() {
    var badArr = [];
    for (var index = 0; index < 10; index++) {
        badArr[index] = function () {
            return 'n = ' + index;
        };
    }
    return badArr;
}
var badArr = createClosureArray();
for (var index in badArr) {
    console.log(badArr[index]());
}



In the above code the loop was moved within the createClosureArray() function and the function now just returns the completed array, which at first glance seems more intuitive.
What might not be obvious is that since createClosureArray() is only invoked once only one scope is created for this function instead of one for every iteration of the loop.
Within this function a variable named index is defined. The loop runs and adds functions to the array that return index.
Because there was only one scope within the createClosureArray() function, the index variable is only bound to a value within that scope. In other words, each time the loop changes the value of index, it changes it for everything that references it within that scope.
All of the functions added to the array return the SAME index variable instead of 10 different ones from 10 different scopes. The end result is that all 10 functions return the same variable from the same scope.
After the loop finished and index was done being modified the end value was 10, therefore every function added to the array returns the value of the single index variable which is now set to 10.


Result
AnswerKenntnis:
I know there are plenty of solutions already, but I guess that this small and simple script can be useful to demonstrate the concept:

// makeSequencer will return a "sequencer" function
var makeSequencer = function() {
    var _count = 0; // not accessible outside this function
    var sequencer = function () {
        return _count++;
    }
    return sequencer;
}

var fnext = makeSequencer();
var v0 = fnext();     // v0 = 0;
var v1 = fnext();     // v1 = 1;
var vz = fnext._count // vz = undefined
AnswerKenntnis:
JavaScript functions can access their:


arguments
locals (i.e., their local variables and local functions)
environment, which includes:

globals, including the DOM
anything in outer functions



If a function accesses its environment, then the function is a closure.

Note that outer functions are not required, though they do offer benefits I don't discuss here. By accessing data in its environment, a closure keeps that data alive. In the subcase of outer/inner functions, an outer function can create local data and eventually exit, and yet, if any inner function(s) survive after the outer function exits, then the inner function(s) keep the outer function's local data alive.

Example of a closure that uses the global environment:

Imagine that the StackOverflow Vote-Up and Vote-Down button events are implemented as closures, voteUp_click and voteDown_click, that have access to external variables isVotedUp and isVotedDown, which are defined globally. (For simplicity's sake, I am referring to StackOverflow's Question Vote buttons, not the array of Answer Vote buttons.) When the user clicks the VoteUp button, the voteUp_click function checks whether isVotedDown == true to determine whether to vote up or merely cancel a down vote. Function voteUp_click is a closure because it is accessing its environment.

var isVotedUp = false;
var isVotedDown = false;

function voteUp_click()
{
  if (isVotedUp)
    return;
  else if (isVotedDown)
    SetDownVote(false);
  else
    SetUpVote(true);
}

function voteDown_click()
{
  if (isVotedDown)
    return;
  else if (isVotedUp)
    SetUpVote(false);
  else
    SetDownVote(true);
}

function SetUpVote(status)
{
  isVotedUp = status;
  // do some css stuff to Vote-Up button
}

function SetDownVote(status)
{
  isVotedDown = status;
  // do some css stuff to Vote-Down button
}


All four of these functions are closures as they all access their environment.
AnswerKenntnis:
Okay, talking with 6-year old child, I would possibly use following associations.


  Imaging - you are playing with your little brothers and sisters in entire house, you are moving around with your toys and brought some of them into your older brother's room. After a while your brother returned from the school and went to his room, and he locked inside it, so now you could not access toys left there anymore in a direct way. But you could knock the door and ask your brother for that toys, this is called toy's  closure, your brother made it up for you and he is now into outer scope.


Compare with situation when door was locked by draft and nobody inside (general function execution), and than some local fire occur and burnt down the room (garbage collector:D), and than new room was build and now you may leave another toys there (new function instance) but never get the same toys which were left in first room instance.

For advanced child I would put something like this, it is not perfect but makes you feel about what's it:

function playingInBrothersRoom (withToys) {
  // we closure toys which we played in brother's room, when he come back and lock the door
  // your brother suppose to be into outer [[scope]] object now, thanks god you could communicate with him
  var closureToys = withToys || [],
      returnToy, countIt, toy; // just another closure helpers, for brother's inner use

  var brotherGivesToyBack = function (toy) {
    // new request, there is not yet closureToys on brother's hand yet, give him a time
    returnToy = null;
    if (toy && closureToys.length > 0) { // if we ask for specefic toy brother going search it

      for ( countIt = closureToys.length; countIt; countIt--) {
        if (closureToys[countIt - 1] == toy) {
          returnToy = 'Take your ' + closureToys.splice(countIt - 1, 1) + ', little boy!';
          break;
        }
      }
      returnToy = returnToy || 'Hey, I could not find any ' + toy + ' here, look for another rooms.';
    }
    else if (closureToys.length > 0) { //  otherwise just give back everything he has in the room
      returnToy = 'Behold! ' + closureToys.join(', ') + '.';
      closureToys = [];
    }
    else {
      returnToy = 'Hey, lil shrimp, I gave you everything!';
    }
    console.log(returnToy);
  }
  return brotherGivesToyBack;
}
// you are playing in the house, including brother's room
var toys = ['teddybear', 'car', 'jumpingrope'],
    askBrotherForClosuredToy = playingInBrothersRoom(toys);

// door is locked, brother came from the shcool, you could not cheat and take it out directly
console.log(askBrotherForClosuredToy.closureToys); // underfined

// but you could ask your brother politely, to give it back
askBrotherForClosuredToy('teddybear'); // hooray, here it is, teddybear
askBrotherForClosuredToy('ball'); // brother would not be able to find it
askBrotherForClosuredToy(); // brother gives you all the rest
askBrotherForClosuredToy(); // nothing left in there


As you can see, the toys left in the room still accessible via brother and no matter the room is locked. Here is jsbin to play around it.
AnswerKenntnis:
I'd simply point them to the Mozilla Closures page. It's the best, most concise and simple explanation of Closure basics and practical usage that I've found. Highly recommended to anyone learning JavaScript.

Edited to add: And yes, I'd even recommend it to a 6-year old -- if the 6-year old is learning about Closures, then it's logical they're ready to comprehend the concise and simple explanation provided in the article.
AnswerKenntnis:
A function in JavaScript is not just a reference to a set of instructions (as in C language) but also includes a hidden data structure which is composed of references to all nonlocal variables it uses. Such two-piece functions are called closures. Every function in JavaScript can be considered a closure.

Closures are functions with a state. It is somewhat similar to "this" in the sense that "this" also provides state for a function but function and "this" are separate objects ("this" is just a fancy parameter, and the only way to bind it to a function is to create a closure). While "this" and function always live separately, a function cannot be separated from its closure.

Because all these external variables referenced by a lexically nested function are actually local variables in the chain of its lexically enclosing functions (global variables can be assumed to be local variables of some root function), and every single execution of a function creates new instances of its local variables, - every execution of a function returning (or otherwise transferring it out, such as registering it as a callback) a nested function creates a new closure (with its own potentially unique set of referenced nonlocal variables which represent its execution context).

Also, it must be understood that local variables in JavaScript are created not on the stack frame but in the heap and destroyed only when no one is referencing them. When a function returns, references to its local variables are decremented but can still be non-null if during the current execution they became part of a closure and are still referenced by its (lexically) nested functions (which can happen only if the references to these nested functions were returned or otherwise transferred to some external code).

An example:

function foo (initValue) {
   //This variable is not destroyed when the foo function exits.
   //It is 'captured' by the two nested functions returned below.
   var value = initValue; 

   //Note that the two returned functions are created right now.
   //If the foo function is called again, it will return 
   //new functions referencing a different 'value' variable.
   return {
       getValue: function () { return value; },
       setValue: function (newValue) { value = newValue; }
   }
}

function bar () {
    //foo sets its local variable 'value' to 5 and returns an object with
    //two functions still referencing that local variable
    var obj = foo(5);

    //extracting functions just to show that no 'this' is involved here
    var getValue = obj.getValue;
    var setValue = obj.setValue;

    alert(getValue()); //displays 5
    setValue(10);
    alert(getValue()); //displays 10

    //At this point getValue and setValue functions are destroyed 
    //(in reality they are destroyed at the next iteration of the garbage collector).
    //The local variable 'value' in the foo is no longer referenced by 
    //anything and is destroyed too.
}

bar();
AnswerKenntnis:
An answer for a 6 year old (assuming he knows what a function is and what a variable is, and what data is):

Functions can return data. One kind of data you can return from a function is another function. When that new function gets returned, all the variables and arguments used in the function that created it don't go away.  Instead, that parent function "closes." In other words, nothing can look inside of it and see the variables it used except for the function it returned.  That new function has a special ability to look back inside the function that created it and see the data inside of it.

function the_closure() {
  var x = 4;
  return function () {
    return x; // here, we look back inside the_closure for the value of x
  }
}

var myFn = the_closure();
myFn(); //=> 4


Another really simple way to explain it is in terms of scope:

Any time you create a smaller scope inside of a larger scope, the smaller scope will always be able to see what is in the larger scope.
AnswerKenntnis:
This is copyrighted material so I can't copy it, but I will supply the reference and say that I found it very clear.  Chapter 8 section 6, "Closures," of Javascript the Definitive Guide by David Flanagan, 6th edition, O'Reilly, 2011.  I'll try to paraphrase.  


When a function is invoked, a new object is created to hold the local variables for that invocation.  
A function's scope depends on its declaration location, not its execution location.


Now, assume an inner function declared within an outer function and referring to variables of that outer function.  Further assume the outer function returns the inner function, as a function.  Now there is an external reference to whatever values were in the inner function's scope (which, by our assumptions, includes values from the outer function).  Javascript will preserve those values, as they have remained in scope of current execution thanks to being passed out of the completed outer function.  All functions are closures, but the closures of interest are the inner functions which, in our assumed scenario, preserve outer function values within their "enclosure" (I hope I'm using language correctly here) when they (the inner functions) are returned from outer functions.  I know this doesn't meet the six-year-old requirement, but hopefully is still helpful.
AnswerKenntnis:
I'm sure, Einstein didn't say it with a direct expectation for us to pick any esoteric brainstormer thing and run over six-year olds with futile attempts to get those 'crazy' (and what is even worse for them-boring) things to their childish minds :) If I were 6 years old I wouldn't like to have such parents or wouldn't make friendship with such boring philanthropists, sorry :)

Anyways, for babies, closure is simply a hug, I guess, whatever way you try to explain :) And when you hug a friend of yours then you both kinda share anything you guys have at the moment. It's a rite of passage, once you've hugged somebody you're showing her trust and willingness to let her do with you a lot of things you don't allow and would hide from others. It's an act of friendship :).

Really don't know how to explain it to 5-6 years old babies. I neither think they will appreciate any JavaScript code snippets like

function Baby(){
  this.iTrustYou = true;
}

Baby.prototype.hug = function (baby) {
    var smiles = 0;

    if(baby.iTrustYou){
        return function(){
            smiles++;
            alert(smiles);
        };
    }
};

var 
   arman = new Baby("Arman")
  ,morgan = new Baby("Morgana");

var hug = arman.hug(morgan);
hug();
hug();


For children only:

Closure is Hug

Bug is fly

KISS is smooch! :)
AnswerKenntnis:
Given the following function

function person(name, age){

    var name=name;
    var age=age;

    function introduce(){
        alert("My name is "+name+", and I'm "+age);
    }

    return introduce;
}

var a = person("Jack",12);
var b = person("Matt",14);


Everytime the function person is called a new closure is created.
While variables a and b have the same introduce function, it is linked to different closures. And that closure will still exist even after the function person finishes execution.



a(); //My name is Jack, and I'm 12
b(); //My name is Matt, and I'm 14


An abstract closures could be represented to something like this

closure a = {
    name: "Jack",
    age: 12,
    call: function introduce(){
        alert("My name is "+name+", and I'm "+age);
    }
}

closure b = {
    name: "Matt",
    age: 14,
    call: function introduce(){
        alert("My name is "+name+", and I'm "+age);
    }
}




Assuming you know how a class in another language work. I will make an analogy.

Think like


javascript function as a constructor
local variables as instance properties
these properties are private
inner functions as instance methods


Everytime a function is called


a new object containing all local variables will be created.
methods of this object have access to "properties" of that instance object.
AnswerKenntnis:
After a function is invoked, it goes out of scope. If that function contains something like a callback function, then that callback function is still in scope. If the callback function references some local variable in the immediate environment of the parent function, then naturally you'd expect that variable to be inaccesible to the callback function and return undefined.

Closures ensure that any property that is referenced by the callback function is available for use by that function, even when it's parent function may have gone out of scope
AnswerKenntnis:
The more I think about closure the more I see it as a 2-step process: init - action

init: pass first what's needed...
action: in order to achieve something for later execution.


To a 6-year old, I'd emphasize on the practical aspect of closure:

Daddy: Listen. Could you bring mum some milk (2).
Tom: No problem.
Daddy: Take a look at the map that Daddy has just made: mum is there and daddy is here. 
Daddy: But get ready first. And bring the map with you (1), it may come in handy
Daddy: Then off you go (3). Ok?
Tom: A piece of cake!


Example: Bring some milk to mum (=action). First get ready and bring the map (=init).

function getReady(map) { 
    var cleverBoy = 'I examine the ' + map;
    return function(what, who) {
        return 'I bring ' + what + ' to ' + who + 'because + ' cleverBoy; //I can access the map
    }
}
var offYouGo = getReady('daddy-map');
offYouGo('milk', 'mum');


Because you bring with you a very important piece of information (the map), you're enough knowledgeable to execute other similar actions:

offYouGo('potatoes', 'great mum');


To a developer I'd make a parallelism between closure and OOP. 
The init phase is similar to passing arguments to a constructor in a traditional OO language; the action phase is ultimately the method you call to achieve what you want. And the method has access these init arguments using a mechanism called closure.

See my another answer illustrating the parallelism between OO and closure:

How to "properly" create a custom object in JavaScript?
QuestionKenntnis:
Testing if something is hidden, using jQuery
qn_description:
In jQuery, it is possible to toggle the visibility of an element? You can use the functions .hide(), .show() or .toggle().

Using jQuery, how would you test if an element is visible or hidden?
AnswersKenntnis
AnswerKenntnis:
As, the question refers to a single element, this code might be more suitable:

// Checks for display:[none|block], ignores visible:[true|false]
$(element).is(":visible") 


Same as twernt's suggestion, but applied to a single element.
AnswerKenntnis:
You can use the hidden selector:

// Matches all elements that are hidden
$('element:hidden')


And the visible selector:

// Matches all elements that are visible
$('element:visible')
AnswerKenntnis:
$(element).css('display') == 'none'


Functions don't work with the visibility attribute.
AnswerKenntnis:
None of these answers address what I understand to be the question, which is what I was searching for, "how do I handle items that have visibility: hidden?". Neither :visible nor :hidden will handle this, as they are both looking for display per the documentation.  As far as I could determine, there is no selector to handle CSS visibility.  Here is how I resolved it (standard jQuery selectors, there may be a more condensed syntax):

$(".item").each(function() {
    if ($(this).css("visibility") == "hidden") {
        // handle non visible state
    } else {
        // handle visible state
    }
});
AnswerKenntnis:
From How do I determine the state of a toggled element? :



You can determine whether an element is collapsed or not by using the :visible and :hidden selectors.

var isVisible = $('#myDiv').is(':visible');
var isHidden = $('#myDiv').is(':hidden');


If you're simply acting on an element based on its visibility, just include ":visible" or ":hidden" in the selector expression. For example:

 $('#myDiv:visible').animate({left: '+=200px'}, 'slow');
AnswerKenntnis:
Often when checking if something is visible or not, you are going to go right ahead immediately and do something else with it. jQuery chaining makes this easy.

So if you have a selector and you want to perform some action on it only if is visible or hidden, you can use filter(":visible") or filter(":hidden") followed by chaining it with the action you want to take.

So instead of an if statement, like this:

if ($('#btnUpdate').is(":visible"))
{
     $('#btnUpdate').animate({ width: "toggle" });   // Hide button
}


Or more efficient, but even uglier:

var button = $('#btnUpdate');
if ($(button).is(":visible"))
{
     $(button).animate({ width: "toggle" });   // Hide button
}


You can do it all in one line:

$('#btnUpdate').filter(":visible").animate({ width: "toggle" });
AnswerKenntnis:
It's worth mentioning (even after all this time), that $(element).is(":visible") works for jQuery 1.4.4, but not for jQuery 1.3.2, under Internet Explorer 8.

This can be tested using Tsvetomir Tsonev's helpful test snippet. Just remember to change the version of jQuery, to test under each one.
AnswerKenntnis:
The :visible selector according to the jQuery documentation:


They have a CSS display value of none.
They are form elements with type="hidden".
Their width and height are explicitly set to 0.
An ancestor element is hidden, so the element is not shown on the page.
Elements with visibility: hidden or opacity: 0 are considered to be visible, since they still consume space in the layout.


This is useful in some cases and useless in others, because if you want to check if the element is visible (display != none), ignoring the parents visibility, you will find that doing .css("display") == 'none' is not only faster, but will also return the visibility check correctly.

If you want to check visibility instead of display, you should use: .css("visibility") == "hidden".

Also take into consideration the additional jQuery notes:


  Because :visible is a jQuery extension and not part of the CSS specification, queries using :visible cannot take advantage of the performance boost provided by the native DOM querySelectorAll() method. To achieve the best performance when using :visible to select elements, first select the elements using a pure CSS selector, then use .filter(":visible").


Also, if you are concerned about performance, you should check Now you see meΓÇª show/hide performance (2010-05-04). And use other methods to show and hide elements.
AnswerKenntnis:
How element visibility and jQuery works;

An element could be hidden with "display:none", "visibility:hidden" or "opacity:0". The difference between those methods:


display:none hides the element, and it does not take up any space;
visibility:hidden hides the element, but it still takes up space in the layout;
opacity:0 hides the element as "visibility:hidden", and it still takes up space in the layout; the only difference is that opacity lets one to make an element partly transparent;  

        if ($('.target').is(':hidden')) {
            $('.target').show();
        } 
        else {
            $('.target').hide();
        }
        if ($('.target').is(':visible')) {
            $('.target').hide();
        } 
        else {
            $('.target').show();
        }

        if ($('.target-visibility').css('visibility') == 'hidden') {
            $('.target-visibility').css({ visibility: "visible", display: "" });
        }
         else {
            $('.target-visibility').css({ visibility: "hidden", display: "" });
        }

        if ($('.target-visibility').css('opacity') == "0") {
            $('.target-visibility').css({ opacity: "1", display: "" });
        } 
        else {
            $('.target-visibility').css({ opacity: "0", display: "" });
        }



Useful jQuery toggle methods:

$('.click').click(function() {
    $('.target').toggle();
});

$('.click').click(function() {
    $('.target').slideToggle();
});

$('.click').click(function() {
    $('.target').fadeToggle();
});
AnswerKenntnis:
This works for me, and I am using show() and hide() to make my div hidden/visible:

if( $(this).css("display") == 'none' ){

    /* your code here*/
}
else{

    /*  alternate logic   */
}
AnswerKenntnis:
I would use CSS class .hide { display: none!important; }. 

For hiding/showing, I call .addClass("hide")/.removeClass("hide"). For checking visibility, I use .hasClass("hide").

It's a simple and clear way to check/hide/show elements, if you don't plan to use .toggle() or .animate() methods.
AnswerKenntnis:
You can also do this using plain JavaScript:

function isRendered(domObj) {
    if ((domObj.nodeType != 1) || (domObj == document.body)) {
        return true;
    }
    if (domObj.currentStyle && domObj.currentStyle["display"] != "none" && domObj.currentStyle["visibility"] != "hidden") {
        return isRendered(domObj.parentNode);
    } else if (window.getComputedStyle) {
        var cs = document.defaultView.getComputedStyle(domObj, null);
        if (cs.getPropertyValue("display") != "none" && cs.getPropertyValue("visibility") != "hidden") {
            return isRendered(domObj.parentNode);
        }
    }
    return false;
}


Notes:


Works everywhere
Works for nested elements
Works for CSS and inline styles
Doesn't require a framework
AnswerKenntnis:
One can simply use the hidden or visible attribute, like:

$('element:hidden')
$('element:visible')


Or you can simplify the same with is as follows.

$(element).is(":visible")
AnswerKenntnis:
ebdiv should be set to style="display:none;". It is works for show and hide:

$(document).ready(function(){
    $("#eb").click(function(){
        $("#ebdiv").toggle();
    });    
});
AnswerKenntnis:
Another answer you should put into consideration is if you are hiding an element, you should use jQuery, but instead of actually hiding it, you remove the whole element, but you copy its HTML content and the tag itself into a jQuery variable, and then all you need to do is test if there is such a tag on the screen, using the normal if (!$('#thetagname').length).
AnswerKenntnis:
This may work:

expect($("#message_div").css("display")).toBe("none");
AnswerKenntnis:
HTML

<div id="clickme">
 Click here
</div>
<img id="book" src="http://www.chromefusion.com/wp-content/uploads/2012/06/chrome-logo.jpg" alt="" />


jQuery

<script>

$('#clickme').click(function() {
$('#book').toggle('slow', function() {
    // Animation complete.
     alert( $('#book').is(":visible"));//<--- TRUE if Visible False if Hidden
   });
});

</script>


Source: 

Blogger Plug n Play - jQuery Tools and Widgets: How to See if Element is hidden or Visible Using jQuery

jsFiddle: 

JSFiddle - ipsjolly - k4WWj
AnswerKenntnis:
Use class toggling, not style editing . . .

Using classes designated for "hiding" elements is easy and also one of the most efficient methods. Toggling a class 'hidden' with a Display style of 'none' will perform faster than editing that style directly. I explained some of this pretty thoroughly in Stack Overflow question Turning two elements visible/hidden in the same div.



JavaScript Best Practices and Optimization

Here is a truly enlightening video of a Google Tech Talk by Google front-end engineer Nicholas Zakas:


Speed Up Your Javascript (YouTube)
AnswerKenntnis:
To check if it is not visible I use !:

if ( !$('#book').is(':visible')) {
    alert('#book is not visible')
}


Or the following is also the sam, saving the jQuery selector in a variable to have better performance when you need it multiple times:

var $book = $('#book')

if(!$book.is(':visible')) {
    alert('#book is not visible')
}
AnswerKenntnis:
Also here's a ternary conditional expression to check the state of the element and then to toggle it:

$('someElement').on('click', function(){ $('elementToToggle').is(':visible') ? $('elementToToggle').hide('slow') : $('elementToToggle').show('slow'); });
AnswerKenntnis:
Try the hidden selector:

$('element:hidden')


And the visible selector

$('element:visible')
AnswerKenntnis:
When testing an element against :hidden selector in jQuery it should be considered that an absolute positioned element may be recognized as hidden although their child elements are visible.

This seems somewhat counter-intuitive at first place ΓÇô though having a closer look at the jQuery documentation gives the relevant information:


  Elements can be considered hidden for several reasons: [...] Their width and height are explicitly set to 0. [...]


So this actually makes sense in regards of the box-model and the computed style for the element. Even if width and height are not set explicitly to 0 they may be set implicitly.

Have a look at the following example:

JSFiddle:

http://jsfiddle.net/pM2q3/1/

HTML:

<div class="foo">
    <div class="bar"></div>
</div>


CSS:

.foo {
    position: absolute;
    left: 10px;
    top: 10px;
    background: #ff0000;
}

.bar {
    position: absolute;
    left: 10px;
    top: 10px;
    width: 20px;
    height: 20px; 
    background: #0000ff;
}


Javascript:

console.log($('.foo').is(':hidden')); // true
console.log($('.bar').is(':hidden')); // false
AnswerKenntnis:
Example:

<div id="checkme" class="product" style="display:none">
 <span class="itemlist"><!-- Shows Results for Fish --></span>
 Category:Fish
 <br>Product: Salmon Atlantic
 <br>Specie: Salmo salar
 <br>Form: Steaks
</div>


<script>
  $(document).ready(function(){
     if($("#checkme:hidden").length)
     {
          alert('Hidden');
     }
   });
</script>
AnswerKenntnis:
After all, none of examples suits me, so I wrote my own.

Tests (no support of Internet Explorer filter:alpha):

a) Check if the document is not hidden

b) Check if an element has zero width / height / opacity or display:none / visibility:hidden in inline styles

c) Check if the center (also because it is faster than testing every pixel / corner) of element is not hidden by other element (and all ancestors, example: overflow:hidden / scroll / one element over enother) or screen edges

d) Check if an element has zero width / height / opacity or display:none / visibility:hidden in computed styles (among all ancestors)

Tested on

Android 4.4 (Native browser/Chrome/Firefox), Firefox (Windows/Mac), Chrome (Windows/Mac), Opera (Windows Presto/Mac Webkit), Internet Explorer (Internet Explorer 5-11 document modes + Internet Explorer 8 on a virtual machine), Safari (Windows/Mac/iOS)

var is_visible = (function () {
    var x = window.pageXOffset ? window.pageXOffset + window.innerWidth - 1 : 0,
        y = window.pageYOffset ? window.pageYOffset + window.innerHeight - 1 : 0,
        relative = !!((!x && !y) || !document.elementFromPoint(x, y));
        function inside(child, parent) {
            while(child){
                if (child === parent) return true;
                child = child.parentNode;
            }
        return false;
    };
    return function (elem) {
        if (
            document.hidden ||
            elem.offsetWidth==0 ||
            elem.offsetHeight==0 ||
            elem.style.visibility=='hidden' ||
            elem.style.display=='none' ||
            elem.style.opacity===0
        ) return false;
        var rect = elem.getBoundingClientRect();
        if (relative) {
            if (!inside(document.elementFromPoint(rect.left + elem.offsetWidth/2, rect.top + elem.offsetHeight/2),elem)) return false;
        } else if (
            !inside(document.elementFromPoint(rect.left + elem.offsetWidth/2 + window.pageXOffset, rect.top + elem.offsetHeight/2 + window.pageYOffset), elem) ||
            (
                rect.top + elem.offsetHeight/2 < 0 ||
                rect.left + elem.offsetWidth/2 < 0 ||
                rect.bottom - elem.offsetHeight/2 > (window.innerHeight || document.documentElement.clientHeight) ||
                rect.right - elem.offsetWidth/2 > (window.innerWidth || document.documentElement.clientWidth)
            )
        ) return false;
        if (window.getComputedStyle || elem.currentStyle) {
            var el = elem,
                comp = null;
            while (el) {
                if (el === document) {break;} else if(!el.parentNode) return false;
                comp = window.getComputedStyle ? window.getComputedStyle(el, null) : el.currentStyle;
                if (comp && (comp.visibility=='hidden' || comp.display == 'none' || (typeof comp.opacity !=='undefined' && comp.opacity != 1))) return false;
                el = el.parentNode;
            }
        }
        return true;
    }
})();


How to use:

is_visible(elem) // boolean
AnswerKenntnis:
if($('#postcode_div').is(':visible')) {
    if($('#postcode_text').val()=='') {
        $('#spanPost').text('\u00a0');
    } else {
        $('#spanPost').text($('#postcode_text').val());
}
AnswerKenntnis:
Because Elements with visibility: hidden or opacity: 0 are considered visible, since they still consume space in the layout (as described for jQuery :visible Selector) - we can check if element is really visible in this way:

function isElementReallyHidden (el) {
    return $(el).is(":hidden") || $(el).css("visibility") == "hidden" || $(el).css('opacity') == 0;
}

var booElementReallyShowed = !isElementReallyHidden(someEl);
$(someEl).parents().each(function () {
    if (isElementReallyHidden(this)) {
        booElementReallyShowed = false;
    }
});
AnswerKenntnis:
You need to check both... Display as well as visibility:

if ($(this).css("display") == "none" || $(this).css("visibility") == "hidden") {
    // The element is not visible
} else {
    // The element is visible
}
AnswerKenntnis:
.is(":not(':hidden')") /*if shown*/
QuestionKenntnis:
How can I prevent SQL-injection in PHP?
qn_description:
If user input is inserted without modification into an SQL query, then the application becomes vulnerable to SQL injection, like in the following example:

$unsafe_variable = $_POST['user_input']; 

mysql_query("INSERT INTO `table` (`column`) VALUES ('$unsafe_variable')");


That's because the user can input something like value'); DROP TABLE table;--, and the query becomes:

INSERT INTO `table` (`column`) VALUES('value'); DROP TABLE table;--')


What can be done to prevent this from happening?
AnswersKenntnis
AnswerKenntnis:
Use prepared statements and parameterized queries. These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.

You basically have two options to achieve this:


Using PDO:

$stmt = $pdo->prepare('SELECT * FROM employees WHERE name = :name');

$stmt->execute(array('name' => $name));

foreach ($stmt as $row) {
    // do something with $row
}

Using MySQLi:

$stmt = $dbConnection->prepare('SELECT * FROM employees WHERE name = ?');
$stmt->bind_param('s', $name);

$stmt->execute();

$result = $stmt->get_result();
while ($row = $result->fetch_assoc()) {
    // do something with $row
}



PDO

Note that when using PDO to access a MySQL database real prepared statements are not used by default. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:

$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8', 'user', 'pass');

$dbConnection->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);


In the above example the error mode isn't strictly necessary, but it is advised to add it. This way the script will not stop with a Fatal Error when something goes wrong. And it gives the developer the chance to catch any error(s) which are thrown as PDOExceptions.

What is mandatory however is the first setAttribute() line, which tells PDO to disable emulated prepared statements and use real prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).

Although you can set the charset in the options of the constructor, it's important to note that 'older' versions of PHP (< 5.3.6) silently ignored the charset parameter in the DSN.

Explanation

What happens is that the SQL statement you pass to prepare is parsed and compiled by the database server. By specifying parameters (either a ? or a named parameter like :name in the example above) you tell the database engine where you want to filter on. Then when you call execute, the prepared statement is combined with the parameter values you specify. 

The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend. Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the $name variable contains 'Sarah'; DELETE FROM employees the result would simply be a search for the string "'Sarah'; DELETE FROM employees", and you will not end up with an empty table.

Another benefit with using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.

Oh, and since you asked about how to do it for an insert, here's an example (using PDO):

$preparedStatement = $db->prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement->execute(array('column' => $unsafeValue));
AnswerKenntnis:
You've got two options - escaping the special characters in your unsafe_variable, or using a parameterized query. Both would protect you from SQL injection. The parameterized query is considered the better practice, but escaping characters in your variable will require fewer changes.

We'll do the simpler string escaping one first.

//Connect

$unsafe_variable = $_POST["user-input"];
$safe_variable = mysql_real_escape_string($unsafe_variable);

mysql_query("INSERT INTO table (column) VALUES ('" . $safe_variable . "')");

//Disconnect


See also, the details of the mysql_real_escape_string function.



Warning:

As of PHP 5.5.0 mysql_real_escape_string and the mysql extension are deprecated.
Please use mysqli extension and mysqli::escape_string function instead



To use the parameterized query, you need to use MySQLi rather than the MySQL functions. To rewrite your example, we would need something like the following.

<?php
    $mysqli = new mysqli("server", "username", "password", "database_name");

    // TODO - Check that connection was successful.

    $unsafe_variable = $_POST["user-input"];

    $stmt = $mysqli->prepare("INSERT INTO table (column) VALUES (?)");

    // TODO check that $stmt creation succeeded

    // "s" means the database expects a string
    $stmt->bind_param("s", $unsafe_variable);

    $stmt->execute();

    $stmt->close();

    $mysqli->close();
?>


The key function you'll want to read up on there would be mysqli::prepare.

Also, as others have suggested, you may find it useful/easier to step up a layer of abstraction with something like PDO.

Please note that the case you asked about is a fairly simple one, and that more complex cases may require more complex approaches. In particular:


If you want to alter the structure of the SQL based on user input, parameterised queries are not going to help, and the escaping required is not covered by mysql_real_escape_string. In this kind of case you would be better off passing the user's input through a whitelist to ensure only 'safe' values are allowed through.
If you use integers from user input in a condition and take the mysql_real_escape_string approach, you will suffer from the problem described by Polynomial in the comments below. This case is trickier because integers would not be surrounded by quotes, so you could deal with by validating that the user input contains only digits.
There are likely other cases I'm not aware of. You might find http://webappsec.org/projects/articles/091007.txt a useful resource on some of the more subtle problems you can encounter.
AnswerKenntnis:
I'd recommend using PDO (PHP Data Objects) to run parameterized SQL queries. 

Not only does this protect against SQL injection, it also speeds up queries. 

And by using PDO rather than mysql_, mysqli_, and pgsql_ functions, you make your app a little more abstracted from the database, in the rare occurrence that you have to switch database providers.
AnswerKenntnis:
Every answer here covers only part of the problem.
In fact, there are four different query parts which we can add to it dynamically:


a string
a number
an identifier
a syntax keyword.


and prepared statements covers only 2 of them

But sometimes we have to make our query even more dynamic, adding operators or identifiers as well.
So, we will need different protection techniques.

In general, such a protection approach is based on whitelisting.
In this case every dynamic parameter should be hardcoded in your script and chosen from that set.
For example, to do dynamic ordering:

$orders  = array("name","price","qty"); //field names
$key     = array_search($_GET['sort'],$orders)); // see if we have such a name
$orderby = $orders[$key]; //if not, first one will be set automatically. smart enuf :)
$query   = "SELECT * FROM `table` ORDER BY $orderby"; //value is safe


However, there is another way to secure identifiers - escaping. As long as you have an identifier quoted, you can escape backticks inside by doubling them. 

As a further step we can borrow a truly brilliant idea of using some placeholder (a proxy to represent the actual value in the query) from the prepared statements and invent a placeholder of another type - an identifier placeholder.

So, to make long story short: it's a placeholder, not prepared statement can be considered as a silver bullet.  

So, a general recommendation may be phrased as
As long as you are adding dynamic parts to the query using placeholders (and these placeholders properly processed of course), you can be sure that your query is safe.

Still there is an issue with SQL syntax keywords (such as AND, DESC and such) but whitelisting seems the only approach in this case.
AnswerKenntnis:
Use PDO and prepared queries.

($conn is a PDO object)

$stmt = $conn->prepare("INSERT INTO tbl VALUES(:id, :name)");
$stmt->bindValue(':id', $id);
$stmt->bindValue(':name', $name);
$stmt->execute();
AnswerKenntnis:
As you can see, people suggest you to use prepared statements at the most. Its not wrong, but when your query is executed just once per process, there would be a slightly performance penalty. 

I was facing this issue but I think i solved it in very sophisticated way - the way hackers use to avoid using quotes. I used this in conjuction with emulated prepared statements. I use it to prevent all kinds of possible sql injection attacks.

My approach:


if you expect input to be integer make sure its really integer. In variable-type language like is php is this very important. You can use for example this very simple but powerful solution: sprintf("SELECT 1,2,3 FROM table WHERE 4 = %u", $input);  
if you expect anything else from integer hex it. If you hex it, you will perfectly escape all input. In C/C++ there's a function called mysql_hex_string(), in php you can use bin2hex().

Dont worry about that the escaped string will have 2x size of its original length because even if you use mysql_real_escape_string, php has to allocate same capacity ((2*input_length)+1), which is the same.
This hex method is often used when you transfer binary data but I see no reason why not use it to all data to prevent sql injection attacks. Note that you have to prepend data with 0x or use mysql function UNHEX instead.


So for example query:

SELECT password FROM users WHERE name = 'root'


Will become:

SELECT password FROM users WHERE name = 0x726f6f74


or

SELECT password FROM users WHERE name = UNHEX('726f6f74')


Hex is the perfect escape. No way to inject.

Difference between UNHEX function and 0x prefix

There was some discussion in comments, so I finally want to make it clear. These two approaches are very similar but are a little different in some ways:

0x prefix can be only used on data columns such as char, varchar, text, block, binary, etc.
Also its use is a little complicated if you are about to insert an empty string, you'll have to entirely replace it with '', or you'll get an error.

UNHEX() works on any column; you do not have to worry about the empty string.



Hex methods are often used as Attacks

Note that this hex method is often used as a sql injection attack where integers are just like strings and escaped just with mysql_real_escape_string, then you can avoid use of quotes.

For example if you just do something like this:

"SELECT title FROM article WHERE id = " . mysql_real_escape_string($_GET["id"])


attack can inject you very easily. Consider the following injected code returned from your script:


  SELECT ... WHERE id = -1 union all select table_name from information_schema.tables


and now just extract table structure:


  SELECT ... WHERE id = -1 union all select column_name from information_schema.column where table_name = 0x61727469636c65


and then just select whatever data ones want. Cool isn't it ?

But if would the coder of injectable site hex it, no injection would be possible because query would look like this: SELECT ... WHERE id = UNHEX('2d312075...3635')
AnswerKenntnis:
Injection prevention - mysql_real_escape_string()

PHP has a specially-made function to prevent these attacks. All you need to do is use the mouthful of a function, mysql_real_escape_string.

mysql_real_escape_string takes a string that is going to be used in a MySQL query and return the same string with all SQL injection attempts safely escaped. Basically, it will replace those troublesome quotes(') a user might enter with a MySQL-safe substitute, an escaped quote \'.

NOTE: you must be connected to the database to use this function!

// Connect to MySQL

$name_bad = "' OR 1'"; 

$name_bad = mysql_real_escape_string($name_bad);

$query_bad = "SELECT * FROM customers WHERE username = '$name_bad'";
echo "Escaped Bad Injection: <br />" . $query_bad . "<br />";


$name_evil = "'; DELETE FROM customers WHERE 1 or username = '"; 

$name_evil = mysql_real_escape_string($name_evil);

$query_evil = "SELECT * FROM customers WHERE username = '$name_evil'";
echo "Escaped Evil Injection: <br />" . $query_evil;


You can find more details in MySQL - SQL Injection Prevention.
AnswerKenntnis:
You could do something basic like this:

$safe_variable = mysql_real_escape_string($_POST["user-input"]);
mysql_query("INSERT INTO table (column) VALUES ('" . $safe_variable . "')");


This won't solve every problem, but it's a very good stepping stone. I left out obvious items such as checking the variable's existence, format (numbers, letters, etc.).
AnswerKenntnis:
Whatever you do end up using, make sure that you check your input hasn't already been mangled by magic_quotes or some other well-meaning rubbish, and if necessary, run it through stripslashes or whatever to sanitise it.
AnswerKenntnis:
Parameterized query AND input validation is the way to go. There is many scenarios under which SQL injection may occur, even though mysql_real_escape_string() has been used.

Those examples are vulnerable to SQL injection:

$offset = isset($_GET['o']) ? $_GET['o'] : 0;
$offset = mysql_real_escape_string($offset);
RunQuery("SELECT userid, username FROM sql_injection_test LIMIT $offset, 10");


or

$order = isset($_GET['o']) ? $_GET['o'] : 'userid';
$order = mysql_real_escape_string($order);
RunQuery("SELECT userid, username FROM sql_injection_test ORDER BY `$order`");


In both cases, you can't use ' to protect the encapsulation.

Source: The Unexpected SQL Injection (When Escaping Is Not Enough)
AnswerKenntnis:
In my opinion, the best way to generally prevent SQL injection in your PHP app (or any web app, for that matter) is to think about your application's architecture. If the only way to protect against SQL injection is to remember to use a special method or function that does The Right Thing every time you talk to the database, you are doing it wrong. That way, it's just a matter of time until you forget to correctly format your query at some point in your code.

Adopting the MVC pattern and a framework like CakePHP or CodeIgniter is probably the right way to go: Common tasks like creating secure database queries have been solved and centrally implemented in such frameworks. They help you to organize your web app in a sensible way and make you think more about loading and saving objects than about securely constructing single SQL queries.
AnswerKenntnis:
There are many ways of preventing SQL injections and other SQL hacks. You can easily find it on the Internet (Google Search). Of course PDO is one of the good solution. But I would like to suggest you some good links prevention from SQL Injection.

What is SQL injection and how to prevent

PHP manual for SQL injection

Microsoft explanation of SQL injection and prevention in PHP

and some other like Preventing SQL injection with MySQL and PHP

Now, why you do you need to prevent your query from SQL injection?

I would like to let you know: Why do we try for preventing SQL injection with a short example below:

Query for login authentication match:

$query="select * from users where email='".$_POST['email']."' and password='".$_POST['password']."' ";


Now, if someone (a hacker) puts

$_POST['email']= admin@emali.com' OR '1=1


and password anything....

The query will be parsed in the system only upto:

$query="select * from users where email='admin@emali.com' OR '1=1';


The other part will be discarded. So, what will happen? A non-authorized user (hacker) will be able to login as admin without having his password. Now, he can do anything what admin/email person can do. See, it's very dangerous if SQL injection is not prevented.
AnswerKenntnis:
I favor stored procedures (MySQL has had stored procedures support since 5.0) from a security point of view - the advantages are -


Most databases (including MySQL) enable user access to be restricted to executing stored procedures. The fine grained security access control is useful to prevent escalation of privileges attacks. This prevents compromised applications from being able to run SQL directly against the database.
They abstract the raw SQL query from the application so less information of the database structure is available to the application. This makes it harder for people to understand the underlying structure of the database and design suitable attacks.
They accept only parameters, so the advantages of parameterized queries are there. Of course - IMO you still need to sanitize your input - especially if you are using dynamic SQL inside the stored procedure.


The disadvantages are -


They (stored procedures) are tough to maintain and tend to multiply very quickly. This makes managing them an issue.
They are not very suitable for dynamic queries - if they are built to accept dynamic code as parameters then a lot of the advantages are negated.
AnswerKenntnis:
Type cast if possible your parameters. But it's only working on simple types like int, bool and float.

$unsafe_variable = $_POST['user_id'];

$safe_variable = (int)$unsafe_variable ;

mysql_query("INSERT INTO table (column) VALUES ('" . $safe_variable . "')");
AnswerKenntnis:
For those unsure of how to use PDO (coming from the mysql_ functions), I made a very, very simple PDO wrapper that is a single file. It exists to show how easy it is to do all the common things applications need done. Works with PostgreSQL, MySQL, and SQLite.

Basically, read it while you read the manual to see how to put the PDO functions to use in real life to make it simple to store and retrieve values in the format you want.


  I want a single column

$count = DB::column('SELECT COUNT(*) FROM `user`);

  
  I want an array(key => value) results (i.e. for making a selectbox)

$pairs = DB::pairs('SELECT `id`, `username` FROM `user`);

  
  I want a single row result

$user = DB::row('SELECT * FROM `user` WHERE `id` = ?', array($user_id));

  
  I want an array of results

$banned_users = DB::fetch('SELECT * FROM `user` WHERE `banned` = ?', array(TRUE));
AnswerKenntnis:
I think if someone want use PHP and MySQL or other DataBase Server: 


Think about learning PDO (PHP Data Objects) ΓÇô is a database access layer providing a uniform method of access to multiple databases.
Think about learning mysqli
Use native php functions like: strip_tags, mysql_real_escape_string or if variable numeric just (int)$foo. Read more about type of variables in PHP read here. If you using libs such as PDO or MySQLi always use PDO::quote() and mysqli_real_escape_string().




Libraries examples:

---- PDO


  ----- no placeholders - ripe for SQL Injection! It's bad


$request = $pdoConnection->("INSERT INTO parents (name, addr, city) values ($name, $addr, $city)");



  ----- unnamed placeholders  


$request = $pdoConnection->("INSERT INTO parents (name, addr, city) values (?, ?, ?);



  ----- named placeholders 


$request = $pdoConnection->("INSERT INTO parents (name, addr, city) value (:name, :addr, :city)");


--- MySQLi

$request = $mysqliConnection->prepare('
       SELECT * FROM trainers
       WHERE name = ?
       AND email = ?
       AND last_login > ?');

    $query->bind_param('first_param', 'second_param', $mail, time() - 3600);
    $query->execute();




P.S


  PDO wins this battle with ease. With support for twelve
  different database drivers  and named parameters, we can ignore the
  small performance loss, and get used to its API. From a security
  standpoint, both of them are safe as long as the developer uses them
  the way they are supposed to be used
  
  But while both PDO and MySQLi are quite fast, MySQLi performs
  insignificantly faster in benchmarks ΓÇô ~2.5% for non-prepared
  statements, and ~6.5% for prepared ones.
  
  And please test every query to your database - it's better way to prevent injection
AnswerKenntnis:
If you want to take advantage of cache engines like redis or memcache maybe DALMP could be a choice, it uses pure mysqli, check this: DALMP Database Abstraction Layer for MySQL using PHP. 

Also you can 'prepare' your arguments before preparing your query so that you can build dynamic queries and at the end have a full prepared statements query. DALMP Database Abstraction Layer for MySQL using PHP.
AnswerKenntnis:
Using this PHP function mysql_escape_string() you can get a good prevention in a fast way.

For example: 

SELECT * FROM users WHERE name = '".mysql_escape_string($name_from_html_form)."'


mysql_escape_string ΓÇö Escapes a string for use in a mysql_query

For more prevention you can add at the end ... 

wHERE 1=1   or  LIMIT 1


Finally you get:

SELECT * FROM users WHERE name = '".mysql_escape_string($name_from_html_form)."' LIMIT 1
AnswerKenntnis:
Regarding to many useful answers, I hope to add some values to this thread.
SQL injection is type of attack that can be done through user inputs (Inputs that filled by user and then used inside queries), The SQL injection patterns are correct query syntax while we can call it: bad queries for bad reasons, we assume that there might be bad person that try to get secret information (by passing access control) that affect the three principles of security (Confidentiality, Integrity, Availability). 

Now, our point is to prevent security threats such as SQL injection attacks, the question asking (How to prevent SQL injection attack using PHP), be more realistic, data filtering or clearing input data is the case when using user-input data inside such query, using PHP or any other programming language is not the case, or as recommended by more people to use modern technology such as prepared statement or any other tools that currently supporting SQL injection prevention, consider that these tools not available anymore? how you secure your application?

My approach against SQL injection is: clearing user-input data before sending it to database (before using it inside any query).

Data filtering for (Converting unsafe data to safe data)
Consider that PDO and MySQLi not available, how can you secure your application? do you force me to use them? what about other languages other than PHP? I prefer to provide general ideas as it can be used for wider border not just for specific language.


SQL user (limiting user privilege): most common SQL operations are (SELECT, UPDATE, INSERT), then, why giving UPDATE privilege to a user that not require it? for example: login, and search pages are only using SELECT, then, why using db users in these pages with high privileges? 
RULE: do not create one database user for all privileges, for all SQL operations, you can create your scheme like (deluser, selectuser, updateuser) as usernames for easy usage.


see Principle of least privilege


Data filtering: before building any query user input should be validated and filtered, for programmers, it's important to define some properties for each user-input variables:
data type, data pattern, and data length. a field that is a number between (x and y) must be exactly validated using exact rule, for a field that is a string (text): pattern is the case, for example: username must contain only some characters lets say [a-zA-Z0-9_-.] the length vary between (x and n) where x and n (integers, x <=n ).
Rule: creating exact filters and validation rules are best practice for me. 
Use other tools: Here, I will also agree with you that prepared statement (parametrized query) and Stored procedures, the disadvantages here is these ways requires advanced skills which are not exist in most users, the basic idea here is to distinguish between SQL query and the data that being used inside, both approach can be used even with unsafe data, because the user-input data here not add anything to the original query such as (any or x=x).
for more information please read OWASP SQL Injection Prevention Cheat Sheet.


Now, if you are an advanced user, start using these defense as you like, but, for beginners, if they can't quickly implement stored procedure and prepared statement, it's better to filter input data as much they can.

Finally, let's consider that user sends this text below instead of entering his  username:

[1] UNION SELECT IF(SUBSTRING(Password,1,1)='2',BENCHMARK(100000,SHA1(1)),0) User,Password FROM mysql.user WHERE User = 'root'


This input can be checked early without any prepared statement and stored procedures, but to be on safe side, using them starts after user-data filtering and validation.

Last point is detecting unexpected behavior which requires more effort and complexity, it's not recommended for normal web applications.
Unexpected behavior in above user input is: SELECT, UNION, IF, SUBSTRING, BENCHMARK, SHA, root once these words detected, you can avoid the input.

UPDATE1:

A user commented that this post is useless, OK! Here is what OWASP.ORG provided:


  Primary defenses: 
   
      Option #1: Use of Prepared Statements (Parameterized Queries) 
      Option #2: Use of Stored Procedures 
      Option #3: Escaping all User Supplied Input  
   
  Additional defenses: 
   
      Also Enforce: Least Privilege 
      Also Perform: White List Input Validation 


As you may knew, claiming on any article should be supported by valid argument, at least one reference! Otherwise it's considered as attack and bad claim!

Update2:

From the PHP manual, PHP: Prepared Statements - Manual:


  Escaping and SQL injection 
  
  Bound variables will be escaped automatically by the server. The
  server inserts their escaped values at the appropriate places into the
  statement template before execution. A hint must be provided to the
  server for the type of bound variable, to create an appropriate
  conversion. See the mysqli_stmt_bind_param() function for more
  information. 
  
  The automatic escaping of values within the server is sometimes
  considered a security feature to prevent SQL injection. The same
  degree of security can be achieved with non-prepared statements, if
  input values are escaped correctly. 


Update3:

I created test cases for knowing how PDO and MySQLi sends the query to MySQL server when using prepared statement:

PDO:

$user = "''1''"; //Malicious keyword
$sql = 'SELECT * FROM awa_user WHERE userame =:username';
$sth = $dbh->prepare($sql, array(PDO::ATTR_CURSOR => PDO::CURSOR_FWDONLY));
$sth->execute(array(':username' => $user));


Query Log:


    189 Query SELECT * FROM awa_user WHERE userame ='\'\'1\'\''
    189 Quit



MySQLi:

$stmt = $mysqli->prepare("SELECT * FROM awa_user WHERE username =?")) {
$stmt->bind_param("s", $user);
$user = "''1''";
$stmt->execute();


Query Log:


    188 Prepare   SELECT * FROM awa_user WHERE username =?
    188 Execute   SELECT * FROM awa_user WHERE username ='\'\'1\'\''
    188 Quit



It's clear that a prepared statement is also escaping the data, nothing else.

As also mentioned in above statement The automatic escaping of values within the server is sometimes considered a security feature to prevent SQL injection. The same degree of security can be achieved with non-prepared statements, if input values are escaped correctly, therefore, this proves that data validation such as intval() is a good idea for integer values before sending any query, in addition, preventing malicious user data before sending the query is correct and valid approach.

Please see this question for more detail: PDO sends raw query to MySQL while Mysqli sends prepared query, both produce the same result

References:


SQL Injection Cheat Sheet
SQL Injection
Information security
Security Principles
Data validation
AnswerKenntnis:
I use three different ways to prevent my web application from being vulnerable to SQL injection.


Use of mysql_real_escape_string(), which is a pre-defined function in PHP, and this code add backslashes to the following characters: \x00, \n, \r, \, ', " and \x1a. Pass the input values as parameters to minimize the chance of SQL injection.
The most advanced way is to use PDOs.


I hope this will help you.

Consider the following query:

$iId = mysql_real_escape_string("1 OR 1=1");
 $sSql = "SELECT * FROM table WHERE id = $iId";

mysql_real_escape_string() will not protect here. If you use single quotes (' ') around your variables inside your query is what protects you against this. Here is an solution below for this:

$iId = (int) mysql_real_escape_string("1 OR 1=1");
 $sSql = "SELECT * FROM table WHERE id = $iId";

This question has some good answers about this.

I suggest, using PDO is the best option.
AnswerKenntnis:
A few guidelines for escaping special characters in SQL statements.

Don't use MySQL, this extension is deprecated, use MySQLi or PDO.

MySQLi

For manually escaping special characters in string you can use mysqli_real_escape_string function. The function will not work properly unless the correct character set is set with mysqli_set_charset.

Example:

$mysqli = new mysqli( 'host', 'user', 'password', 'database' );
$mysqli->set_charset( 'charset');

$string = $mysqli->real_escape_string( $string );
$mysqli->query( "INSERT INTO table (column) VALUES ('$string')" );


For automatic escaping of values with prepared statements, use mysqli_prepare, and mysqli_stmt_bind_param where types for the corresponding bind variables must be provided for an appropriate conversion:

Example:

$stmt = $mysqli->prepare( "INSERT INTO table ( column1, column2 ) VALUES (?,?)" );

$stmt->bind_param( "is", $integer, $string );

$stmt->execute();


No matter if you use prepared statements or mysqli_real_escape_string, you always have to know the type of input data you're working with. 

So if you use prepared statement, you must specify the types of the variables for mysqli_stmt_bind_param function.

And use of mysqli_real_escape_string is for, as the name says, escaping special characters in a string, so it will not make integers safe. The purpose of this function is to prevent breaking the strings in sql statements, and the damage to the database that it could cause. mysqli_real_escape_string is a useful function when used properly, especially when combined with sprintf.

Example:

$string = "x' OR name LIKE '%John%";
$integer = '5 OR id != 0';

$query = sprintf( "SELECT id, email, pass, name FROM members WHERE email ='%s' AND id = %d", $mysqli->real_escape_string( $string ), $integer );

echo $query;
// SELECT id, email, pass, name FROM members WHERE email ='x\' OR name LIKE \'%John%' AND id = 5

$integer = '99999999999999999999';
$query = sprintf( "SELECT id, email, pass, name FROM members WHERE email ='%s' AND id = %d", $mysqli->real_escape_string( $string ), $integer );

echo $query;
// SELECT id, email, pass, name FROM members WHERE email ='x\' OR name LIKE \'%John%' AND id = 2147483647
AnswerKenntnis:
The simple alternative to this problem could be solved by granting appropriate permissions in the database itself.
For example: if you are using mysql database. then enter into the database through terminal or the ui provided and just follow this command:

 GRANT SELECT, INSERT, DELETE ON database TO username@'localhost' IDENTIFIED BY 'password';


This will restrict the user to only get confined with the specified query's only. Remove the delete permission and so the data would never get deleted from the query fired from the php page.
The second thing to do is to flush the privileges so that the mysql refreshes the permissions and updates.

FLUSH PRIVILEGES; 


more information about flush.

To see the current privileges for the user fire the following query.

    select * from mysql.user where User='username';


Learn more about GRANT.
AnswerKenntnis:
A simple way would be to use a PHP framework like CodeIgniter or Laravel which have in-built features like filtering and active-record, so that you dont have to worry about these nuances.
AnswerKenntnis:
How protect index.php by sql-injection using .htaccess was not really a duplicate! Some have reported it as a duplicate question to this question. So, I'll post my answer here, an admin can even move my answer from here to a so called duplicate question:

However, if the attackers is trying to hack with the form via PHP $_GET variable or with the URL's query string, you can able to catch them if they're not secure.

RewriteCond %{QUERY_STRING} ([0-9]+)=([0-9]+)
RewriteRule ^(.*) ^/track.php


Because 1=1, 2=2, 1=2, 2=1, 1+1=2, etc... are the common questions to SQL database of an attacker. Maybe also it's used by many hacking applications. But you must be careful, that you must not rewrite a safe queries from your site. The code above is giving you a tip, to rewrite or redirect (depends on you) that hacking-specific dynamic query string into a page that will store the attacker's I.P. address, or EVEN THEIR COOKIES, history, browser, or any other sensitive informations, and try to hack them back for security purpose.
AnswerKenntnis:
There are so many answers for PHP + MYSQL but here there is code for PHP+Oracle for preventing sql injection as well as regular use of oci8 drivers might be this is useful for anyone

$c = oci_connect($userName, $password, "(DESCRIPTION=(ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST =$serverName)(PORT = 1521)))(CONNECT_DATA=(SID=$databaseName)))");
$strQuery = "UPDATE table SET field = :xx WHERE ID = 123"
$stmt = OCIParse($c, $strQuery);

OCIBindByName($stmt, ':xx', $fieldval);

$ok = OCIExecute($stmt);
AnswerKenntnis:
Using PDO and MYSQLi is a good practice to prevent sql injections. but you really want work with mysql functions and queries it would be better to use

mysql_real_escape_string

$unsafe_variable = mysql_real_escape_string($_POST['user_input']);


There are more ability to prevent this:-
like identify is input is string, number, char or array there are so many inbuilt functions to detect this:-
Also would be better to use these functions to check a input data.

is_string

$unsafe_variable = (is_string($_POST['user_input']) ? $_POST['user_input'] : '');


is_numeric

$unsafe_variable = (is_numeric($_POST['user_input']) ? $_POST['user_input'] : '');


and so many so better to use those functions to check input data with mysql_real_escape_string
AnswerKenntnis:
A good idea is to use an 'object-relational mapper' like Idiorm:

$user = ORM::for_table('user')
->where_equal('username', 'j4mie')
->find_one();

$user->first_name = 'Jamie';
$user->save();

$tweets = ORM::for_table('tweet')
    ->select('tweet.*')
    ->join('user', array(
        'user.id', '=', 'tweet.user_id'
    ))
    ->where_equal('user.username', 'j4mie')
    ->find_many();

foreach ($tweets as $tweet) {
    echo $tweet->text;
}


It not only saves you from SQL injections, but from syntax errors too!
AnswerKenntnis:
I've written this little function several years ago:

function runEscapedQuery($preparedQuery /*, ...*/)
{
    global $DB_LINK;
    $ctr = 0;

    ensureConnection(); // Make sure we are connected.

    $params = array_slice(func_get_args(), 1);
    foreach ($params as $value)
    {
        if (is_string($value))
        {
            $value = "'" . mysqli_real_escape_string($DB_LINK, $value) . "'";
        }
        else if (is_null($value))
        {
            $value = 'NULL';
        }
        else if (!is_int($value) && !is_float($value))
        {
            die('Only basic typed arguments allowed in a query. Argument '.($ctr+1).' is not a basic type, it\'s type is '. gettype($value). '.');
        }
        $preparedQuery = str_replace('{'.$ctr.'}', $value, $preparedQuery);
        $ctr++;
    }
    $results = runQuery($preparedQuery); // executes query, dies on error, fetches results in an array.

    return $results;
}


This allows running statements in an one-liner C#-ish String.Format like:

runEscapedQuery("INSERT INTO Whatever (id, foo, bar) VALUES ({0}, {1}, {2})", $numericVar, $stringVar1, $stringVar2);


It escapes considering the variable type. If you try to parameterize table, column names, it would fail as it puts every string in quotes which is invalid syntax.
QuestionKenntnis:
What does "use strict" do in JavaScript, and what is the reasoning behind it?
qn_description:
What is "use strict"; all about, what does it imply, and is it still relevant?

Do any of the current browsers respond to the "use strict"; string or is it for future use?
AnswersKenntnis
AnswerKenntnis:
This article about that might interest you: John Resig - ECMAScript 5 Strict Mode, JSON, and More

To quote some interesting parts:


  Strict Mode is a new feature in ECMAScript 5 that allows you to place a program, or a function, in a "strict" operating context. This strict context prevents certain actions from being taken and throws more exceptions.


And:


  Strict mode helps out in a couple ways:
  
  
  It catches some common coding bloopers, throwing exceptions.
  It prevents, or throws errors, when relatively "unsafe" actions are taken (such as gaining access to the global object).
  It disables features that are confusing or poorly thought out.
  


Also note you can apply "strict mode" to the whole file... Or you can use it only for a specific function (still quoting from John Resig's article):

// Non-strict code...

(function(){
  "use strict";

  // Define your library strictly...
})();

// Non-strict code... 


Which might be helpful if you have to mix old and new code ;-)

So, I suppose it's a bit like the "use strict" you can use in Perl (hence the name?): it helps you make fewer errors, by detecting more things that could lead to breakages.
AnswerKenntnis:
It's a new feature of ECMAScript5.  John Resig wrote up a nice summary of it.

It's just a string you put in your js files (either at the top of your file or inside of a function) that looks like this:

"use strict";


Putting it in your code now shouldn't cause any problems with current browsers as it's just a string.  It may cause problems with your code in the future if your code violates the pragma.  For instance, if you currently have foo = "bar" without defining foo first, your code will start failing...which is a good thing in my opinion.
AnswerKenntnis:
If people are worried about using use strict it might be worth checking out this article: 

http://www.novogeek.com/post/ECMAScript-5-Strict-mode-support-in-browsers-What-does-this-mean.aspx

It talks about browser support, but more importantly how to deal with it safely:

function isStrictMode(){
    return !this;
} 
//returns false, since 'this' refers to global object and '!this' becomes false

function isStrictMode(){   
    "use strict";
    return !this;
} 
//returns true, since in strict mode, the keyword 'this' does not refer to global object, unlike traditional JS. So here,'this' is null and '!this' becomes true.
AnswerKenntnis:
I strongly recommend every developer to start using strict mode now. There are enough browsers supporting it that strict mode will legitimately help save us from errors we didnΓÇÖt even know where in your code. Apparently, at initial stage there will be errors we have never encountered before. To get full benefit, we need to do a proper testing after switching to strict mode to make sure we have caught everything. Definitely we donΓÇÖt just throw "use strict" in our code and assume there are no errors. So the churn is that itΓÇÖs time to start using this incredibly useful language feature to write better code.

eg. 

var person = {
    name : 'xyz',
    position : 'abc',
    fullname : function () {  "use strict"; return this.name; }
};


JSLint is a debugger written by Douglas Crockford. Simply paste in your script, and itΓÇÖll quickly scan for any noticeable issues and errors in your code.
AnswerKenntnis:
If you use a browser released in the last year or so then it most likely supports JavaScript Strict mode. Only older browsers around before ECMAScript 5 became the current standard don't support it. 

The quotes around the command make sure that the code will still work in older browsers as well (although the things that generate a syntax error in strict mode will generally just cause the script to malfunction in some hard to detect way in those older browsers).
AnswerKenntnis:
A word of caution, all you hard-charging programmers:  applying "use strict" to existing code can be hazardous!  This thing is not some feel-good, happy-face sticker that you can slap on the code to make it 'better'.  With the "use strict" pragma, the browser will suddenly THROW exceptions in random places that it never threw before just because at that spot you are doing something that default/loose JavaScript happily allows but strict JavaScript abhors!  You may have strictness violations hiding in seldom used calls in your code that will only throw an exception when they do eventually get run - say, in the production environment that your paying customers use!

If you are going to take the plunge, it is a good idea to apply "use strict" alongside comprehensive unit tests and a strictly configured JSHint build task that will give you some confidence that there is no dark corner of your module that will blow up horribly just because you've turned on Strict Mode.  Or, hey, here's another option:  just don't add "use strict" to any of your legacy code, it's probably safer that way, honestly.  DEFINITELY DO NOT add "use strict" to any modules you do not own or maintain, like third party modules.

I think even though it is a deadly caged animal, "use strict" can be good stuff, but you have to do it right.  The best time to go strict is when your project is greenfield and you are starting from scratch.  Configure JSHint/JSLint with all the warnings and options cranked up as tight as your team can stomach, get a good build/test/assert system du jour rigged like Grunt+Karma+Chai, and only THEN start marking all your new modules as "use strict".  Be prepared to cure lots of niggly errors and warnings.  Make sure everyone understands the gravity by configuring the build to FAIL if JSHint/JSLint produces any violations.

My project was not a greenfield project when I adopted "use strict".  As a result, my IDE is full of red marks because I don't have "use strict" on half my modules, and JSHint complains about that.  It's a reminder to me about what refactorings I should do in the future.  My goal is to be red mark free due to all of my missing "use strict" statements, but that is years away now.
AnswerKenntnis:
"Use Strict"; is an insurance that programmer will not use the loose or the bad properties of JavaScript, it is a guide, just like a ruler will help you make straight lines, "Use Strict" will help you do "Straight coding".
Those that prefer not to use rulers to do their lines straight usually end up in those pages asking for others to debug their codes.
Believe me the overhead is negligible compared to poorly designed code.
Doug Crockford whom been a Senior JavaScript developer for several years, has a very interesting post here, personally i like to return to his site all the time to make sure i don't forget my good practice.
Modern JavaScript practice should always evoke the "Use Strict"; pragma.
The only reason that the ECMA Group has made the "Strict" mode optional is to permit less experienced coders access to JavaScript and give then time to adapt to the new and safer coding practices.
AnswerKenntnis:
To invoke strict mode for an entire script, put the exact statement "use strict"; (or 'use strict';) before any other statements.

// Whole-script strict mode syntax
"use strict";
var v = "Hi!  I'm a strict mode script!";


This syntax has a trap that has already bitten a major site: it isn't possible to blindly concatenate non-conflicting scripts. Consider concatenating a strict mode script with a non-strict mode script: the entire concatenation looks strict! The inverse is also true: non-strict plus strict looks non-strict. Concatenation of strict mode scripts with each other is fine, and concatenation of non-strict mode scripts is fine. Only concatenating strict and non-strict scripts is problematic. It is thus recommended that you enable strict mode on a function-by-function basis (at least during the transition period).

You can also take the approach of wrapping the entire contents of a script in a function and having that outer function use strict mode. This eliminates the concatenation problem but it means that you have to explicitly export any global variables out of the function scope.
AnswerKenntnis:
Strict mode makes several changes to normal JavaScript semantics.


strict mode eliminates some JavaScript silent errors by changing them
to throw errors.
strict mode fixes mistakes that make it difficult for JavaScript
engines to perform optimizations.
strict mode prohibits some syntax likely to be defined in future
versions of ECMAScript.


for more information vistit Strict Mode- Javascript
AnswerKenntnis:
There's a good talk by some people who were on the ECMAScript committee: Changes to JavaScript, Part 1: ECMAScript 5" about how incremental use of the "use strict" switch allows JavaScript implementers to clean up a lot of the dangerous features of JavaScript without suddenly breaking every website in the world.

Of course it also talks about just what a lot of those misfeatures are (were) and how ECMAScript 5 fixes them.
QuestionKenntnis:
The definitive guide to form based website authentication
qn_description:
Form based authentication for websites

We believe that Stack Overflow should not just be a resource for very specific technical questions, but also for general guidelines on how to solve variations on common problems. "Form based authentication for websites" should be a fine topic for such an experiment. 

It should include topics such as:


How to log in
How to remain logged in
How to store passwords
Using secret questions
Forgotten username/password functionality
OpenID
"Remember me" checkbox
Browser autocompletion of usernames and passwords
Secret URLs (public URLs protected by digest)
Checking password strength
E-mail validation
and much more about form based authentication ...


It should not include things like:


roles and authorization
HTTP basic authentication


Please help us by


Suggesting subtopics
Submitting good articles about this subject
Editing the official answer
AnswersKenntnis
AnswerKenntnis:
PART I: How To Log In


As a rule, CAPTCHAs should be a last resort. They tend to be annoying, often aren't human-solvable, most of them are ineffective against bots, all of them are ineffective against cheap third-world labor (according to OWASP, the current sweatshop rate is $12 per 500 tests), and some implementations are technically illegal in some countries (see link number 1 from the MUST-READ list). If you must use a CAPTCHA, use reCAPTCHA, since it is OCR-hard by definition (since it uses already OCR-misclassified book scans).
It is possible to prevent some browsers from storing/retrieving a password with the autocomplete tag for forms/input fields. However in the real world, your customers will have many accounts on different systems; it compromises their security if they use the same password for every site. Can you expect them to remember different passwords for every site? There are some good password managers out there, however there are also bad ones - which will become a target for attackers. Latest versions of Chrome also allow users to over-ride autocomplete=off, negating any efforts made to prevent this.
The only (currently practical) way to protect against login interception (packet sniffing) during login is by using a certificate-based encryption scheme (for example, SSL) or a proven & tested challenge-response scheme (for example, the Diffie-Hellman-based SRP). Any other method can be easily circumvented by an eavesdropping attacker.
On that note: hashing the password client-side (for example, with JavaScript) is useless (and almost universally a security flaw) unless it is combined with one of the above - that is, either securing the line with strong encryption or using a tried-and-tested challenge-response mechanism (if you don't know what that is, just know that it is one of the most difficult to prove, most difficult to design, and most difficult to implement concepts in digital security). Hashing the password is effective against password disclosure, but not against replay attacks, Man-In-The-Middle attacks / hijackings, or brute-force attacks (since we are handing the attacker both username, salt and hashed password).
After sending the authentication tokens, the system needs a way to remember that you have been authenticated - this fact should only ever be stored serverside in the session data. A cookie can be used to reference the session data. Wherever possible, the cookie should have the secure and HTTP Only flags set when sent to the browser. The httponly flag provides some protection against the cookie being read by a XSS attack. The secure flag ensures that the cookie is only sent back via HTTPS, and therefore protects against network sniffing attacks. The value of the cookie should not be predictable. Where a cookie referencing a non-existent session is presented, its value should be replaced immediately to prevent session fixation.


PART II: How To Remain Logged In - The Infamous "Remember Me" Checkbox

Persistent Login Cookies ("remember me" functionality) are a danger zone; on the one hand, they are entirely as safe as conventional logins when users understand how to handle them; and on the other hand, they are an enormous security risk in the hands of most users, who use them on public computers, forget to log out, don't know what cookies are or how to delete them, etc.

Personally, I want my persistent logins for the web sites I visit on a regular basis, but I know how to handle them safely. If you are positive that your users know the same, you can use persistent logins with a clean conscience. If not - well, then you're more like me; subscribing to the philosophy that users who are careless with their login credentials brought it upon themselves if they get hacked. It's not like we go to our user's houses and tear off all those facepalm-inducing Post-It notes with passwords they have lined up on the edge of their monitors, either. If people are idiots, then let them eat idiot cake.

Of course, some systems can't afford to have any accounts hacked; for such systems, there is no way you can justify having persistent logins.

If you DO decide to implement persistent login cookies, this is how you do it:


First, follow Charles Miller's 'Best Practices' article Do not get tempted to follow the 'Improved' Best Practices linked at the end of his article. Sadly, the 'improvements' to the scheme are easily thwarted (all an attacker has to do when stealing the 'improved' cookie is remember to delete the old one. This will require the legitimate user to re-login, creating a new series identifier and leaving the stolen one valid).
And DO NOT STORE THE PERSISTENT LOGIN COOKIE (TOKEN) IN YOUR DATABASE, ONLY A HASH OF IT! The login token is Password Equivalent, so if an attacker got his hands on your database, he/she could use the tokens to log in to any account, just as if they were cleartext login-password combinations. Therefore, use strong salted hashing (bcrypt / phpass) when storing persistent login tokens.


PART III: Using Secret Questions

Don't. Never ever use 'secret questions'. Read the paper from link number 5 from the MUST-READ list. You can ask Sarah Palin about that one, after her Yahoo! email account got hacked during the presidential campaign because the answer to her 'security' question was... (wait for it) ... "Wasilla High School"!

Even with user-specified questions, it is highly likely that most users will choose either:


A 'standard' secret question like mother's maiden name or favourite pet
A simple piece of trivia that anyone could lift from their blog, LinkedIn profile, or similar
Any question that is easier to answer than guessing their password. Which, for any decent password, is every question conceivable.


In conclusion, security questions are inherently insecure in all their forms and variations, and should never be employed in an authentication scheme for any reason.

A secondary question is often considered as adequate for fulfilling a requirement for two-factor authentication. While capturing some of the response via clicks rather than typing in theory provides protection against keylogger attacks, it is still just an extension to the password mechanism - and when users are presented with a text box instead of drop-downs on a phishing site, they rarely perceive this as abnormal. Note that you may be able to fulfill your two-factor obligations by using a long-lasting cookie (granted on submission of multiple authentication questions) in place of a security question asked each and every time - but at the expense of user convenience.

The only reason anyone still uses security questions by choice is that is saves the cost of a few support calls from users who can't remember their email passwords to get to their reactivation codes. At the expense of security and Sara Palin's reputation, that is. Worth it? You be the judge.

PART IV: Forgotten Password Functionality

I already mentioned why you should never use security questions for handling forgotten/lost user passwords; it also goes without saying that you should never e-mail users their passwords. There are at least two more all-too-common pitfalls to avoid in this field:


Don't RESET user's passwords no matter what - 'reset' passwords are harder for the user to remember, which means he/she MUST either change it OR write it down - say, on a bright yellow Post-It on the edge of his monitor. Instead, just let users pick a new one right away - which is what they want to do anyway.
Always hash the lost password code/token in the database. AGAIN, this code is another example of a Password Equivalent, so it MUST be hashed in case an attacker got his hands on your database. When a lost password code is requested, send the plaintext code to the user's email address, then hash it, save the hash in your database -- and throw away the original. Just like a password or a persistent login token.


A final note: always make sure your interface for entering the 'lost password code' is at least as secure as your login form itself, or an attacker will simply use this to gain access instead. Making sure you generate very long 'lost password codes' (for example, 16 case sensitive alphanumeric characters) is a good start, but consider adding the same throttling that you do for logins.

PART V: Checking Password Strength

First, you'll want to read this small article for a reality check: The 500 most common passwords

Okay, so maybe the list isn't the canonical list of most common passwords on any system anywhere ever, but it's a good indication of how poorly people will choose their passwords when there is no enforced policy in place. Plus, the list looks frighteningly close to home when you compare it to the publicly available analyses of 40.000+ recently stolen MySpace passwords.

Well, enough MySpace-bashing for now. Moving on..

So: With no minimum password strength requirements, 2% of users use one of the top 20 most common passwords. Meaning: if an attacker gets just 20 attempts, 1 in 50 accounts on your website will be crackable.

Thwarting this requires calculating the entropy of a password and then applying a threshold.  The National Institute of Standards and Technology (NIST) Special Publication 800-63 has a set of very good suggestions.  That, when combined with a dictionary and keyboard layout analysis (for example, 'qwertyuiop' is a bad password), can reject 99% of all poorly selected passwords at a level of 18 bits of entropy.  Simply calculating password strength and showing a visual strength meter to a user is insufficient.  Unless it is enforced, users will ignore it.

PART VI: Much More - Or: Preventing Rapid-Fire Login Attempts

First, have a look at the numbers: Password Recovery Speeds - How long will your password stand up

If you don't have the time to look through the tables in that link, here's the list of them:


It takes virtually no time to crack a weak password, even if you're cracking it with an abacus
It takes virtually no time to crack an alphanumeric 9-character password, if it is case insensitive
It takes virtually no time to crack an intricate, symbols-and-letters-and-numbers, upper-and-lowercase password, if it is less than 8 characters long (a desktop PC can search the entire keyspace up to 7 characters in a matter of days or even hours)
It would, however, take an inordinate amount of time to crack even a 6-character password, if you were limited to one attempt per second!


So what can we learn from these numbers? Well, lots, but we can focus on the most important part: the fact that preventing large numbers of rapid-fire successive login attempts (ie. the brute force attack) really isn't that difficult. But preventing it right isn't as easy as it seems.

Generally speaking, you have three choices that are all effective against brute-force attacks (and dictionary attacks, but since you are already employing a strong passwords policy, they shouldn't be an issue):


Present a CAPTCHA after N failed attempts (annoying as hell and often ineffective -- but I'm repeating myself here)
Locking accounts and requiring email verification after N failed attempts (this is a DoS attack waiting to happen)
And finally, login throttling: that is, setting a time delay between attempts after N failed attempts (yes, DoS attacks are still possible, but at least they are far less likely and a lot more complicated to pull off).


Best practice #1: A short time delay that increases with the number of failed attempts, like:


1 failed attempt = no delay
2 failed attempts = 2 sec delay
3 failed attempts = 4 sec delay
4 failed attempts = 8 sec delay
5 failed attempts = 16 sec delay
etc.


DoS attacking this scheme would be very impractical, but on the other hand, potentially devastating, since the delay increases exponentially. A DoS attack lasting a few days could suspend the user for weeks.


  To clarify: The delay is not a delay before returning the response to the browser. It is more like a timeout or refractory period during which login attempts to a specific account or from a specific IP address will not be accepted or evaluated at all. That is, correct credentials will not return in a successful login, and incorrect credentials will not trigger a delay increase.


Best practice #2: A medium length time delay that goes into effect after N failed attempts, like:


1-4 failed attempts = no delay
5 failed attempts = 15-30 min delay


DoS attacking this scheme would be quite impractical, but certainly doable. Also, it might be relevant to note that such a long delay can be very annoying for a legitimate user. Forgetful users will dislike you.

Best practice #3: Combining the two approaches - either a fixed, short time delay that goes into effect after N failed attempts, like:


1-4 failed attempts = no delay
5+ failed attempts = 20 sec delay


Or, an increasing delay with a fixed upper bound, like:


1 failed attempt = 5 sec delay
2 failed attempts = 15 sec delay
3+ failed attempts = 45 sec delay


This final scheme was taken from the OWASP best-practices suggestions (link 1 from the MUST-READ list), and should be considered best practice, even if it is admittedly on the restrictive side.


  As a rule of thumb however, I would say: the stronger your password policy is, the less you have to bug users with delays. If you require strong (case-sensitive alphanumerics + required numbers and symbols) 9+ character passwords, you could give the users 2-4 non-delayed password attempts before activating the throttling.


DoS attacking this final login throttling scheme would be very impractical. And as a final touch, always allow persistent (cookie) logins (and/or a CAPTCHA-verified login form) to pass through, so legitimate users won't even be delayed while the attack is in progress. That way, the very impractical DoS attack becomes an extremely impractical attack.

Additionally, it makes sense to do more aggressive throttling on admin accounts, since those are the most attractive entry points

PART VII: Distributed Brute Force Attacks

Just as an aside, more advanced attackers will try to circumvent login throttling by 'spreading their activities':


Distributing the attempts on a botnet to prevent IP address flagging
Rather than picking one user and trying the 50.000 most common passwords (which they can't, because of our throttling), they will pick THE most common password and try it against 50.000 users instead. That way, not only do they get around maximum-attempts measures like CAPTCHAs and login throttling, their chance of success increases as well, since the number 1 most common password is far more likely than number 49.995
Spacing the login requests for each user account, say, 30 seconds apart, to sneak under the radar


Here, the best practice would be logging the number of failed logins, system-wide, and using a running average of your site's bad-login frequency as the basis for an upper limit that you then impose on all users.

Too abstract? Let me rephrase:

Say your site has had an average of 120 bad logins per day over the past 3 months. Using that (running average), your system might set the global limit to 3 times that -- ie. 360 failed attempts over a 24 hour period. Then, if the total number of failed attempts across all accounts exceeds that number within one day (or even better, monitor the rate of acceleration and trigger on a calculated treshold), it activates system-wide login throttling - meaning short delays for ALL users (still, with the exception of cookie logins and/or backup CAPTCHA logins).

I also posted a question with more details and a really good discussion of how to avoid tricky pitfals in fending off distributed brute force attacks

PART VIII: Two-Factor Authentication and Authentication Providers

Credentials can be compromised, whether by exploits, passwords being written down and lost, laptops with keys being stolen, or users entering logins into phishing sites.  Logins can be protected with two-factor authentication, which use out-of-band factors such as single-use codes received from a phone call, SMS message, or dongle. Several providers offer two-factor authentication services.

Authentication can be completely delegated to a single-sign-on service such as OAuth, OpenID or Persona (nee BrowserID), where another provider handles collecting credentials.  This pushes the problem to a trusted third party.   Twitter is an example of an OAuth provider, while Facebook provides a similar proprietary solution.

MUST-READ LINKS About Web Authentication


OWASP Guide To Authentication
Dos and DonΓÇÖts of Client Authentication on the Web (very readable MIT research paper)
Charles Miller's Persistent Login Cookie Best Practice
Wikipedia: HTTP cookie
Personal knowledge questions for fallback authentication: Security questions in the era of Facebook (very readable Berkeley research paper)
AnswerKenntnis:
Definitive Article

Sending credentials

The only practical way to send credentials 100% securely is by using SSL. Using JavaScript to hash the password is not safe. (TODO citation required). There's another secure method called SRP, but it's patented (although it is freely licensed) and there are few good implementations available.

Storing passwords

Don't ever store passwords as plaintext in the database. Not even if you don't care about the security of your own site. Assume that some of your users will reuse the password of their online bank account. So, store the hashed password, and throw away the original. And make sure the password doesn't show up in access logs or application logs. The best hashing function seems to be bcrypt. 

Hashes by themselves are also insecure. For instance, identical passwords mean identical hashes--this makes hash lookup tables an effective way of cracking lots of passwords at once. Instead, store the salted hash. A salt is a string appended to the password prior to hashing - use a different (random) salt per user. The salt is a public value, so you can store them with the hash in the database. See here for more on this.

This means that you can't send the user their forgotten passwords (because you only have the hash). Don't reset the user's password unless you have authenticated the user (users must prove that they know the answer to the security question, or are able to read emails sent to the stored (and validated) email address.)

Security questions

Security questions are insecure - avoid using them. Why? Read PART III: Using Secret Questions in @Jens Roland answer here in this wiki.

Session cookies

After the user logs in, the server sends the user a session cookie. The server can retrieve the username or id from the cookie, but nobody else can generate such a cookie (TODO explain mechanisms). Cookies can be hijacked (TODO really? how?), so don't send persistent cookies. If you want to autologin your users, you can set a persistent cookie, but you should set a flag that the user has auto-logged in, and needs to login for real for sensitive operations (TODO is this correct? I think this is what Amazon does.)

List of external resources


Dos and Don'ts of Client Authentication on the Web (PDF)
21 page academic article with many great tips.  
Ask YC: Best Practices for User Authentication
Forum discussion on the subject  
You're Probably Storing Passwords Incorrectly
Introductory article about storing passwords
Discussion: Coding Horror: You're Probably Storing Passwords Incorrectly
Forum discussion about a Coding Horror article.
Never store passwords in a database!
Another warning about storing passwords in the database.
Password cracking
Wikipedia article on weaknesses of several password hashing schemes.
Enough With The Rainbow Tables: What You Need To Know About Secure Password Schemes
Discussion about rainbow tables and how to defend against them, and against other threads. Includes extensive discussion.
AnswerKenntnis:
First, a strong caveat that this answer is not the best fit for this exact question. It should definitely not be the top answer!

I will go ahead and mention MozillaΓÇÖs proposed BrowserID (or perhaps more precisely, the Verified Email Protocol) in the spirit of finding an upgrade path to better approaches to authentication in the future.

IΓÇÖll summarize it this way:


Mozilla is a nonprofit with values that align well with finding good solutions to this problem.
The reality today is that most websites use form-based authentication
Form-based authentication has a big drawback, which is increased risk of phishing. Users are asked to enter sensitive information into an area controlled by a remote entity, rather than an area controlled by their User Agent (browser).
Since browsers are implicitly trusted (the whole idea of a User Agent is to act on behalf of the User), they can help improve this situation.
The primary force holding back progress here is deployment deadlock. Solutions must be decomposed into steps which provide some incremental benefit on their own.
The simplest decentralized method for expressing identity that is built into the internet infrastructure is the domain name.
As a second level of expressing identity, each domain manages its own set of accounts.
The form ΓÇ£account@domainΓÇ¥ is concise and supported by a wide range of protocols and URI schemes. Such an identifier is, of course, most universally recognized as an email address.
Email providers are already the de-facto primary identity providers online. Current password reset flows usually let you take control of an account if you can prove that you control that accountΓÇÖs associated email address.
The Verified Email Protocol was proposed to provide a secure method, based on public key cryptography, for streamlining the process of proving to domain B that you have an account on domain A.
For browsers that donΓÇÖt support the Verified Email Protocol (currently all of them), Mozilla provides a shim which implements the protocol in client-side JavaScript code.
For email services that donΓÇÖt support the Verified Email Protocol, the protocol allows third parties to act as a trusted intermediary, asserting that theyΓÇÖve verified a userΓÇÖs ownership of an account. It is not desirable to have a large number of such third parties; this capability is intended only to allow an upgrade path, and it is much preferred that email services provide these assertions themselves.
Mozilla offers their own service to act as such a trusted third party. Service Providers (that is, Relying Parties) implementing the Verified Email Protocol may choose to trust Mozilla's assertions or not. MozillaΓÇÖs service verifies usersΓÇÖ account ownership using the conventional means of sending an email with a confirmation link.
Service Providers may, of course, offer this protocol as an option in addition to any other method(s) of authentication they might wish to offer.
A big user interface benefit being sought here is the ΓÇ£identity selectorΓÇ¥. When a user visits a site and chooses to authenticate, their browser shows them a selection of email addresses (ΓÇ£personalΓÇ¥, ΓÇ£workΓÇ¥, ΓÇ£political activismΓÇ¥, etc.) they may use to identify themselves to the site.
Another big user interface benefit being sought as part of this effort is helping the browser know more about the userΓÇÖs session ΓÇô who theyΓÇÖre signed in as currently, primarily ΓÇô so it may display that in the browser chrome.
Because of the distributed nature of this system, it avoids lock-in to major sites like Facebook, Twitter, Google, etc. Any individual can own their own domain and therefore act as their own identity provider.


This is not strictly ΓÇ£form-based authentication for websitesΓÇ¥. But it is an effort to transition from the current norm of form-based authentication to something more secure: browser-supported authentication.
AnswerKenntnis:
I just thought I'd share this solution that I found to be working just fine.

I call it the Dummy Field (though I haven't invented this so don't credit me).

In short: you just have to insert this into your <form> and check for it to be empty at when validating:

<input type="text" name="email" style="display:none" />


The trick is to fool a bot into thinking it has to insert data into a required field, that's why I named the input "email". If you already have a field called email that you're using you should try naming the dummy field something else like "company", "phone" or "emailaddress". Just pick something you know you don't need and what sounds like something people would normally find logical to fill in into a web form. Now hide the input field using CSS or JavaScript/jQuery - whatever fits you best - just don't set the input type to hidden or else the bot won't fall for it.

When you are validating the form (either client or server side) check if your dummy field has been filled to determine if it was send by a human or a bot.

Example:

In case of a human:
The user will not see the dummy field (in my case named "email") and will not attempt to fill it. So the value of the dummy field should still be empty when the form has been send.

In case of a bot: The bot will see a field whose type is text and a name email (or whatever it is you called it) and will logically attempt to fill it with appropriate data. It doesn't care if you styled the input form with some fancy CSS, web-developers do it all the time. Whatever the value in the dummy field is, we don't care as long as it's larger than 0 characters.

I used this method on a guestbook in combination with CAPTCHA, and I haven't seen a single spam post since. I had used a CAPTCHA-only solution before, but eventually it resulted in about five spam posts every hour. Adding the dummy field in the form has stopped (at least till now) all the spam from appearing. 

I believe this can also be used just fine with a login/authentication form.

Warning: Of course this method is not 100% fool proof. Bots can be programmed to ignore input fields with the style display:none applied to it. You also have to think about people who use some form of auto-completion (like most browsers have built-in!) to auto-fill all form fields for them. They might just as well pick up a dummy field.

You can also vary this up a little by leaving the dummy field visible but outside the boundaries of screen, but this is totally up to you. 

Be creative!
AnswerKenntnis:
I do not think the above answer is "wrong" but there are large areas of authentication that are not touched upon (or rather the emphasis is on "how to implement cookie sessions", not on "what options are available and what are the trade offs".

My suggested edits / answers are


The problem lies more in account setup than in password checking.
The use of two factor authenitication is much more secure than more clever means of password encryption
Do NOT try to implement your own login form or database storage of passwords, unless 
the data being stored is valueless at account creation and self-generated (that is, web 2.0 style like Facebook, Flickr, etc.)


Digest Authentication is a standards based approach supported in all major browsers and servers, that will not send a password even over a secure channel.



This avoids any need to have "sessions" or cookies as the browser itself will re-encrypt the communication each time. It is the most "lightweight" development approach.

However, I do not recommend this, except for public, low value services. This is an issue with some of the other answers above - do not try an re-implement server-side authetication mechanisms - this problem has been solved and is supported by most major browsers. Do not use cookies. Do not store anything in your own hand-rolled database. Just ask, per request, if the request is autheticated. Everything else should be supported by configuration and third-party trusted software.

So ...

First, we are confusing the initial creation of an account (with a password) with the 
re-checking of the password subsequently. If I am Flickr and creating your site for the first time, the new user has access to zero value (blank web space). I truly do not care if the person creating the account is lying about their name. If I am creating an account of the hospital intranet / extranet, the value lies in all the medical records, and so I do care about the identity (*) of the account creator.

This is the very very hard part. The only decent solution is a web of trust. For example, you join the hospital as a doctor. You create a web page hosted somewhere with your photo, your passport number and a public key, and hash them all with the private key. You then visit the hospital and the system administrator looks at your passport, sees if the photo matches you, and then hashes the web page / photo hash with the hospital private key. From now on we can securely exchange keys and tokens. As can anyone who trusts the hospital (there is the secret sauce BTW). The system administrator can also give you an RSA dongle or other two-factor authentication.

But this is a lot of hassle, and not very web 2.0. However, it is the only secure way to create new accounts that have access to valuable information that is not self-created.


Kerberos and SPNEGO - single sign on mechanisms with a trusted third party - basically the user verifies against a trusted third party. (NB this is not in any way the not to be trusted OAuth)
SRP - sort of clever password authentication without a trusted third party. But here we are getting into the realms of "it's safer to use two factor authentication, even if that's costlier"
SSL client side - give the clients a public key certificate (support in all major browsers - but raises questions over client machine security).


In the end it's a tradeoff - what is the cost of a security breach vs the cost of implementing more secure approaches. One day, we may see a proper PKI widely accepted and so no more own rolled authentication forms and databases. One day...
AnswerKenntnis:
See also Wikibooks PHP Programming: User login systems.
AnswerKenntnis:
User Authentication on the World Wide Web is old, but a good primer read nonetheless.
AnswerKenntnis:
When hashing, don't use fast hash algorithms such as MD5 (many hardware implementations exist).  Use something like SHA-512.  For passwords, slower hashes are better.

The faster you can create hashes, the faster any brute force checker can work. Slower hashes will therefore slow down brute forcing. A slow hash algorithm will make brute forcing impractical for longer passwords (8 digits +)
AnswerKenntnis:
good article about realistic password strength estimation:

Dropbox Tech Blog  » Blog Archive   » zxcvbn: realistic password strength estimation
AnswerKenntnis:
Not quite 'definitive', but OWASP has some good stuff as well...
AnswerKenntnis:
My favourite rule in regards to authentication systems: use passphrases, not passwords. Easy to remember, hard to crack.
More info: Coding Horror: Passwords vs. Pass Phrases
AnswerKenntnis:
Simple protection Against Brute Force Attack

A very simple method to protect against brute force attack without much code. 

Make your server return randomly false for login-in as if the user did a wrong typing in the password. Doing so once every 40 login would give a 2.5% chance that the code is never found thus a 100% doubt that the right code has already been tried by the hacker and a 98.5% chance that the real user is not bothered. Write that information on your website so that attackers knows. Not perfect at all but simple enough to be implemented by everyone.

EDIT

Better not advertise that such a system is in place... as Mat Quinlan pointed out. And of course it will not help if you have thousands of zombie machine trying the same passwords multiple time. but if each machine try a different combination... who knows, it might help! If you advertise that this system is in place you will however gain some time.
QuestionKenntnis:
How can I get query string values in JavaScript?
qn_description:
Is there a plugin-less way of retrieving query string values via jQuery (or without)? 

If so, how? If not, is there a plugin which can do so?
AnswersKenntnis
AnswerKenntnis:
You don't need jQuery for that purpose. You can use just some pure JavaScript:

function getParameterByName(name) {
    name = name.replace(/[\[]/, "\\[").replace(/[\]]/, "\\]");
    var regex = new RegExp("[\\?&]" + name + "=([^&#]*)"),
        results = regex.exec(location.search);
    return results == null ? "" : decodeURIComponent(results[1].replace(/\+/g, " "));
}


Usage:

var prodId = getParameterByName('prodId');
AnswerKenntnis:
Some of the solutions posted here are inefficient. Repeating the regular expression search every time the script needs to access a parameter is completely unnecessary, one single function to split up the parameters into an associative-array style object is enough. If you're not working with the HTML 5 History API, this is only necessary once per page load.  The other suggestions here also fail to decode the URL correctly.

var urlParams;
(window.onpopstate = function () {
    var match,
        pl     = /\+/g,  // Regex for replacing addition symbol with a space
        search = /([^&=]+)=?([^&]*)/g,
        decode = function (s) { return decodeURIComponent(s.replace(pl, " ")); },
        query  = window.location.search.substring(1);

    urlParams = {};
    while (match = search.exec(query))
       urlParams[decode(match[1])] = decode(match[2]);
})();

Example querystring: 


  ?i=main&mode=front&sid=de8d49b78a85a322c4155015fdce22c4&enc=+Hello%20&empty


Result:

urlParams = {
    enc: " Hello ",
    i: "main",
    mode: "front",
    sid: "de8d49b78a85a322c4155015fdce22c4",
    empty: ""
}

alert(urlParams["mode"]);
// -> "front"

alert("empty" in urlParams);
// -> true


This could easily be improved upon to handle array-style query strings too.  An example of this is here, but since array-style parameters aren't defined in RFC 3986 I won't pollute this answer with the source code. For those interested in a "polluted" version, look at campbeln's answer below.

Also, as pointed out in the comments, ; is a legal delimiter for key=value pairs.  It would require a more complicated regex to handle ; or &, which I think is unnecessary because it's rare that ; is used and I would say even more unlikely that both would be used.  If you need to support ; instead of &, just swap them in the regex.

 
If you're using a server-side preprocessing language, you might want to use its native JSON functions to do the heavy lifting for you.  For example, in PHP you can write:

<script>var urlParams = <?php echo json_encode($_GET, JSON_HEX_TAG);?>;</script>

Much simpler!
AnswerKenntnis:
Without jQuery

var qs = (function(a) {
    if (a == "") return {};
    var b = {};
    for (var i = 0; i < a.length; ++i)
    {
        var p=a[i].split('=');
        if (p.length != 2) continue;
        b[p[0]] = decodeURIComponent(p[1].replace(/\+/g, " "));
    }
    return b;
})(window.location.search.substr(1).split('&'));


With an URL like ?topic=123&name=query+string, the following will return:

qs["topic"];    // 123
qs["name"];     // query string
qs["nothere"];  // undefined (object)




Google method

Tearing Google's code I found the method they use: getUrlParameters

function (b) {
    var c = typeof b === "undefined";
    if (a !== h && c) return a;
    for (var d = {}, b = b || k[B][vb], e = b[p]("?"), f = b[p]("#"), b = (f === -1 ? b[Ya](e + 1) : [b[Ya](e + 1, f - e - 1), "&", b[Ya](f + 1)][K](""))[z]("&"), e = i.dd ? ia : unescape, f = 0, g = b[w]; f < g; ++f) {
        var l = b[f][p]("=");
        if (l !== -1) {
            var q = b[f][I](0, l),
                l = b[f][I](l + 1),
                l = l[Ca](/\+/g, " ");
            try {
                d[q] = e(l)
            } catch (A) {}
        }
    }
    c && (a = d);
    return d
}


It is obfuscated, but it is understandable.

They start to look for parameters on the url from ? and also from the hash #. Then for each parameter they split in the equal sign b[f][p]("=") (which looks like indexOf, they use the position of the char to get the key/value). Having it split they check whether the parameter has a value or not, if it has they store the value of d, if not it just continue.

In the end the object d is returned, handling escaping and the + sign. This object is just like mine, it has the same behavior.



My method as a jQuery plugin

(function($) {
    $.QueryString = (function(a) {
        if (a == "") return {};
        var b = {};
        for (var i = 0; i < a.length; ++i)
        {
            var p=a[i].split('=');
            if (p.length != 2) continue;
            b[p[0]] = decodeURIComponent(p[1].replace(/\+/g, " "));
        }
        return b;
    })(window.location.search.substr(1).split('&'))
})(jQuery);


Usage

$.QueryString["param"]




Performance test (split method against regex method) (jsPerf)

Preparation code: methods declaration

Split test code

var qs = window.GetQueryString(query);

var search = qs["q"];
var value = qs["value"];
var undef = qs["undefinedstring"];


Regex test code

var search = window.getParameterByName("q");
var value = window.getParameterByName("value");
var undef = window.getParameterByName("undefinedstring");


Testing in Firefox 4.0 x86 on Windows Server 2008 R2 / 7 x64


Split method: 144,780 ┬▒2.17% fastest
Regex method: 13,891 ┬▒0.85% | 90% slower
AnswerKenntnis:
Improved version of Artem Barger's answer:

function getParameterByName(name) {
    var match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}


For more information on improvement see: http://james.padolsey.com/javascript/bujs-1-getparameterbyname/
AnswerKenntnis:
Just another recommendation. The plugin jQuery-URL-Parser allows to retrieve all parts of URL, including anchor, host, etc.

Usage is very simple and cool:

$.url().param("itemID")
AnswerKenntnis:
Roshambo on snipplr.com has a really hot and simple script to achieve this described in Get URL Parameters with jQuery | Improved. With his script you also easily get to pull out just the parameters you want.

Here's the gist:

$.urlParam = function(name, url) {
    if (!url) {
     url = window.location.href;
    }
    var results = new RegExp('[\\?&]' + name + '=([^&#]*)').exec(url);
    if (!results) { 
        return 0; 
    }
    return results[1] || 0;
}


Then just get your parameters from the query string.

So if the URL/query string was xyz.com/index.html?lang=de.

Just call var langval = $.urlParam('lang');, and you've got it.

UZBEKJON has a great blog post on this as well, Get URL parameters & values with jQuery.
AnswerKenntnis:
If you're using jQuery, you can use a library, such as jQuery BBQ: Back Button & Query Library.


  ...jQuery BBQ provides a full .deparam() method, along with both hash state management, and fragment / query string parse and merge utility methods.


If you want to just use plain JavaScript, you could use...

var getParamValue = (function() {
    var params;
    var resetParams = function() {
            var query = window.location.search;
            var regex = /[?&;](.+?)=([^&;]+)/g;
            var match;

            params = {};

            if (query) {
                while (match = regex.exec(query)) {
                    params[match[1]] = decodeURIComponent(match[2]);
                }
            }    
        };

    window.addEventListener
    && window.addEventListener('popstate', resetParams);

    resetParams();

    return function(param) {
        return params.hasOwnProperty(param) ? params[param] : null;
    }

})();ΓÇï


Because of the new HTML History API and specifically history.pushState() and history.replaceState(), the URL can change which will invalidate the cache of parameters and their values.

This version will update its internal cache of parameters each time the history changes.
AnswerKenntnis:
Here's my stab at making Andy E's excellent solution into a full fledged jQuery plugin:

;(function ($) {
    $.extend({      
        getQueryString: function (name) {           
            function parseParams() {
                var params = {},
                    e,
                    a = /\+/g,  // Regex for replacing addition symbol with a space
                    r = /([^&=]+)=?([^&]*)/g,
                    d = function (s) { return decodeURIComponent(s.replace(a, " ")); },
                    q = window.location.search.substring(1);

                while (e = r.exec(q))
                    params[d(e[1])] = d(e[2]);

                return params;
            }

            if (!this.queryStringParams)
                this.queryStringParams = parseParams(); 

            return this.queryStringParams[name];
        }
    });
})(jQuery);


The syntax is:

var someVar = $.getQueryString('myParam');


Best of both worlds!
AnswerKenntnis:
Why not just use 2 splits ?

function get(n) {
      var half = location.search.split(n + '=')[1];
      return half !== undefined ? decodeURIComponent(half.split('&')[0]) : null;
  }


I was reading all previous and more complete answer. But I think that is the simplest and faster method. You can check in this jsPerf benchmark 

To solve the problem in Rup's comment, add a conditional split by changing the first line to the two below.  But absolute accuracy means it's now slower than regexp (see jsPerf).

function get(n) {
    var half = location.search.split('&' + n + '=')[1];
    if (!half) half = location.search.split('?' + n + '=')[1];
    return half !== undefined ? decodeURIComponent(half.split('&')[0]) : null;
}


So if you know you won't run into Rup's counter-case, this wins.  Otherwise, regexp.
AnswerKenntnis:
If you're doing more URL manipulation than simply parsing the querystring, you may find URI.js helpful. It is a library for manipulating URLs - and comes with all the bells and whistles. (Sorry for self-advertising here)

to convert your querystring into a map:

var data = URI('?foo=bar&bar=baz&foo=world').query(true);
data == {
  "foo": ["bar", "world"],
  "bar": "baz"
}


(URI.js also "fixes" bad querystrings like ?&foo&&bar=baz& to ?foo&bar=baz)
AnswerKenntnis:
I like Ryan Phelan's solution. But I don't see any point of extending jQuery for that? There is no usage of jQuery functionality.

On other hand I like the built-in function in Google Chrome: window.location.getParameter.

So why not to use this? Okay, other browsers don't have. So let's create this function if it does not exist:

if (!window.location.getParameter ) {
  window.location.getParameter = function(key) {
    function parseParams() {
        var params = {},
            e,
            a = /\+/g,  // Regex for replacing addition symbol with a space
            r = /([^&=]+)=?([^&]*)/g,
            d = function (s) { return decodeURIComponent(s.replace(a, " ")); },
            q = window.location.search.substring(1);

        while (e = r.exec(q))
            params[d(e[1])] = d(e[2]);

        return params;
    }

    if (!this.queryStringParams)
        this.queryStringParams = parseParams(); 

    return this.queryStringParams[key];
  };
}


This function is more or less from Ryan Phelan, but it is wrapped differently: clear name and no dependencies of other javascript libraries. More about this function on my blog.
AnswerKenntnis:
Keep it simple in plain javascript:

function qs(key) {
    var vars = [], hash;
    var hashes = window.location.href.slice(window.location.href.indexOf('?') + 1).split('&');
    for(var i = 0; i < hashes.length; i++)
    {
        hash = hashes[i].split('=');
        vars.push(hash[0]);
        vars[hash[0]] = hash[1];
    }
    return vars[key];
}


Call it from anywhere in the JavaScript code:

var result = qs('someKey');
AnswerKenntnis:
Here is a fast way to get an object similar to the PHP $_GET array:

function get_query(){
    var url = location.href;
    var qs = url.substring(url.indexOf('?') + 1).split('&');
    for(var i = 0, result = {}; i < qs.length; i++){
        qs[i] = qs[i].split('=');
        result[qs[i][0]] = decodeURIComponent(qs[i][1]);
    }
    return result;
}


Usage:

var $_GET = get_query();


For the query string x=5&y&z=hello&x=6 this returns the object:

{
  x: "6",
  y: undefined,
  z: "hello"
}
AnswerKenntnis:
Use the following to get a query param value given a key name:

function getParam(key) 
  {
  // Find the key and everything up to the ampersand delimiter
  var value=RegExp(""+key+"[^&]+").exec(window.location.search);

  // Return the unescaped value minus everything starting from the equals sign or an empty string
  return unescape(!!value ? value.toString().replace(/^[^=]+./,"") : "");
  }
AnswerKenntnis:
These are all great answers, but I needed something a bit more robust, and thought you all might like to have what I created. It is a simple library method that does dissection and manipulation of url parameters. The static method has the following sub methods that can be called on the subject url:


getHost
getPath
getHash
setHash
getParams
getQuery
setParam
getParam
hasParam
removeParam


Example:

URLParser(url).getParam('myparam1')




var url = "http://www.test.com/folder/mypage.html?myparam1=1&myparam2=2#something";

function URLParser(u){
    var path="",query="",hash="",params;
    if(u.indexOf("#") > 0){
        hash = u.substr(u.indexOf("#") + 1);
        u = u.substr(0 , u.indexOf("#"));
    }
    if(u.indexOf("?") > 0){
        path = u.substr(0 , u.indexOf("?"));        
        query = u.substr(u.indexOf("?") + 1);
        params= query.split('&');
    }else
        path = u;
    return {
        getHost: function(){
            var hostexp = /\/\/([\w.-]*)/;
            var match = hostexp.exec(path);
            if (match != null && match.length > 1)
                return match[1];
            return "";
        },
        getPath: function(){
            var pathexp = /\/\/[\w.-]*(?:\/([^?]*))/;
            var match = pathexp.exec(path);
            if (match != null && match.length > 1)
                return match[1];
            return "";
        },
        getHash: function(){
            return hash;
        },
        getParams: function(){
            return params
        },
        getQuery: function(){
            return query;
        },
        setHash: function(value){
            if(query.length > 0)
                query = "?" + query;
            if(value.length > 0)
                query = query + "#" + value;
            return path + query;
        },
        setParam: function(name, value){
            if(!params){
                params= new Array();
            }
            params.push(name + '=' + value);
            for (var i = 0; i < params.length; i++) {
                if(query.length > 0)
                    query += "&";
                query += params[i];
            }
            if(query.length > 0)
                query = "?" + query;
            if(hash.length > 0)
                query = query + "#" + hash;
            return path + query;
        },
        getParam: function(name){
            if(params){
                for (var i = 0; i < params.length; i++) {
                    var pair = params[i].split('=');
                    if (decodeURIComponent(pair[0]) == name)
                        return decodeURIComponent(pair[1]);
                }
            }
            console.log('Query variable %s not found', name);
        },
        hasParam: function(name){
            if(params){
                for (var i = 0; i < params.length; i++) {
                    var pair = params[i].split('=');
                    if (decodeURIComponent(pair[0]) == name)
                        return true;
                }
            }
            console.log('Query variable %s not found', name);
        },
        removeParam: function(name){
            query = "";
            if(params){
                var newparams = new Array();
                for (var i = 0;i < params.length;i++) {
                    var pair = params[i].split('=');
                    if (decodeURIComponent(pair[0]) != name)
                          newparams .push(params[i]);
                }
                params = newparams ;
                for (var i = 0; i < params.length; i++) {
                    if(query.length > 0)
                        query += "&";
                    query += params[i];
                }
            }
            if(query.length > 0)
                query = "?" + query;
            if(hash.length > 0)
                query = query + "#" + hash;
            return path + query;
        },
    }
}


document.write("Host: " + URLParser(url).getHost() + '<br>');
document.write("Path: " + URLParser(url).getPath() + '<br>');
document.write("Query: " + URLParser(url).getQuery() + '<br>');
document.write("Hash: " + URLParser(url).getHash() + '<br>');
document.write("Params Array: " + URLParser(url).getParams() + '<br>');
document.write("Param: " + URLParser(url).getParam('myparam1') + '<br>');
document.write("Has Param: " + URLParser(url).hasParam('myparam1') + '<br>');

document.write(url + '<br>');

// Remove first param
url = URLParser(url).removeParam('myparam1');
document.write(url + ' - Remove first param<br>');

// Add third param
url = URLParser(url).setParam('myparam3',3);
document.write(url + ' - Add third param<br>');

// Remove second param
url = URLParser(url).removeParam('myparam2');
document.write(url + ' - Add third param<br>');

// Add hash 
url = URLParser(url).setHash('newhash');
document.write(url + ' - Set Hash<br>');

// Remove last param
url = URLParser(url).removeParam('myparam3');
document.write(url + ' - Remove last param<br>');

// Remove a param that doesnt exist
url = URLParser(url).removeParam('myparam3');
document.write(url + ' - Remove a param that doesnt exist<br>');

ΓÇï
AnswerKenntnis:
Code golf:

var a = location.search&&location.search.substr(1).replace(/\+/gi," ").split("&");
for (var i in a) {
    var s = a[i].split("=");
    a[i]  = a[unescape(s[0])] = unescape(s[1]);
}


Display it!

for (i in a) {
    document.write(i + ":" + a[i] + "<br/>");   
};


On my Mac: test.htm?i=can&has=cheezburger displays

0:can
1:cheezburger
i:can
has:cheezburger
AnswerKenntnis:
I use regular expressions a lot but not for that.

It seems easier and more efficient to me to read the query string once in my application, and build an object from all the key/value pairs like:

var search = function() {
  var s = window.location.search.substr(1),
    p = s.split(/\&/), l = p.length, kv, r = {};
  if (l === 0) {return false;}
  while (l--) {
    kv = p[l].split(/\=/);
    r[kv[0]] = decodeURIComponent(kv[1] || '') || true;
  }
  return r;
}();


For an URL like http://domain.com?param1=val1&param2=val2 you can get their value later in your code as search.param1 and search.param2.
AnswerKenntnis:
function GET() {
        var data = [];
        for(x = 0; x < arguments.length; ++x)
            data.push(location.href.match(new RegExp("/\?".concat(arguments[x],"=","([^\n&]*)")))[1])
                return data;
    }


example:
data = GET("id","name","foo");
query string : ?id=3&name=jet&foo=b
returns:
    data[0] // 3
    data[1] // jet
    data[2] // b
or
    alert(GET("id")[0]) // return 3
AnswerKenntnis:
Roshambo jQuery method wasn't taking care of decode URL

http://snipplr.com/view/26662/get-url-parameters-with-jquery--improved/

Just added that capability also while adding in  the return statement 

return decodeURIComponent(results[1].replace(/\+/g, " ")) || 0;


Now you can find the updated gist:

$.urlParam = function(name){
var results = new RegExp('[\\?&]' + name + '=([^&#]*)').exec(window.location.href);
if (!results) { return 0; }
return decodeURIComponent(results[1].replace(/\+/g, " ")) || 0;
}
AnswerKenntnis:
Doesn't have enough reputation for comment, sigh.

Here's my edit to this excellent answer - with added ability to parse query strings with keys without values.

var url = 'http://sb.com/reg/step1?param';
var qs = (function(a) {
    if (a == "") return {};
    var b = {};
    for (var i = 0; i < a.length; ++i) {
        var p=a[i].split('=', 2);
        if (p[1]) p[1] = decodeURIComponent(p[1].replace(/\+/g, " "));
        b[p[0]] = p[1];
    }
    return b;
})((url.split('?'))[1].split('&'));


IMPORTANT! Parameter for that func in last line is different, it's just example how one can pass arbitrary url to it. You can use last line from Bruno answer to parse current url.

So what exactly changed? With url http://sb.com/reg/step1?param= results will be same. But with url http://sb.com/reg/step1?param Bruno solution returns object without keys, while mine returns object with key param and undefined value.
AnswerKenntnis:
From the MDN:

function loadPageVar (sVar) {
┬á┬áreturn unescape(window.location.search.replace(new RegExp("^(?:.*[&\\?]" + escape(sVar).replace(/[\.\+\*]/g, "\\$&") + "(?:\\=([^&]*))?)?.*$", "i"), "$1"));
}

alert(loadPageVar("name"));
AnswerKenntnis:
This is a function I created a while back and I'm quite happy with. It is not case sensitive - which is handy. Also, if the requested QS doesn't exist, it just returns an empty string.

I use a compressed version of this. I'm posting uncompressed for the novice types to better explain what's going on.

I'm sure this could be optimized or done differently to work faster, but it's always worked great for what I need.

Enjoy.

    function getQSP(sName, sURL) {
        var theItmToRtn = "";
        var theSrchStrg = location.search;
        if (sURL) theSrchStrg = sURL;

        var sOrig = theSrchStrg;

        theSrchStrg = theSrchStrg.toUpperCase();
        sName = sName.toUpperCase();
        theSrchStrg = theSrchStrg.replace("?", "&")
        theSrchStrg = theSrchStrg + "&";
        var theSrchToken = "&" + sName + "=";
        if (theSrchStrg.indexOf(theSrchToken) != -1) {
            var theSrchTokenLth = theSrchToken.length;
            var theSrchTokenLocStart = theSrchStrg.indexOf(theSrchToken) + theSrchTokenLth;
            var theLocOfNextAndSign = theSrchStrg.indexOf("&", theSrchTokenLocStart);
            theItmToRtn = unescape(sOrig.substring(theSrchTokenLocStart, theLocOfNextAndSign));
        }
        return unescape(theItmToRtn);
    }
AnswerKenntnis:
I like this one:

// get an array with all querystring values
// example: var valor = getUrlVars()["valor"];
function getUrlVars() {
    var vars = [], hash;
    var hashes = window.location.href.slice(window.location.href.indexOf('?') + 1).split('&');
    for (var i = 0; i < hashes.length; i++) {
        hash = hashes[i].split('=');
        vars.push(hash[0]);
        vars[hash[0]] = hash[1];
    }
    return vars;
}


Works great for me.
AnswerKenntnis:
I developed a small library using techniques listed here to create an easy to use, drop-in solution to anyones troubles; It can be found here: 

https://github.com/Nijikokun/query-js

Usage

Fetching specific parameter/key:

query.get('param');


Using the builder to fetch the entire object:

var storage = query.build();
console.log(storage.param);


and tons more... check the github link for more examples.

Features


Caching on both decoding and parameters
Supports hash query strings #hello?page=3
Supports passing custom queries
Supports Array / Object Parameters user[]="jim"&user[]="bob"
Supports empty management &&
Supports declaration parameters without values name&hello="world"
Supports repeated parameters param=1&param=2
Clean, compact, and readable source 4kb
AMD, Require, Node support
AnswerKenntnis:
I'm gonna throw my hat in the ring - I needed an object from the query string, and I hate lots of code. may not be the most robust in the universe but it's just a few lines of code.

var q = {};
location.href.split('?')[1].split('&').forEach(function(i){
    q[i.split('=')[0]]=i.split('=')[1];
});


a URL like this.htm?hello=world&foo=bar will create:

{hello:'world', foo:'bar'}
AnswerKenntnis:
http://someurl.com?key=value&keynovalue&keyemptyvalue=&&keynovalue=nowhasvalue#somehash



Regular key/value pair (?param=value)
Keys w/o value (?param : no equal sign or value)
Keys w/ empty value (?param= : equal sign, but no value to right of equal sign)
Repeated Keys (?param=1&param=2)
Removes Empty Keys (?&& : no key or value)


Code:


var queryString = window.location.search || '';
var keyValPairs = [];
var params      = {};
queryString     = queryString.substr(1);

if (queryString.length)
{
   keyValPairs = queryString.split('&');
   for (pairNum in keyValPairs)
   {
      var key = keyValPairs[pairNum].split('=')[0];
      if (!key.length) continue;
      if (typeof params[key] === 'undefined')
         params[key] = [];
      params[key].push(keyValPairs[pairNum].split('=')[1]);
   }
}



How to Call:


params['key'];  // returns an array of values (1..n)



Output:


key            ["value"]
keyemptyvalue  [""]
keynovalue     [undefined, "nowhasvalue"]
AnswerKenntnis:
tl;dr solution using vanilla javascript

*To access different parts of url use location.(search|hash)

var queryDict = {}
location.search.substr(1).split("&").forEach(function(item) {queryDict[item.split("=")[0]] = item.split("=")[1]})



Handles empty keys correctly.
Overrides multi-keys with last value found.


"?a=1&b=2&c=3&d&e&a=5"
> queryDict
a: "5"
b: "2"
c: "3"
d: undefined
e: undefined


multi-valued keys

Simple key check (item in dict) ? dict.item.push(val) : dict.item = [val,]

var qd = {}
location.search.substr(1).split("&").forEach(function(item) {(item.split("=")[0] in qd) ? qd[item.split("=")[0]].push(item.split("=")[1]) : qd[item.split("=")[0]] = [item.split("=")[1],]})



Now returns arrays instead.
Access values by qd.key[index] or qd[key][index]


> qd
a: ["1", "5"]
b: ["2"]
c: ["3"]
d: [undefined]
e: [undefined]

> qd.a[1]    // "5"
> qd["a"][1] // "5"


encoded characters?

Enclose the item.split("=")[1] by decodeURIComponent(item.split("=")[1])

"?a=1&b=2&c=3&d&e&a=5&a=t%20e%20x%20t&e=http%3A%2F%2Fw3schools.com%2Fmy%20test.asp%3Fname%3Dst├Ñle%26car%3Dsaab"
> qd
a: ["1", "5", "t e x t"]
b: ["2"]
c: ["3"]
d: [undefined]
e: [undefined, "http://w3schools.com/my test.asp?name=st├Ñle&car=saab"]
AnswerKenntnis:
function GetQueryStringParams(sParam)
{
    var sPageURL = window.location.search.substring(1);
    var sURLVariables = sPageURL.split('&');

    for (var i = 0; i < sURLVariables.length; i++)
    {
        var sParameterName = sURLVariables[i].split('=');
        if (sParameterName[0] == sParam)
        {
            return sParameterName[1];
        }
    }
}ΓÇï


And this is how you can use this function assuming the URL is


  http://dummy.com/?stringtext=jquery&stringword=jquerybyexample


var tech = GetQueryStringParams('stringtext');
var blog = GetQueryStringParams('stringword');
AnswerKenntnis:
Here is my version of query string parsing code on github

It's "prefixed" with jquery.*, but the parsing function itself don't use jQuery. Its pretty fast but still open for few simple performance optimizations.

Also it supports list & hash-tables encoding in URL, like: 

arr[]=10&arr[]=20&arr[]=100


or

hash[key1]=hello&hash[key2]=moto&a=How%20are%20you
AnswerKenntnis:
One line code to get Query 

var value = location.search.match(new RegExp(key + "=(.*?)($|\&)", "i"))[1];
QuestionKenntnis:
Why is subtracting these two times (in 1927) giving a strange result?
qn_description:
If I run the following program, which parses two date strings referencing times one second apart and compares them:

public static void main(String[] args) throws ParseException {
    SimpleDateFormat sf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");  
    String str3 = "1927-12-31 23:54:07";  
    String str4 = "1927-12-31 23:54:08";  
    Date sDt3 = sf.parse(str3);  
    Date sDt4 = sf.parse(str4);  
    long ld3 = sDt3.getTime() /1000;  
    long ld4 = sDt4.getTime() /1000; 
    System.out.println(ld3);  
    System.out.println(ld4);  
    System.out.println(ld4-ld3);
}


The output is:

-1325491905
-1325491552
353


Why is ld4-ld3 not 1 (as I would expect from the one-second difference in the times), but 353?

If I change the dates to times one second later:

String str3 = "1927-12-31 23:54:08";  
String str4 = "1927-12-31 23:54:09";  


Then ld4-ld3 will be 1



UPDATE

Java version:

java version "1.6.0_22"
Java(TM) SE Runtime Environment (build 1.6.0_22-b04)
Dynamic Code Evolution Client VM (build 0.2-b02-internal, 19.0-b04-internal, mixed mode)


Timezone(TimeZone.getDefault()):

sun.util.calendar.ZoneInfo[id="Asia/Shanghai",
offset=28800000,dstSavings=0,
useDaylight=false,
transitions=19,
lastRule=null]

Locale(Locale.getDefault()): zh_CN
AnswersKenntnis
AnswerKenntnis:
It's a time zone change on December 31st in Shanghai.

See this page for details of 1927 in Shanghai. Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds. So "1927-12-31 23:54:08" actually happened twice, and it looks like Java is parsing it as the later possible instant for that local date/time - hence the difference.

Just another episode in the often weird and wonderful world of time zones.

EDIT: Stop the press! History changes...

The original question would no longer demonstrate quite the same behaviour, if rebuilt with  version 2013a of TZDB. In 2013a, the result would be 358 seconds, with a transition time of 23:54:03 instead of 23:54:08.

I only noticed this because I'm collecting questions like this in Noda Time, in the form of unit tests... The test has now been changed, but it just goes to show - not even historical data is safe.

EDIT: To answer Ken Kin's question around a transition at 1900... it looks like the Java timezone implementation treats all time zones as simply being in their standard time for any instant before the start of 1900 UTC:

import java.util.TimeZone;

public class Test {
    public static void main(String[] args) throws Exception {
        long startOf1900Utc = -2208988800000L;
        for (String id : TimeZone.getAvailableIDs()) {
            TimeZone zone = TimeZone.getTimeZone(id);
            if (zone.getRawOffset() != zone.getOffset(startOf1900Utc - 1)) {
                System.out.println(id);
            }
        }
    }
}


The code above produces no output on my Windows machine. So any time zone which has any offset other than its standard one at the start of 1900 will count that as a transition. TZDB itself has some data going back earlier than that, and doesn't rely on any idea of a "fixed" standard time (which is what getRawOffset assumes to be a valid concept) so other libraries needn't introduce this artificial transition.
AnswerKenntnis:
You've encountered a local time discontinuity:


  When local standard time was about to reach Sunday, 1. January 1928,
  00:00:00 clocks were turned backward 0:05:52 hours to Saturday, 31.
  December 1927, 23:54:08 local standard time instead


This is not particularly strange and has happened pretty much everywhere at one time or another as timezones were switched or changed due to political or administrative actions.
AnswerKenntnis:
The moral of this strangeness is:


Use dates and times in UTC wherever possible.
If you can not display a date or time in UTC, always indicate the time-zone.
If you can not require an input date/time in UTC, require an explicitly indicated time-zone.
AnswerKenntnis:
I ran your code, the output is:

-1325466353
-1325466352
1


And:

-1325466352
-1325466351
1


on:

$ java -version
java version "1.6.0_20"
OpenJDK Runtime Environment (IcedTea6 1.9.7) (fedora-52.1.9.7.fc14-i386)
OpenJDK Client VM (build 19.0-b09, mixed mode)
AnswerKenntnis:
When incrementing time you should convert back to UTC and then add or subtract. Use the local time only for display.

This way you will be able to walk through any periods where hours or minutes happen twice.

If you converted to UTC, add each second, and convert to local time for display. You would go through 11:54:08 p.m. LMT - 11:59:59 p.m. LMT and then 11:54:08 p.m. CST - 11:59:59 p.m. CST.
AnswerKenntnis:
Instead of converting each date, you use the following code 

long i = (sDt4.getTime() - sDt3.getTime()) / 1000;
System.out.println(i);


And see the result is:

1
AnswerKenntnis:
I'm sorry to say that, but this question is not valid any more.

JDK 6 has fixed this as an issue two years ago, and JDK 7 just recently in update 25.

The described discontinuity was obviously considered a bug by Oracle and is not there any more.
AnswerKenntnis:
As explained by others, there's a time discontinuity there. There are two possible timezone offsets for 1927-12-31 23:54:08 at Asia/Shanghai, but only one offset for 1927-12-31 23:54:07. So, depending on which offset is used, there's either a one second difference or a 5 minutes and 53 seconds difference.

This slight shift of offsets, instead of the usual one-hour daylight savings (summer time) we are used to, obscures the problem a bit.

Note that the 2013a update of the timezone database moved this discontinuity a few seconds earlier, but the effect would still be observable.

The new java.time package on Java 8 let use see this more clearly, and provide tools to handle it. Given:

DateTimeFormatterBuilder dtfb = new DateTimeFormatterBuilder();
dtfb.append(DateTimeFormatter.ISO_LOCAL_DATE);
dtfb.appendLiteral(' ');
dtfb.append(DateTimeFormatter.ISO_LOCAL_TIME);
DateTimeFormatter dtf = dtfb.toFormatter();
ZoneId shanghai = ZoneId.of("Asia/Shanghai");

String str3 = "1927-12-31 23:54:07";  
String str4 = "1927-12-31 23:54:08";  

ZonedDateTime zdt3 = LocalDateTime.parse(str3, dtf).atZone(shanghai);
ZonedDateTime zdt4 = LocalDateTime.parse(str4, dtf).atZone(shanghai);

Duration durationAtEarlierOffset = Duration.between(zdt3.withEarlierOffsetAtOverlap(), zdt4.withEarlierOffsetAtOverlap());

Duration durationAtLaterOffset = Duration.between(zdt3.withLaterOffsetAtOverlap(), zdt4.withLaterOffsetAtOverlap());


Then durationAtEarlierOffset will be one second, while durationAtLaterOffset will be five minutes and 53 seconds.

Also, these two offsets are the same:

// Both have offsets +08:05:52
ZoneOffset zo3Earlier = zdt3.withEarlierOffsetAtOverlap().getOffset();
ZoneOffset zo3Later = zdt3.withLaterOffsetAtOverlap().getOffset();


But these two are different:

// +08:05:52
ZoneOffset zo4Earlier = zdt4.withEarlierOffsetAtOverlap().getOffset();

// +08:00
ZoneOffset zo4Later = zdt4.withLaterOffsetAtOverlap().getOffset();


You can see the same problem comparing 1927-12-31 23:59:59 with 1928-01-01 00:00:00, though, in this case, it is the earlier offset that produce the longer divergence, and it is the earlier date that has two possible offsets.

Another way to approach this is to check whether there's a transition going on. We can do this like this:

// Null
ZoneOffsetTransition zot3 = shanghai.getRules().getTransition(ld3.toLocalDateTime);

// An overlap transition
ZoneOffsetTransition zot4 = shanghai.getRules().getTransition(ld3.toLocalDateTime);


You can check whether the transition is an overlap - in which case there's more than one valid offset for that date/time - or a gap - in which case that date/time is not valid for that zone id - by using the isOverlap() and isGap() methods on zot4.

I hope this helps people handle this sort of issue once Java 8 becomes widely available, or to those using Java 7 who adopt the JSR 310 backport.
QuestionKenntnis:
The Python yield keyword explained
qn_description:
What is the use of the yield keyword in Python? What does it do?

For example, I'm trying to understand this code (**):

def node._get_child_candidates(self, distance, min_dist, max_dist):
    if self._leftchild and distance - max_dist < self._median:
        yield self._leftchild
    if self._rightchild and distance + max_dist >= self._median:
        yield self._rightchild  


And this is the caller:

result, candidates = list(), [self]
while candidates:
    node = candidates.pop()
    distance = node._get_dist(obj)
    if distance <= max_dist and distance >= min_dist:
        result.extend(node._values)
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))
return result


What happens when the method _get_child_candidates is called?
A list is returned? A single element is returned? Is it called again? When subsequent calls do stop?

** The code comes from Jochen Schulz (jrschulz), who made a great Python library for metric spaces. This is the link to the complete source: Module mspace.
AnswersKenntnis
AnswerKenntnis:
To understand what yield does, you must understand what generators are. And before generators come iterables.

Iterables

When you create a list, you can read its items one by one, and it's called iteration:

>>> mylist = [1, 2, 3]
>>> for i in mylist:
...    print(i)
1
2
3


Mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:

>>> mylist = [x*x for x in range(3)]
>>> for i in mylist:
...    print(i)
0
1
4


Everything you can use "for... in..." on is an iterable: lists, strings, files...
These iterables are handy because you can read them as much as you wish, but you store all the values in memory and it's not always what you want when you have a lot of values.

Generators

Generators are iterators, but you can only iterate over them once. It's because they do not store all the values in memory, they generate the values on the fly:

>>> mygenerator = (x*x for x in range(3))
>>> for i in mygenerator:
...    print(i)
0
1
4


It is just the same except you used () instead of []. BUT, you can not perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.

Yield

Yield is a keyword that is used like return, except the function will return a generator.

>>> def createGenerator():
...    mylist = range(3)
...    for i in mylist:
...        yield i*i
...
>>> mygenerator = createGenerator() # create a generator
>>> print(mygenerator) # mygenerator is an object!
<generator object createGenerator at 0xb7555c34>
>>> for i in mygenerator:
...     print(i)
0
1
4


Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.

To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky :-)

Then, your code will be run each time the for uses the generator.

Now the hard part:

The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each other call will run the loop you have written in the function one more time, and return the next value, until there is no value to return.

The generator is considered empty once the function runs but does not hit yield anymore. It can be because the loop had come to an end, or because you do not satisfy a "if/else" anymore.

Your code explained

Generator:

# Here you create the method of the node object that will return the generator
def node._get_child_candidates(self, distance, min_dist, max_dist):

  # Here is the code that will be called each time you use the generator object:

  # If there is still a child of the node object on its left
  # AND if distance is ok, return the next child
  if self._leftchild and distance - max_dist < self._median:
      yield self._leftchild

  # If there is still a child of the node object on its right
  # AND if distance is ok, return the next child
  if self._rightchild and distance + max_dist >= self._median:
      yield self._rightchild

  # If the function arrives here, the generator will be considered empty
  # there is no more than two values: the left and the right children


Caller:

# Create an empty list and a list with the current object reference
result, candidates = list(), [self]

# Loop on candidates (they contain only one element at the beginning)
while candidates:

    # Get the last candidate and remove it from the list
    node = candidates.pop()

    # Get the distance between obj and the candidate
    distance = node._get_dist(obj)

    # If distance is ok, then you can fill the result
    if distance <= max_dist and distance >= min_dist:
        result.extend(node._values)

    # Add the children of the candidate in the candidates list
    # so the loop will keep running until it will have looked
    # at all the children of the children of the children, etc. of the candidate
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))

return result


This code contains several smart parts:


The loop iterates on a list but the list expands while the loop is being iterated :-) It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.
The extend() method is a list object method that expects an iterable and adds its values to the list.


Usually we pass a list to it:

>>> a = [1, 2]
>>> b = [3, 4]
>>> a.extend(b)
>>> print(a)
[1, 2, 3, 4]


But in your code it gets a generator, which is good because:


You don't need to read the values twice.
You can have a lot of children and you don't want them all stored in memory.


And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples and generators! This is called duck typing and is one of the reason why Python is so cool. But this is another story, for another question...

You can stop here, or read a little bit to see a advanced use of generator:

Controlling a generator exhaustion

>>> class Bank(): # let's create a bank, building ATMs
...    crisis = False
...    def create_atm(self):
...        while not self.crisis:
...            yield "$100"
>>> hsbc = Bank() # when everything's ok the ATM gives you as much as you want
>>> corner_street_atm = hsbc.create_atm()
>>> print(corner_street_atm.next())
$100
>>> print(corner_street_atm.next())
$100
>>> print([corner_street_atm.next() for cash in range(5)])
['$100', '$100', '$100', '$100', '$100']
>>> hsbc.crisis = True # crisis is coming, no more money!
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> wall_street_atm = hsbc.create_atm() # it's even true for new ATMs
>>> print(wall_street_atm.next())
<type 'exceptions.StopIteration'>
>>> hsbc.crisis = False # trouble is, even post-crisis the ATM remains empty
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> brand_new_atm = hsbc.create_atm() # build a new one to get back in business
>>> for cash in brand_new_atm:
...    print cash
$100
$100
$100
$100
$100
$100
$100
$100
$100
...


It can be useful for various things like controlling access to a resource.

Itertools, your best friend

The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?
Chain two generators? Group values in a nested list with a one liner? Map / Zip without creating another list?

Then just import itertools.

An example? Let's see the possible orders of arrival for a 4 horse race:

>>> horses = [1, 2, 3, 4]
>>> races = itertools.permutations(horses)
>>> print(races)
<itertools.permutations object at 0xb754f1dc>
>>> print(list(itertools.permutations(horses)))
[(1, 2, 3, 4),
 (1, 2, 4, 3),
 (1, 3, 2, 4),
 (1, 3, 4, 2),
 (1, 4, 2, 3),
 (1, 4, 3, 2),
 (2, 1, 3, 4),
 (2, 1, 4, 3),
 (2, 3, 1, 4),
 (2, 3, 4, 1),
 (2, 4, 1, 3),
 (2, 4, 3, 1),
 (3, 1, 2, 4),
 (3, 1, 4, 2),
 (3, 2, 1, 4),
 (3, 2, 4, 1),
 (3, 4, 1, 2),
 (3, 4, 2, 1),
 (4, 1, 2, 3),
 (4, 1, 3, 2),
 (4, 2, 1, 3),
 (4, 2, 3, 1),
 (4, 3, 1, 2),
 (4, 3, 2, 1)]


Understanding the inner mechanisms of iteration

Iteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).
Iterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.

More about it in this article about how does the for loop work.
AnswerKenntnis:
Shortcut to Grokking yield

When you see a function with yield statements, apply this easy trick to understand what will happen:


Insert a line result = [] at the start of the function.
Replace each yield expr with result.append(expr).
Insert a line return result at the bottom of the function.
Yay - no more yield statements! Read and figure out code.
Revert function to original definition.


This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different that what happens in the list based approach. In many cases the yield approach will be a lot more memory efficient and faster too. In other cases this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...

Don't confuse your Iterables, Iterators and Generators

First, the iterator protocol - when you write

for x in mylist:
    ...loop body...


Python performs the following two steps:


Gets an iterator for mylist:

Call iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).

[This is the step most people forget to tell you about]
Uses the iterator to loop over items:

Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.


The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).

Here mylist is an iterable because it implements the iterator protocol. In a user defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.

So that's the iterator protocol, many objects implement this protocol:


Built-in lists, dictionaries, tuples, sets, files.
User defined classes that implement __iter__().
Generators.


Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:

def f123():
    yield 1
    yield 2
    yield 3

for item in f123():
    print item


Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state, runs until the next yield statement and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits. 

So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.

Why Use Generators?

Usually you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps state in instance members and performs the next logical step in it's next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and be prone to bugs. Here generators provide a clean and easy solution.
AnswerKenntnis:
Think of it this way:

An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:

Original version:

def some_function():
    for i in xrange(4):
        yield i

for i in some_function():
    print i


This is basically what the python interpreter does with the above code:

class it:
    def __init__(self):
        #start at -1 so that we get 0 when we add 1 below.
        self.count = -1
    #the __iter__ method will be called once by the for loop.
    #the rest of the magic happens on the object returned by this method.
    #in this case it is the object itself.
    def __iter__(self):
        return self
    #the next method will be called repeatedly by the for loop
    #until it raises StopIteration.
    def next(self):
        self.count += 1
        if self.count < 4:
            return self.count
        else:
            #a StopIteration exception is raised
            #to signal that the iterator is done.
            #This is caught implicitly by the for loop.
            raise StopIteration 

def some_func():
    return it()

for i in some_func():
    print i


For more insight as to what's happening behind the scenes, the for loop can be rewritten to this:

iterator = some_func()
try:
    while 1:
        print iterator.next()
except StopIteration:
    pass


Does that make more sense or just confuse you more?  :)

EDIT: I should note that this IS an oversimplification for illustrative purposes.  :)

EDIT 2: Forgot to throw the StopIteration exception
AnswerKenntnis:
I feel like I post a link to this presentation every day:  David M. Beazly's Generator Tricks for Systems Programmers.  If you're a Python programmer and you're not extremely familiar with generators, you should read this.  It's a very clear explanation of what generators are, how they work, what the yield statement does, and it answers the question "Do you really want to mess around with this obscure language feature?"  

SPOILER ALERT.  The answer is:  Yes.  Yes, you do.
AnswerKenntnis:
The yield keyword is reduced to two simple facts:


If the compiler detects the yield keyword anywhere inside a function, that function no longer returns via the return statement. Instead, it immediately returns a lazy "pending list" object called a generator
A generator is iterable. What is an iterable? It's anything like a list or set or range or dict-view, with a built-in protocol for visiting each element in a certain order.


In a nutshell: a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out.

generator = myYieldingFunction(...)
x = list(generator)

   generator
       v
[x[0], ..., ???]

         generator
             v
[x[0], x[1], ..., ???]

               generator
                   v
[x[0], x[1], x[2], ..., ???]

                       StopIteration exception
[x[0], x[1], x[2]]     done

list==[x[0], x[1], x[2]]




Example

Let's define a function makeRange that's just like Python's range. Calling makeRange(n) RETURNS A GENERATOR:

def makeRange(n):
    # return 0,1,2,...,n-1
    i = 0
    while i < n:
        yield i
        i += 1

>>> makeRange(5)
<generator object makeRange at 0x19e4aa0>


To force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable):

>>> list(makeRange(5))
[0, 1, 2, 3, 4]




Comparing example to "just returning a list"

The above example can be thought of as merely creating a list which you append to and return:

# list-version                   #  # generator-version
def makeRange(n):                #  def makeRange(n):
    """return [0,1,2,...,n-1]""" #~     """return 0,1,2,...,n-1"""
    TO_RETURN = []               #>
    i = 0                        #      i = 0
    while i < n:                 #      while i < n:
        TO_RETURN += [i]         #~         yield i
        i += 1                   #      i += 1
    return TO_RETURN             #>

>>> makeRange(5)
[0, 1, 2, 3, 4]


There is one major difference though; see the last section.



How you might use generators

An iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so:

#                   _ITERABLE_
>>> [x+10 for x in makeRange(5)]
[10, 11, 12, 13, 14]


To get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop.

Please note: generators can actually be used for many more things, such as implementing coroutines or non-deterministic programming or other elegant things. However, the "lazy lists" viewpoint I present here is the most common use you will find.



Behind the scenes

This is how the "Python iteration protocol" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a "lazy, incremental list".

>>> x=iter(range(5))
>>> next(x)
0
>>> next(x)
1
>>> next(x)
2
>>> next(x)
3
>>> next(x)
4
>>> next(x)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration


The built-in function next() just calls the objects .next() function, which is a part of the "iteration protocol" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...



Minutiae

Normally, most people would not care about the following distinctions and probably want to stop reading here.

In Python-speak, an iterable is any object which "understands the concept of a for-loop" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax).

When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.

Thus, in the unlikely event that you are failing to do something like this...

> x = myRange(5)
> list(x)
[0, 1, 2, 3, 4]
> list(x)
[]


... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred.
AnswerKenntnis:
There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this:

def fib():
    last, cur = 0, 1
    while True: 
        yield cur
        last, cur = cur, last + cur


Then I can use it in other code like this:

for f in fib():
    if some_condition: break
    coolfuncs(f);


It really helps simplify some problems, and makes some things easier to work with.
AnswerKenntnis:
yield is just like return.  It returns whatever you tell it to.  The only difference is that the next time you call the function, execution starts from the last call to the yield statement.

In the case of your code, the function get_child_candidates is acting like an iterator so that when you extend your list, it adds one element at a time to the new list.

list.extend calls an iterator until it's exhausted.  In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list.
AnswerKenntnis:
Yield gives you a generator. 

def get_odd_numbers(i):
    return range(1, i, 2)
def yield_odd_numbers(i):
    for x in range(1, i, 2):
       yield x
foo = get_odd_numbers(10)
bar = yield_odd_numbers(10)
foo
[1, 3, 5, 7, 9]
bar
<generator object yield_odd_numbers at 0x1029c6f50>
bar.next()
1
bar.next()
3
bar.next()
5


As you can see, in the first case foo holds the entire list in memory at once. It's not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called. In the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object "remembers" where it was in the looping the last time you called it--this way, if you're using an iterable to (say) count to 50 billion, you don't have to count to 50 billion all at once and store the 50 billion numbers to count through. Again, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :)

This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things.
AnswerKenntnis:
It's returning a generator. I'm not particularly familiar with Python, but I believe it's the same kind of thing as C#'s iterator blocks if you're familiar with those.

There's an IBM article which explains it reasonably well (for Python) as far as I can see.

The key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - as if the generator method was paused. Now obviously you can't really "pause" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself.
AnswerKenntnis:
An example in plain language. I will provide a correspondence between high-level human concepts to low-level python concepts.

I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following:


I call you and tell you that I want a sequence of numbers which is produced in a specific way, and I let you know what the algorithm is.
This step corresponds to defining the generator function, i.e. the function containing a yield.
Sometime later, I tell you, "ok, get ready to tell me the sequence of numbers".
This step corresponds to calling the generator function which returns a generator object. Note that you don't tell me any numbers yet, you just grab your paper and pencil.
I ask you, "tell me the next number", and you tell me the first number; after that, you wait for me to ask you for the next number. It's your job to remember where you were, what numbers you have already said, what is the next number. I don't care about the details.
This step corresponds to calling .next() on the generator object.
ΓÇª repeat previous step, untilΓÇª
eventually, you might come to an end. You don't tell me a number, you just shout, "hold your horses! I'm done! No more numbers!"
This step corresponds to the generator object ending its job, and raising a StopIteration exception The generator function does not need to raise the exception, it's raised automatically when the function ends or issues a return.


This is what a generator does (a function that contains a yield); it starts executing, pauses whenever it does a yield, and when asked for a .next() value it continues from the point it was last. It fits perfectly by design with the iterator protocol of python, which describes how to sequentially request for values.

The most famous user of the iterator protocol is the for command in python. So, whenever you do a:

for item in sequence:


it doesn't matter if sequence is a list, a string, a dictionary or a generator object like described above; the result is the same: you read items off a sequence one by one.

Note that defining a function which contains a yield keyword is not the only way to create a generator; it's just the easiest way to create one.

For more accurate information, read about iterator types, the yield statement and generators in the Python documentation.
AnswerKenntnis:
For those who prefer a minimal working example, meditate on this interactive Python session:

>>> def f():
...   yield 1
...   yield 2
...   yield 3
... 
>>> g = f()
>>> for i in g:
...   print i
... 
1
2
3
>>> for i in g:
...   print i
... 
>>> # Note that this time nothing was printed
AnswerKenntnis:
There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators.  Here is the PL theory answer:

The yield statement in python returns a generator.  A generator in python is a function that returns continuations (and specifically a type of coroutines, but continuations represent the more general mechanism to understand what is going on).

Continuations in programming languages theory are a much more fundamental kind of computation, but they are not often used because they are extremely hard to reason about and also very difficult to implement.  But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state are saved the current values of variables and the operations that have yet to be performed, and so on. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out.

Continuations, in this more general form, can be implemented in two ways. In the call/cc way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored.

In continuation passing style (CPS), continuations are just normal (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards.

The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.



Now let's talk about generators in python. Generators are a specific subtype of continuation. Whereas continuations are able in general to save the state of a computation (i.e., the program's call stack), generators are only able to save the state of an iterable. Although, saying that a generator can only save the state of an iterable is slightly misleading. For instance

def f():
  while True:
    yield 4


clearly isn't a conventional iterable in the sense of for x in y: do_something(), but it can still be iterated over, the iterator will just return 4 forever.

To reiterate the last point: Continuations can save the state of a program's stack and generators can save the state of iterables. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand this page about continuations and call/cc).

But you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style: 

Whenever yield is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode  (i.e., not pseudocode but not code) the generator's next method is basically as follows: 

class Generator():
  def __init__(self,iterable,generatorfun):
    self.next_continuation = lambda:generatorfun(iterable)

  def next(self):
    value, next_continuation = self.next_continuation()
    self.next_continuation = next_continuation
    return value


where yield keyword is actually syntactic sugar for the real generator function, basically something like:

def generatorfun(iterable):
  if len(iterable) == 0:
    raise StopIteration
  else:
    return (iterable[0], lambda:generatorfun(iterable[1:]))


Remember that this is just pseudocode and the actual implementation of generators in python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the yield keyword.
AnswerKenntnis:
Here are some Python examples of how to actually implement generators as if Python did not provide syntactic sugar for them (or in a language without native syntax, like JavaScript). Snippets from that link is below.

As a Python generator:

from itertools import islice

def fib_gen():
    a, b = 1, 1
    while True:
        yield a
        a, b = b, a + b

assert [1, 1, 2, 3, 5] == list(islice(fib_gen(), 5))


Using lexical closures instead of generators

def ftake(fnext, last):
    return [fnext() for _ in xrange(last)]

def fib_gen2():
    #funky scope due to python2.x workaround
    #for python 3.x use nonlocal
    def _():
        _.a, _.b = _.b, _.a + _.b
        return _.a
    _.a, _.b = 0, 1
    return _

assert [1,1,2,3,5] == ftake(fib_gen2(), 5)


Using object closures instead of generators (because ClosuresAndObjectsAreEquivalent)

class fib_gen3:
    def __init__(self):
        self.a, self.b = 1, 1

    def __call__(self):
        r = self.a
        self.a, self.b = self.b, self.a + self.b
        return r

assert [1,1,2,3,5] == ftake(fib_gen3(), 5)
AnswerKenntnis:
I was going to post "read page 19 of Beazley's 'Python: Essential Reference' for a quick description of generators", but so many others have posted good descriptions already.

Also, note that yield can be used in coroutines as the dual of their use in generator functions.  Although it isn't the same use as your code snippet, (yield) can be used as an expression in a function.  When a caller sends a value to the method using the send() method, then the coroutine will execute until the next (yield) statement is encountered.

Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the yield statement in functions.
AnswerKenntnis:
While a lot of answers show why you'd use a yield to create a generator, there are more uses for yield.  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using yield to create a generator.

To help understand what a yield does in the following code, you can use your finger to trace the cycle through any code that has a yield.  Every time your finger hits the yield, you have to wait for a next or a send to be entered.  When a next is called, you trace through the code until you hit the yieldΓÇª the code on the right of the yield is evaluated and returned to the callerΓÇª then you wait.  When next is called again, you perform another loop through the code.  However, you'll note that in a coroutine, yield can also be used with a sendΓÇª which will send a value from the caller into the yielding function. If a send is given, then yield receives the value sent, and spits it out the left hand sideΓÇª then the trace through the code progresses until you hit the yield again (returning the value at the end, as if next was called).

For example:

>>> def coroutine():
...     i = -1
...     while True:
...         i += 1
...         val = (yield i)
...         print("Received %s" % val)
...
>>> sequence = coroutine()
>>> sequence.next()
0
>>> sequence.next()
Received None
1
>>> sequence.send('hello')
Received hello
2
>>> sequence.close()
AnswerKenntnis:
Here is a mental image of what yield does.

I like to think of a thread as having a stack (even if it's not implemented that way).

When a normal function is called, it puts its local variables on the stack, does some computation, returns and clears the stack. The values of its local variables are never seen again.

With a yield function, when it's called first, it similarly adds its local variables to the stack, but then takes its local variables to a special hideaway instead of clearing them, when it returns via yield. A possible place to put them would be somewhere in the heap.

Note that it's not the function any more, it's a kind of an imprint or ghost of the function that the for loop is hanging onto.

When it is called again, it retrieves its local variables from its special hideaway and puts them back on the stack and computes, then hides them again in the same way.
AnswerKenntnis:
From a programming viewpoint, the iterators are implemented as thunks 

http://en.wikipedia.org/wiki/Thunk_(functional_programming)

To implement thunks (also called anonymous functions), one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to "messages".

http://en.wikipedia.org/wiki/Message_passing

"next" is a message sent to a closure, created by "iter" call.
AnswerKenntnis:
here is simple example with result:

def isPrimeNumber(n):
        print "isPrimeNumber({}) call".format(n)
        if n==1:
            return False
        for x in range(2,n):
            if n % x == 0:
                return False
        return True


def primes (n=1):
        while(True):
            print "loop step ---------------- {}".format(n)
            if isPrimeNumber(n): yield n
            n += 1

for n in primes():
        if n> 10:break
        print "wiriting result {}".format(n)   


output :

loop step ---------------- 1
isPrimeNumber(1) call
loop step ---------------- 2
isPrimeNumber(2) call
loop step ---------------- 3
isPrimeNumber(3) call
wiriting result 3
loop step ---------------- 4
isPrimeNumber(4) call
loop step ---------------- 5
isPrimeNumber(5) call
wiriting result 5
loop step ---------------- 6
isPrimeNumber(6) call
loop step ---------------- 7
isPrimeNumber(7) call
wiriting result 7
loop step ---------------- 8
isPrimeNumber(8) call
loop step ---------------- 9
isPrimeNumber(9) call
loop step ---------------- 10
isPrimeNumber(10) call
loop step ---------------- 11
isPrimeNumber(11) call


I am not an python developer but it looks to me "yield"  holds the position of program flow and next time  loop start from "yield" position. Seems like waiting at that position and just before that returning value outside and next time continue to work.

Seems to me interesting and nice ability :D
QuestionKenntnis:
How can I make a redirect page in jQuery/JavaScript?
qn_description:
How can I redirect the user from one page to another using jQuery?
AnswersKenntnis
AnswerKenntnis:
jQuery is not necessary, and window.location.replace(...) will best simulate an HTTP redirect.  

It is better than using window.location.href =, because replace() does not put the originating page in the session history, meaning the user won't get stuck in a never-ending back-button fiasco.  If you want to simulate someone clicking on a link, use location.href.  If you want to simulate an HTTP redirect, use location.replace.

For example:

// similar behavior as an HTTP redirect
window.location.replace("http://stackoverflow.com");

// similar behavior as clicking on a link
window.location.href = "http://stackoverflow.com";
AnswerKenntnis:
WARNING: This answer has been provided as a possible solution. Although, obviously, the pure JavaScript approach is the best one, as this requires jQuery.

var url = "http://stackoverflow.com";    
$(location).attr('href',url);
AnswerKenntnis:
You don't need jQuery to do just that:

window.location = "http://www.page-2.com";





  Note: This is similar to "clicking" a link and will record page change in browser's history.To replace current page in history, use location.replace
AnswerKenntnis:
It would help if you were a little more descriptive in what you are trying to do.  If you are trying to generate paged data, there are some options in how you do this.  You can generate separate links for each page that you want to be able to get directly to.

<a href='/path-to-page?page=1' class='pager-link'>1</a>
<a href='/path-to-page?page=2' class='pager-link'>2</a>
<span class='pager-link current-page'>3</a>
...


Note that the current page in the example is handled differently in the code and with CSS.

If you want the paged data to be changed via AJAX, this is where jQuery would come in.  What you would do is add a click handler to each of the anchor tags corresponding to a different page.  This click handler would invoke some jQuery code that goes and fetches the next page via AJAX and updates the table with the new data.  The example below assumes that you have a web service that returns the new page data.

$(document).ready( function() {
    $('a.pager-link').click( function() {
        var page = $(this).attr('href').split(/\?/)[1];
        $.ajax({
            type: 'POST',
            url: '/path-to-service',
            data: page,
            success: function(content) {
               $('#myTable').html(content);  // replace
            }
        });
        return false; // to stop link
    });
});
AnswerKenntnis:
This works for every browser:

window.location.href = 'your_url';


Good luck!
AnswerKenntnis:
All of these answers are correct, but I'll post this for those who might run into the same strange issue that I did.  I was having an issue with HTTP_REFERER getting lost when using simply location.href.

In IE8 and lower, location.href (or any & all variations location - will lose referrer), which for secure sites is important to maintain, because testing for it (url pasting/ session / etc) can be helpful in telling whether a request is legitimate. 
(Note :: there are also ways to work-around / spoof these referrers, as noted by droop's link in the comments)

My cross-browser fix is this simple function. Assuming you, of course, are worried about losing HTTP_REFERER as I stated. (otherwise you can just use location.href etc)

Usage: Redirect('anotherpage.aspx');

function Redirect (url) {
    var ua        = navigator.userAgent.toLowerCase(),
        isIE      = ua.indexOf('msie') !== -1,
        version   = parseInt(ua.substr(4, 2), 10);

    // IE8 and lower
    if (isIE && version < 9) {
        var link = document.createElement('a');
        link.href = url;
        document.body.appendChild(link);
        link.click();
    }

    // All other browsers
    else { window.location.href = url; }
}
AnswerKenntnis:
I also think that location.replace(url) is the best way, but if you want to notify the search engines about you redirection (they don't analyze javascript to see the redirection) you should add the rel="canonical" meta tag to your website.

Adding a noscript section with a html refresh meta tag in it, is also a good solution. I suggest you to use this javascript redirection tool to create redirections. It also have an IE support to pass the http referrer.

A sample code without delay look like this:

<!--Pleace this snippet right after opening the head tag to make it work properly-->
<!--REDIRECTING STARTS-->
<link rel="canonical" href="https://yourdomain.com"/>
<noscript>
<meta http-equiv="refresh" content="0;URL=https://yourdomain.com">
</noscript>
<script type="text/javascript">
    var url = "https://yourdomain.com";

    // IE8 and lower fix
    if (navigator.userAgent.match(/MSIE\s(?!9.0)/))
    {
        var referLink = document.createElement("a");
        referLink.href = url;
        document.body.appendChild(referLink);
        referLink.click();
    }

    // All other browsers
    else { window.location.replace(url); }
</script>
<!--REDIRECTING ENDS-->
AnswerKenntnis:
var url = 'asdf.html';
window.location.href = url;
AnswerKenntnis:
You can do that without jQuery as:

window.location = "http://yourdomain.com";


And if you want only jQuery then you can do it like :

$jq(window).attr("location","http://yourdomain.com");
AnswerKenntnis:
But if some one wants to redirect back to home page then he may use the following snippet.

window.location = window.location.host


It would be helpful if you have three different environments as development, staging, and production.

You can explore this window or window.location object by just putting these words in Chrome Console or Firebug's Console.
AnswerKenntnis:
$jq(window).attr("location","http://google.fr");


This version works well with jQuery 1.6.2.
AnswerKenntnis:
JavaScript provides you many methods to retrieve and change the current URL which is displayed in browser's address bar. All these methods uses the Location object, which is  a property of the Window object. You can create a new Location object that has the current URL as follows..

var currentLocation = window.location;


Basic Structure of a URL

<protocol>//<hostname>:<port>/<pathname><search><hash>





Protocol -- Specifies the protocol name be used to access the resource on the Internet. (HTTP (without SSL) or HTTPS (with SSL))
hostname -- Host name specifies the host that owns the resource. For example, www.stackoverflow.com. A server provides services using the name of the host.
port -- A port number used to recognize a specific process to which an Internet or other network message is to be forwarded when it arrives at a server.
pathname -- The path gives info about the specific resource within the host that the Web client wants to access. For example, stackoverflow.com/index.html.
query --  A query string follows the path component, and provides a string of information that the resource can utilize for some purpose (for example, as parameters for a search or as data to be processed). 
hash -- The anchor portion of a URL, includes the hash sign (#).


With these Location object properties you can access all of these URL components


hash   -Sets or returns the anchor portion of a URL.  
host   -Sets
or returns the hostname and port of a URL.   
hostname   -Sets or
returns the hostname of a URL.   
href   -Sets or returns the entire
URL.  
pathname   -Sets or returns the path name of a URL.   
port -Sets or returns the port number the server uses for a URL.  
protocol   -Sets or returns the protocol of a URL.   
search   -Sets
or returns the query portion of a URL


Now If you want to change a page or redirect the user to some other page you can use the href property of the Location object like this

You can use the href property of the Location object.

window.location.href = "http://www.stackoverflow.com";


Demo Fiddle

Location Object also have these three methods


assign() -- Loads a new document.
reload() -- Reloads the current document.
replace() -- Replaces the current document with a new one


You can use assign() and replace methods also to redirect to other pages like these

location.assign("http://www.stackoverflow.com");

location.replace("http://www.stackoverflow.com");


How assign() and replace() differs -- The difference between replace() method and assign() method(), is that replace() removes the URL of the current document from the document history, means it is not possible to use the "back" button to navigate back to the original document. So Use the assign() method if you want to load a new document, andwant to give the option to navigate back to the original document.

You can change the location object href property using jQuery also  like this

$(location).attr('href',url);


And hence you can redirect the user to some other url.
AnswerKenntnis:
On your click function just add 

window.location.href = "the url where you want to redirect";
$('#id').click(function(){
    window.location.href = "http://www.google.com";
});
AnswerKenntnis:
1.

    window.location.href="login.jsp?backurl="+window.location.href;


2.

    window.history.back(-1);


3.

    window.navigate("top.jsp");


4.

    self.location="top.htm";


5.

    top.location="error.jsp";


6.

    window.location = window.location.host;


7.

    $(location).attr('href',"http://www.google.com");           //jquery
AnswerKenntnis:
jQuery is not needed. You can do this:

window.open("URL","_self","","")


It is that easy!

The best way to initiate an HTTP request is with document.loacation.href.replace('URL').
AnswerKenntnis:
So, the question is how to make a redirect page, and not how to redirect to a website?

You only need to use Javascript for this. Here is some tiny code that will create a dynamic redirect page.

<script>
    var url = window.location.search.split('url=')[1]; // Get the url after ?url=
    if( url ) window.location.replace(url);
</script>


So say you just put this snippet into a redirect/index.html file on your website you can use it like so.


  http://www.mywebsite.com/redirect?url=http://stackoverflow.com


And if you go to that link it will automatically redirect you to stackoverflow.com.


  Link to Documentation


And that's how you make a Simple redirect page with javasctript

Edit:

There is also 1 thing to note, I have added window.location.replace in my code because I think it suits a redirect page, but, you must know that when using window.location.replace and you get redirected, when you press the back button in your browser it will not got back to the redirect page and it will go back to the page before it, take a look at this little demo thing

Example:


  The process: store home => redirect page to google => google
  
  When at google: google => back button in browser => store home


So, if this suits your needs then everything should be fine, if you want to include the redirect page in the browser history replace this 

if( url ) window.location.replace(url);


with 

if( url ) window.location.href = url;
AnswerKenntnis:
First write properly. You want to navigate within an application for another link from your application for another link. Here is the code:

window.location.href = "http://www.google.com";


And if you want to navigate pages within your application then I also have code, if you want.
AnswerKenntnis:
You need to put this line on your code

 $(location).attr('href',"http://stackoverflow.com");
AnswerKenntnis:
Write the below code after the PHP, HTML or jQuery section. If in the middle of the PHP or HTML section, then use the <script> tag.

location.href = "http://google.com"
AnswerKenntnis:
In JavaScript and jQuery we can use the following code to redirect the one page to another page:

window.location.href="http://google.com";
window.location.replace("page1.html");
AnswerKenntnis:
Here is a time-delay redirection. You can set the delay time to whatever you want:

    <!doctype html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <title>Your Document Title</title>
      <script type="text/javascript">
        function delayer(delay){
          onLoad=setTimeout('window.location.href = "http://www.google.com/"',delay);
        }   
      </script>
    </head>
    <body>
      <script>delayer(8000)</script>
      <div>You will be redirected in 8 seconds!</div>
    </body>
AnswerKenntnis:
<script type="text/javascript">
var url = "https://yourdomain.com";

// IE8 and lower fix
if (navigator.userAgent.match(/MSIE\s(?!9.0)/))
{
    var referLink = document.createElement("a");
    referLink.href = url;
    document.body.appendChild(referLink);
    referLink.click();
}

// All other browsers
else { window.location.replace(url); }
</script>
AnswerKenntnis:
Try this:

location.assign("http://www.google.com");

Reference
AnswerKenntnis:
Page redirect in jQuery/JavaScript using PHP pages

This is very simple. See this code:

function myfunction()
{
   var i = document.getElementById('Login').value;
   if (i == 'Login')
   {
       window.location = "login.php";
   }
   else
       window.location = "Logout.php";
}


If you want to give a complete URL as window.location = "www.stackoverflow.com";.
AnswerKenntnis:
Instead of redirecting, you can replace the window the user is on with the page you want to redirect to using JavaScript code like this:

window.open('url', '_self');


You can also do this:

window.location.href = "http://example.com";
AnswerKenntnis:
This is how I use it.

   window.location.replace('yourPage.aspx');   
   // If you're on root and redirection page is also on the root

   window.location.replace(window.location.host + '/subDirectory/yourPage.aspx');

   // If you're in sub directory and redirection page is also in some other sub directory.
AnswerKenntnis:
Check this link
// similar behavior as an HTTP redirect

window.location.replace("http://stackoverflow.com");


// similar behavior as clicking on a link

window.location.href = "http://stackoverflow.com";
AnswerKenntnis:
User

location.href = "http://google.com";


dont write window its optional.
AnswerKenntnis:
You can use it like in the following code where getGuestHouseRequestToForward is the request mapping (URL). You can also use your URL.

function savePopUp(){
    $.blockUI();
    $.ajax({
        url:"saveForwardingInformationForGuestHouse?roomType="+$("#roomType").val(),
        data: $("#popForm").serialize(),
        dataType: "json",
        error: (function() {
            alert("Server Error");
            $.unblockUI();
    }),
    success: function(map) {
        $("#layer1").hide();
        $.unblockUI();
        window.location = "getGuestHouseRequestToForward";
    }
});


This is for the same context of the application.
AnswerKenntnis:
Use the following code:

$("#id").click(function(){
   window.location.href("https://www.google.com");
});
QuestionKenntnis:
List of freely available programming books
qn_description:
I'm trying to amass a list of programming books that are freely available on the Internet. The books can be about a particular programming language or about computers in general. 

What are some freely available programming books on the Internet?
AnswersKenntnis
AnswerKenntnis:
As of October 2013, this list is now maintained on GitHub by Victor Felder (and you).
QuestionKenntnis:
How can I pass the string "Null" through WSDL (SOAP) from ActionScript 3 to a ColdFusion web service without receiving a "missing parameter error"?
qn_description:
We have an employee whose last name is Null. He kills our employee lookup application when his last name is used as the search term (which happens to be quite often now). The error received (thanks Fiddler!) is 

  <soapenv:Fault>
   <faultcode>soapenv:Server.userException</faultcode>
   <faultstring>coldfusion.xml.rpc.CFCInvocationException: [coldfusion.runtime.MissingArgumentException : The SEARCHSTRING parameter to the getFacultyNames function is required but was not passed in.]</faultstring>


Cute, huh?

The parameter's type is string.

I am using:


WSDL (SOAP).
Flex 3.5
ActionScript 3
ColdFusion 8


Note that the error DOES NOT occur when calling the webservice as an object from a ColdFusion page.
AnswersKenntnis
AnswerKenntnis:
Tracking it down 

At first I thought this was a coercion bug where null was getting coerced to "null" and a test of "null" == null was passing. It's not. I was close, but so very, very wrong. Sorry about that!

I've since done lots of fiddling on wonderfl.net and tracing through the code in mx.rpc.xml.*. At line 1795 of XMLEncoder (in the 3.5 source), in setValue, all of the XMLEncoding boils down to 

currentChild.appendChild(xmlSpecialCharsFilter(Object(value)));


which is essentially the same as:

currentChild.appendChild("null");


This code, according to my original fiddle, returns an empty XML element. But why?

 Cause 

According to commenter Justin Mclean on bug report FLEX-33664, the following is the culprit (see last two tests in my fiddle which verify this):

var nullXML:XML = <root>null</root>;
if(nullXML == null){
    //always branches here, as (nullXML == null) == true
}


When currentChild.appendChild is passed the string "null", it first converts it to a root XML element with text null, and then tests that element against the null literal. This is a weak equality test, so either the XML containing null is coerced to the null type, or the null type is coerced to a root xml element containing the string "null", and the test passes where it arguably should fail. One fix might be to always use strict equality tests when checking XML (or anything, really) for "nullness."

Solution
The only reasonable workaround I can think of, short of fixing this bug in every damn version of ActionScript, is to test fields for "null" and escape them as CDATA values. 

CDATA values are the most appropriate way to mutate an entire text value that would otherwise cause encoding/decoding problems. Hex encoding, for instance, is meant for individual characters. CDATA values are preferred when you're escaping the entire text of an element. The biggest reason for this is that it maintains human readability.
AnswerKenntnis:
The problem could be in Flex's SOAP encoder. Try extending the SOAP encoder in your Flex application and debug the program to see how the null value is handled. My guess is, it's passed as NaN (Not a Number). This will mess up SOAP message unmarshalling process sometime (most notably in JBoss 5 server...). I remember extending the SOAP encoder and performing an explicit check on how NaN is handled. 

(On a side note, are you expected to do something useful if employee id is Null, is this not an validation issue? I could be wrong, since I hardly know the requirement...)
AnswerKenntnis:
On the xkcd note, the Bobby Tables website has good advice for avoiding the improper interpretation of user data (in this case, the string "Null") in SQL queries in various languages, including ColdFusion.

It is not clear from the question that this is the source of the problem, and given the solution noted in a comment to the first answer (embedding the parameters in a structure) it seems likely that it was something else.
AnswerKenntnis:
@doc_180 had the right concept, except he is focused on numbers, whereas the original poster had issues with strings.  

The solution is to change the mx.rpc.xml.XMLEncoder file. This is line 121 

    if (content != null)
        result += content;


[I looked at Flex 4.5.1 SDK; line numbers may differ in other versions]

Basically, the validation fails because 'content is null' and therefore your argument is not added to the outgoing SOAP Packet; thus causing the missing parameter error.  

You have to extend this class to remove the validation.  Then there is a big snowball up the chain, modifying SOAPEncoder to use your modified XMLEncoder, and then modifying Operation to use your modified SOAPEncoder, and then moidfying WebService to use your alternate Operation class.  

I spent a few hours on it, but need to move on.  It'll probably take a day or two.

You may be able to just fix the XMLEncoder line and do some monkey patching to use your own class.  

I'll also add that if you switch to using RemoteObject/AMF with ColdFusion, the null is passed without problems.



11/16/2013 update:

I have one more recent addition to my last comment about RemoteObject/AMF.  If you are using CF10; then properties with a null value on an object are removed from the server side object.  So, you have to check for the properties existence before accessing it or you will get a runtime error.  Check like this:

<cfif (structKeyExists(arguments.myObject,'propertyName')>
 <!--- no property code --->
<cfelse>
 <!--- handle property  normally --->
</cfif>


This is a change in behavior from CF9; where the null properties would turn into empty strings.



Edit 12/6/2013

Since there was a question about how nulls are treated here is a quick sample application to demonstrate how a string "null" will relate to the reserved word null.

<?xml version="1.0" encoding="utf-8"?>
<s:Application xmlns:fx="http://ns.adobe.com/mxml/2009" 
               xmlns:s="library://ns.adobe.com/flex/spark" 
               xmlns:mx="library://ns.adobe.com/flex/mx" minWidth="955" minHeight="600" initialize="application1_initializeHandler(event)">
    <fx:Script>
        <![CDATA[
            import mx.events.FlexEvent;

            protected function application1_initializeHandler(event:FlexEvent):void
            {
                var s :String = "null";
                if(s != null){
                    trace('null string is not equal to null reserved word using the != condition');
                } else {
                    trace('null string is equal to null reserved word using the != condition');
                }

                if(s == null){
                    trace('null string is equal to null reserved word using the == condition');
                } else {
                    trace('null string is not equal to null reserved word using the == condition');
                }

                if(s === null){
                    trace('null string is equal to null reserved word using the === condition');
                } else {
                    trace('null string is not equal to null reserved word using the === condition');
                }

            }

        ]]>
    </fx:Script>
    <fx:Declarations>
        <!-- Place non-visual elements (e.g., services, value objects) here -->
    </fx:Declarations>
</s:Application>


The trace output is:


  null string is not equal to null reserved word using the != condition
  
  null string is not equal to null reserved word using the == condition
  
  null string is not equal to null reserved word using the === condition
AnswerKenntnis:
Translate all characters into their hex-entity equivalents. In this case, Null would be converted into &#4E;&#75;&#6C;&#6C;
AnswerKenntnis:
As a hack, you could consider having a special handling on the client side, converting 'Null' string to something that will never occur, for example, XXNULLXX and converting back on the server. 

It is not pretty, but it may solve the issue for such a boundary case.
AnswerKenntnis:
Stringifying a null value in actionscript will give the string "NULL".  My suspicion is that someone has decided that it is, therefore, a good idea to decode the string "NULL" as null, causing the breakage you see here -- probably because they were passing in null objects and getting strings in the database, when they didn't want that (so be sure to check for that kind of bug, too).
AnswerKenntnis:
Well, I guess that Flex' implementation of the SOAP Encoder seems to serialize null values incorrectly. Serializing them as a String Null doesn't seem to be a good solution. The formally correct version seems to be to pass a null value as:

<childtag2 xsi:nil="true" />


So the value of "Null" would be nothing else than a valid string, which is exactly what you are looking for.

I guess getting this fixed in Apache Flex shouldn't be that hard to get done. I would recommend opening a Jira issue or to contact the guys of the apache-flex mailinglist. However this would only fix the client side. I can't say if ColdFusion will be able to work with null values encoded this way.

See also Radu Cotescu's blog post How to send null values in soapUI requests.
AnswerKenntnis:
It's a kludge, but assuming there's a minimum length for SEARCHSTRING, for example 2 characters, substring the SEARCHSTRING parameter at the second character and pass it as two parameters instead: SEARCHSTRING1 ("Nu") and SEARCHSTRING2 ("ll").  Concatenate them back together when executing the query to the database.
QuestionKenntnis:
Undo 'git add' before commit
qn_description:
I mistakenly added files using the command 

git add file


I have not yet run git commit. Is there a way to undo this or remove these files from the commit?
AnswersKenntnis
AnswerKenntnis:
You can also

git reset <file>


which will remove it from the current index (the "about to be committed" area) without changing anything else. Note that git reset <file> is short for git reset HEAD <file>.

You can use git reset without any file name to undo all due changes. This can come handy when there are too many files to be listed one by one.
AnswerKenntnis:
You want:

git rm --cached <added_file_to_undo>


Reasoning:

When I was new this, I first tried

git reset .


(to undo my entire initial add), only to get this (not so) helpful message:

fatal: Failed to resolve 'HEAD' as a valid ref.


It turns out that this is because the HEAD ref (branch?) doesn't exist until after the first commit. That is, you'll run into the same beginner's problem as me if your workflow, like mine, was something like:


cd to my great new project directory to try out Git, the new hotness
git init
git add .
git status

... lots of crap scrolls by ...

=> Damn, I didn't want to add all of that.
google "undo git add"

=> find Stack Overflow - yay
git reset .

=>    fatal: Failed to resolve 'HEAD' as a valid ref.


It further turns out that there's a bug logged against the unhelpfulness of this in the mailing list.

And that the correct solution was right there in the Git status output (which, yes, I glossed over as 'crap)


...
# Changes to be committed:
#   (use "git rm --cached <file>..." to unstage)
...



And the solution indeed is to use git rm --cached FILE.

Note the warnings elsewhere here - git rm deletes your local working copy of the file, but not if you use --cached.  Here's the result of git help rm:


  --cached
      Use this option to unstage and remove paths only from the index.
      Working tree files, whether modified or not, will be left.


I proceed to use

git rm --cached .


to remove everything and start again. Didn't work though, because while add . is recursive, turns out rm needs -r to recurse. Sigh.

git rm -r --cached .


Okay, now I'm back to where I started. Next time I'm going to use -n to do a dry run and see what will be added:

git add -n .


I zipped up everything to a safe place before trusting git help rm about the --cached not destroying anything (and what if I misspelled it).
AnswerKenntnis:
If you type:

git status


git will tell you what is staged, etc, including instructions on how to unstage:

use "git reset HEAD <file>..." to unstage


I find git does a pretty good job of nudging me to do the right thing in situations like this.

Note: Recent git versions (1.8.4.x) have changed this message:

(use "git rm --cached <file>..." to unstage)
AnswerKenntnis:
To clarify: git add moves changes from the current working directory to the staging area (index). 

This process is called staging. So the most natural command to stage the changes (changed files) is the obvious one:

git stage


git add is just an easier to type alias for git stage

Pity there is no git unstage nor git unadd commands. The relevant one is harder to guess or remember,
but is pretty obvious:

git reset HEAD --


We can easily create an alias for this:

git config --global alias.unadd 'reset HEAD --'
git config --global alias.unstage 'reset HEAD --'


And finally, we have new commands:

git add file1
git stage file2
git unadd file2
git unstage file1


Personally I use even shorter aliases:

git a #for staging
git u #for unstaging
AnswerKenntnis:
git rm --cached . -r


will "un-add" everything you've added from your current directory recursively
AnswerKenntnis:
Run

git gui


and remove all the files manually or by selecting all of them and clicking on the unstage from commit button.
AnswerKenntnis:
An addition to the accepted anwser: if your mistakenly added file was huge, you'll probably notice that, even after removing it from the index with 'git reset', it still seems to occupy space in the .git directory. This is nothing to be worried about, the file is indeed still in the repository, but only as a "loose object", it will no be copied to other repositories (via clone, push), and the space will be eventually reclaimed - though perhaps not very soon. If you are anxious, you can run:

git gc --prune=now




Update (what follows is my attempt to clear some confusion that can arise from the most upvoted answers):

So, which of the followint is the real undo of git add? 

git reset HEAD <file> ?

git rm --cached <file>? 

Strictly speaking, and if I'm not mistaken: none. git add cannot be undone -safely, in general.

Let's recall first what git add <file> actually does:


If <file> was not previously tracked, git add  adds it to the cache, with its current content.
If <file> was already tracked, git add  saves the current content (snapshot, version) to the cache. In GIT, this action is still called add, (not mere update it), because two different versions (snapshots) of a file are regarded as two different items: hence, we are indeed adding a new item to the cache, to be eventually commited later.


In light of this, the question is slightly ambiguous: 


  I mistakenly added files using the command...


The OP's scenario seems to be the first one (untracked file),  we want the "undo" to remove the file (not just the current contents) from the tracked items. If this is the case, then it's ok to run  git rm --cached <file>.  

And we could also run git reset HEAD <file>. This is in general preferrable, because it works in both scenarios: it also does the undo when we wrongly added a version of an already tracked item.

But there are two caveats. 

First: There is (as pointed out in the answer) only one scenario in which git reset HEAD doesn't work, but git rm --cached does: a new repository (no commits). But, really, this a practically irrelevant case.

Second: Be aware that git reset HEAD  can't magically recover the previously cached file contents, it just resyncs it from the HEAD. If our misguided git add overwrote a previous staged uncommited version, we can't recover it. That's why, strictly speaking, we cannot undo.

Example:

$ git init
$ echo "version 1" > file.txt
$ git add file.txt   # first add  of file.txt
$ git commit -m 'first commit'
$ echo "version 2" > file.txt
$ git add  file.txt   # stage (don't commit) "version 2" of file.txt
$ git diff --cached file.txt
-version 1
+version 2
$ echo "version 3" > file.txt   
$ git diff  file.txt
-version 2
+version 3
$ git add  file.txt    # oops we didn't mean this
$ git reset HEAD file.txt  # undo ?
$ git diff --cached file.txt  # no dif, of course. stage == HEAD
$ git diff file.txt   # we have lost irrevocably "version 2"
-version 1
+version 3


Of course, this is not very critical if we just follow the usual lazy workflow of doing 'git add' only for adding new files (case 1), and we update new contents via the commit, git commit -a command.
AnswerKenntnis:
If you're on your initial commit and you can't use git reset, just declare "Git bankruptcy" and delete the .git folder and start over
AnswerKenntnis:
Git has commands for every action imaginable, but needs extensive knowledge to get things right and because of that it is counter-intuitive at best...

What you did before:


Changed a file and used git add ., or git add <file>.


What you want:


Remove the file from the index, but keep it versioned and left with uncommitted changes in working copy:

git reset head <file>

Reset the file to the last state from HEAD, undoing changes and removing them from the index:


    # Think `svn revert <file>` IIRC.
    git reset HEAD <file>
    git checkout <file>

    # If you have a `<branch>` named like `<file>`, use:
    git checkout -- <file>


This is needed since git reset --hard HEAD won't work with single files.


Remove <file> from index and versioning, keeping the un-versioned file with changes in working copy:

git rm --cached <file>

Remove <file> from working copy and versioning completely:

git rm <file>
AnswerKenntnis:
Use git add -i to remove just-added files from your upcoming commit.  Example:

Adding the file you didn't want:

$ git add foo
$ git status
# On branch master
# Changes to be committed:
#   (use "git reset HEAD <file>..." to unstage)
#
#       new file:   foo
#
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
# [...]#


Going into interactive add to undo your add (the commands typed at git here are "r" (revert), "1" (first entry in the list revert shows), 'return' to drop out of revert mode, and "q" (quit):

$ git add -i
           staged     unstaged path
  1:        +1/-0      nothing foo

*** Commands ***
  1: [s]tatus     2: [u]pdate     3: [r]evert     4: [a]dd untracked
  5: [p]atch      6: [d]iff       7: [q]uit       8: [h]elp
What now> r
           staged     unstaged path
  1:        +1/-0      nothing [f]oo
Revert>> 1
           staged     unstaged path
* 1:        +1/-0      nothing [f]oo
Revert>> 
note: foo is untracked now.
reverted one path

*** Commands ***
  1: [s]tatus     2: [u]pdate     3: [r]evert     4: [a]dd untracked
  5: [p]atch      6: [d]iff       7: [q]uit       8: [h]elp
What now> q
Bye.
$


That's it!  Here's your proof, showing that "foo" is back on the untracked list:

$ git status
# On branch master
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
# [...]
#       foo
nothing added to commit but untracked files present (use "git add" to track)
$
AnswerKenntnis:
git remove or git rm can be used for this, with the --cached flag. Try:

git help rm
AnswerKenntnis:
Maybe Git has evolved since you posted your question.

$> git --version
git version 1.6.2.1


Now, you can try:

git reset HEAD .


This should be what you are looking for.
AnswerKenntnis:
Here's a way to avoid this vexing problem when you start a new project:


Create the main directory for your new project.
Run git init.
Now create a .gitignore file (even if it's empty).
Commit your .gitignore file.


Git makes it really hard to do git reset if you don't have any commits.  If you create a tiny initial commit just for the sake of having one, after that you can git add -A and git reset as many times as you want in order to get everything right.

Another advantage of this method is that if you run into line-ending troubles later and need to refresh all your files, it's easy:


Check out that initial commit.  This will remove all your files.
Then check out your most recent commit again.  This will retrieve fresh copies of your files, using your current line-ending settings.
AnswerKenntnis:
As per many of the other answers you can use git reset

BUT:

I found this great little post that actually adds the Git command (well an alias) for "git unadd", git unadd:

Simply,

git config --global alias.unadd "reset HEAD"


Now you can

git unadd foo.txt bar.txt
AnswerKenntnis:
To remove new files from the staging area (and only in case of a new file), as suggested above:

git rm --cached FILE


Use rm --cached only for new files accidentally added.
AnswerKenntnis:
Note that if you fail to specify a revision then you have to include a separator. Example from my console:

git reset <path_to_file>
fatal: ambiguous argument '<path_to_file>': unknown revision or path not in the working tree.
Use '--' to separate paths from revisions

git reset -- <path_to_file>
Unstaged changes after reset:
M   <path_to_file>


(git version 1.7.5.4)
AnswerKenntnis:
To reset every file in a particular folder (and its subfolders), you can use the following command:

git reset *
AnswerKenntnis:
The question is not clearly posed. The reason is that git add has two meanings:


adding a new file to the staging area, then undo with git rm --cached file.
adding a modified file to the staging area, then undo with git reset HEAD file.


if in doubt, use

git reset HEAD file


Because it does the expected thing in both cases.

Warning: if you do git rm --cached file on a file that was modified (a file that existed before in the repository), then the file will be removed on git commit! But it will still exist in your file system.

git status will tell you it the file was a new file or modified:

On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

    new file:   my_new_file.txt
    modified:   my_modified_file.txt
AnswerKenntnis:
This command will unstash your changes:

git reset HEAD filename.txt


You can also use 

git add -p 


to add parts of files.
AnswerKenntnis:
Just type "git reset" and it is like you never typed "git add ." since your last commit.
AnswerKenntnis:
The command git reset --hard HEAD should work.  The one thing to note is that you need to changed directory (cd) back into your normal working directory. Otherwise if you run the command from the directory you mistakenly did the git add .   .... you will not be able to revert out and instead get the errors mentioned in other posts regard "unknown revision or path not in the working tree".
AnswerKenntnis:
use the * command to handle multiple files at a time

git reset HEAD *.prj
git reset HEAD *.bmp
git reset HEAD *gdb*


etc
AnswerKenntnis:
git clean -f -d should help as it cleans up untracked files along with directories.
AnswerKenntnis:
You can simply reset the code by using the following code 

git reset HEAD .
AnswerKenntnis:
As this is your first commit, you can run this instead: 

rm -fr .git


And start again from scratch! (this is not the first time I'm, going through this cycle of init, add, commit, sh!t, google add undo, oh, why not remove the repository and start again)

:)
QuestionKenntnis:
Plain English explanation of Big O
qn_description:
What is a plain English explanation of Big O? With as little formal definition as possible and simple mathematics.
AnswersKenntnis
AnswerKenntnis:
Quick note, this is almost certainly confusing Big O notation (which is an upper bound) with Theta notation (which is a two-side bound). In my experience this is actually typical of discussions in non-academic settings. Apologies for any confusion caused.



The simplest definition I can give for Big-O notation is this:

Big-O notation is a relative representation of the complexity of an algorithm.

There are some important and deliberately chosen words in that sentence:


  
  relative: you can only compare apples to apples.  You can't compare an algorithm to do arithmetic multiplication to an algorithm that sorts a list of integers.  But a comparison of two algorithms to do arithmetic operations (one multiplication, one addition) will tell you something meaningful;
  representation: Big-O (in its simplest form) reduces the comparison between algorithms to a single variable.  That variable is chosen based on observations or assumptions.  For example, sorting algorithms are typically compared based on comparison operations (comparing two nodes to determine their relative ordering).  This assumes that comparison is expensive.  But what if comparison is cheap but swapping is expensive?  It changes the comparison; and
  complexity: if it takes me one second to sort 10,000 elements how long will it take me to sort one million?  Complexity in this instance is a relative measure to something else.
  


Come back and reread the above when you've read the rest.

The best example of Big-O I can think of is doing arithmetic.  Take two numbers (123456 and 789012).  The basic arithmetic operations we learnt in school were:


  
  addition;
  subtraction;
  multiplication; and
  division.
  


Each of these is an operation or a problem.  A method of solving these is called an algorithm.

Addition is the simplest.  You line the numbers up (to the right) and add the digits in a column writing the last number of that addition in the result.  The 'tens' part of that number is carried over to the next column.

Let's assume that the addition of these numbers is the most expensive operation in this algorithm.  It stands to reason that to add these two numbers together we have to add together 6 digits (and possibly carry a 7th).  If we add two 100 digit numbers together we have to do 100 additions.  If we add two 10,000 digit numbers we have to do 10,000 additions.

See the pattern?  The complexity (being the number of operations) is directly proportional to the number of digits n in the larger number.  We call this O(n) or linear complexity.

Subtraction is similar (except you may need to borrow instead of carry).

Multiplication is different.  You line the numbers up, take the first digit in the bottom number and multiply it in turn against each digit in the top number and so on through each digit.  So to multiply our two 6 digit numbers we must do 36 multiplications.  We may need to do as many as 10 or 11 column adds to get the end result too.

If we have two 100-digit numbers we need to do 10,000 multiplications and 200 adds.  For two one million digit numbers we need to do one trillion (1012) multiplications and two million adds.

As the algorithm scales with n-squared, this is O(n2) or quadratic complexity.  This is a good time to introduce another important concept:

We only care about the most significant portion of complexity.

The astute may have realized that we could express the number of operations as: n2 + 2n.  But as you saw from our example with two numbers of a million digits apiece, the second term (2n) becomes insignificant (accounting for 0.0002% of the total operations by that stage).

One can notice that we've assumed the worst case scenario here. While multiplying 6 digit numbers if one of them is 4 digit and the other one is 6 digit, then we only have 24 multiplications. Still we calculate the worst case scenario for that 'n', i.e when both are 6 digit numbers. Hence Big-O notation is about the Worst-case scenario of an algorithm

The Telephone Book

The next best example I can think of is the telephone book, normally called the White Pages or similar but it'll vary from country to country.  But I'm talking about the one that lists people by surname and then initials or first name, possibly address and then telephone numbers.

Now if you were instructing a computer to look up the phone number for "John Smith" in a telephone book that contains 1,000,000 names, what would you do?  Ignoring the fact that you could guess how far in the S's started (let's assume you can't), what would you do?

A typical implementation might be to open up to the middle, take the 500,000th and compare it to "Smith".  If it happens to be "Smith, John", we just got real lucky.  Far more likely is that "John Smith" will be before or after that name.  If it's after we then divide the last half of the phone book in half and repeat.  If it's before then we divide the first half of the phone book in half and repeat.  And so on.

This is called a binary search and is used every day in programming whether you realize it or not.

So if you want to find a name in a phone book of a million names you can actually find any name by doing this at most 20 times.  In comparing search algorithms we decide that this comparison is our 'n'.


  
  For a phone book of 3 names it takes 2 comparisons (at most).
  For 7 it takes at most 3.
  For 15 it takes 4.
  ΓÇª
  For 1,000,000 it takes 20.
  


That is staggeringly good isn't it?

In Big-O terms this is O(log n) or logarithmic complexity.  Now the logarithm in question could be ln (base e), log10, log2 or some other base.  It doesn't matter it's still O(log n) just like O(2n2) and O(100n2) are still both O(n2).

It's worthwhile at this point to explain that Big O can be used to determine three cases with an algorithm:


  
  Best Case: In the telephone book search, the best case is that we find the name in one comparison.  This is O(1) or constant complexity;
  Expected Case: As discussed above this is O(log n); and
  Worst Case: This is also O(log n).
  


Normally we don't care about the best case.  We're interested in the expected and worst case.  Sometimes one or the other of these will be more important.

Back to the telephone book.

What if you have a phone number and want to find a name?  The police have a reverse phone book but such look-ups are denied to the general public.  Or are they?  Technically you can reverse look-up a number in an ordinary phone book.  How?

You start at the first name and compare the number.  If it's a match, great, if not, you move on to the next.  You have to do it this way because the phone book is unordered (by phone number anyway).

So to find a name:


  
  Best Case: O(1);
  Expected Case: O(n) (for 500,000); and
  Worst Case: O(n) (for 1,000,000).
  


The Travelling Salesman

This is quite a famous problem in computer science and deserves a mention.  In this problem you have N towns. Each of those towns is linked to 1 or more other towns by a road of a certain distance. The Travelling Salesman problem is to find the shortest tour that visits every town.

Sounds simple?  Think again.

If you have 3 towns A, B and C with roads between all pairs then you could go:


  
  A ΓåÆ B ΓåÆ C
  A ΓåÆ C ΓåÆ B
  B ΓåÆ C ΓåÆ A
  B ΓåÆ A ΓåÆ C
  C ΓåÆ A ΓåÆ B
  C ΓåÆ B ΓåÆ A
  


Well actually there's less than that because some of these are equivalent (A ΓåÆ B ΓåÆ C and C ΓåÆ B ΓåÆ A are equivalent, for example, because they use the same roads, just in reverse).

In actuality there are 3 possibilities.


  
  Take this to 4 towns and you have (iirc) 12 possibilities.
  With 5 it's 60.
  6 becomes 360.
  


This is a function of a mathematical operation called a factorial.  Basically:


  
  5! = 5 ├ù 4 ├ù 3 ├ù 2 ├ù 1 = 120
  6! = 6 ├ù 5 ├ù 4 ├ù 3 ├ù 2 ├ù 1 = 720
  7! = 7 ├ù 6 ├ù 5 ├ù 4 ├ù 3 ├ù 2 ├ù 1 = 5040
  ΓÇª
  25! = 25 ├ù 24 ├ù ΓÇª ├ù 2 ├ù 1 = 15,511,210,043,330,985,984,000,000
  ΓÇª
  50! = 50 ├ù 49 ├ù ΓÇª ├ù 2 ├ù 1 = 3.04140932 ├ù 1064
  


So the Big-O of the Travelling Salesman problem is O(n!) or factorial or combinatorial complexity.

By the time you get to 200 towns there isn't enough time left in the universe to solve the problem with traditional computers.

Something to think about.

Polynomial Time

Another point I wanted to make quick mention of is that any algorithm that has a complexity of O(na) is said to have polynomial complexity or is solvable in polynomial time.

O(n), O(n2) etc are all polynomial time. Some problems cannot be solved in polynomial time.  Certain things are used in the world because of this.  Public Key Cryptography is a prime example.  It is computationally hard to find two prime factors of a very large number.  If it wasn't, we couldn't use the public key systems we use.

Anyway, that's it for my (hopefully plain English) explanation of Big O (revised).
AnswerKenntnis:
It shows how an algorithm scales. 

O(n2):


1 item: 1 second
10 items: 100 seconds
100 items: 10000 seconds


Notice that the number of items increases by a factor of 10, but the time increases by a factor of 102. Basically, n=10 and so O(n2) gives us the scaling factor n2 which is 102.

O(n):


1 item: 1 second 
10 items: 10 seconds
100 items: 100 seconds


This time the number of items increases by a factor of 10, and so does the time. n=10 and so O(n)'s scaling factor is 10.

O(1):


1 item: 1 second
10 items: 1 second
100 items: 1 second


The number of items is still increasing by a factor of 10, but the scaling factor of O(1) is always 1.

That's the gist of it. They reduce the maths down so it might not be exactly n2 or whatever they say it is, but that'll be the dominating factor in the scaling.
AnswerKenntnis:
EDIT: Quick note, this is almost certainly confusing Big O notation (which is an upper bound) with Theta notation (which is both an upper and lower bound). In my experience this is actually typical of discussions in non-academic settings. Apologies for any confusion caused.

In one sentence: As the size of your job goes up, how much longer does it take to complete it?

Obviously that's only using "size" as the input and "time taken" as the output — the same idea applies if you want to talk about memory usage etc.

Here's an example where we have N T-shirts which we want to dry. We'll assume it's incredibly quick to get them in the drying position (i.e. the human interaction is negligible). That's not the case in real life, of course...


Using a washing line outside: assuming you have an infinitely large back yard, washing dries in O(1) time. However much you have of it, it'll get the same sun and fresh air, so the size doesn't affect the drying time.
Using a tumble dryer: you put 10 shirts in each load, and then they're done an hour later. (Ignore the actual numbers here — they're irrelevant.) So drying 50 shirts takes about 5 times as long as drying 10 shirts.
Putting everything in an airing cupboard: If we put everything in one big pile and just let general warmth do it, it will take a long time for the middle shirts to get dry. I wouldn't like to guess at the detail, but I suspect this is at least O(N^2) — as you increase the wash load, the drying time increases faster.


One important aspect of "big O" notation is that it doesn't say which algorithm will be faster for a given size. Take a hashtable (string key, integer value)  vs an array of pairs (string, integer). Is it faster to find a key in the hashtable or an element in the array, based on a string? (i.e. for the array, "find the first element where the string part matches the given key.") Hashtables are generally amortised (~= "on average") O(1) — once they're set up, it should take about the same time to find an entry in a 100 entry table as in a 1,000,000 entry table. Finding an element in an array (based on content rather than index) is linear, i.e. O(N) — on average, you're going to have to look at half the entries.

Does this make a hashtable faster than an array for lookups? Not necessarily. If you've got a very small collection of entries, an array may well be faster — you may be able to check all the strings in the time that it takes to just calculate the hashcode of the one you're looking at. As the data set grows larger, however, the hashtable will eventually beat the array.
AnswerKenntnis:
Big-O notation (also called "asymptotic growth" notation) is what functions "look like" when you ignore constant factors and stuff near the origin. We use it to talk about how thing scale.



Basics

for "sufficiently" large inputs...


f(x) Γêê O(upperbound) means f "grows no faster than" upperbound
f(x) Γêê ╞ƒ(justlikethis) mean f "grows exactly like" justlikethis
f(x) Γêê ╬⌐(lowerbound) means f "grows no slower than" lowerbound


big-O notation doesn't care about constant factors: the function 9x┬▓ is said to "grow exactly like" 10x┬▓. Neither does big-O asymptotic notation care about non-asymptotic stuff ("stuff near the origin" or "what happens when the problem size is small"): the function 10x┬▓ is said to "grow exactly like" 10x┬▓ - x + 2.

Why would you want to ignore the smaller parts of the equation? Because they become completely dwarfed by the big parts of the equation as you consider larger and larger scales; their contribution becomes dwarfed and irrelevant. (See example section.)

Put another way, it's all about the ratio. If you divide the actual time it takes by the O(...), you will get a constant factor in the limit of large inputs. Intuitively this makes sense: functions "scale like" one another if you can multiply one to get the other. That is, when we say...

actualAlgorithmTime(N) Γêê O(bound(N))
                                       e.g. "time to mergesort N elements 
                                             is O(N log(N))"


... this means that for "large enough" problem sizes N (if we ignore stuff near the origin), there exists some constant (e.g. 2.5, completely made up) such that:

actualAlgorithmTime(N)                 e.g. "mergesort_duration(N)       "
ΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇ < constant            ΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇ < 2.5 
       bound(N)                                    N log(N)         


There are many choices of constant; often the "best" choice is known as the "constant factor" of the algorithm... but we often ignore it like we ignore non-largest terms (see Constant Factors section for why they don't usually matter). You can also think of the above equation as a bound, saying "In the worst-case scenario, the time it takes will never be worse than roughly N*log(N), within a factor of 2.5 (a constant factor we don't care much about)".

In general, O(...) is the most useful one because we often care about worst-case behavior. If f(x) represents something "bad" like processor or memory usage, then "f(x) Γêê O(upperbound)" means "upperbound is the worse-case scenario of processor/memory usage".



Intuition

This lets us make statements like...

"For large enough inputsize=N, and a constant 
 factor of 1, if I double the input size...
  ... I double the time it takes."        ( O(N) )
  ... I quadruple the time it takes."     ( O(N┬▓) )
  ... I add 1 to the time it takes."      ( O(log(N)) )
  ... I don't change the time it takes."  ( O(1) )


(with credit to http://stackoverflow.com/a/487292/711085 )



Applications

As a purely mathematical construct, big-O notation is not limited to talking about processing time and memory. You can use it to discuss the asymptotics of anything where scaling is meaningful, such as:


the number of possibly handshakes among N people at a party (╞ƒ(N┬▓), specifically N(N-1)/2, but what matters is that it "scales like" N┬▓)
probabilistic expected number of people who have seen some viral marketing as a function of time
how website latency scales with the number of processing units in a CPU or GPU or computer cluster
how heat output scales on CPU dies as a function of transistor count, voltage, etc.




Example

For the handshake example, #handshakes Γêê ╞ƒ(N┬▓). The number of handshakes is exactly n-choose-2 or (N┬▓-N)/2 (each of N people shakes the hands of N-1 other people, but this double-counts handshakes so divide by 2). However, for very large numbers of people, the linear term N is dwarfed and effectively contributes 0 to the ratio. Therefore the scaling behavior is order N┬▓, or the number of handshakes "grows like N┬▓".

#handshakes(N)
ΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇ Γëê 1/2
     N┬▓


If you wanted to prove this to yourself, you could perform some simple algebra on the ratio to split it up into multiple terms (lim means "considered in the limit of", you can ignore it if it makes you feel better):

    N┬▓/2 - N/2         (N┬▓)/2   N/2         1/2
lim ΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇ = lim ( ΓöÇΓöÇΓöÇΓöÇΓöÇΓöÇ - ΓöÇΓöÇΓöÇ ) = lim ΓöÇΓöÇΓöÇ = 1/2
NΓåÆΓê₧     N┬▓       NΓåÆΓê₧     N┬▓     N┬▓      NΓåÆΓê₧  1
                               ΓöòΓöüΓöüΓöüΓöÖ
             this is 0 in the limit of NΓåÆΓê₧:
             graph it, or plug in a really large number for N




Constant factors

Usually we don't care what the specific constant factors are, because they don't affect the way the function grows. For example, two algorithm may both take O(N) time to complete, but one may be twice as slow as the other. We usually don't care too much unless the factor is very large, since optimizing is tricky business ( When is optimisation premature? ); also the mere act of picking an algorithm with a better big-O will often improve performance by orders of magnitude.

Some asymptotically superior algorithms (e.g. a non-comparison O(N log(log(N))) sort) can have so large a constant factor (e.g. 100000*N log(log(N))), or overhead that is relatively large like O(N log(log(N))) with a hidden + 100*N, that they are rarely worth using even on "big data".



Why O(N) is sometimes the best you can do, i.e. why we need datastructures

O(N) algorithms are in some sense the "best" algorithms if you need to read all your data. The very act of reading a bunch of data is an O(N) operation. Loading it into memory is usually O(N) (or faster if you have hardware support, or no time at all if you've already read the data). However if you touch or even look at every piece of data (or even every other piece of data), your algorithm will take O(N) time to perform this looking. Nomatter how long your actual algorithm takes, it will be at least O(N) because it spent that time looking at all the data.

The same can be said for the very act of writing. For example, all algorithms which print out all permutations of a number N are O(N!) because the output is at least that long.

This motivates the use of data structures: a data structure requires reading the data only once (usually O(N) time), plus some arbitrary amount of preprocessing (e.g. O(N) or O(N log(N)) or O(N┬▓)) which we try to keep small. Thereafter, modifying the data structure (insertions / deletions / etc.) and making queries on the data take very little time, such as O(1) or O(log(N)). You then proceed to make a large number of queries! In general, the more work you're willing to do ahead of time, the less work you'll have to do later on.

For example, say you had the latitude and longitude coordinates of millions of roads segments, and wanted to find all street intersections.


Naive method: If you had the coordinates of a street intersection, and wanted to examine nearby streets, you would have to go through the millions of segments each time, and check each one for adjacency.
If you only needed to do this once, it would not be a problem to have to do the naive method of O(N) work only once, but if you want to do it many times (in this case, N times, once for each segment), we'd have to do O(N┬▓) work, or 1000000┬▓=1000000000000 operations. Not good (a modern computer can perform about a billion operations per second).
If we use a simple structure called a hash table (an instant-speed lookup table, also known as a hashmap or dictionary), we pay a small cost by preprocessing everything in O(N) time. Thereafter, it only takes constant time on average to look up something by its key (in this case, our key is the latitude and longitude coordinates, rounded into a grid; we search the adjacent gridspaces of which there are only 9, which is a constant).
Our task went from an infeasible O(N┬▓) to a manageable O(N), and all we had to do was pay a minor cost to make a hash table.


The moral of the story: a data structure lets us speed up operations. Even more advanced data structures can let you combine, delay, or even ignore operations in incredibly clever ways, like leaving the equivalent of "to-do" notes at junctions in a tree.



Amortized / average-case complexity

There is also the concept of "amortized" or "average case". This is no more than using big-O notation for the expected value of a function, rather than the function itself. For example, some data structures may have a worse-case complexity of O(N) for a single operation, but guarantee that if you do many of these operations, the average-case complexity will be O(1).



Multidimensional big-O

Most of the time, people don't realize that there's more than one variable at work. For example, in a string-search algorithm, your algorithm may take time O([length of text] + [length of query]), i.e. it is linear in two variables like O(N+M). Other more naive algorithms may be O([length of text]*[length of query]) or O(N*M). Ignoring multiple variables is one of the most common oversights I see in algorithm analysis, and can handicap you when designing an algorithm.



The whole story

Keep in mind that big-O is not the whole story. You can drastically speed up some algorithms by using caching, making them cache-oblivious, avoiding bottlenecks by working with RAM instead of disk, using parallelization, or doing work ahead of time -- these techniques are often independent of the order-of-growth "big-O" notation, though you will often see the number of cores in the big-O notation of parallel algorithms.

Also keep in mind that due to hidden constraints of your program, you might not really care about asymptotic behavior. You may be working with a bounded number of values, for example:


If you're sorting something like 5 elements, you don't want to use the speedy O(N log(N)) quicksort; you want to use insertion sort, which happens to perform well on small inputs. These situations often comes up in divide-and-conquer algorithms, where you split up the problem into smaller and smaller subproblems, such as recursive sorting, fast Fourier transforms, or matrix multiplication.
If some values are effectively bounded due to some hidden fact (e.g. the average human name is softly bounded at perhaps 40 letters, and human age is softly bounded at around 150). You can also impose bounds on your input to effectively make terms constant.


In practice, even among algorithms which have the same or similar asymptotic performance, their relative merit may actually be driven by other things, such as: other performance factors (quicksort and mergesort are both O(N log(N)), but quicksort takes advantage of CPU caches); non-performance considerations, like ease of implementation; whether a library is available, and how reputable and maintained the library is.

Many things can implicitly contribute to the running time's constant factor, such as whether you run your algorithm on a 500MHz computer vs 2GHz computer, whether your programming language is interpreted or using a JIT compiler, whether you are doing a constant amount of extra work in a critical section of code, etc. The effect may be small (e.g. 0.9x speed) or large (e.g. 0.01x speed) compared to a different implementation and/or environment. Do you switch languages to eek out that little extra constant factor of work? That literally depends on a hundred other reasons (necessity, skills, coworkers, programmer productivity, the monetary value of your time, familiarity, workarounds, why not assembly or GPU, etc...), which may be more important than performance.

The above issues, like programming language, are almost never considered as part of the constant factor (nor should they be); yet one should be aware of them, because sometimes (though rarely) they may not be constant. For example in cpython, the native priority queue implementation is asymptotically non-optimal (O(log(N)) rather than O(1) for your choice of insertion or find-min); do you use another implementation? Probably not, since the C implementation is probably faster, and there are probably other similar issues elsewhere. There are tradeoffs; sometimes they matter and sometimes they don't.



Math addenda

For completeness, the precise definition of big-O notation is as follows: f(x) Γêê O(g(x)) means that "f is asymptotically upper-bounded by const*g": ignoring everything below some finite value of x, there exists a constant such that |f(x)| Γëñ const * |g(x)|. (The other symbols are as follows: just like O means Γëñ, ╬⌐ means ΓëÑ. There are lowercase variants: o means <, and ╧ë means >.) f(x) Γêê ╞ƒ(g(x)) means both f(x) Γêê O(g(x)) and f(x) Γêê ╬⌐(g(x)) (upper- and lower-bounded by g): there exists some constants such that f will always lie in the "band" between const1*g(x) and const2*g(x). It is the strongest asymptotic statement you can make and roughly equivalent to ==. (Sorry, I elected to delay the mention of the absolute-value symbols until now, for clarity's sake; especially because I have never seen negative values come up in a computer science context.)

People will often use = O(...). It is technically more correct to use Γêê O(...). Γêê means "is an element of". O(N┬▓) is actually an equivalence class, that is, it is a set of things which we consider to be the same. In this particular case, O(N┬▓) contains elements like {2 N┬▓, 3 N┬▓, 1/2 N┬▓, 2 N┬▓ + log(N), - N┬▓ + N^1.9, ...} and is infinitely large, but it's still a set. People will know what you mean if you use = however. Additionally, it is often the case that in a casual setting, people will say O(...) when they mean ╞ƒ(...); this is technically true since the set of things ╞ƒ(exactlyThis) is a subset of O(noGreaterThanThis)... and it's easier to type. ;-)
AnswerKenntnis:
Big O describes an upper limit on the growth behaviour of a function, for example the runtime of a program, when inputs become large.

Examples:


O(n): If I double the input size the runtime doubles
O(n2): If the input size doubles the runtime quadruples
O(log n): If the input size doubles the runtime increases by one
O(2n): If the input size increases by one, the runtime doubles


The input size is usually the space in bits needed to represent the input.
AnswerKenntnis:
Big O notation is most commonly used by programmers as an approximate measure of how long a computation (algorithm) will take to complete expressed as a function of the size of the input set. 

Big O is useful to compare how well two algorithms will scale up as the number of inputs is increased. 

More precisely Big O notation is used to express the asymptotic behavior of a function. That means how the function behaves as it approaches infinity. 

In many cases the "O" of an algorithm will fall into one of the following cases:


O(1) - Time to complete is the same regardless of the size of input set. An example is accessing an array element by index.
O(Log N) - Time to complete increases roughly in line with the log2(n). For example 1024 items takes roughly twice as long as 32 items, because Log2(1024) = 10 and Log2(32) = 5. An example is finding an item in a binary search tree (BST). 
O(N) - Time to complete that scales linearly with the size of the input set. In other words if you double the number of items in the input set, the algorithm takes roughly twice as long. An example is counting the number of items in a linked list. 
O(N Log N) - Time to complete increases by the number of items times the result of Log2(N). An example of this is heap sort and quick sort. 
O(N^2) - Time to complete is roughly equal to the square of the number of items. An example of this is bubble sort.
O(N!) - Time to complete is the factorial of the input set. An example of this is the traveling salesman problem brute-force solution.  


Big O ignores factors that do not contribute in a meaningful way to the growth curve of a function as the input size increases towards infinity. This means that constants that are added to or multiplied by the function are simply ignored.
AnswerKenntnis:
Big O is just a way to "Express" yourself in a common way, "How much time / space does it take to run my code?".

You may often see O(n), O(n^2), O(nlogn) and so forth, all these are just ways to show; How does an algorithm change?

O(n) means Big O is n, and now you might think, "What is n!?" Well "n" is the amount of elements. Imaging you want to search for an Item in an Array. You would have to look on Each element and as "Are you the correct element/item?" in the worst case, the item is at the last index, which means that it took as much time as there are items in the list, so to be generic, we say "oh hey, n is a fair given amount of values!".

So then you might understand what "n^2" means, but to be even more specific, play with the thought you have a simple, the simpliest of the sorting algorithms; bubblesort. This algorithm needs to look through the whole list, for each item.

My list


1
6
3


The flow here would be:


Compare 1 and 6, which is biggest? Ok 6 is in the right position, moving forward!
Compare 6 and 3, oh, 3 is less! Let's move that, Ok the list changed, we need to start from the begining now!


This is O n^2 because, you need to look at all items in the list there are "n" items. For each item, you look at all items once more, for comparing, this is also "n", so for every item, you look "n" times meaning n*n = n^2

I hope this is as simple as you want it.

But remember, Big O is just a way to experss yourself in the manner of time and space.
AnswerKenntnis:
Big O describes the fundamental scaling nature of an algorithm.

There is a lot of information that Big O does not tell you about a given algorithm. It cuts to the bone and gives only information about the scaling nature of an algorithm, specifically how the resource use (think time or memory) of an algorithm scales in response to the "input size".

Consider the difference between a steam engine and a rocket. They are not merely different varieties of the same thing (as, say, a Prius engine vs. a Lamborghini engine) but they are dramatically different kinds of propulsion systems, at their core. A steam engine may be faster than a toy rocket, but no steam piston engine will be able to achieve the speeds of an orbital launch vehicle. This is because these systems have different scaling characteristics with regards to the relation of fuel required ("resource usage") to reach a given speed ("input size").

Why is this so important? Because software deals with problems that may differ in size by factors up to a trillion. Consider that for a moment. The ratio between the speed necessary to travel to the Moon and human walking speed is less than 10,000:1, and that is absolutely tiny compared to the range in input sizes software may face. And because software may face an astronomical range in input sizes there is the potential for the Big O complexity of an algorithm, it's fundamental scaling nature, to trump any implementation details.

Consider the canonical sorting example. Bubble-sort is O(n^2) while merge-sort is O(n log n). Let's say you have two sorting applications, application A which uses bubble-sort and application B which uses merge-sort, and let's say that for input sizes of around 30 elements application A is 1,000x faster than application B at sorting. If you never have to sort much more than 30 elements then it's obvious that you should prefer application A, as it is much faster at these input sizes. However, if you find that you may have to sort ten million items then what you'd expect is that application B actually ends up being thousands of times faster than application A in this case, entirely due to the way each algorithm scales.
AnswerKenntnis:
Ok, my 2cents.

Big-O, is rate of increase of resource consumed by program, w.r.t. problem-instance-size

Resource : Could be total-CPU time, could be maximum RAM space. By default refers to CPU time.

Say the problem is "Find the sum", 

int Sum(int*arr,int size){
      int sum=0;
      while(size-->0) 
         sum+=arr[size]; 

      return sum;
}


problem-instance= {5,10,15}  ==> problem-instance-size = 3, iterations-in-loop= 3

problem-instance= {5,10,15,20,25}  ==> problem-instance-size = 5 iterations-in-loop = 5

For input of size "n" the program is growing at speed of "n" iterations in array. Hence Big-O is N expressed as  O(n)

Say the problem is "Find the Combination", 

    void Combination(int*arr,int size)
    { int outer=size,inner=size;
      while(outer -->0) {
        inner=size;
        while(inner -->0)
          cout<<arr[outer]<<"-"<<arr[inner]<<endl;
      }
    }


problem-instance= {5,10,15}  ==> problem-instance-size = 3, total-iterations = 3*3 = 9

problem-instance= {5,10,15,20,25}  ==> problem-instance-size = 5, total-iterations= 5*5 =25

For input of size "n" the program is growing at speed of "n*n" iterations in array. Hence Big-O is N^2 expressed as  O(n^2)
AnswerKenntnis:
What is a plain English explanation of Big O? With as little formal definition as possible and simple mathematics.


A Plain English Explanation of the Need for Big-O Notation:

When we program, we are trying to solve a problem. What we code is called an algorithm. Big O notation allows us to compare the worse case performance of our algorithms in a standardized way. Hardware specs vary over time and improvements in hardware can reduce the time it takes an algorithms to run. But replacing the hardware does not mean our algorithm is any better or improved over time, as our algorithm is still the same. So in order to allow us to compare different algorithms, to determine if one is better or not, we use Big O notation.

A Plain English Explanation of What Big O Notation is:

Not all algorithms run in the same amount of time, and can vary based on the number of items in the input, which we'll call n. Based on this, we consider the worse case analysis, or an upper-bound of the run-time as n get larger and larger. We must be aware of what n is, because many of the Big O notations reference it.
AnswerKenntnis:
Big O is a measure of how much time/space an algorithm uses relative to the size of its input.  

If an algorithm is O(n) then the time/space will increase at the same rate as its input.

If an algorithm is O(n^2) then the time/space increase at the rate of its input squared.

and so on.
AnswerKenntnis:
Big O notation is a way of describing the upper bound of an algorithm in terms of space or running time.  The n is the number of elements in the the problem (i.e size of an array, number of nodes in a tree, etc.)  We are interested in describing the running time as n gets  big.

When we say some algorithm is O(f(n)) we are saying that the running time (or space required) by that algorithm is always lower than some constant times f(n).

To say that binary search has a running time of O(logn) is to say that there exists some constant c which you can multiply log(n) by that will always be larger than the running time of binary search.  In this case you will always have some constant factor of log(n) comparisons.

In other words where g(n) is the running time of your algorithm, we say that g(n) = O(f(n)) when g(n) <= c*f(n) when n > k, where c and k are some constants.
AnswerKenntnis:
It is very difficult to measure the speed of software programs, and when we try, the answers can be very complex and filled with exceptions and special cases. This is a big problem, because all those exceptions and special cases are distracting and unhelpful when we want to compare two different programs with one another to find out which is "fastest".

As a result of all this unhelpful complexity, people try to describe the speed of software programs using the smallest and least complex (mathematical) expressions possible. These expressions are very very crude approximations: Although, with a bit of luck, they will capture the "essence" of whether a piece of software is fast or slow.

Because they are approximations, we use the letter "O" (Big Oh) in the expression, as a convention to signal to the reader that we are making a gross oversimplification. (And to make sure that nobody mistakenly thinks that the expression is in any way accurate).

If you read the "Oh" as meaning "on the order of" or "approximately" you will not go too far wrong. (I think the choice of the Big-Oh might have been an attempt at humour).

The only thing that these "Big-Oh" expressions try to do is to describe how much the software slows down as we increase the amount of data that the software has to process. If we double the amount of data that needs to be processed, does the software need twice as long to finish it's work? Ten times as long? In practice, there are a very limited number of big-Oh expressions that you will encounter and need to worry about:

The good:


O(1) Constant: The program takes the same time to run no matter how big the input is.
O(log n) Logarithmic: The program run-time increases only slowly, even with big increases in the size of the input.


The bad:


O(n) Linear: The program run-time increases proportionally to the size of the input.
O(n^k) Polynomial: - Processing time grows faster and faster - as a polynomial function - as the size of the input increases.


... and the ugly:


O(k^n) Exponential The program run-time increases very quickly with even moderate increases in the size of the problem - it is only practical to process small data sets with exponential algorithms.
O(n!) Factorial The program run-time will be longer than you can afford to wait for anything but the very smallest and most trivial-seeming datasets.
AnswerKenntnis:
Here is the plain English bestiary I tend to use when explaining the common varieties of Big-O

In all cases, prefer algorithms higher up on the list to those lower on the list.  However, the cost of moving to a more expensive complexity class varies significantly.

O(1):

No growth.  Regardless of how big as the problem is, you can solve it in the same amount of time.  This is somewhat analogous to broadcasting where it takes the same amount of energy to broadcast over a given distance, regardless of the number of people that lie within the broadcast range.

O(log n):

This complexity is the same as O(1) except that it's just a little bit worse.  For all practical purposes, you can consider this as a very large constant scaling.  The difference in work between processing 1 thousand and 1 billion items is only a factor six.

O(n):

The cost of solving the problem is proportional to the size of the problem.  If your problem doubles in size, then the cost of the solution doubles.  Since most problems have to be scanned into the computer in some way, as data entry, disk reads, or network traffic, this is generally an affordable scaling factor.

O(n log n):

This complexity is very similar to O(n).  For all practical purposes, the two are equivalent.  This level of complexity would generally still be considered scalable.  By tweaking assumptions some O(n log n) algorithms can be transformed into O(n) algorithms.  For example, bounding the size of keys reduces sorting from O(n log n) to O(n).

O(n2):

Grows as a square, where n is the length of the side of a square.  This is the same growth rate as the "network effect", where everyone in a network might know everyone else in the network.  Growth is expensive.  Most scalable solutions cannot use algorithms with this level of complexity without doing significant gymnastics.  This generally applies to all other polynomial complexities - O(nk) - as well.

O(2n):

Does not scale.  You have no hope of solving any non-trivially sized problem.  Useful for knowing what to avoid, and for experts to find approximate algorithms which are in O(nk).
AnswerKenntnis:
A simple straightforward answer can be:

Big O represents the worst possible time/space for that algorithm. The algorithm will never take more space/time above that limit. Big O represents time/space complexity in the extreme case.
AnswerKenntnis:
Not sure I'm further contributing to the subject but still thought I'd share: I once found this blog post to have some quite helpful (though very basic) explanations & examples on Big O: 

Via examples, this helped get the bare basics into my tortoiseshell-like skull, so I think it's a pretty descent 10-minute read to get you headed in the right direction.
AnswerKenntnis:
"What is a plain English explanation of Big O? With as little formal
  definition as possible and simple mathematics."


Such a beautifully simple and short question seems at least to deserve an equally short answer, like a student might receive during tutoring.


  Big O notation simply tells how much time* an algorithm can run within,
  in terms of only the amount of input data**.


( *in a wonderful, unit-free sense of time!)
(**which is what matters, because people will always want more, whether they live today or tomorrow)

Well, what's so wonderful about Big O notation if that's what it does?


Practically speaking, Big O analysis is so useful and important because Big O puts the focus squarely on the algorithm's own complexity and completely ignores anything that is merely a proportionality constant—like a JavaScript engine, the speed of a CPU, your Internet connection, and all those things which become quickly become as laughably outdated as a Model T. Big O focuses on performance only in the way that matters equally as much to people living in the present or in the future.
Big O notation also shines a spotlight directly on the most important principle of computer programming/engineering, the fact which inspires all good programmers to keep thinking and dreaming: the only way to achieve results beyond the slow forward march of technology is to invent a better algorithm.
AnswerKenntnis:
Big O

f(x) = O(g(x)) when x goes to a (for example, a = +Γê₧) means that there is a function k such that:


f(x) = k(x)g(x)
k is bounded in some neighborhood of a (if a = +Γê₧, this means that there are numbers N and M such that for every x > N, |k(x)| < M).


In other words, in plain English: f(x) = O(g(x)), x ΓåÆ a, means that in a neighborhood of a, f decomposes into the product of g and some bounded function.

Small o

By the way, here is for comparison the definition of small o.

f(x) = o(g(x)) when x goes to a means that there is a function k such that:


f(x) = k(x)g(x)
k(x) goes to 0 when x goes to a.


Examples


sin x = O(x) when x ΓåÆ 0.
sin x = O(1) when x ΓåÆ +Γê₧,
x^2 + x = O(x) when x ΓåÆ 0,
x^2 + x = O(x^2) when x ΓåÆ +Γê₧,
ln(x) = o(x) = O(x) when x ΓåÆ +Γê₧.


Attention! The notation with the equal sign "=" uses a "fake equality": it is true that o(g(x)) = O(g(x)), but false that O(g(x)) = o(g(x)).  Similarly, it is ok to write "ln(x) = o(x) when x ΓåÆ +Γê₧", but the formula "o(x) = ln(x)" would make no sense.

More examples


O(1) = O(n) = O(n^2) when n ΓåÆ +Γê₧ (but not the other way around, the equality is "fake"),
O(n) + O(n^2) = O(n^2) when n ΓåÆ +Γê₧
O(O(n^2)) = O(n^2) when n ΓåÆ +Γê₧
O(n^2)O(n^3) = O(n^5) when n ΓåÆ +Γê₧




Here is the Wikipedia article: https://en.wikipedia.org/wiki/Big_O_notation
AnswerKenntnis:
Algorithm example (Java):

public boolean simple_search (ArrayList<Integer> list, int key)
{
    for (Integer i : list)
    {
        if (i == key)
        {
            return true;
        }
    }

    return false;
}


Algorithm description:


This algorithm search a list, item by item, looking for a key,
Iterating on each item in the list, if it's the key then return True,
If the loop has finished without finding the key, return False.


Big-O notation represent the upper-bound on the Complexity (Time, Space, ..)

To find The Big-O on Time Complexity:


Calculate how much time (regarding input size) the worst case takes:
Worst-Case: the key doesn't exist in the list.
Time(Worst-Case) = 4n+1
Time: O(4n+1) = O(n) | in Big-O, constants are neglected
O(n) ~ Linear


There's also Big-Omega, which represent complexity of the Best-Case:


Best-Case: the key is the first item.
Time(Best-Case) = 4
Time: ╬⌐(4) = O(1) ~ Instant\Constant
AnswerKenntnis:
Big O notation seeks to describe the relative complexity of an algorithm by reducing the growth rate to the key factors when the key factor tends towards infinity. For this reason, you will often hear the phrase asymptotic complexity. In doing so, all other factors are ignored. It is a  relative representation of complexity.
AnswerKenntnis:
Assume we're talking about an algorithm A, which should do something with a dataset of size n. 

Then O( <some expression X involving n> ) means, in simple English:


  If you're unlucky when executing A, it might take X(n) operations to
  complete.


As it happens, there are certain functions (think of them as implementations of X(n)) that tend to occur quite often. These are well known and easily compared (Examples: 1, Log N, N, N^2, N!, etc..)

By comparing these when talking about A and other algorithms, it is easy to rank the algorithms according to the number of operations they may (worst-case) require to complete. 

In general, our goal will be to find or structure an algorithm A in such a way that it will have a function X(n) that returns as low a number as possible.
QuestionKenntnis:
How can I check if one string contains another substring in JavaScript?
qn_description:
How can I check if one string contains another substring in JavaScript? 

Usually I would expect a String.contains() method, but there doesn't seem to be one.
AnswersKenntnis
AnswerKenntnis:
indexOf returns the position of the string in the other string. If not found, it will return -1:

var s = "foo";
alert(s.indexOf("oo") > -1);
AnswerKenntnis:
You can easily add a contains method to String with this statement:

String.prototype.contains = function(it) { return this.indexOf(it) != -1; };


Note: see the comments below for a valid argument for not using this. My advice: use your own judgement.
AnswerKenntnis:
The problem with your code is that JavaScript is case sensitive. Your method call

indexof()


should actually be

indexOf()


Try fixing it and see if that helps:

if (test.indexOf("title") !=-1) {
    alert(elm);
    foundLinks++;
}
AnswerKenntnis:
You could use the JavaScript search() method.

Syntax is: string.search(regexp)

It returns the position of the match, or -1 if no match is found.

See examples there: jsref_search

You don't need a complicated regular expression syntax. If you are not familiar with them a simple st.search("title") will do. If you want your test to be case insensitive, then you should do st.search(/title/i).
AnswerKenntnis:
var index = haystack.indexOf(needle);
AnswerKenntnis:
A contains method has been added to Strings in Javascript 1.8.6:

"potato".contains("to");
> true


Of course, we will have to wait decades for mainstream support :P

(source)
AnswerKenntnis:
You can use jQuery's :contains selector.

$("div:contains('John')")


Check it here: http://api.jquery.com/contains-selector/
AnswerKenntnis:
This piece of code should work well:

var str="This is testing for javascript search !!!";
if(str.search("for") != -1) {
   //logic
}
AnswerKenntnis:
A common way to write a contains method in JS is:

if (!String.prototype.contains) {
    String.prototype.contains = function (arg) {
        return !!~this.indexOf(arg);
    };
}


The bitwise negation operator (~) is used to turn -1 into 0 (falsey), and all other values will be non-zero (truthy).

The double boolean negation operators are used to cast the number into a boolean.
AnswerKenntnis:
This just worked for me. It selects for strings that do not contain the term "Deleted:"

if (eventString.indexOf("Deleted:") == -1)
AnswerKenntnis:
You need to call indexOf with a capital "O" as mentioned. It should also be noted, that in JavaScript class is a reserved word, you need to use className to get this data attribute. The reason it's probably failing is because it's returning a null value. You can do the following to get your class value...

var test = elm.getAttribute("className");
//or
var test = elm.className
AnswerKenntnis:
String.contains() introduced in JavaScript 1.8.6


  Determines whether one string may be found within another string, 
  returning true or false as appropriate.


Syntax

var contained = str.contains(searchString [, position]);  


Parameters

searchString


A string to be searched for within this string.

position


The position in this string at which to begin searching for searchString defaults to 0.  

Example

var str = "To be, or not to be, that is the question.";

console.log(str.contains("To be"));    // true
console.log(str.contains("question")); // true
console.log(str.contains("To be", 1)); // false  


Note

Only supported in Firefox from version 18 onwords.
AnswerKenntnis:
Another option of doing this is:

You can use the match function, that is, something like:

x = "teststring";

if (x.match("test")) {
     // Code
}
AnswerKenntnis:
Use regular expression

RegExp.test(string)
AnswerKenntnis:
Since the question is pretty popular, I thought I could add a little modern flavor to the code.

var allLinks = content.document.getElementsByTagName("a")
,   il       = allLinks.length
,   i        = 0
,   test
,   alrt;

while (i < il) {
  elm  = allLinks[i++];
  test = elm.getAttribute("class");

  if (test.indexOf("title") > -1) console.log(elm), foundLinks++;   
}
alrt = foundLinks ? "Found " + foundLinks + " title class" : "No title class found";
console.log(alrt);


Btw the correct answer is misspelling indexOf or the non-standard String.contains.
Loading an external library (especially if the code is written in pure javascript) or messing with String.prototype or using a regex is a little over kill.
AnswerKenntnis:
You were looking for .indexOfMDN.

indexOf is going to return an index to the matched substring. The index will correlate to where the substring starts. If there is no match, a -1 is returned. Here is a simple demo of that concept:

var str = "Hello World";// For example, lets search this string,
var term = "World";// for the term "World",
var index = str.indexOf(term);// and get its index.
if(index != -1){// If the index is not -1 then the term was matched in the string,
 alert(index);// and we can do some work based on that logic. (6 is alerted)
}
AnswerKenntnis:
If you were looking for an alternative to write the ugly -1 check, you prepend a ~ tilde instead. 

if (~haystack.indexOf('needle')) alert('found');


More details here
AnswerKenntnis:
Since there is a complaint about using the prototype, and since using indexOf makes your code less readable, and since regexp is overkill:

function stringContains(inputString, stringToFind) {
    return (inputString.indexOf(stringToFind) != -1);
}


That is the compromise I ended up going for.
AnswerKenntnis:
JavaScript code to use contain method in an array

 <html>
<head>
<h2>Use of contains() method</h2>
<script>
Array.prototype.contains = function (element) {
  for (var i = 0; i < this.length; i++) {
    if (this[i] == element) {
     return true;
    }
  }
  return false;
}
arr1 = ["Rose", "India", "Technologies"];
document.write("The condition is "+arr1.contains("India")+"<br>");
</script>
</head>
<b>[If the specified element is present in the array, it returns true otherwise 
returns false.]</b>
</html>


In the given code the contain method determines whether the specified element is present in the array or not. If the specified element is present in the array, it returns true otherwise it returns false.
AnswerKenntnis:
indexOf didn't work for me in Internet Explorer 8, and so I used jQuery's inArray() method:

$.inArray("search_string", in_array)
AnswerKenntnis:
Instead of using code snippets found here and there on the web, you can also use a well-tested and documented library like Underscore.string for this. It has an include method that does what you want:

_.str.include("foobar", "ob")
=> true


Here is the description of the library, it just adds 9kb but gives you all the advantages a well-tested and documented library has over copy'n'paste code snippets:


  Underscore.string is JavaScript library for comfortable manipulation
  with strings, extension for Underscore.js inspired by Prototype.js,
  Right.js, Underscore and beautiful Ruby language.
  
  Underscore.string provides you several useful functions: capitalize,
  clean, includes, count, escapeHTML, unescapeHTML, insert, splice,
  startsWith, endsWith, titleize, trim, truncate and so on.


Note well, Underscore.string is influenced by Underscore.js but can be used without it.
AnswerKenntnis:
If you don't like the !!~, etc. tricks, you can simply add +1 to the result of .indexOf(). This way if a string is not found, -1 + 1 = 0 will be falsy, 0.. + 1 = 1.. will be truthy:

if( "StackOverflow".indexOf("Stack") + 1 )
     alert('contains');
else alert('does not contain');
AnswerKenntnis:
Use the inbuilt and simplest one i.e match() on the string. To achieve what you are looking forward do this:

var stringData ="anyString Data";

var subStringToSearch = "any";

// This will give back the substring if matches and if not returns null
var doesContains = stringData.match(subStringToSearch);

if(doesContains !=null) {
    alert("Contains Substring");
}
AnswerKenntnis:
Try this:

if ('Hello, World!'.indexOf('orl') !== -1)
    alert("The string 'Hello World' contains the substring 'orl'!");
else
    alert("The string 'Hello World' does not contain the substring 'orl'!");


Here is an example: http://jsfiddle.net/oliverni/cb8xw/
AnswerKenntnis:
this is function to check a substring is exists in a string or not

function isStringMatch(str, str_to_match) {
   return (str.indexOf(str_to_match) > 0);
}
AnswerKenntnis:
The easyest way is indeed using indexOf.

To just check a string string for a substring substr you can use this method

string = "asdf";
substr = "as";
alert(string.indexOf(substr) == -1 ? false : true);




As you wanted the function string.contains(), you can implement it yourself like this:

String.prototype.contains = function(test) {
    return this.indexOf(test) == -1 ? false : true;
};


now you can use this ecen shorter method to check if a string contains a special substring:

string = "asdf";
alert(string.contains("as"));


Here is a JSFiddle as well.
QuestionKenntnis:
How to pair socks from a pile efficiently?
qn_description:
Yesterday I was pairing the socks from the clean laundry, and figured out the way I was doing it is not very efficient. I was doing a naive search ΓÇö picking one sock and "iterating" the pile in order to find its pair. This requires iterating over n/2 * n/4 = n2/8 socks on average.

As a computer scientist I was thinking what I could do? Sorting (according to size/color/...) of course came into mind to achieve an O(NlogN) solution.

Hashing or other not-in-place solutions are not an option, because I am not able to duplicate my socks (though it could be nice if I could).

So, the question is basically:

Given a pile of n pairs of socks, containing 2n elements (assume each sock has exactly one matching pair), what is the best way to pair them up efficiently with up to logarithmic extra space? (I believe I can remember that amount of info if needed.)

I will appreciate an answer that addresses the following aspects:


A general theoretical solution for a huge number of socks.
The actual number of socks is not that large, I don't believe me and my spouse have more than 30 pairs. (And it is fairly easy to distinguish between my socks and hers, can this be utilized as well?)
Is it equivalent to the element distinctness problem?
AnswersKenntnis
AnswerKenntnis:
Sorting solutions have been proposed but sorting is a little too much: We don't need order, we just need equality groups.

So hashing would be enough (and faster).


For each color of socks, form a pile. Iterate over all socks in your input basket and distribute them onto the color piles.
Iterate over each pile and distribute it by some other metric (e.g. pattern) into a second set of piles
Recursively apply this scheme until you have distributed all socks onto very small piles that you can visually process immediately


This kind of recursive hash partitioning is actually being done by SQL Server when it needs to hash join or hash aggregate over huge data sets. It distributes its build input stream into many partitions which are independent. This scheme scales to arbitrary amounts of data and multiple CPUs linearly.

You don't need recursive partitioning if you can find a distribution key (hash key) that provides enough buckets that each bucket is small enough to be processed very quickly. Unfortunately, I don't think socks have such a property.

If each sock had an integer called "PairID" one could easily distribute them into 10 buckets according to PairID % 10 (the last digit).

The best real-world partitioning I can think of is creating a rectangle of piles: one dimension is color, the other is pattern. Why a rectangle? Because we need O(1) random-access to piles. (A 3D cuboid would also work, but that is not very practical.)



Update:

What about parallelism? Can multiple humans match the socks faster?


The simplest parallization strategy is to have multiple workers take from the input basket and put the socks onto the piles. This only scales up so much - imagine 100 people fighting over 10 piles. The synchronization costs (manifesting themselves as hand-collisions and human communication) destroy efficiency and speed-up (see the Universal Scalability Law!). Is this prone to deadlocks? No, because each work only needs to access one pile at a time. With just one "lock" there cannot be a deadlock. Livelocks might be possible depending on how the humans coordinate access to piles. They might just use random backoff like network cards do that on a physical level to determine what card can exclusively access the network wire. If it works for NICs, it should work for humans as well.
It scales nearly indefinitely if each worker has its own set of piles. Workers can then take from the input basket big chunks of socks (very little contention as they are doing it rarely) and they do not need to sync when distributing the socks at all (because they have thread-local piles). At the end all workers need to union their pile-sets. I believe that can be done in O(log (worker count * piles per worker)) if the workers form an aggregation tree.


What about the element distinctness problem? As the article states, the element distinctness problem can be solved in O(N). This is the same for the socks problem (also O(N), if you need only one distribution step (I proposed multiple steps only because humans are bad at calculations - one step is enough if you distribute on md5(color, length, pattern, ...), i.e. a perfect hash of all attributes)).

Clearly, one cannot go faster than O(N) so we have reached the optimal lower bound.

Although the outputs are not exactly the same (in one case, just a boolean. In the other case, the pairs of socks), the asymptotic complexities are the same.
AnswerKenntnis:
As architecture of human brain is completely different than of modern CPU this question makes no practical sense.

Humans can win over CPU algorithms using the fact that "finding a matching pair" can be one operation for a set that isn't too big.

My algorithm:

spread_all_socks_on_flat_surface();
while (socks_left_on_a_surface()) {
     // thanks to human visual SIMD, this is one, quick operation
     pair = notice_any_matching_pair();
     remove_socks_pair_from_surface(pair);
}


At least this is what I am using in real life and I find it very efficient. The downside is it requires flat surface, but it's usually abundant.
AnswerKenntnis:
Case 1: All socks are identical (this is what I do in real life by the way). 

Pick any two of them to make a pair. Constant time.

Case 2: There are a constant number of combinations (ownership, color, size, texture, etc.).

Use radix sort. This is only linear time since comparison is not required.

Case 3: The number of combinations is not known in advance (general case).

We have to do comparison to check whether two socks come in pair. Pick one of the O(n log n) comparison-based sorting algorithms.

However in real life when the number of socks is relatively small (constant), these theoretically optimal algorithms wouldn't work well. It might take even more time than sequential search, which theoretically requires quadratic time.
AnswerKenntnis:
Non-algorithmic answer, yet "efficient" when I do it:


step 1) discard all your existing socks
step 2) go to Walmart and buy them by packets of 10 - n packet of
white and m packets of black. No need for other colors in everyday's
life.


Yet times to times, I have to do this again (lost socks, damaged socks, etc.), and I hate to discard perfectly good socks too often (and I wished they kept selling the same socks reference!), so I recently took a different approach.

Algorithmic answer:

Consider than if you draw only one sock for the second stack of socks, as you are doing, your odds of finding the matching sock in a naive search is quite low.


So pick up five of them at random, and memorize their shape or their length.


Why five? Usually humans are good are remembering between five and seven different elements in the working memory - a bit like the human equivalent of a RPN stack - five is a safe default.


Pick up one from the stack of 2n-5.
Now look for a match (visual pattern matching - humans are good at that with a small stack) inside the five you drew, if you don't find one, then add that to your five.
Keep randomly picking socks from the stack and compare to your 5+1 socks for a match. As your stack grows, it will reduce your performance but raise your odds. Much faster.


Feel free to write down the formula to calculate how many samples you have to draw for a 50% odds of a match. IIRC it's an hypergeometric law.

I do that every morning and rarely need more than three draws - but I have n similar pairs (around 10, give or take the lost ones) of m shaped white socks. Now you can estimate the size of my stack of stocks :-)

BTW, I found that the sum of the transaction costs of sorting all the socks every time I needed a pair were far less than doing it once and binding the socks. A just-in-time works better because then you don't have to bind the socks, and there's also a diminishing marginal return (that is, you keep looking for that two or three socks that when somewhere in the laundry and that you need to finish matching your socks and you lose time on that).
AnswerKenntnis:
What I do is that I pick up the first sock and put it down (say, on the edge of the laundry bowl). Then I pick up another sock and check to see if it's the same as the first sock. If it is, I remove them both. If it's not, I put it down next to the first sock. Then I pick up the third sock and compare that to the first two (if they're still there). Etc.

This approach can be fairly easily be implemented in an array, assuming that "removing" socks is an option. Actually, you don't even need to "remove" socks. If you don't need sorting of the socks (see below), then you can just move them around and end up with an array that has all the socks arranged in pairs in the array.

Assuming that the only operation for socks is to compare for equality, this algorithm is basically still an n2 algorithm, though I don't know about the average case (never learned to calculate that).

Sorting, of course improves efficiency, especially in real life where you can easily "insert" a sock between two other socks. In computing the same could be achieved by a tree, but that's extra space. And, of course, we're back at NlogN (or a bit more, if there are several socks that are the same by sorting criteria, but not from the same pair).

Other than that, I cannot think of anything, but this method does seem to be pretty efficient in real life. :)
AnswerKenntnis:
The theoretical limit is O(n) because you need to touch each sock (unless some are already paired somehow).

You can achieve O(n) with radix sort. You just need to pick some attributes for the buckets. 


First you can choose (hers, mine) - split them into 2 piles,
then use colors (can have any order for the colors, e.g. alphabetically by color name) - split them into piles by color (remember to keep the initial order from step 1 for all socks in the same pile),
then length of the sock,
then texture,
....


If you can pick a limited number of attributes, but enough attributes that can uniquely identify each pair, you should be done in O(k * n), which is O(n) if we can consider k is limited.
AnswerKenntnis:
This is asking the wrong question. Right question to ask is, why am I spending time sorting socks? How much does it cost on yearly basis, when you value your free time for X monetary units of your choice? And more often than not, this is not just any free time, it's morning free time, which you could be spending in bed, or sipping your coffee, or leaving a bit early and not being caught in the traffic.

It's often good to take a step back, and think a way around the problem.

And there is a way!

Find sock you like. Take all relevant features into account: color in different lighting conditions, overall quality and durability, comfort in different climatic conditions, odor absorption. Also important is, they should not lose elasticity in storage, so natural fabrics are good, and they should be available in plastic wrapping.

It's better if there's no difference between left and right foot socks, but it's not critical. If socks are left-right symmetrical, finding a pair is O(1) operation, and sorting the socks is approximate O(M) operation, where M is amount of places in your house, which you have littered with socks, ideally some small constant number.

If you chose a fancy pair with different left and right sock, doing a full bucket sort to left and right foot buckets takes O(N+M), where N is amount of socks and M is same as above. Somebody else can give the formula for average iterations of finding first pair, but worst case for finding a pair with blind search is N/2+1, which becomes astronomically unlikely case for reasonable N. This can be sped up by using advanced image recognition algorithms and heuristics, when scanning the pile of unsorted socks with Mk1 Eyeball.

So, algorithm for achieving O(1) sock pairing efficiency (assuming symmetrical sock):


You need to estimate how many pairs of socks you will need for the rest of your life, or perhaps until you retire and move to warmer climes with no need to wear socks ever again. If you are young, you could also estimate how long it takes before we'll all have sock-sorting robots in our homes, and the whole problem becomes irrelevant.
You need to find out how you can order your selected sock in bulk, and how much it costs, and do they deliver.
Order the socks!
Get rid of your old socks.


Alternative step 3 would involve comparing costs of buying same amount of perhaps cheaper socks a few pairs at a time over the years, and adding the cost of sorting socks, but take my word for it: buying in bulk is cheaper! Also, socks in storage increase in value at the rate of sock price inflation, which is more than you would get on many investments. Then again there is also storage cost, but socks really do not take much space at the top shelf of a closet.

Problem solved. So, just get new socks, throw/donate your old ones away, and live happily ever after knowing you are saving money and time every day for the rest of your life.
AnswerKenntnis:
As a practical solution:


Quickly make piles of easily distinguishable socks. (Say by color)
Quicksort every pile and use the length of the sock for comparison. As a human you can make a fairly quick decision which sock to use to partition that avoids worst case. (You can see multiple socks in parallel, use that to your advantage!)
Stop sorting piles when they reached a threshold at which you are comfortable to find spot pairs and unpairable socks instantly


If you have 1000 socks, with 8 colors and an average distribution, you can make 4 piles of each 125 socks in c*n time. With a threshold of 5 socks you can sort every pile in 6 runs. (Counting 2 seconds to throw a sock on the right pile it will take you little under 4 hours.)

If you have just 60 socks, 3 colors and 2 sort of socks (yours / your wife's) you can sort every pile of 10 socks in 1 runs (Again threshold = 5). (Counting 2 seconds it will take you 2 min).

The initial bucket sorting will speed up your process, because it divides your n socks into k buckets in c*n time so than you will only have to do c*n*log(k) work. (Not taking into account the threshold). So all in all you do about n*c*(1 + log(k)) work, where c is the time to throw a sock on a pile.

This approach will be favourable compared to any c*x*n + O(1) method roughly as long as log(k) < x - 1.



In computer science this can be helpful:
We have a collection of n things, an order on them (length) and also an equivalence relation (extra information, for example the color of socks). The equivalence relation allows us to make a partition of the original collection, and in every equivalence class our order is still maintained. The mapping of a thing to it's equivalence class can be done in O(1), so only O(n) is needed to assign each item to a class. Now we have used our extra information and can proceed in any manner to sort every class. The advantage is that the data sets are already significantly smaller.

The method can also be nested, if we have multiple equivalence relations -> make colour piles, than within every pile partition on texture, than sort on length. Any equivalence relation that creates a partition with more than 2 elements that have about even size will bring a speed improvement over sorting (provided we can directly assign a sock to its pile), and the sorting can happen very quickly on smaller data sets.
AnswerKenntnis:
Here's an Omega(n log n) lower bound in comparison based model. (The only valid operation is comparing two socks.)

Suppose that you know that your 2n socks are arranged this way:

p1 p2 p3 ... pn pf(1) pf(2) ... pf(n)

where f is an unknown permutation of the set {1,2,...,n}. Knowing this cannot make the problem harder. There are n! possible outputs (matchings between first and second half), which means you need log(n!) = Omega(n log n) comparisons. This is obtainable by sorting.

Since you are interested in connections to element distinctness problem: proving the Omega(n log n) bound for element distinctness is harder, because the output is binary yes/no. Here, the output has to be a matching and the number of possible outputs suffices to get a decent bound. However, there's a variant connected to element distinctness. Suppose you are given 2n socks and wonder if they can be uniquely paired. You can get a reduction from ED by sending (a1, a2, ..., an) to (a1, a1, a2, a2, ..., an, an). (Parenthetically, the proof of hardness of ED is very interesting, via topology.)

I think that there should be an Omega(n2) bound for the original problem if you allow equality tests only. My intuition is: Consider a graph where you add an edge after a test, and argue that if the graph is not dense the output is not uniquely determined.
AnswerKenntnis:
This question is actually deeply philosophical. At heart it's about whether the power of people to solve problems (the "wetware" of our brains) is equivalent to what can be accomplished by algorithms.

An obvious algorithm for sock sorting is:

Let N be the set of socks that are still unpaired, initially empty
for each sock s taken from the dryer
  if s matches a sock t in N
    remove t from N, bundle s and t together, and throw them in the basket
  else
    add s to N


Now the computer science in this problem is all about the steps


"if s pairs with a sock t in N".  How quickly can we "remember" what we've seen so far?
"remove t from N" and "add s to N".  How expensive is keeping track of what we've seen so far?


Human beings will use various strategies to effect these. Human memory is associative, something like a hash table where feature sets of stored values are paired with the corresponding values themselves. For example, the concept of "red car" maps to all the red cars a person is capable of remembering. Someone with a perfect memory has a perfect mapping.  Most people are imperfect in this regard (and most others).  The associative map has a limited capacity. Mappings may bleep out of existence under various circumstances (one beer too many), be recorded in error ("I though her name was Betty, not Nettie"), or never be overwritten even though we observe that the truth has changed ("dad's car" evokes "orange Firebird" when we actually knew he'd traded that in for the red Camaro).

In the case of socks, perfect recall means looking at a sock s always produces the memory of its sibling t, including enough information (where it is on the ironing board) to locate t in constant time.  A person with photographic memory accomplishes both 1 and 2 in constant time without fail.

Someone with less than perfect memory might use a few commonsense equivalence classes based on features within his capability to track: size (papa, mama, baby), color (greenish, redish, etc.), pattern (argyle, plain, etc.), style (footie, knee-high, etc.).  So the ironing board would be divided into sections for the categories. This usually allows the category to be located in constant time by memory, but then a linear search through the category "bucket" is needed. 

Someone with no memory or imagination at all (sorry) will just keep the socks in one pile and do a linear search of the whole pile.

A neat freak might use numeric labels for pairs as someone suggested.  This opens the door to a total ordering, which allows the human to use exactly the same algorithms we might with a CPU: binary search, trees, hashes, etc.

So the "best" algorithm depends on the qualities of the wetware/hardware/software that is running it and our willingness to "cheat" by imposing a total order on pairs.  Certainly a "best" meta-algorithm is to hire the worlds best sock-sorter: a person or machine that can aquire and quickly store a huge set N of sock attribute sets in a 1-1 associative memory with constant time lookup, insert, and delete. Both people and machines like this can be procured. If you have one, you can pair all the socks in O(N) time for N pairs, which is optimal. The total order tags allow you to use standard hashing to get the same result with either a human or hardware computer.
AnswerKenntnis:
Cost: Moving socks -> high, finding/search socks in line -> small

What we want to do is reduce the number of moves, and compensate with the number of searches. Also, we can utilize the multithreded environment of the Homo Sapiens to hold more things in the descision cache. 

X = Yours, Y = Your spouses

From pile A of all socks:

Pick two socks, place corresponding X sock in X line, and Y sock in Y line at next available position.

Do until A is empty.

For each line X and Y


Pick the first sock in line, search along the line until it finds the corresponding sock.
Put into the corresponding finished line of socks. 
Optional While you are searching the line and and the current sock you are looking at is identical to the previous, do step 2 for these socks.


Optionally to step one, you pick up two sock from that line instead of two, as the caching memory is large enough we can quickly identify if either sock matches the current one on the line you are observing. If you are fortunate enough to have three arms, you could possibly parse three socks at the same time given that the memory of the subject is large enough.

Do until both X and Y is empty.

Done

However, as this have simillar complexity as selection sort, the time taken is far less due to the speeds of I/O(moving socks) and search(searching the line for a sock).
AnswerKenntnis:
This is how I actually do it, for p pairs of socks (n = 2p individual socks):


Grab a sock at random from the pile.
For the first sock, or if all previously-chosen socks have been paired, simply place the sock into the first "slot" of an "array" of unpaired socks in front of you.
If you have one or more selected unpaired socks, check your current sock against all the unpaired socks in the array. 

It is possible to separate socks into general classes or types (white/black, ankle/crew, athletic/dress) when building your array, and "drill-down" to only compare like-for-like.
If you find an acceptable match, put both socks together and remove them from the array.
If you do not, put the current sock into the first open slot in the array.

Repeat with every sock.


The worst-case scenario of this scheme is that every pair of socks is different enough that it must be matched exactly, and that the first n/2 socks you pick are all different. This is your O(n2) scenario, and it's extremely unlikely. If the number of unique types of sock t is less than the number of pairs p = n/2, and the socks in each type are alike enough (usually in wear-related terms) that any sock of that type can be paired with any other, then as I inferred above, the maximum number of socks you will ever have to compare to is t, after which the next one you pull will match one of the unpaired socks. This scenario is much more likely in the average sock drawer than the worst-case, and reduces the worst-case complexity to O(n*t) where usually t << n.
AnswerKenntnis:
From your question it is clear you don't have much actual experience with laundry :). You need an algorithm that works well with a small number of non-pairable socks.

The answers till now don't make good use of our human pattern recognition capabilities. The game of Set provides a clue of how to do this well: put all socks in a two-dimensional space so you can both recognize them well and easily reach them with your hands. This limits you to an area of about 120 * 80 cm or so. From there select the pairs you recognize and remove them. Put extra socks in the free space and repeat. If you wash for people with easily recognizable socks (small kids come to mind), you can do a radix sort by selecting those socks first. This algorithm works well only when the  number of single socks is low
AnswerKenntnis:
In order to say how efficient it is to pair socks from a pile, we have to define the machine first, because the pairing isn't done whether by a turing nor by a random access machine, which are normally used as the basis for an algorithmic analysis.

The machine

The machine is an abstraction of a the real world element called human being. It is able to read from the environment via a pair of eyes. And our machine model is able to manipulate the environment by using 2 arms. Logical and arithmetic operations are calculated using our brain (hopefully ;-)).

We also have to consider the intrinsic runtime of the atomic operations that can be carried out with these instruments. Due to physical constraints, operations which are carried out by an arm or eye have non constant time complexity. This is because we can't move an endlessly large pile of socks with an arm nor can an eye see the top sock on an endlessly large pile of socks.

However mechanical physics give us some goodies as well. We are not limited to move at most one sock with an arm. We can move a whole couple of them at once. 

So depending on the previous analysis following operations should be used in descending order:


logical and arithmetic operations
environmental reads
environmental modifications


We can also make use of the fact that people only have a very limited amount of socks. So an environmental modification can involve all socks in the pile.

The algorithm

So here is my suggestion:


Spread all socks in the pile over the floor.
Find a pair by looking at the socks on the floor.
Repeat from 2 until no pair can be made.
Repeat from 1 until there are no socks on the floor.


Operation 4 is necessary, because when spreading socks over the floor some socks may hide others. Here is the analysis of the algorithm:

The analysis

The algorithm terminates with high probability. This is due to the fact that one is unable to find pairs of socks in step number 2.

For the following runtime analysis of pairing n pairs of socks, we suppose that at least half of the 2n socks aren't hidden after step 1. So in the average case we can find n/2 pairs. This means that the loop is step 4 is executed O(log n) times. Step 2 is executed O(n^2) times. So we can conclude:


The algorithm involves O(ln n + n) environmental modifications (step 1 O(ln n) plus picking every pair of sock from the floor)
The algorithm involves O(n^2) environmental reads from step 2
The algorithm involves O(n^2) logical and arithmetic operations for comparing a sock with another in step 2


So we have a total runtime complexity of O(r*n^2 + w*(ln n + n)) where r and w are the factors for environmental read and environmental write operations respectively for a reasonable amount of socks. The cost of the logical and arithmetical operations are omitted, because we suppose that it takes a constant amount of logical and arithmetical operations to decide whether 2 socks belong to the same pair. This may not be feasible in every scenario.
AnswerKenntnis:
List<Sock> UnSearchedSocks = getAllSocks();
List<Sock> UnMatchedSocks = new list<Sock>();
List<PairOfSocks> PairedSocks = new list<PairOfSocks>();

foreach (Sock newSock in UnsearchedSocks)
{
  Sock MatchedSock = null;
  foreach(Sock UnmatchedSock in UnmatchedSocks)
  {
    if (UnmatchedSock.isPairOf(newSock))
    {
      MatchedSock = UnmatchedSock;
      break;
    }
  }
  if (MatchedSock != null)
  {
    UnmatchedSocks.remove(MatchedSock);
    PairedSocks.Add(new PairOfSocks(MatchedSock, NewSock));
  }
  else
  {
    UnmatchedSocks.Add(NewSock);
  }
}
AnswerKenntnis:
Whenever you pick up a sock, put it in one place.  Then the next sock you pick up, if it doesn't match the first sock, set it beside the first one.  If it does, there's a pair.  This way it doesn't really matter how many combinations there are, and there are only two possibilities for each sock you pick up -- either it has a match that's already in your array of socks, or it doesn't, which means you add it to a place in the array.

This also means that you will almost certainly never have all your socks in the array, because socks will get removed as they're matched.
AnswerKenntnis:
Consider a hash-table of  size  'N'.

If we assume normal distribution, then the estimated number of 'insertions'  to have atleast one sock mapped to one bucket is NlogN  (ie, all buckets are full) 

I had derived this as a part of another puzzle,but I would be happy to be proven wrong. 
Here's my blog article on the same   

Let 'N' correspond to an approximate upper-bound on the number of  number of unique colors/pattern of socks that you have.

Once you have a collision(a.k.a  : a match) simply remove that pair of socks. 
 Repeat the same experiment with the next batch of NlogN socks. 
The beauty of it is that you could be making NlogN parallel comparisons(collision-resolution) because of the way the human mind works.  :-)
AnswerKenntnis:
Socks, whether real ones or some analogous data structure, would be supplied in pairs. 

The simplest answer is prior to allowing the pair to be separated, a single data structure for the pair should have been initialized that contained a pointer to the left and right sock, thus enabling socks to be referred to directly or via their pair. A sock may also be extended to contain a pointer to its partner.

This solves any computational pairing problem by removing it with a layer of abstraction.

Applying the same idea to the practical problem of pairing socks, the apparent answer is: don't allow your socks to ever be unpaired. Socks are provided as a pair, put in the drawer as a pair (perhaps by balling them together), worn as a pair. But the point where unpairing is possible is in the washer, so all that's required is a physical mechanism that allows the socks to stay together and be washed efficiently. 

There are two physical possibilities:

For a 'pair' object that keeps a pointer to each sock we could have a cloth bag that we use to keep the socks together. This seems like massive overhead.

But for each sock to keep a reference to the other, there is a neat solution: a popper (or a 'snap button' if you're American), such as these:

http://www.aliexpress.com/compare/compare-invisible-snap-buttons.html

Then all you do is snap your socks together right after you take them off and put them in your washing basket, and again you've removed the problem of needing to pair your socks with a physical abstraction of the 'pair' concept.
AnswerKenntnis:
Pick up a first sock and place it on a table. Now pick another sock, if it matches the first picked place it on top of the first, if not place it on the table a small distance from the first. Pick a third sock, if it matches either of the previous 2 place it on top of them or else place it a small distance from the third. Repeat until you have picked up all the socks.
AnswerKenntnis:
The answers above are amazing, so this answer may interpreted as trolling.

In short: Don't sort your socks; use a safety pin.

Pin each pair before tossing into the laundry basket.  I haven't sorted socks in years.
AnswerKenntnis:
There's actually no need for searching, nor sorting.

Buy sock clips and wash bag to keep socks together in laundry:
http://www.amazon.co.uk/SOCK-CLIPS-SOCKS-TOGETHER-LAUNDRY/dp/B00BG0JUF4

Every time you have dirty socks clip them and put them in the wash bag.

The bottom line is:

Instead of optimizing the reading operation to sort through data in a random layout, optimize the write operation so that at reading time the data is already in order.
AnswerKenntnis:
I came out with another solution which would not promise less operations, neither less time consumption but should be tried to see if it can be good enough heuristic to provide less time consumption in huge series of sock pairing.

Preconditions:
There is no guarantee that there are the same socks. If they are of the same color it doesn't mean they have the same size or pattern. Socks are randomly shuffled. There can be odd number of socks(some are missing, we don't know how many). Prepare to remember a variable "index" and set it to 0.

The result will have one or two piles: 1. "matched" and 2. "missing"

Heuristic:


Find most distinctive sock.
Find its match.
If there is no match put it on the "missing" pile.
Repeat from 1. until there are no more most distinctive socks.
If there are less then 6 socks, go to 11.
Pair blindly all socks to its neighbor (do not pack it)
Find all matched pairs, pack it and move packed pairs to "matched" pile; 
If there were no new matches - increment "index" by 1
If "index" is greater then 2 ( this could be value dependent on sock 
number because with greater number of socks there are less chance to
pair them blindly ) go to 11
Shuffle the rest 
Go to 1
Forget "index"
Pick a sock
Find its pair
If there is no pair for the sock move it to the "missing" pile
If match found pair it, pack pair and move it to the "matched" pile
If there are still more then one socks go to 12
If there is just one left go to 14
Smile satisfied :)


Also, there could be added check for damaged socks also, as if the removal of those. It could be inserted between 2 and 3, and between 13 and 14 

I have never tried this and looking forward to hear about any experiences or corrections.
AnswerKenntnis:
Real world approach:

As rapidly as possible, remove socks from the unsorted pile one at a time and place in piles in front of you. The piles should be arranged somewhat space-efficiently, with all socks pointing the same direction; the number of piles is limited by the distance you can easily reach. The selection of a pile on which to put a sock should be -- as rapidly as possible -- putting a sock on a pile of apparently like socks; the occasional type I (putting a sock on a pile it doesn't belong to) or type II (putting a sock in its own pile when there's an existing pile of like socks) error can be tolerated -- the most important consideration is speed. Once all the sock are in piles, rapidly go through the multi-sock piles creating pairs and removing them (these are heading for the drawer). If there are non-matching socks in the pile, re-pile them to their best (within the as-fast-as-possible constraint) pile. When all the multi-sock piles have been processed, match up remaining pairable socks that weren't paired due to type II errors. Whoosh, you're done -- and I have a lot of socks and don't wash them until a large fraction are dirty. Another practical note: I flip the top of one of a pair of socks down over the other, taking advantage of their elastic properties, so they stay together while being transported to the drawer and while in the drawer.
AnswerKenntnis:
I have taken simple steps to reduce my effort into a process taking O(1) time.

By reducing my inputs to one of two types of socks (white socks for recreation, black socks for work), I only need to determine which of two socks I have in hand. (Technically, since they are never washed together, I have reduced the process to O(0) time)

Some upfront effort is required to find desirable socks, and to purchase in sufficient quantity as to eliminate need for your existing socks. As I'd done this before my need for black socks, my effort was minimal, but mileage may vary.

Such an upfront effort has been seen many times in very popular and effective code. Examples include #DEFINE'ing pi to several decimals (other examples exist, but that's the one that comes to mind right now).
AnswerKenntnis:
Create a hash table which will be used for unmatched socks, using the pattern as the hash. Iterate over the socks one by one. If the sock has a pattern match in the hash table, take the sock out of the table and make a pair. If the sock does not have a match, put it into the table.
AnswerKenntnis:
The problem of sorting your n pairs of socks is O(n). Before you throw them in the laundry basket, you thread the left one to the right one. On taking them out, you cut the thread and put each pair into your drawer - 2 operations on n pairs, so O(n).

Now the next question is simply whether you do your own laundry and your wife does hers. That is a problem likely in an entirely different domain of problems. :)
AnswerKenntnis:
I've finished to pair my socks just right now, and I found that the best way to do it is the following:
- choose one of the socks and put it away (create a 'bucket' for that pair)
- if the next one is the pair of the previous one, than put it to the existing bucket, otherwise create a new one

in the worst case it means that you will have n/2 different buckets, and you will have n-2 determinations about that which bucket contains the pair of the current sock. Obviously this algorithm works well if you have just a few pairs, I did it with 12 pairs.

Not so scientific, but works well:)
AnswerKenntnis:
What about doing some preprocess?. I would stitch a mark or id number in every sock in a way that every pair has the same mark/id number. This process might be done every time you buy a new pair of socks and if you ask your mom/grandma, maybe it wouldn't cost any effort. Then, you could do radix sort to get O(n) total cost. Find a place for every mark/id number and just pick all the socks one by one and put them into the correct place.
AnswerKenntnis:
I tought about this very often during my PhD (in Computer Sciences). I came up with multiple solutions, depending on the ability of distinguishing socks and thus find correct pairs as fast as possible.

Suppose the cost of looking at socks and memorizing their distinctive patterns is negligible (╬╡). Then the best solution is simply to throw all socks on a table. This involves those steps:


Throw all socks on a table (1) and create a hashmap {pattern: position} (╬╡)
While there are remaining socks (n/2):

Pick up one random sock (1)
Find position of corresponding sock (╬╡)
Retrieve sock (1) and store pair



This is indeed the fastest possibility and is executed in n + 1 = O(n) complexity. But it supposes that you perfectly remember all patterns... In practice, this is not the case, and my personal experience is that you sometimes don't find the matching pair at first attempt:


Throw all socks on a table (1)
While there are remaining socks (n/2):

Pick up one random sock (1)
while it is not paired (1/P):

Find sock with similar pattern
Take sock and compare both (1)
If ok, store pair




This now depends on our ability to find matching pairs. This is particularly true if you have dark/grey pairs or white sport socks that often have very similar patterns! Let's admit that you have a probability of P of finding the corresponding sock. You'll need, on average, 1/P tries before finding the corresponding sock to form a pair. The overall complexity is 1 + (n/2) * (1 + 1/P) = O(n).

Both are linear in the number of socks, and are very similar solutions. Let's slightly modify the problem and admit you have multiple pairs of similar socks in the set, and that it is easy to store multiple pairs of socks in one move (1+╬╡). For K distinct patterns, you may implement:


For each sock (n):

Pick up one random sock (1)
Put it on its pattern's cluster

For each cluster (K):

Take cluster and store pairs of socks (1+╬╡)



The overall complexity becomes n+K = O(n). Still linear, but choosing correct algorithm may now greatly depend on the values of P and K! But one may object again that you may have difficulties to find (or create) cluster for each ssock.

Besides, you may also loose time by looking on websites what is the best algorithm and proposing your own solution :)
AnswerKenntnis:
The real problem here doesn't lie in pairing the socks, but the fact that your wife finds it funny to buy all kinds of different socks. As you already pointed out, you're a computer scientist, so it's likely all your socks are so similar that pairing is a trivial operation. 

Therefore, I propose a hybrid solution. 


Make 2 piles. Put your socks on pile 1, put her socks on pile 2.
If pile 1 already has a sock, instead of putting your sock on pile 1, pair them.
Let your wife deal with her own mess (which probably means a naive search).


Basically you're done in O(n) and you only need 2 'buckets'.

If pairing her socks is apparently a requirement to save your marriage, I would suggest using one of the many methods that have been proposed earlier.
QuestionKenntnis:
Flash CS4 refuses to let go
qn_description:
I have a Flash project, and it has many source files. I have a fairly heavily-used class, call it Jenine. I recently (and, perhaps, callously) relocated Jenine from one namespace to another. I thought we were ready - I thought it was time. The new Jenine was better in every way - she had lost some code bloat, she had decoupled herself from a few vestigial class relationships, and she had finally come home to the namespace that she had always secretly known in her heart was the one she truly belonged to. She was among her own kind.

Unfortunately, Flash would have none of that. Perhaps it had formed an attachment. Perhaps it didn't want Jenine to be decoupled. Either way, it clung to the old, perfect version of Jenine in its memory. It refused to move on. It ignored her (function) calls. It tried to forget her new, public interfaces. Instead, every instance of Jenine that it constructed was always a copy of the old version, down to its classpath:

var jenineInstance:Jenine = new Jenine();
trace( getQualifiedClassName(jenineInstance));
// Should print: com.newnamespace.subspace::Jenine
// Prints: com.oldnamespace.subspace::Jenine
// Ah, young love!


We fought. I'm not proud of some of the things I said or did. In the end, in a towering fit of rage, I deleted all references of Jenine completely. She was utterly, completely erased from the system. My cursor fell upon the "Empty Trash" menu option like the cold lid of a casket.

I don't think Flash ever recovered. To this day it still clings to the memory of Jenine. Her old, imperfect definitions still float through my project like abandoned ghosts. Whenever I force Flash to compile, it still lovingly inserts her into my movie, nestling her definition in amongst the other, living classes, like a small shrine. I wonder if they can see her.

Flash and I don't really talk anymore. I write my code, it compiles it. There's a new girl in town named Summer who looks almost identical to Jenine, as if someone had just copied her source-code wholesale into a new class, but Flash hasn't shown any interest. Most days it just mopes around and writes bad poetry in my comments when it thinks I'm not looking.

I hope no one else has had a similar experience, that this is just a singular, painful ripple in the horrifying dark lagoon that is the Flash code-base. If, by some fluke chance you have, or you have any idea how to erase whatever damn cache the compiler is using, please, please help.
AnswersKenntnis
AnswerKenntnis:
Flash still has the ASO file, which is the compiled byte code for your classes. On Windows, you can see the ASO files here:

C:\Documents and Settings\username\Local Settings\Application Data\Adobe\Flash CS4\en\Configuration\Classes\aso


On a Mac, the directory structure is similar in /Users/username/Library/Application Support/ 



You can remove those files by hand, or in Flash you can select Control->Delete ASO files to remove them.
AnswerKenntnis:
Try deleting your ASO files.

ASO files are cached compiled versions of your class files. Although the IDE is a lot better at letting go of old caches when changes are made, sometimes you have to manually delete them. To delete ASO files: Control>Delete ASO Files.

This is also the cause of the "I-am-not-seeing-my-changes-so-let-me-add-a-trace-now-everything-works" bug that was introduced in CS3.
AnswerKenntnis:
What if you compile it using another machine? A fresh installed one would be lovely. I hope your machine is not jealous.
AnswerKenntnis:
I have found one related behaviour that may help (sounds like your specific problem runs deeper though):

Flash checks whether a source file needs recompiling by looking at timestamps. If its compiled version is older than the source file, it will recompile. But it doesn't check whether the compiled version was generated from the same source file or not.

Specifically, if you have your actionscript files under version control, and you Revert a change, the reverted file will usually have an older timestamp, and Flash will ignore it.
AnswerKenntnis:
Also, to use your new namespaced class you can also do

var jenine:com.newnamespace.subspace.Jenine = com.newnamespace.subspace.Jenine()
AnswerKenntnis:
Do you have several swf-files? If your class is imported in one of the swf's, other swf's will also use the same version of the class. One old import with * in one swf will do it. Recompile everything and see if it works.
AnswerKenntnis:
Use a grep analog to find the strings oldnamespace and Jenine inside the files in your whole project folder. Then you'd know what step to do next.
QuestionKenntnis:
Fix merge conflicts in Git?
qn_description:
Is there a good way to explain how to resolve merge conflicts in Git?
AnswersKenntnis
AnswerKenntnis:
Try: git mergetool

It opens a GUI that steps you through each conflict and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  Much better than doing the whole thing by hand certainly.

By @JoshGlover


  Well, it doesn't necessarily open a GUI unless you install one. Running git mergetool for me resulted in vimdiff being used. You can install one of the following tools to use it instead: meld opendiff kdiff3 tkdiff xxdiff tortoisemerge gvimdiff diffuse ecmerge p4merge araxis vimdiff emerge


p4merge is working great for me. Others I've not tried
AnswerKenntnis:
Here's a probable use-case, from the top:

You're going to pull some changes, but oops, you're not up to date:

git fetch origin
git pull origin master

From ssh://gitosis@example.com:22/projectname
 * branch            master     -> FETCH_HEAD
Updating a030c3a..ee25213
error: Entry 'filename.c' not uptodate. Cannot merge.


So you get up-to-date and try again, but have a conflict:

git add filename.c
git commit -m "made some wild and crazy changes"
git pull origin master

From ssh://gitosis@example.com:22/projectname
 * branch            master     -> FETCH_HEAD
Auto-merging filename.c
CONFLICT (content): Merge conflict in filename.c
Automatic merge failed; fix conflicts and then commit the result.


So you decide to take a look at the changes:

git mergetool


Oh me, oh my, upstream changed some things, but just to use my changes...no...their changes...

git checkout --ours filename.c
git checkout --theirs filename.c
git add filename.c
git commit -m "using theirs"


And then we try a final time

git pull origin master

From ssh://gitosis@example.com:22/projectname
 * branch            master     -> FETCH_HEAD
Already up-to-date.


Ta-da!
AnswerKenntnis:
I find merge tools rarely help me understand the conflict or the resolution. I'm usually more successful looking at the conflict markers in a text editor and using git log as a supplement.

Here are a few tips:

Tip One

The best thing I have found is to use the "diff3" merge conflict style:

git config merge.conflictstyle diff3

This produces conflict markers like this:

<<<<<<<
Changes made on the branch that is being merged into. In most cases,
this is the branch that I have currently checked out (i.e. HEAD).
|||||||
The common ancestor version.
=======
Changes made on the branch that is being merged in. This is often a 
feature/topic branch.
>>>>>>>


The middle section is what the common ancestor looked like. This is useful because you can compare it to the top and bottom versions to get a better sense of what was changed on each branch, which gives you a better idea for what the purpose of each change was.

If the conflict is only a few lines, this generally makes the conflict very obvious. (Knowing how to fix a conflict is very different; you need to be aware of what other people are working on. If you're confused, it's probably best to just call that person into your room so they can see what you're looking at.)

If the conflict is longer, then I will cut and paste each of the three sections into three separate files, such as "mine", "common" and "theirs".

Then I can run the following commands to see the two diff hunks that caused the conflict:

diff common mine
diff common theirs


This is not the same as using merge tool, since merge tool will include all of the non-conflicting diff hunks too. I find that to be distracting.

Tip Two

Somebody already mentioned this, but understanding the intention behind each diff hunk is generally very helpful for understanding where a conflict came from and how to handle it.

git log --merge -p <name of file>


This shows all of the commits that touched that file in between the common ancestor and the two heads you are merging. (So it doesn't include commits that already exist in both branches before merging.) This helps you ignore diff hunks that clearly are not a factor in your current conflict.

Tip Three

Verify your changes with automated tools.

If you have automated tests, run those. If you have a lint, run that. If it's a buildable project, then build it before you commit, etc. In all cases, you need to do a bit of testing to make sure your changes didn't break anything. (Heck, even a merge without conflicts can break working code.)

Tip Four

Plan ahead; communicate with co-workers.

Planning ahead and being aware of what others are working on can help prevent merge conflicts and/or help resolve them earlier -- while the details are still fresh in mind. 

For example, if you know that you and another person are both working on different refactoring that will both affect the same set of files, you should talk to each other ahead of time and get a better sense for what types of changes each of you is making. You might save considerable time and effort if you conduct your planned changes serially rather than in parallel. 

For major refactorings that cut across a large swath of code, you should strongly consider working serially: everybody stops working on that area of the code while 1 person performs the complete refactoring.

If you can't work serially (due to time pressure, maybe), then communicating about expected merge conflicts at least helps you solve the problems sooner while the details are still fresh in mind. For example, if a co-worker is making a disruptive series of commits over the course of a 1-week period, you may choose to merge/rebase on that co-workers branch once or twice each day during that week. That way, if you do find merge/rebase conflicts, you can solve them more quickly than if you wait a few weeks to merge everything together in one big lump.

Tip Five

If you're unsure of a merge, don't force it.

Merging can feel overwhelming, especially when there are a lot of conflicting files and the conflict markers cover hundreds of lines. Often times when estimating software projects we don't include enough time for overhead items like handling a gnarly merge, so it feels like a real drag to spend several hours dissecting each conflict.

In the long run, planning ahead and being aware of what others are working on are the best tools for anticipating merge conflicts and prepare yourself to resolve them correctly in less time.
AnswerKenntnis:
Identify which files are in conflict (Git should tell you this).
Open each file and examine the diffs; Git demarcates them.  Hopefully it will be obvious which version of each block to keep.  You may need to discuss it with fellow developers who committed the code.
Once you've resolved the conflict in a file git add the_file.
Once you've resolved all conflicts, do git rebase --continue or whatever command 
Git said to do when you completed.
AnswerKenntnis:
Check out the answers in Aborting a merge in Git, especially Charles Bailey's answer which shows how to view the different versions of the file with problems, for example, 

# Common base version of the file.
git show :1:some_file.cpp

# 'Ours' version of the file.
git show :2:some_file.cpp

# 'Theirs' version of the file.
git show :3:some_file.cpp
AnswerKenntnis:
If you're making frequent small commits, then start by looking at the commit comments with git log --merge. Then git diff will show you the conflicts.

For conflicts that involve more than a few lines, it's easier to see what's going on in an external gui tool. I like opendiff -- git also supports vimdiff, gvimdiff, kdiff3, tkdiff, meld, xxdiff, emerge out of the box and you can install others: git config merge.tool "your.tool" will set your chosen tool and then git mergetool after a failed merge will  show you the diffs in context.

Each time you edit a file to resolve a conflict git add filename will update the index and your diff will no longer show it. When all the conflicts are handled and their files have been git add-ed, git commit will complete your merge.
AnswerKenntnis:
See How Conflicts Are Presented in Git the git merge docs to understand what merge conflict markers are.

Also, the How to Resolve Conflicts section explains how to resolve the conflicts:


  After seeing a conflict, you can do two things:
  
  
  Decide not to merge. The only clean-ups you need are to reset the index file to the HEAD commit to reverse 2. and to clean up working tree changes made by 2. and 3.; git merge --abort can be used for this.
  Resolve the conflicts. Git will mark the conflicts in the working tree. Edit the files into shape and git add them to the index. Use git commit to seal the deal.
  
  
  You can work through the conflict with a number of tools:
  
  
  Use a mergetool. git mergetool to launch a graphical mergetool which will work you through the merge.
  Look at the diffs. git diff will show a three-way diff, highlighting changes from both the HEAD and MERGE_HEAD versions.
  Look at the diffs from each branch. git log --merge -p <path> will show diffs first for the HEAD version and then the MERGE_HEAD version.
  Look at the originals. git show :1:filename shows the common ancestor, git show :2:filename shows the HEAD version, and git show :3:filename shows the MERGE_HEAD version.
  


You can also read about merge conflict markers and how to resolve them in the Pro Git book section Basic Merge Conflicts.
AnswerKenntnis:
For Emacs users which want to resolve merge conflicts semi-manually:

git diff --name-status --diff-filter=U


shows all files which require conflict resolution.
Open each of those file one by one, or all at once by:

emacs $(git diff --name-only --diff-filter=U)


When visiting a buffer requiring edits in emacs type

ALT+x vc-resolve-conflicts


This will open 3 buffers (mine, theirs, and the output buffer). Navigate by pressing 'n' (next region), 'p' (prevision region). Press 'a' and 'b' to copy mine or theirs region to output buffer, respectively. And/or edit the output buffer directly.
When finished: Press 'q', emacs asks you if you want to save this buffer: yes.
After finishing a buffer mark it as resolved by running from the teriminal:

git add FILENAME


When finished with all buffers type

git commit


to finish the merge.
AnswerKenntnis:
You could fix merge conflicts in a number of ways as other have detailed.

I think the real key is knowing how changes flow with local and remote repositories.  The key to this is understanding tracking branches.  I have found that I think of the tracking branch as the 'missing piece in the middle' between me my local, actual files directory and the remote defined as origin.  

I've personally got into the habit of 2 things to help avoid this.

Instead of:

git add .
git commit -m"some msg"


Which has two drawbacks - 

a) All new/changed files get added and that might include some unwanted changes.
b) You don't get to review the file list first.

So instead I do:

git add file,file2,file3...
git commit # Then type the files in the editor and save-quit.


This way you are more deliberate about which files get added and you also get to review the list and think a bit more while using the editor for the message.  I find it also improves my commit messages when I use a full screen editor rather than the -m option.

[Update - as time has passed I've switched more to:

git status # Make sure I know whats going on
git add .
git commit # Then use the editor


]

Also (and more relevant to your situation), I try to avoid:

git pull


or

git pull origin master.


because pull implies a merge and if you have changes locally that you didn't want merged you can easily end up with merged code and/or merge conflicts for code that shouldn't have been merged.

Instead I try to do

git checkout master
git fetch   
git rebase --hard origin/master # or whatever branch I want.


You may also find this helpful:

git branch, fork, fetch, merge, rebase and clone, what are the differences?
AnswerKenntnis:
CoolAJ86's answer sums up pretty much everything. In case you have changes in both branches in smae piece of code you will have to do a manual merge. 
Open the file in conflict in any text editor and you should see following structure

(Code not in Conflict)
>>>>>>>>>>>
(first alternative for conflict starts here)
Multiple code lines here
===========
(second alternative for conflict starts here)
Multiple code lines here too    
<<<<<<<<<<<
(Code not in conflict here)


Choose one of the alternatives or a combination of both in a way that you want new code to be, while removing equal signs and angle brackets. 

git commit -a -m "commit message"
git push origin master
AnswerKenntnis:
git fetch origin/branch
git reset --hard orgin/branch
git merge master
QuestionKenntnis:
var functionName = function() {} vs function functionName() {}
qn_description:
I've recently started maintaining someone else's JavaScript code. I'm fixing bugs, adding features and also trying to tidy up the code and make it more consistent.

The previous developer uses two ways of declaring functions and I can't work out if there is a reason behind it or not.

The two ways are:

var functionOne = function() {
    // Some code
};




function functionTwo() {
    // Some code
}


What are the reasons for using these two different methods and what are the pros and cons of each? Is there anything that can be done with one method that can't be done with the other?
AnswersKenntnis
AnswerKenntnis:
The difference is that functionOne is defined at run-time, whereas functionTwo is defined at parse-time for a script block.  For example:

<script>
  // Error
  functionOne();

  var functionOne = function() {
  };
</script>

<script>
  // No error
  functionTwo();

  function functionTwo() {
  }
</script>

<script>
  "use strict";
  if (test) {
     // Error
     function functionThree() { doSomething(); }
  }
  else {
     // Error
     function functionThree() { doSomethingElse(); }
  }
</script>
AnswerKenntnis:
First I want to correct Greg: function abc(){} is scoped too — the name abc is defined in the scope where this definition is encountered. Example:

function xyz(){
  function abc(){};
  // abc is defined here...
}
// ...but not here


Secondly, it is possible to combine both styles:

var xyz = function abc(){};


xyz is going to be defined as usual, abc is undefined in all browsers but IE — do not rely on it being defined. But it will be defined inside its body:

var xyz = function abc(){
  // xyz is visible here
  // abc is visible here
}
// xyz is visible here
// abc is undefined here


If you want to alias functions on all browsers use this kind of declaration:

function abc(){};
var xyz = abc;


In this case both xyz and abc are aliases of the same object:

console.log(xyz === abc); // prints "true"


One compelling reason to use the combined style is the "name" attribute of function objects (not supported by IE). Basically when you define a function like this:

function abc(){};
console.log(abc.name); // prints "abc"


its name is automatically assigned. But when you define it like this:

var abc = function(){};
console.log(abc.name); // prints ""


its name is empty — we created an anonymous function and assigned it to some variable.

Another good reason to use the combined style is to use a short internal name to refer to itself, while providing a long non-conflicting name for external users:

// assume really.long.external.scoped is {}
really.long.external.scoped.name = function shortcut(n){
  // let's call itself recursively:
  shortcut(n - 1);
  // ...
  // let's pass itself as a callback:
  someFunction(shortcut);
  // ...
}


In the example above we can do the same with an external name, but it'll be too unwieldy (and slower).

(Another way to refer to itself is to use arguments.callee, which is still relatively long, and not supported in the strict mode.)

Deep down JavaScript treats both statements differently. This is a function declaration:

function abc(){}


abc here is defined everywhere in the current scope:

// we can call it here
abc(); // works
// yet it is defined down there
function abc(){}
// we can call it again
abc(); // works


This is a function expression:

var xyz = function(){};


xyz here is defined from the point of assignment:

// we can't call it here
xyz(); // UNDEFINED!!!
// now it is defined
xyz = function(){}
// we can call it here
xyz(); // works


Function declaration vs. function expression is the real reason why there is a difference demonstrated by Greg.

Fun fact:

var xyz = function abc(){};
console.log(xyz.name); // prints "abc"


Personally I prefer the "function expression" declaration because this way I can control the visibility. When I define the function like that:

var abc = function(){};


I know that I defined the function locally. When I define the function like that:

abc = function(){};


I know that I defined it globally providing that I didn't define abc anywhere in the chain of scopes. This style of definition is resilient even when used inside eval(). While this definition:

function abc(){};


depends on the context and may leave you guessing where it is actually defined, especially in the case of eval() — the answer is: it depends on browser.
AnswerKenntnis:
Speaking about the global context, both, the var statement and a FunctionDeclaration at the end will create a non-deleteable property on the global object, but the value of both can be overwritten.

The subtle difference between the two ways is that when the Variable Instantiation process runs (before the actual code execution) all identifiers declared with var will be initialized with undefined, and the ones used by the FunctionDeclaration's will be available since that moment, for example:

 alert(typeof foo); // 'function', it's already available
 alert(typeof bar); // 'undefined'
 function foo () {}
 var bar = function () {};
 alert(typeof bar); // 'function'


The assignment of the bar FunctionExpression takes place until runtime.

A global property created by a FunctionDeclaration can be overwritten without any problems just like a variable value, e.g.:

 function test () {}
 test = null;


Another obvious difference between your two examples is that the first function doesn't have a name, but the second has it, which can be really useful when debugging (i.e. inspecting a call stack).

About your edited first example (foo = function() { alert('hello!'); };), it is an undeclared assignment, I would highly encourage you to always use the var keyword.

With an assignment, without the var statement, if the referenced identifier is not found in the scope chain, it will become a deleteable property of the global object.

Also, undeclared assignments throw a ReferenceError on ECMAScript 5 under Strict Mode.

A must read:


Named function expressions demystified


Note: This answer has been merged from another question, in which the major doubt and misconception from the OP was that identifiers declared with a FunctionDeclaration, couldn't be overwritten which is not the case.
AnswerKenntnis:
The two code snippets you've posted there will, for almost all purposes, behave the same way.

However, the difference in behaviour is that with the first variant, that function can only be called after that point in the code.

With the second variant, the function is available to code that runs above where the function is declared.

This is because with the first variant, the function is assigned to the variable foo at run time.  In the second, the function is assigned to that identifier foo at parse time.

More technical info

Javascript has three ways of defining functions.


Your first snippet shows a function expression.  This involves using the "function" operator to create a function - the result of that operator can be stored in any variable or object property.  The function expression is powerful that way.  The function expression is often called an "anonymous function" because it does not have to have a name,
Your second example is a function declaration.  This uses the "function" statement to create a function.  The function is made available at parse time and can be called anywhere in that scope.  You can still store it in a variable or object property later.
The third way of defining a function is the "Function()" constructor, which is not shown in your original post.  It's not recommended to use this as it works the same way as eval(), which has its problems.
AnswerKenntnis:
Other commenters have already covered the semantic difference of the two variants above. I wanted to note a stylistic difference: Only the "assignment" variation can set a property of another object.

I often build javascript modules with a pattern like this:

(function(){
        var exports = {};

        function privateUtil() {
                ...
        }

        exports.publicUtil = function() {
                ...
        };

        return exports;
})();


With this pattern, your public functions will all use assignment, while your private functions use declaration.

(Note also that assignment should require a semicolon after the statement, while declaration prohibits it.)
AnswerKenntnis:
Here's the rundown on the standard forms that create functions: (Originally written for another question, but adapted after being moved into the canonical question.)

Function Declaration

The first form is a function declaration, which looks like this:

function x() {
    console.log('x');
}


A function declaration is a declaration; it's not a statement or expression. As such, you don't follow it with a ; (although doing so is harmless).

A function declaration is processed when execution enters the context in which it appears, before any step-by-step code is executed. The function it creates is given a proper name (x in the example above), and that name is put in the scope in which the declaration appears.

Because it's processed before any step-by-step code in the same context, you can do things like this:

x(); // Works even though it's above the declaration
function x() {
    console.log('x');
}


Also because it's not part of the step-by-step execution of the code, you can't put it inside a control structure like try, if, switch, while, etc.

if (someCondition) {
    function foo() {    // <===== INVALID AND WILL FAIL ON
    }                   //        MANY ENGINES
}


Some engines will handle the above even though it's invalid, by rewriting it as a function expression on-the-fly. There's talk of adding a function statement to the next spec (ECMAScript6) to codify that. But with current engines, it will not work reliably; don't do it.

Anonymous Function Expression

The second common form is called an anonymous function expression:

var y = function () {
    console.log('y');
};


The function this creates has no name (it's anonymous). Like all expressions, it's evaluated when it's reached in the step-by-step execution of the code.

I should note that the current draft of the next version of the JavaScript standard, ECMAScript6, will actually assign a name to that function by inferring it from the context. The name in this case would be y. Mozilla's (Firefox's) JavaScript engine does that already, and it will be in the next spec. Basically any time the parser can make a reasonable guess, as it can above, it will (once engines are doing this new thing).

Named Function Expression

The third form is a named function expression ("NFE"):

var z = function w() {
    console.log('zw')
};


The function this creates has a proper name (w in this case). Like all expressions, this is evaluated when it's reached in the step-by-step execution of the code. The name of the function is not added to the scope in which the expression appears; the name is in scope within the function itself:

var z = function w() {
    console.log(typeof w); // "function"
};
console.log(typeof w);     // "undefined"


Note that NFEs have frequently been a source of bugs for JavaScript implementations. IE8 and earlier, for instance, handle NFEs completely incorrectly, creating two different functions at two different times. Early versions of Safari had issues as well. The good news is that current versions of browsers (IE9 and up, current Safari) don't have those issues any more. (But as of this writing, sadly, IE8 remains in widespread use, and so using NFEs with code for the web in general is still problematic.)
AnswerKenntnis:
An important reason is to add one and only one variable as the "Root" of your namespace...

var MyNamespace = {}
MyNamespace.foo= function() {

}


or

var MyNamespace {
  foo: function() {
  },
  ...
}


There are many techniques for namespacing.  Its become more important with the plethora of JavaScript modules available.

Also see Javascript Namespace Declaration
AnswerKenntnis:
An illustration of when to prefer the first method on the second one is when you need to avoid overriding a function's previous definition.

With :

if (condition){
    function myfunction(){
        // some code 
    }
}


, this definition of myfunction will override any previous definition, since it will be done at parse-time.

While :

if (condition){
    var myfunction = function (){
        // some code
    }
}


does the correct job of defining myfunction only when condition is met.
AnswerKenntnis:
A function declaration and a function expression assigned to a variable behave the same once the binding is established.

There is a difference however at how and when the function object is actually associated with its variable. This difference is due to the mechanism called variable hoisting in JavaScript.

Basically, all function declarations and variable declarations are hoisted to the top of the function in which the declaration occurs (this is why we say that JavaScript has function scope).


When a function declaration is hoisted, the function body "follows"
so when the function body is evaluated, the variable will immediately
be bound to a function object.
When a variable declaration is hoisted, the initialization does not
follow, but is "left behind". The variable is initialized to
undefined at the start of the function body, and will be assigned
a value at its original location in the code. (Actually, it will be assigned a value at every location where a declaration of a variable with the same name occurs.)


The order of hoisting is also important: function declarations take precedence over variable declarations with the same name, and the last function declaration takes precedence over previous function declarations with the same name.

Some examples...

var foo = 1;
function bar() {
  if (!foo) {
    var foo = 10 }
  return foo; }
bar() // 10


Variable foo is hoisted to the top of the function, initialized to undefined, so that !foo is true, so foo is assigned 10. The foo outside of bar's scope plays no role and is untouched. 

function f() {
  return a; 
  function a() {return 1}; 
  var a = 4;
  function a() {return 2}}
f()() // 2

function f() {
  return a;
  var a = 4;
  function a() {return 1};
  function a() {return 2}}
f()() // 2


Function declarations take precedence over variable declarations, and the last function declaration "sticks".

function f() {
  var a = 4;
  function a() {return 1}; 
  function a() {return 2}; 
  return a; }
f() // 4


In this example a is initialized with the function object resulting from evaluating the second function declaration, and then is assigned 4.

var a = 1;
function b() {
  a = 10;
  return;
  function a() {}}
b();
a // 1


Here the function declaration is hoisted first, declaring and initializing variable a. Next, this variable is assigned 10. In other words: the assignment does not assign to outer variable a.
AnswerKenntnis:
I use the variable approach in my code for a very specific reason, the theory of which has been covered in an abstract way above, but an example might help some people like me, with limited javascript expertise.

I have code that I need to run with 160 independently-designed brandings. Most of the code is in shared files, but branding-specific stuff is in a separate file, one for each branding.

Some brandings require specific functions, some do not.  Sometimes I have to add new functions to do new branding-specific things.  I am happy to change the shared coded, but I don't want to have to change all 160 sets of branding files.

By using the variable syntax, I can declare the variable (a function pointer essentially) in the shared code and either assign a trivial stub function, or set to null.  

The one or two brandings that need a specific implementation of the function can then define their version of the function and assign this to the variable if they want, the rest do nothing.  I can test for a null function before I execute it in the shared code.

From people's comments above, I gather it may be possible to redefine a static function too, but I think the variable solution is nice and clear.
AnswerKenntnis:
In computer science terms, we are talking about anonymous functions and named functions. I think the most important difference is that an anonymous function is not bound to an name, hence the name anonymous function. In Javascript it is a first class object dynamically declared at runtime.

For more informationen on anonymous functions and lambda calculus, Wikipedia is a good start (http://en.wikipedia.org/wiki/Anonymous_function).
AnswerKenntnis:
In terms of code maintenance cost named functions are more preferable:


independent from place where they are declared( but still limited by scope). 
More resistant to mistakes like conditional initialization.(You are still able to override if wanted to).
The code becomes more readable by allocating local functions separately of scope functionality. Usually in the scope the functionality goes first, followed by declarations of local functions.
in debugger you will clearly see on call stack the function name instead of "anonymous/evaluated" function.


I suspect more PROS for named functions are follow.
And what is listed as advantage of named functions is disadvantage for anonymous ones.

Historically anonymous functions appeared from inability of JS as language to list members with named functions: 

{ member:function(){/* how to make this.member a named function? */}
}
AnswerKenntnis:
@EugeneLazutkin gives an example where he names an assigned function to be able to use shortcut() as an internal reference to itself. John Resig gives another example - copying a recursive function assigned to another object in his Learning Advanced Javascript tutorial. While assigning functions to properties isn't strictly the question here, I recommend actively trying the tutorial out - run the code by clicking the button in the upper right corner, and double click the code to edit to your liking.

Examples from the tutorial: recursive calls in yell():

Tests fail when the original ninja object is removed. (page 13)

var ninja = { 
  yell: function(n){ 
    return n > 0 ? ninja.yell(n-1) + "a" : "hiy"; 
  } 
}; 
assert( ninja.yell(4) == "hiyaaaa", "A single object isn't too bad, either." ); 

var samurai = { yell: ninja.yell }; 
var ninja = null; 

try { 
  samurai.yell(4); 
} catch(e){ 
  assert( false, "Uh, this isn't good! Where'd ninja.yell go?" ); 
}


If you name the function that will be called recursively, the tests will pass. (page 14)

var ninja = { 
  yell: function yell(n){ 
    return n > 0 ? yell(n-1) + "a" : "hiy"; 
  } 
}; 
assert( ninja.yell(4) == "hiyaaaa", "Works as we would expect it to!" ); 

var samurai = { yell: ninja.yell }; 
var ninja = {}; 
assert( samurai.yell(4) == "hiyaaaa", "The method correctly calls itself." );
AnswerKenntnis:
First one(function doSomething(x)) should be part of an object notation.

The second one(var doSomething = function(x){ alert(x);}) is simply creating an anonymous function and assigning it to a variable doSomething . So doSomething() will call the function.

you may want to What is a Function Declaration and  Function Expression 

A Function Declaration defines a named function variable without requiring variable assignment. Function Declarations occur as standalone constructs and cannot be nested within non-function blocks. 

function foo() {
    return 3;
}



  ECMA 5 (13.0) defines the syntax as
  function Identifier ( FormalParameterListopt ) { FunctionBody }


in above condition the function name is visible within itΓÇÖs scope and the scope of itΓÇÖs parent (otherwise it would be unreachable)

and in function expression 

A Function Expression defines a function as a part of a larger expression syntax (typically a variable assignment ). Functions defined via Functions Expressions can be named or anonymous. Function Expressions should not start with ΓÇ£functionΓÇ¥  

//anonymous function expression
var a = function() {
    return 3;
}

//named function expression
var a = function foo() {
    return 3;
}

//self invoking function expression
(function foo() {
    alert("hello!");
})();



  ECMA 5 (13.0) defines the syntax as
  function Identifieropt ( FormalParameterListopt ) { FunctionBody }
AnswerKenntnis:
If you would use those functions to create objects, you would get:

var objectOne = new functionOne();
console.log(objectOne.__proto__); // prints "Object {}" because constructor is an anonymous function

var objectTwo = new functionTwo();
console.log(objectTwo.__proto__); // prints "functionTwo {}" because constructor is a named function
AnswerKenntnis:
Another difference that is not mentioned in the other answers is that if you use the anonymous function

var functionOne = function() {
    // Some code
};


and use that as a constructor as in

var one = new functionOne();


then one.constructor.name will not be defined. Function.name is non-standard but is supported by Firefox, Chrome, other Webkit-derived browsers and IE 9+.

With 

function functionTwo() {
    // Some code
}
two = new functionTwo();


it is possible to retrieve the name of the constructor as a string with two.constructor.name.
AnswerKenntnis:
The first is function declaration

function abc(){}


And the second is function expression

var abc = function() {};


The main difference is with hoisting. In the first example, the whole function declaration get hoisted, when in the second only the var abc is hoisted and undefined is assigned to it, while the function itself remains at the position that is declared. In a simple words:

//this will work
abc(param);
function abc(){}

//this would fail
abc(param);
var abc = function() {}


To study more about this topics visit this 
link
QuestionKenntnis:
Does it matter which equals operator (== vs ===) I use in JavaScript comparisons?
qn_description:
I'm using JSLint to go through some horrific JavaScript at work and it's returning a huge number of suggestions to replace == (two equals signs) with === (three equals signs) when doing things like comparing idSele_UNVEHtype.value.length == 0 inside of an if statement.

Is there a performance benefit to replacing == with ===? 

Any performance improvement would probably be welcomed as there are hundreds (if not thousands) of these comparison operators being used throughout the file.

Would I be correct in assuming that if no type conversion takes place, there would be a small (probably extremely small) performance gain over ==?
AnswersKenntnis
AnswerKenntnis:
The identity (===) operator behaves identically to the equality (==) operator except no type conversion is done, and the types must be the same to be considered equal.

Reference: Javascript Tutorial: Comparison Operators

The == operator will compare for equality after doing any necessary type conversions.  The === operator will not do the conversion, so if two values are not the same type === will simply return false. It's this case where === will be faster, and may return a different result than ==. In all other cases performance will be the same.

To quote Douglas Crockford's excellent JavaScript: The Good Parts,


  JavaScript has two sets of equality operators: === and !==, and their evil twins == and !=.  The good ones work the way you would expect.  If the two operands are of the same type and have the same value, then === produces true and !== produces false.  The evil twins do the right thing when the operands are of the same type, but if they are of different types, they attempt to coerce the values.  the rules by which they do that are complicated and unmemorable.  These are some of the interesting cases:

'' == '0'           // false
0 == ''             // true
0 == '0'            // true

false == 'false'    // false
false == '0'        // true

false == undefined  // false
false == null       // false
null == undefined   // true

' \t\r\n ' == 0     // true

  
  The lack of transitivity is alarming.  My advice is to never use the evil twins.  Instead, always use === and !==.  All of the comparisons just shown produce false with the === operator.




Update:

A good point was brought up by @Casebash in the comments and in @Phillipe Laybaert's answer concerning reference types.  For reference types == and === act consistently with one another (except in a special case).

var a = [1,2,3];
var b = [1,2,3];

var c = { x: 1, y: 2 };
var d = { x: 1, y: 2 };

var e = "text";
var f = "te" + "xt";

a == b            // false
a === b           // false

c == d            // false
c === d           // false

e == f            // true
e === f           // true


The special case is when you compare a literal with an object that evaluates to the same literal, due to its toString or valueOf method. For example, consider the comparison of a string literal with a string object created by the String constructor.

"abc" == new String("abc")    // true
"abc" === new String("abc")   // false


Here the == operator is checking the values of the two objects and returning true, but the === is seeing that they're not the same type and returning false.  Which one is correct?  That really depends on what you're trying to compare.  My advice is to bypass the question entirely and just don't use the String constructor to create string objects.
AnswerKenntnis:
Using the == operator (Equality)

true == 1; //true, because 'true' is converted to 1 and then compared
"2" == 2;  //true, because 2 is converted to "2" and then compared


Using the === operator (Identity)

true === 1; //false
"2" === 2;  //false


This is because the equality operator == does type coercion, meaning that the interpreter implicitly tries to convert the values before comparing.

On the other hand, the identity operator === does not do type coercion, and thus does not convert the values when comparing.
AnswerKenntnis:
In the answers here, I didn't read anything about what equal means. Some will say that === means equal and of the same type, but that's not really true. It actually means that both operands reference the same object, or in case of value types, have the same value.

So, let's take the following code:

var a = [1,2,3];
var b = [1,2,3];
var c = a;

var ab_eq = (a === b); // false (even though a and b are the same type)
var ac_eq = (a === c); // true


The same here:

var a = { x: 1, y: 2 };
var b = { x: 1, y: 2 };
var c = a;

var ab_eq = (a === b); // false (even though a and b are the same type)
var ac_eq = (a === c); // true


Or even:

var a = { };
var b = { };
var c = a;

var ab_eq = (a === b); // false (even though a and b are the same type)
var ac_eq = (a === c); // true


This behavior is not always obvious. There's more to the story than being equal and being of the same type.

The rule is:

For value types (numbers):
   a === b returns true if a and b have the same value and are of the same type

For reference types:
   a === b returns true if a and b reference the exact same object

For strings:
   a === b returns true if a and b are both strings and contain the exact same characters



Strings: the special case...

Strings are not value types, but in Javascript they behave like value types, so they will be "equal" when the characters in the string are the same and when they are of the same length (as explained in the third rule)

Now it becomes interesting:

var a = "12" + "3";
var b = "123";

alert(a === b); // returns true, because strings behave like value types


But how about this?:

var a = new String("123");
var b = "123";

alert(a === b); // returns false !! (but they are equal and of the same type)


I thought strings behave like value types? Well, it depends who you ask... In this case a and b are not the same type. a is of type Object, while b is of type string. Just remember that creating a string object using the String constructor creates something of type Object that behaves as a string most of the time.
AnswerKenntnis:
This question is more than a year old, but please let me add this counsel:

If in doubt, read the specification! 

ECMA-262 is the specification for a scripting language of which Javascript is a dialect. Of course in the practice it matters more how the most important browsers behave than an esoteric definition how something is supposed to be handled. But it is helpful to understand why new String("a") !== "a".

Please let me explain how to read the specification to clarify this question. I see that in this very old topic nobody had an answer for the very strange effect. So, if you can read a specification, this will help you in your profession tremendously. It is an acquired skill. So, let's continue.

Searching the PDF file for === brings me to page 56 of the specification: 11.9.4. The Strict Equals Operator ( === ), and after wading through the specificationalese I find:


  11.9.6 The Strict Equality Comparison Algorithm
  The comparison x === y, where x and y are values, produces true or false. Such a comparison is performed as follows:
    1. If Type(x) is different from Type(y), return false.
    2. If Type(x) is Undefined, return true.
    3. If Type(x) is Null, return true.
    4. If Type(x) is not Number, go to step 11.
    5. If x is NaN, return false.
    6. If y is NaN, return false.
    7. If x is the same number value as y, return true.
    8. If x is +0 and y is ΓêÆ0, return true.
    9. If x is ΓêÆ0 and y is +0, return true.
    10. Return false.
    11. If Type(x) is String, then return true if x and y are exactly the same sequence of characters (same length and same characters in corresponding positions); otherwise, return false.
    12. If Type(x) is Boolean, return true if x and y are both true or both false; otherwise, return false.
    13. Return true if x and y refer to the same object or if they refer to objects joined to each other (see 13.1.2). Otherwise, return false.


Interesting ist step 11. Yes, strings are treated as value types. But this does not explain why new String("a") !== "a". Do we have a browser not conforming to ECMA-262?

Not so fast!

Let's check the types of the operands. Try it out for yourself by wrapping them in typeof(). I find that new String("a") is an object, and step 1 is used: return false if the types are different.

If you wonder why new String("a") does not return a string, how about some exercise reading a specification? Have fun!



Aidiakapi wrote this in a comment below:


  From the specification 
  
  11.2.2 The new Operator:
  
  If Type(constructor) is not Object, throw a TypeError exception.
  
  With other words, if String wouldn't be of type Object it couldn't be used with the new operator. 


new always returns an Object, even for String constructors, too. And alas! The value semantics for strings (see step 11) is lost.

And this finally means: new String("a") !== "a".
AnswerKenntnis:
In PHP and JavaScript, it is a strict equality operator. Which means, it will compare both type and values.
AnswerKenntnis:
I tested this in Firefox with Firebug using code like this:

console.time("testEquality");
var n = 0;
while(true) {
    n++;
    if(n==100000) break;
}
console.timeEnd("testEquality");


and

console.time("testTypeEquality");
var n = 0;
while(true) {
    n++;
    if(n===100000) break;
}
console.timeEnd("testTypeEquality");


My results (tested 5 times each and averaged):

==: 115.2
===: 114.4


so I'd say that the miniscule difference (this is over 100000 iterations, remember) is negligible. Performance ISN'T a reason to do ===. Type safety (well as safe as you're gonna get in JS), and code quality is.
AnswerKenntnis:
In Javascript it means of the same value and type

for example 

4 == "4" will return true


but

4 === "4" will return false
AnswerKenntnis:
The === operator is called a strict comparison operator, it does differ from the == operator.

Lets take 2 vars a and b.

For "a == b" to evaluate to true a and b need to be the same value.

In the case of "a === b" a and b must be the same value and also the same type for it to evaluate to true.  

Take the following example

var a = 1;
var b = "1";

if (a == b) //evaluates to true as a and b are both 1
{
    alert("a == b");
}

if (a === b) //evaluates to false as a is not the same type as b
{
    alert("a === b");
}


In summary; using the == operator might evaluate to true in situations where you do not want it to so using the === operator would be safer.  

In the 90% usage scenario it won't matter which one you use, but it is handy to know the difference when you get some unexpected behaviour one day.
AnswerKenntnis:
It means equality without type coercion

0==false   // true
0===false  // false, different types
AnswerKenntnis:
it checked for the same type also for both sides. 

var x = '1';
var y = 1;
x === y // false


'string' != 'number' for example
AnswerKenntnis:
An interesting pictorial representation of the equality comparison between == and ===.  

Source: http://dorey.github.io/JavaScript-Equality-Table/



var1===var2


  When using three equals signs for JavaScript equality testing,
  everything is as is. Nothing gets converted before being evaluated.






var1==var2


  When using two equals signs for JavaScript equality testing, some
  funky conversions take place.







  Moral of the story: Use three equals unless you fully understand the
  conversions that take place for two-equals.
AnswerKenntnis:
In a typical script there will be no performance difference. More important may be the fact that thousand "===" is 1KB heavier than thousand "==" :) Javascript profilers can tell you if there is performance difference in your case.

But personally i would do what JSLint suggests. This recommendation is there not because of performance issues, but because type coercion means ('\t\r\n' == 0) is true.
AnswerKenntnis:
The equal comparison operator == is confusing and should be avoided. 

If you HAVE TO live with it, then remember the following 3 things: 


It is not transitive: (a == b) and (b == c) does not lead to (a == c)
It's mutually exclusive to its negation: (a == b) and (a != b) always hold opposite Boolean values, with all a and b.
In case of doubt, learn by heart the following truth table:


EQUAL OPERATOR TRUTH TABLE IN JAVASCRIPT


Each row in the table is a set of 3 mutually "equal" values, meaning that any 2 values among them are equal using the equal == sign*


** STRANGE: note that any two values on the first column are not equal in that sense.**

''       == 0 == false   // Any two values among these 3 ones are equal with the == operator
'0'      == 0 == false   // Also a set of 3 equal values, note that only 0 and false are repeated
'\t'     == 0 == false   // -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
'\r'     == 0 == false   // -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
'\n'     == 0 == false   // -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
'\t\r\n' == 0 == false   // -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

null == undefined  // These two "default" values are not-equal to any of the listed values above
NaN                // NaN is not equal to any thing, even to itself.
AnswerKenntnis:
There is unlikely to be any performance difference between the two operations in your usage. There is no type-conversion to be done because both parameters are already the same type. Both operations will have a type comparison followed by a value comparison.
AnswerKenntnis:
From the core javascript reference
=== Returns true if the operands are strictly equal (see above) with no type conversion.
AnswerKenntnis:
it checks the values well as type of the variable to for equality
AnswerKenntnis:
It's a strict check test.

It's a good thing especially if you're checking between 0 and false and null. 

For example, if you have:

$a = 0;

Then:

$a==0; 
$a==NULL;
$a==false;


All returns true and you may not want this. Let's suppose you have a function that can return the 0th index of an array or false on failure. If you check with "==" false, you can get a confusing result.

So with the same thing as above, but a strict test:

$a = 0;

$a===0; // returns true
$a===NULL; // returns false
$a===false; // returns false
AnswerKenntnis:
JSLint sometimes gives you unrealistic reasons to modify stuff. === has the exactly same performance as == if the types are already the same. 

It is faster only when the types are not the same, in which case it does not try to convert types but directly returns a false.

So, IMHO, JSLint maybe used to write new code, but useless over-optimizing should be avoided at all costs. 

Meaning, there is no reason to change == to === in a check like if (a == 'test') when you know it for a fact that a can only be a String. 

Modifying a lot of code that way wastes developers' and reviewers' time and achieves nothing.
AnswerKenntnis:
As a rule of thumb, I would generally use === instead of == (and !== instead of !=).

Reasons are explained in in the answers above and also Douglas Crockford is pretty clear about it (JavaScript: The Good Parts).

However there is one single exception:
== null is an efficient way to check for 'is null or undefined':

if( value == null ){
    // value is either null or undefined
}


For example jQuery 1.9.1 uses this pattern 43 times, and  the JSHint syntax checker even provides the eqnull relaxing option for this reason.

From the jQuery style guide:


  Strict equality checks (===) should be used in favor of ==. The only
  exception is when checking for undefined and null by way of null.

// Check for both undefined and null values, for some important reason. 
undefOrNull == null;
AnswerKenntnis:
JavaScript === vs == .

    0==false   // true
    0===false  // false, because they are of a different type
    1=="1"     // true, auto type coercion
    1==="1"    // false, because they are of a different type
AnswerKenntnis:
The problem is that you might easily get into trouble since JS have a lot of implicit conversions meaning ...

var x = 0;
var isTrue = x == null;
var isFalse = x === null;


Which pretty soon becomes a problem. The best sample of why implicit conversion is "evil" can be taken from this code in MFC / C++ which actually will compile due to an implicit conversion from CString to HANDLE which is a pointer typedef type...

CString x;
delete x;


Which obviously during runtime does very undefined things...

Google for impliciti conversions in C++ and STL to get some of the arguments against it...
AnswerKenntnis:
The top 2 answers both mentioned == means equality and === means identity. Unfortunately, this statement is incorrect. 

If both operands of == are objects, then they are compared to see if they are the same object. If both operands point to the same object, then the equal operator returns true. Otherwise,
the two are not equal. 

var a = [1, 2, 3];  
var b = [1, 2, 3];  
console.log(a == b)  // false  
console.log(a === b) // false  


In the code above, both == and === get false because a and b are not the same objects.

That's to say: if both operands of == are objects, == behaves same as ===, which also means identity. The essential difference of this two operators is about type conversion. == has conversion before it checks equality, but === does not.
AnswerKenntnis:
Equality comparison: 

Operator ==

Returns true, when both operands are equal. The operands are converted to the same type before being compared.

>>> 1 == 1
true
>>> 1 == 2
false
>>> 1 == '1'
true


Equality and type comparison: 

Operator ===

Returns true if both operands are equal and of the same type. It's generally 
better and safer if you compare this way, because there's no behind-the-scenes type conversions.

>>> 1 === '1'
false
>>> 1 === 1
true
AnswerKenntnis:
Here is a handy comparison table that shows the conversions that happen and the differences between == and ===.

As the conclusion states:


  "Use three equals unless you fully understand the conversions that take
  place for two-equals."


http://dorey.github.io/JavaScript-Equality-Table/
AnswerKenntnis:
null and undefined are nothingness. i.e.
var a;
var b = null;

Here a and b do not have values. Whereas, 0,false and '' are all values.One thing common beween all these are that they are all falsy values, which means they all satisfy falsy conditions.

So, the 0,false and '' together form a sub-group. And on other hand, null & undefined form the second sub-group. Check the comparisons in below image. null and undefined would equal. the other three would equal to each other. But, they all are treated as falsy conditions in JS.



This is same as any object(like {},arrays etc.), non-empty string & Boolean true are all truthy conditions. But, they are all not equal.
AnswerKenntnis:
*Operators === vs == * 

1 == true    =>    true
true == true    =>    true
1 === true    =>    false
true === true    =>    true
QuestionKenntnis:
Difference between px, dp, dip and sp in Android?
qn_description:
Can anyone explain the differences between the px, dip, dp and sp units in Android?
AnswersKenntnis
AnswerKenntnis:
px is one pixel.
sp is scale-independent pixels.
dip is Density-independent pixels.


You would use 


sp for font sizes 
dip for everything else.

dip==dp


From Android Developers center:


  px
  Pixels - corresponds to actual pixels on the screen.  
  
  in
  Inches - based on the physical size of the screen.  
  
  mm
  Millimeters - based on the physical size of the screen.  
  
  pt
  Points - 1/72 of an inch based on the physical size of the screen.  
  
  dp
  Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. The ratio of dp-to-pixel will change with the screen density, but not necessarily in direct proportion. Note: The compiler accepts both "dip" and "dp", though "dp" is more consistent with "sp".  
  
  sp
  Scale-independent Pixels - this is like the dp unit, but it is also scaled by the user's font size preference. It is recommend you use this unit when specifying font sizes, so they will be adjusted for both the screen density and user's preference.
AnswerKenntnis:
Use dp (== dip) for everything but for fonts use sp


Pretty much everything about this and how to achieve the best support for multiple screens with different sizes and density is very well documented here:


Supporting Multiple Screens


If you are any serious about developing an android app for more than one type of device, you should have read the above at least once. In addition to that it is always a good thing to know the actual number of active devices that have a particular screen configuration.


Screen Sizes and Densities
AnswerKenntnis:
I will elaborate more on how exactly does dp convert to px:


If running on mdpi device 150x150 px image will take up 150*150 dp of screen space.
If running on hdpi device 150x150 px image will take up 100*100 dp of screen space.
If running on xhdpi device 150x150 px image will take up 75*75 dp of screen space.


The other way around: say, you want to add an image to your application and you need it to fill 100*100 dp control, you'll need to create different size images for supported screen sizes:


100*100 px image for mdpi
150*150 px image for hdpi
200*200 px image for xhdpi
AnswerKenntnis:
px
Pixels - point per scale  corresponds to actual pixels on the screen.

in
Inches - based on the physical size of the screen.

mm
Millimeters - based on the physical size of the screen.

pt
Points - 1/72 of an inch based on the physical size of the screen.

dp
Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. The ratio of dp-to-pixel will change with the screen density, but not necessarily in direct proportion. Note: The compiler accepts both "dip" and "dp", though "dp" is more consistent with "sp".

sp
Scale-independent Pixels - this is like the dp unit, but it is also scaled by the user's font size preference. It is recommend you use this unit when specifying font sizes, so they will be adjusted for both the screen density and user's preference.

Take the example of two screens that are the same size but one has a resolution of 160 dpi (dots per inch, i.e. pixels per inch) and the other is 240 dpi.

                          Lower resolution   screen          Higher resolution, same size
Physical Width                      1.5 inches                        1.5 inches
Dots Per Inch (ΓÇ£dpiΓÇ¥)               160                               240
Pixels (=width*dpi)                 240                               360
Density (factor of baseline 160)    1.0                               1.5
Density-independent Pixels          240                               240
(ΓÇ£dipΓÇ¥ or ΓÇ£dpΓÇ¥ or ΓÇ£dpsΓÇ¥)
Scale-independent pixels (ΓÇ£sipΓÇ¥ or ΓÇ£spΓÇ¥)    Depends on user font size settings  same
AnswerKenntnis:
Moreover you should have clear understanding about the following concepts:


  Screen size: Actual physical size, measured as the screen's diagonal. For simplicity, Android groups all actual screen sizes into
  four generalized sizes: small, normal, large, and extra large.
  
  Screen density: The quantity of pixels within a physical area of the screen; usually referred to as dpi (dots per inch). For example, a
  "low" density screen has fewer pixels within a given physical area,
  compared to a "normal" or "high" density screen. For simplicity,
  Android groups all actual screen densities into four generalized
  densities: low, medium, high, and extra high.
  
  Orientation: The orientation of the screen from the user's point of view. This is either landscape or portrait, meaning that the
  screen's aspect ratio is either wide or tall, respectively. Be aware
  that not only do different devices operate in different orientations
  by default, but the orientation can change at runtime when the user
  rotates the device.
  
  Resolution: The total number of physical pixels on a screen. When adding support for multiple screens, applications do not work directly
  with resolution; applications should be concerned only with screen
  size and density, as specified by the generalized size and density
  groups.
  
  Density-independent pixel (dp): A virtual pixel unit that you should use when defining UI layout, to express layout dimensions or
  position in a density-independent way. The density-independent pixel
  is equivalent to one physical pixel on a 160 dpi screen, which is the
  baseline density assumed by the system for a "medium" density screen.
  At runtime, the system transparently handles any scaling of the dp
  units, as necessary, based on the actual density of the screen in use.
  The conversion of dp units to screen pixels is simple: px = dp * (dpi
  / 160). For example, on a 240 dpi screen, 1 dp equals 1.5 physical
  pixels. You should always use dp units when defining your
  application's UI, to ensure proper display of your UI on screens with
  different densities.


Reference: Android developers site
AnswerKenntnis:
Basically the only time where px applies is 1px, and that's if you want exactly 1 pixel on the screen like in the case of a divider. On >160, you may get 2-3 pixels, and on 120dpi, it rounds to 0.
AnswerKenntnis:
px

Pixels - corresponds to actual pixels on the screen.

dp or dip

Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen.

Use of dp:

Density independence -
Your application achieves ΓÇ£density independenceΓÇ¥ when it preserves the physical size (from the userΓÇÖs point of view) of user interface elements when displayed on screens with different densities. (ie) The image should look the same size (not enlarged or shrinked) in different types of screens.

sp

Scale-independent Pixels - this is like the dp unit, but it is also scaled by the user's font size preference.

http://developer.android.com/guide/topics/resources/more-resources.html#Dimension
AnswerKenntnis:
I have calculate below formula to make convertions  dpi to dp & sp
AnswerKenntnis:
Anything related with the size of text and appearance must use sp

whereas,  anything related to the size of the controls or the layouts etc. must be used with dp.

you can use both dp and dip at its places.
AnswerKenntnis:
Source 1

Source 2: (data from source 2 is given below)


  These are dimension values defined in XML. A dimension is specified
  with a number followed by a unit of measure. For example: 10px, 2in,
  5sp. The following units of measure are supported by Android:
  
  dp
  
  Density-independent Pixels - An abstract unit that is based on the
  physical density of the screen. These units are relative to a 160 dpi
  (dots per inch) screen, on which 1dp is roughly equal to 1px. When
  running on a higher density screen, the number of pixels used to draw
  1dp is scaled up by a factor appropriate for the screen's dpi.
  Likewise, when on a lower density screen, the number of pixels used
  for 1dp is scaled down. The ratio of dp-to-pixel will change with the
  screen density, but not necessarily in direct proportion. Using dp
  units (instead of px units) is a simple solution to making the view
  dimensions in your layout resize properly for different screen
  densities. In other words, it provides consistency for the real-world
  sizes of your UI elements across different devices.
  
  sp
  
  Scale-independent Pixels - This is like the dp unit, but it is also
  scaled by the user's font size preference. It is recommend you use
  this unit when specifying font sizes, so they will be adjusted for
  both the screen density and the user's preference.
  
  pt
  
  Points - 1/72 of an inch based on the physical size of the screen.
  
  px
  
  Pixels - Corresponds to actual pixels on the screen. This unit of
  measure is not recommended because the actual representation can vary
  across devices; each devices may have a different number of pixels per
  inch and may have more or fewer total pixels available on the screen.
  
  mm
  
  Millimeters - Based on the physical size of the screen.
  
  in
  
  Inches - Based on the physical size of the screen.


Note: A dimension is a simple resource that is referenced using the value provided in the name attribute (not the name of the XML file). As such, you can combine dimension resources with other simple resources in the one XML file, under one  element.
AnswerKenntnis:
Difference between dp and sp units mentioned as "user's font size preference" by the answers copied from official documentation can be seen at run time by changing Settings->Accessibility->Large Text option.

Large Text option forces text to become 1.3 times bigger.

private static final float LARGE_FONT_SCALE = 1.3f;


This might be well of course vendor dependent since it lies in packages/apps/Settings.
AnswerKenntnis:
Good strategy for images that are in drawable is 

   ΓÇó    1080x1920    save it in ΓÇ£drawable-xxhdpiΓÇ¥ folder
   ΓÇó    480x800      save it in ΓÇ£drawable-hdpiΓÇ¥ folder
   ΓÇó    320x480      save it in ΓÇ£drawable-mdpiΓÇ¥ folder
   ΓÇó    1280x720     save it in ΓÇ£drawable-xhdpiΓÇ¥ folder


save images accordingly
QuestionKenntnis:
What is a metaclass in Python?
qn_description:
What are metaclasses? What do you use them for?
AnswersKenntnis
AnswerKenntnis:
A metaclass is the class of a class. Like a class defines how an instance of the class behaves, a metaclass defines how a class behaves. A class is an instance of a metaclass.

While in Python you can use arbitrary callables for metaclasses (like Jerub shows), the more useful approach is actually to make it an actual class itself. 'type' is the usual metaclass in Python. In case you're wondering, yes, 'type' is itself a class, and it is its own type. You won't be able to recreate something like 'type' purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass 'type'.

A metaclass is most commonly used as a class-factory. Like you create an instance of the class by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefor allow you to do 'extra things' when creating a class, like registering the new class with some registry, or even replace the class with something else entirely.

When the 'class' statement is executed, Python first executes the body of the 'class' statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the '__metaclass__' global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.

However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods, in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the 'type' metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.

Here's an aggregated example of the bits and pieces:

def make_hook(f):
    """Decorator to turn 'foo' method into '__foo__'"""
    f.is_hook = 1
    return f

class MyType(type):
    def __new__(cls, name, bases, attrs):

        if name.startswith('None'):
            return None

        # Go over attributes and see if they should be renamed.
        newattrs = {}
        for attrname, attrvalue in attrs.iteritems():
            if getattr(attrvalue, 'is_hook', 0):
                newattrs['__%s__' % attrname] = attrvalue
            else:
                newattrs[attrname] = attrvalue

        return super(MyType, cls).__new__(cls, name, bases, newattrs)

    def __init__(self, name, bases, attrs):
        super(MyType, self).__init__(name, bases, attrs)

        # classregistry.register(self, self.interfaces)
        print "Would register class %s now." % self

    def __add__(self, other):
        class AutoClass(self, other):
            pass
        return AutoClass
        # Alternatively, to autogenerate the classname as well as the class:
        # return type(self.__name__ + other.__name__, (self, other), {})

    def unregister(self):
        # classregistry.unregister(self)
        print "Would unregister class %s now." % self

class MyObject:
    __metaclass__ = MyType


class NoneSample(MyObject):
    pass

# Will print "NoneType None"
print type(NoneSample), repr(NoneSample)

class Example(MyObject):
    def __init__(self, value):
        self.value = value
    @make_hook
    def add(self, other):
        return self.__class__(self.value + other.value)

# Will unregister the class
Example.unregister()

inst = Example(10)
# Will fail with an AttributeError
#inst.unregister()

print inst + inst
class Sibling(MyObject):
    pass

ExampleSibling = Example + Sibling
# ExampleSibling is now a subclass of both Example and Sibling (with no
# content of its own) although it will believe it's called 'AutoClass'
print ExampleSibling
print ExampleSibling.__mro__
AnswerKenntnis:
Classes as objects

Before understanding metaclasses, you need to master classes in Python. And Python has a very peculiar idea of what classes are, borrowed from the Smalltalk language.

In most languages, classes are just pieces of code that describe how to produce an object. That's kinda true in Python too:

>>> class ObjectCreator(object):
...       pass
... 

>>> my_object = ObjectCreator()
>>> print(my_object)
<__main__.ObjectCreator object at 0x8974f2c>


But classes are more than that in Python. Classes are objects too.

Yes, objects. 

As soon as you use the keyword class, Python executes it and creates
an OBJECT. The instruction

>>> class ObjectCreator(object):
...       pass
... 


creates in memory an object with the name "ObjectCreator". 

This object (the class) is itself capable of creating objects (the instances), 
and this is why it's a class. 

But still, it's an object, and therefore:


you can assign it to a variable
you can copy it
you can add attributes to it
you can pass it as a function parameter


e.g.:

>>> print(ObjectCreator) # you can print a class because it's an object
<class '__main__.ObjectCreator'>
>>> def echo(o):
...       print(o)
... 
>>> echo(ObjectCreator) # you can pass a class as a parameter
<class '__main__.ObjectCreator'>
>>> print(hasattr(ObjectCreator, 'new_attribute'))
False
>>> ObjectCreator.new_attribute = 'foo' # you can add attributes to a class
>>> print(hasattr(ObjectCreator, 'new_attribute'))
True
>>> print(ObjectCreator.new_attribute)
foo
>>> ObjectCreatorMirror = ObjectCreator # you can assign a class to a variable
>>> print(ObjectCreatorMirror.new_attribute)
foo
>>> print(ObjectCreatorMirror())
<__main__.ObjectCreator object at 0x8997b4c>


Creating classes dynamically

Since classes are objects, you can create them on the fly, like any object.

First, you can create a class in a function using class:

>>> def choose_class(name):
...     if name == 'foo':
...         class Foo(object):
...             pass
...         return Foo # return the class, not an instance
...     else:
...         class Bar(object):
...             pass
...         return Bar
...     
>>> MyClass = choose_class('foo') 
>>> print(MyClass) # the function returns a class, not an instance
<class '__main__.Foo'>
>>> print(MyClass()) # you can create an object from this class
<__main__.Foo object at 0x89c6d4c>


But it's not so dynamic, since you still have to write the whole class yourself.

Since classes are objects, they must be generated by something.

When you use the class keyword, Python creates this object automatically. But as
with most things in Python, it gives you a way to do it manually.

Remember the function type? The good old function that lets you know what 
type an object is:

>>> print(type(1))
<type 'int'>
>>> print(type("1"))
<type 'str'>
>>> print(type(ObjectCreator))
<type 'type'>
>>> print(type(ObjectCreator()))
<class '__main__.ObjectCreator'>


Well, type has a completely different ability, it can also create classes on 
the fly. type can take the description of a class as parameters, 
and return a class.

(I  know, it's silly that the same function can have two completely different uses
according to the parameters you pass to it. It's an issue due to backwards 
compatibility in Python)

type works this way:

type(name of the class, 
     tuple of the parent class (for inheritance, can be empty), 
     dictionary containing attributes names and values)


e.g.:

>>> class MyShinyClass(object):
...       pass


can be created manually this way:

>>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object
>>> print(MyShinyClass)
<class '__main__.MyShinyClass'>
>>> print(MyShinyClass()) # create an instance with the class
<__main__.MyShinyClass object at 0x8997cec>


You'll notice that we use "MyShinyClass" as the name of the class
and as the variable to hold the class reference. They can be different,
but there is no reason to complicate things.

type accepts a dictionary to define the attributes of the class. So:

>>> class Foo(object):
...       bar = True


Can be translated to:

>>> Foo = type('Foo', (), {'bar':True})


And used as a normal class:

>>> print(Foo)
<class '__main__.Foo'>
>>> print(Foo.bar)
True
>>> f = Foo()
>>> print(f)
<__main__.Foo object at 0x8a9b84c>
>>> print(f.bar)
True


And of course, you can inherit from it, so:

>>>   class FooChild(Foo):
...         pass


would be:

>>> FooChild = type('FooChild', (Foo,), {})
>>> print(FooChild)
<class '__main__.FooChild'>
>>> print(FooChild.bar) # bar is inherited from Foo
True


Eventually you'll want to add methods to your class. Just define a function
with the proper signature and assign it as an attribute.

>>> def echo_bar(self):
...       print(self.bar)
... 
>>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})
>>> hasattr(Foo, 'echo_bar')
False
>>> hasattr(FooChild, 'echo_bar')
True
>>> my_foo = FooChild()
>>> my_foo.echo_bar()
True


You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.

This is what Python does when you use the keyword class, and it does so by using a metaclass.

What are metaclasses (finally)

Metaclasses are the 'stuff' that creates classes.

You define classes in order to create objects, right?

But we learned that Python classes are objects.

Well, metaclasses are what create these objects. They are the classes' classes,
you can picture them this way:

MyClass = MetaClass()
MyObject = MyClass()


You've seen that type lets you do something like this:

MyClass = type('MyClass', (), {})


It's because the function type is in fact a metaclass. type is the 
metaclass Python uses to create all classes behind the scenes.

Now you wonder why the heck is it written in lowercase, and not Type?

Well, I guess it's a matter of consistency with str, the class that creates
strings objects, and int the class that creates integer objects. type is
just the class that creates class objects.

You see that by checking the __class__ attribute. 

Everything, and I mean everything, is an object in Python. That includes ints, 
strings, functions and classes. All of them are objects. And all of them have
been created from a class:

>>> age = 35
>>> age.__class__
<type 'int'>
>>> name = 'bob'
>>> name.__class__
<type 'str'>
>>> def foo(): pass
>>> foo.__class__
<type 'function'>
>>> class Bar(object): pass
>>> b = Bar()
>>> b.__class__
<class '__main__.Bar'>


Now, what is the __class__ of any __class__ ?

>>> age.__class__.__class__
<type 'type'>
>>> name.__class__.__class__
<type 'type'>
>>> foo.__class__.__class__
<type 'type'>
>>> b.__class__.__class__
<type 'type'>


So, a metaclass is just the stuff that creates class objects.

You can call it a 'class factory' if you wish.

type is the built-in metaclass Python uses, but of course, you can create your
own metaclass.

The __metaclass__ attribute

You can add a __metaclass__ attribute when you write a class:

class Foo(object):
  __metaclass__ = something...
  [...]


If you do so, Python will use the metaclass to create the class Foo.

Careful, it's tricky.

You write class Foo(object) first, but the class object Foo is not created
in memory yet.

Python will look for __metaclass__ in the class definition. If it finds it,
it will use it to create the object class Foo. If it doesn't, it will use
type to create the class.

Read that several times.

When you do:

class Foo(Bar):
  pass


Python does the following:

Is there a __metaclass__ attribute in Foo?

If yes, create in memory a class object (I said a class object, stay with me here),
with the name Foo by using what is in __metaclass__.

If Python can't find __metaclass__, it will look for a __metaclass__ in 
Bar (the parent class), and try to do the same.

If Python can't find __metaclass__ in any parent, it will look for 
a __metaclass__ at the MODULE level, and try to do the same.

Then if it can't find any __metaclass__ at all, it will use type
to create the class object.

Now the big question is, what can you put in __metaclass__ ?

The answer is: something that can create a class.

And what can create a class? type, or anything that subclasses or uses it.

Custom metaclasses

The main purpose of a metaclass is to change the class automatically,
when it's created.

You usually do this for APIs, where you want to create classes matching the
current context.

Imagine a stupid example, where you decide that all classes in your module
should have their attributes written in uppercase. There are several ways to 
do this, but one way is to set __metaclass__ at the module level.

This way, all classes of this module will be created using this metaclass, 
and we just have to tell the metaclass to turn all attributes to uppercase.

Luckily, __metaclass__ can actually be any callable, it doesn't need to be a
formal class (I know, something with 'class' in its name doesn't need to be 
a class, go figure... but it's helpful).

So we will start with a simple example, by using a function.

# the metaclass will automatically get passed the same argument
# that you usually pass to `type`
def upper_attr(future_class_name, future_class_parents, future_class_attr):
  """
    Return a class object, with the list of its attribute turned 
    into uppercase.
  """

  # pick up any attribute that doesn't start with '__' and uppercase it
  uppercase_attr = {}
  for name, val in future_class_attr.items():
      if not name.startswith('__'):
          uppercase_attr[name.upper()] = val
      else:
          uppercase_attr[name] = val

  # let `type` do the class creation
  return type(future_class_name, future_class_parents, uppercase_attr)

__metaclass__ = upper_attr # this will affect all classes in the module

class Foo(): # global __metaclass__ won't work with "object" though
  # but we can define __metaclass__ here instead to affect only this class
  # and this will work with "object" children
  bar = 'bip'

print(hasattr(Foo, 'bar'))
# Out: False
print(hasattr(Foo, 'BAR'))
# Out: True

f = Foo()
print(f.BAR)
# Out: 'bip'


Now, let's do exactly the same, but using a real class for a metaclass:

# remember that `type` is actually a class like `str` and `int`
# so you can inherit from it
class UpperAttrMetaclass(type): 
    # __new__ is the method called before __init__
    # it's the method that creates the object and returns it
    # while __init__ just initializes the object passed as parameter
    # you rarely use __new__, except when you want to control how the object
    # is created.
    # here the created object is the class, and we want to customize it
    # so we override __new__
    # you can do some stuff in __init__ too if you wish
    # some advanced use involves overriding __call__ as well, but we won't
    # see this
    def __new__(upperattr_metaclass, future_class_name, 
                future_class_parents, future_class_attr):

        uppercase_attr = {}
        for name, val in future_class_attr.items():
            if not name.startswith('__'):
                uppercase_attr[name.upper()] = val
            else:
                uppercase_attr[name] = val

        return type(future_class_name, future_class_parents, uppercase_attr)


But this is not really OOP. We call type directly and we don't override
call the parent __new__. Let's do it:

class UpperAttrMetaclass(type): 

    def __new__(upperattr_metaclass, future_class_name, 
                future_class_parents, future_class_attr):

        uppercase_attr = {}
        for name, val in future_class_attr.items():
            if not name.startswith('__'):
                uppercase_attr[name.upper()] = val
            else:
                uppercase_attr[name] = val

        # reuse the type.__new__ method
        # this is basic OOP, nothing magic in there
        return type.__new__(upperattr_metaclass, future_class_name, 
                            future_class_parents, uppercase_attr)


You may have noticed the extra argument upperattr_metaclass. There is
nothing special about it: a method always receives the current instance as
first parameter. Just like you have self for ordinary methods.

Of course, the names I used here are long for the sake of clarity, but like
for self, all the arguments have conventional names. So a real production
metaclass would look like this:

class UpperAttrMetaclass(type): 

    def __new__(cls, clsname, bases, dct):

        uppercase_attr = {}
        for name, val in dct.items():
            if not name.startswith('__'):
                uppercase_attr[name.upper()] = val
            else:
                uppercase_attr[name] = val

        return type.__new__(cls, clsname, bases, uppercase_attr)


We can make it even cleaner by using super, which will ease inheritance (because
yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):

class UpperAttrMetaclass(type): 

    def __new__(cls, clsname, bases, dct):

        uppercase_attr = {}
        for name, val in dct.items():
            if not name.startswith('__'):
                uppercase_attr[name.upper()] = val
            else:
                uppercase_attr[name] = val

        return super(UpperAttrMetaclass, cls).__new__(cls, clsname, bases, uppercase_attr)


That's it. There is really nothing more about metaclasses.

The reason behind the complexity of the code using metaclasses is not because
of metaclasses, it's because you usually use metaclasses to do twisted stuff
relying on introspection, manipulating inheritance, vars such as __dict__, etc.

Indeed, metaclasses are especially useful to do black magic, and therefore
complicated stuff. But by themselves, they are simple:


intercept a class creation
modify the class
return the modified class


Why would you use metaclasses classes instead of functions?

Since __metaclass__ can accept any callable, why would you use a class
since it's obviously more complicated?

There are several reasons to do so:


The intention is clear. When you read UpperAttrMetaclass(type), you know
what's going to follow
You can use OOP. Metaclass can inherit from metaclass, override parent methods.
Metaclasses can even use metaclasses.
You can structure your code better. You never use metaclasses for something as
trivial as the above example. It's usually for something complicated. Having the
ability to make several methods and group them in one class is very useful
to make the code easier to read.
You can hook on __new__, __init__ and __call__. Which will allow
you to do different stuff. Even if usually you can do it all in __new__, 
some people are just more comfortable using __init__.
These are called metaclasses, damn it! It must mean something!


Why the hell would you use metaclasses?

Now the big question. Why would you use some obscure error prone feature?

Well, usually you don't:


  Metaclasses are deeper magic than
  99% of users should never worry about.
  If you wonder whether you need them,
  you don't (the people who actually
  need them know with certainty that
  they need them, and don't need an
  explanation about why). 


Python Guru Tim Peters

The main use case for a metaclass is creating an API. A typical example of this is the Django ORM.

It allows you to define something like this:

class Person(models.Model):
  name = models.CharField(max_length=30)
  age = models.IntegerField()


But if you do this:

guy = Person(name='bob', age='35')
print(guy.age)


It won't return an IntegerField object. It will return an int, and can even take it directly from the database.

This is possible because models.Model defines __metaclass__ and 
it uses some magic that will turn the Person you just defined with simple statements
into a complex hook to a database field. 

Django makes something complex look simple by exposing a simple API
and using metaclasses, recreating code from this API to do the real job
behind the scenes.

The last word

First, you know that classes are objects that can create instances.

Well in fact, classes are themselves instances. Of metaclasses.

>>> class Foo(object): pass
>>> id(Foo)
142630324


Everything is an object in Python, and they are all either instances of classes
or instances of metaclasses.

Except for type.

type is actually its own metaclass. This is not something you could
reproduce in pure Python, and is done by cheating a little bit at the implementation
level.

Secondly, metaclasses are complicated. You may not want to use them for 
very simple class alterations. You can change classes by using two different techniques:


monkey patching
class decorators


99% of the time you need class alteration, you are better off using these.

But 99% of the time, you don't need class alteration at all.
AnswerKenntnis:
Metaclasses are the secret sauce that make 'class' work. The default metaclass for a new style object is called 'type'.

class type(object)
  |  type(object) -> the object's type
  |  type(name, bases, dict) -> a new type


Metaclasses take 3 args. 'name', 'bases' and 'dict'

Here is where the secret starts. Look for where name, bases and the dict come from in this example class definition.

class ThisIsTheName(Bases, Are, Here):
    All_the_code_here
    def doesIs(create, a):
        dict


Lets define a metaclass that will demonstrate how 'class:' calls it.

def test_metaclass(name, bases, dict):
    print 'The Class Name is', name
    print 'The Class Bases are', bases
    print 'The dict has', len(dict), 'elems, the keys are', dict.keys()

    return "yellow"

class TestName(object, None, int, 1):
    __metaclass__ = test_metaclass
    foo = 1
    def baz(self, arr):
        pass

print 'TestName = ', repr(TestName)

# output => 
The Class Name is TestName
The Class Bases are (<type 'object'>, None, <type 'int'>, 1)
The dict has 4 elems, the keys are ['baz', '__module__', 'foo', '__metaclass__']
TestName =  'yellow'


And now, an example that actually means something, this will automatically make the variables in the list "attributes" set on the class, and set to None.

def init_attributes(name, bases, dict):
    if 'attributes' in dict:
        for attr in dict['attributes']:
            dict[attr] = None

    return type(name, bases, dict)

class Initialised(object):
    __metaclass__ = init_attributes
    attributes = ['foo', 'bar', 'baz']

print 'foo =>', Initialised.foo
# output=>
foo => None


Note that the magic behaviour that 'Initalised' gains by having the metaclass init_attributes is not passed onto a subclass of Initalised.

Here is an even more concrete example, showing how you can subclass 'type' to make a metaclass that performs an action when the class is created. This is quite tricky:

class MetaSingleton(type):
    instance = None
    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(MetaSingleton, cls).__call__(*args, **kw)
        return cls.instance

 class Foo(object):
     __metaclass__ = MetaSingleton

 a = Foo()
 b = Foo()
 assert a is b
AnswerKenntnis:
One use for metaclasses is adding new properties and methods to an instance automatically.

For example, if you look at Django models, their definition looks a bit confusing. It looks as if you are only defining class properties:

class Person(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=30)


However, at runtime the Person objects are filled with all sorts of useful methods. See the source for some amazing metaclassery.
AnswerKenntnis:
I think the ONLamp introduction to metaclass programming is well written and gives a really good introduction to the topic despite being several years old already.

http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html

In short: A class is a blueprint for the creation of an instance, a metaclass is a blueprint for the creation of a class. It can be easily seen that in Python classes need to be first-class objects too to enable this behavior.

I've never written one myself, but I think one of the nicest uses of metaclasses can be seen in the Django framework. The model classes use a metaclass approach to enable a declarative style of writing new models or form classes. While the metaclass is creating the class, all members get the possibility to customize the class itself.


Creating a new model
The metaclass enabling this


The thing that's left to say is: If you don't know what metaclasses are, the probability that you will not need them is 99%.
AnswerKenntnis:
Others have explained how metaclasses work and how they fit into the Python type system. Here's an example of what they can be used for. In a testing framework I wrote, I wanted to keep track of the order in which classes were defined, so that I could later instantiate them in this order. I found it easiest to do this using a metaclass.

class MyMeta(type):

    counter = 0

    def __init__(cls, name, bases, dic):
        type.__init__(cls, name, bases, dic)
        cls._order = MyMeta.counter
        MyMeta.counter += 1

class MyType(object):

    __metaclass__ = MyMeta


Anything that's a subclass of MyType then gets a class attribute _order that records the order in which the classes were defined.
AnswerKenntnis:
The best metaclass explanation I have read is "Metaclasses Demystified"  http://cleverdevil.org/computing/78/, which originally appeared in Python Magazine.

EDIT: Here's an archived version of it until Jonathan's site is fixed:

http://web.archive.org/web/20120503014702/http://cleverdevil.org/computing/78/
AnswerKenntnis:
I had a really complicated problem, that probably could have been solved differently, but I chose to solve it using a metaclass.  Because of the complexity, it is one of the few modules I have written where the comments in the module surpass the amount of code that has been written.  Here it is...

#!/usr/bin/env python

# Copyright (C) 2013-2014 Craig Phillips.  All rights reserved.

# This requires some explaining.  The point of this metaclass excercise is to
# create a static abstract class that is in one way or another, dormant until
# queried.  I experimented with creating a singlton on import, but that did
# not quite behave how I wanted it to.  See now here, we are creating a class
# called GsyncOptions, that on import, will do nothing except state that its
# class creator is GsyncOptionsType.  This means, docopt doesn't parse any
# of the help document, nor does it start processing command line options.
# So importing this module becomes really efficient.  The complicated bit
# comes from requiring the GsyncOptions class to be static.  By that, I mean
# any property on it, may or may not exist, since they are not statically
# defined; so I can't simply just define the class with a whole bunch of
# properties that are @property @staticmethods.
#
# So here's how it works:
#
# Executing 'from libgsync.options import GsyncOptions' does nothing more
# than load up this module, define the Type and the Class and import them
# into the callers namespace.  Simple.
#
# Invoking 'GsyncOptions.debug' for the first time, or any other property
# causes the __metaclass__ __getattr__ method to be called, since the class
# is not instantiated as a class instance yet.  The __getattr__ method on
# the type then initialises the class (GsyncOptions) via the __initialiseClass
# method.  This is the first and only time the class will actually have its
# dictionary statically populated.  The docopt module is invoked to parse the
# usage document and generate command line options from it.  These are then
# paired with their defaults and what's in sys.argv.  After all that, we
# setup some dynamic properties that could not be defined by their name in
# the usage, before everything is then transplanted onto the actual class
# object (or static class GsyncOptions).
#
# Another piece of magic, is to allow command line options to be set in
# in their native form and be translated into argparse style properties.
#
# Finally, the GsyncListOptions class is actually where the options are
# stored.  This only acts as a mechanism for storing options as lists, to
# allow aggregation of duplicate options or options that can be specified
# multiple times.  The __getattr__ call hides this by default, returning the
# last item in a property's list.  However, if the entire list is required,
# calling the 'list()' method on the GsyncOptions class, returns a reference
# to the GsyncListOptions class, which contains all of the same properties
# but as lists and without the duplication of having them as both lists and
# static singlton values.
#
# So this actually means that GsyncOptions is actually a static proxy class...
#
# ...And all this is neatly hidden within a closure for safe keeping.
def GetGsyncOptionsType():
    class GsyncListOptions(object):
        __initialised = False

    class GsyncOptionsType(type):
        def __initialiseClass(cls):
            if GsyncListOptions._GsyncListOptions__initialised: return

            from docopt import docopt
            from libgsync.options import doc
            from libgsync import __version__

            options = docopt(
                doc.__doc__ % __version__,
                version = __version__,
                options_first = True
            )

            paths = options.pop('<path>', None)
            setattr(cls, "destination_path", paths.pop() if paths else None)
            setattr(cls, "source_paths", paths)
            setattr(cls, "options", options)

            for k, v in options.iteritems():
                setattr(cls, k, v)

            GsyncListOptions._GsyncListOptions__initialised = True

        def list(cls):
            return GsyncListOptions

        def __getattr__(cls, name):
            cls.__initialiseClass()
            return getattr(GsyncListOptions, name)[-1]

        def __setattr__(cls, name, value):
            # Substitut option names: --an-option-name for an_option_name
            import re
            name = re.sub(r'^__', "", re.sub(r'-', "_", name))
            listvalue = []

            # Ensure value is converted to a list type for GsyncListOptions
            if isinstance(value, list):
                if value:
                    listvalue = [] + value
                else:
                    listvalue = [ None ]
            else:
                listvalue = [ value ]

            type.__setattr__(GsyncListOptions, name, listvalue)

    # Cleanup this module to prevent tinkering.
    import sys
    module = sys.modules[__name__]
    del module.__dict__['GetGsyncOptionsType']

    return GsyncOptionsType

# Our singlton abstract proxy class.
class GsyncOptions(object):
    __metaclass__ = GetGsyncOptionsType()
AnswerKenntnis:
Nice explanation of Python classes, objects, and metaclasses:

http://www.cafepy.com/article/python_types_and_objects/python_types_and_objects.html
QuestionKenntnis:
Clone all remote branches with Git?
qn_description:
I have a master and a development branch, both pushed to GitHub. I've cloned, pulled, and fetched, but I remain unable to get anything other than the master branch back.

I'm sure I'm missing something obvious, but I have read the manual and I'm getting no joy at all.
AnswersKenntnis
AnswerKenntnis:
First, clone a remote Git repository and cd into it:

$ git clone git://example.com/myproject
$ cd myproject


Next, look at the local branches in your repository:

$ git branch
* master


But there are other branches hiding in your repository! You can see these using the -a flag:

$ git branch -a
* master
  remotes/origin/HEAD
  remotes/origin/master
  remotes/origin/v1.0-stable
  remotes/origin/experimental


If you just want to take a quick peek at an upstream branch, you can check it out directly:

$ git checkout origin/experimental


But if you want to work on that branch, you'll need to create a local tracking branch:

$ git checkout -b experimental origin/experimental


and you will see

Branch experimental set up to track remote branch experimental from origin.
Switched to a new branch 'experimental'


That last line throw some people "New branch" - huh?
What it really means is a new local branch that gets the branch from the index and creates it locally for you.  The previous line is actually more informative as it tells you that the branch is being set up to track the remote branch, which usually means the origin/branch_name branch 

Now, if you look at your local branches, this is what you'll see:

$ git branch
* experimental
  master


You can actually track more than one remote repository using git remote.

$ git remote add win32 git://example.com/users/joe/myproject-win32-port
$ git branch -a
* master
  remotes/origin/HEAD
  remotes/origin/master
  remotes/origin/v1.0-stable
  remotes/origin/experimental
  remotes/win32/master
  remotes/win32/new-widgets


At this point, things are getting pretty crazy, so run gitk to see what's going on:

$ gitk --all &
AnswerKenntnis:
If you have many remote branches that you want to fetch at once, do:

$ git remote update
$ git pull --all


Now you can checkout any branch as you need to, without hitting the remote repository.
AnswerKenntnis:
This Bash script helped me out:

#!/bin/bash
for branch in `git branch -a | grep remotes | grep -v HEAD | grep -v master`; do
    git branch --track ${branch##*/} $branch
done


It will create tracking branches for all remote branches, except master (which you probably got from the original clone command). I think you might still need to do a 

git fetch --all
git pull --all


to be sure.
AnswerKenntnis:
You can easily switch to a branch without using the fancy "git checkout -b somebranch origin/somebranch" syntax.  You can just do:

git checkout somebranch


Git will automatically do the right thing:

$ git checkout somebranch
Branch somebranch set up to track remote branch somebranch from origin.
Switched to a new branch 'somebranch'


Git will check whether a branch with the same name exists in exactly one remote, and if it does, it tracks it the same way as if you had explicitly specified that it's a remote branch. From the git-checkout man page of Git 1.8.2.1:


  If <branch> is not found but there does exist a tracking branch in
  exactly one remote (call it <remote>) with a matching name, treat as
  equivalent to

$ git checkout -b <branch> --track <remote>/<branch>
AnswerKenntnis:
The fetch that you are doing should get all the remote branches, but it won't create local branches for them. If you use gitk, you should see the remote branches described as "remotes/origin/dev" or something similar.

To create a local branch based on a remote branch, do something like:

git checkout -b dev refs/remotes/origin/dev

Which should return something like:

Branch dev set up to track remote branch refs/remotes/origin/dev.
Switched to a new branch "dev"

Now, when you are on the dev branch, "git pull" will update your local dev to the same point as the remote dev branch.  Note that it will fetch all branches, but only pull the one you are on to the top of the tree.
AnswerKenntnis:
Using the --mirror option seems to copy the remote tracking branches properly.
However, it sets up the repository as a bare repository, so you have to turn it back into a normal repository afterwards.

git clone --mirror path/to/original path/to/dest/.git
cd path/to/dest
git config --bool core.bare false
AnswerKenntnis:
Regarding,


  $ git checkout -b experimental origin/experimental


using

$ git checkout -t origin/experimental


or the more verbose but easier to remember

$ git checkout --track origin/experimental


might be better, in terms of tracking a remote repository.
AnswerKenntnis:
When you do "git clone git://location", all branches and tags are fetched.

In order to work on top of a specific remote branch, assuming it's the origin remote:

git checkout -b branch origin/branchname
AnswerKenntnis:
Just do this:

$ git clone git://example.com/myproject
$ cd myproject
$ git checkout branchxyz
Branch branchxyz set up to track remote branch branchxyz from origin.
Switched to a new branch 'branchxyz'
$ git pull
Already up-to-date.
$ git branch
* branchxyz
  master
$ git branch -a
* branchxyz
  master
  remotes/origin/HEAD -> origin/master
  remotes/origin/branchxyz
  remotes/origin/branch123


You see, 'git clone git://example.com/myprojectt' fetches everything, even the branches, you just have to checkout them, then your local branch will be created.
AnswerKenntnis:
Use my tool git_remote_branch (you need Ruby installed on your machine). It's built specifically to make remote branch manipulations dead easy.

Each time it does an operation on your behalf, it prints it in red at the console. Over time, they finally stick into your brain :-)

If you don't want grb to run commands on your behalf, just use the 'explain' feature. The commands will be printed to your console instead of executed for you.

Finally, all commands have aliases, to make memorization easier.

Note that this is alpha software ;-)

Here's the help when you run grb help:


git_remote_branch version 0.2.6

  Usage:

  grb create branch_name [origin_server] 

  grb publish branch_name [origin_server] 

  grb rename branch_name [origin_server] 

  grb delete branch_name [origin_server] 

  grb track branch_name [origin_server] 



  Notes:
  - If origin_server is not specified, the name 'origin' is assumed 
    (git's default)
  - The rename functionality renames the current branch

  The explain meta-command: you can also prepend any command with the 
keyword 'explain'. Instead of executing the command, git_remote_branch 
will simply output the list of commands you need to run to accomplish 
that goal.

  Example: 
    grb explain create
    grb explain create my_branch github

  All commands also have aliases:
  create: create, new
  delete: delete, destroy, kill, remove, rm
  publish: publish, remotize
  rename: rename, rn, mv, move
  track: track, follow, grab, fetch
AnswerKenntnis:
A git clone is supposed to copy the entire repository.  Try cloning it, and then run git branch with no additional arguments.  It should list all the branches.  If then you want to switch to branch "foo" instead of "master", use git checkout foo.
AnswerKenntnis:
I needed to do exactly the same. Here is my Ruby script.

#!/usr/bin/env ruby

local = []
remote = {}

# Prepare
%x[git reset --hard HEAD]
%x[git checkout master] # Makes sure that * is on master.
%x[git branch -a].each_line do |line|
  line.strip!
  if /origin\//.match(line)
     remote[line.gsub(/origin\//, '')] = line
   else
     local << line
   end
end
# Update 
remote.each_pair do |loc, rem|
  next if local.include?(loc)
  %x[git checkout --track -b #{loc} #{rem}]
end
%x[git fetch]
AnswerKenntnis:
Use aliases. Though there aren't any native Git one-liners, you can define your own as

git config --global alias.clone-branches '! git branch -a | sed -n "/\/HEAD /d; /\/master$/d; /remotes/p;" | xargs -L1 git checkout -t'


and then use it as

git clone-branches
AnswerKenntnis:
Better late than never, but here is the best way to do this:

mkdir repo
cd repo
git clone --mirror path/to/repo.git .git 
git config --unset core.bare
git reset --hard


At this point you have a complete copy of the remote repo with all of it's branches (verify with git branch).  You can use --mirror instead of --bare if your remote repo has remotes of its own.
AnswerKenntnis:
Why you only see "master"

git clone downloads all remote remote branches but still considers them "remote", even though the files are located in your new repository. There's one exception to this, which is that the cloning process creates a local branch called "master" from the remote branch called "master". By default, git branch only shows local branches, which is why you only see "master".

git branch -a shows all branches, including remote branches.



How to get local branches

If you actually want to work on a branch, you'll probably want a "local" version of it. To simply create local branches from remote branches (without checking them out and thereby changing the contents of your working directory), you can do that like this:

git branch branchone origin/branchone
git branch branchtwo origin/branchtwo
git branch branchthree origin/branchthree


In this example, branchone is the name of a local branch you're creating based on origin/branchone; if you instead want to create local branches with different names, you can do this:

git branch localbranchname origin/branchone


Once you've created a local branch, you can see it with git branch (remember, you don't need -a to see local branches).
AnswerKenntnis:
This isn't too much complicated, very simple and straight forward steps are as follows;

git fetch origin This will bring all the remote branches to your local.

git checkout --track origin/<branch you want to checkout>


Verify whether you are in the desired branch by  this;

git branch


The output will like this;

*your current branch 
some branch2
some branch3 


Notice the * sign that denotes the current branch.
AnswerKenntnis:
I'm going to add my 2 cents here because I got here trying to find out how to pull down a remote branch I had deleted locally. Origin was not mine, and I didn't want to go through the hassle of re-cloning everything 

This worked for me:

assuming you need to recreate the branch locally:

git checkout -b recreated-branch-name
git branch -a (to list remote branches)
git rebase remotes/remote-origin/recreated-branch-name


So if I forked from gituser/master to sjp and then branched it to sjp/mynewbranch it would look like this:

$ git checkout -b mynewbranch
$ git branch -a
  master
  remotes/sjp/master
  remotes/sjp/mynewbranch
$ git fetch (habit to always do before)
$ git rebase remotes/sjp/mynewbranch
AnswerKenntnis:
Here is a bash script for fetching all branches and tags of a git project as snapshots into separate folders. 

https://gist.github.com/hfossli/7562257

Maybe not what was asked directly, but some people might come here looking for this solution.
AnswerKenntnis:
For copy-paste into command line:

git checkout master ; remote=origin ; for brname in `git branch -r | grep $remote | grep -v master | grep -v HEAD | awk '{gsub(/^[^\/]+\//,"",$1); print $1}'`; do git branch -D $brname ; git checkout -b $brname $remote/$brname ; done ; git checkout master


For more readibility:

git checkout master ;
remote=origin ;
for brname in `
    git branch -r | grep $remote | grep -v master | grep -v HEAD 
    | awk '{gsub(/^[^\/]+\//,"",$1); print $1}'
`; do
    git branch -D $brname ;
    git checkout -b $brname $remote/$brname ;
done ;
git checkout master



This will:


check out master (so that we can delete branch we are on)
select remote to checkout (change it to whatever remote you have)
loop through all branches of the remote except master and HEAD

delete local branch (so that we can check out force-updated branches)
check out branch from the remote

check out master (for the sake of it)


Based on answer of VonC.
AnswerKenntnis:
A little late to the party, but I think this does the trick:

mkdir YourRepo
cd YourRepo
git init --bare .git                       # create a bare repo
git remote add origin REMOTE_URL           # add a remote
git fetch origin refs/heads/*:refs/heads/* # fetch heads
git fetch origin refs/tags/*:refs/tags/*   # fetch tags
git init                                   # reinit work tree
git checkout master                        # checkout a branch


If this does something undesirable, I'd love to know. However, so far, this works for me.
QuestionKenntnis:
What is the maximum length of a URL in different browsers?
qn_description:
What is the maximum length of a URL in different browsers? Does it differ between browsers? 

Does the HTTP protocol dictate it?
AnswersKenntnis
AnswerKenntnis:
Short answer - de facto limit of 2000 characters

If you keep URLs under 2000 characters, they'll work in virtually any combination of client and server software.

Longer answer - first, the standards...

RFC 2616 (Hypertext Transfer Protocol HTTP/1.1) section 3.2.1 says


  The HTTP protocol does not place
  any a priori limit on the length of
  a URI. Servers MUST be able to handle
  the URI of any resource they    serve,
  and SHOULD be able to handle URIs of
  unbounded length if they    provide
  GET-based forms that could generate
  such URIs. A server    SHOULD return
  414 (Request-URI Too Long) status if a
  URI is longer    than the server can
  handle (see section 10.4.15).
  
  Note: Servers ought to be cautious
  about depending on URI lengths
        above 255 bytes, because some older client or proxy
        implementations might not properly support these lengths.


...and the reality

That's what the standards say. For the reality, see this research over at boutell.com to see what individual browser and server implementations will support. It's worth a read, but the executive summary is:


  Extremely long URLs are usually a
  mistake. URLs over 2,000 characters
  will not work in the most popular web
  browser. Don't use them if you intend
  your site to work for the majority of
  Internet users.


Search engines like URLs < 2048 chars...

Be aware that the sitemaps protocol, which allows a site to inform search engines about available pages, has a limit of 2048 characters in a URL. If you intend to use sitemaps, a limit has been decided for you! (see Calin-Andrei Burloiu's answer below) 

There's also some research from 2010 into the maximum URL length that search engines will crawl and index. They found the limit was 2047 chars, which appears allied to the sitemap protocol spec. However, they also found the Google SERP tool wouldn't cope with URLs longer than 1855 chars.

Internet Explorer's limitations...

IE8's maximum URL length is 2083 chars, and it seems IE9 has a similar limit. 

I've tested IE10 and the address bar will only accept 2083 chars. You can click a URL which is longer than this, but the address bar will still only show 2083 characters of this link.

Is this information up to date?

This is a popular question, and as the original research is 7 years old I'll try to keep it up to date: As of Mar 2014, the advice still stands, as the IE10 address bar will still not accept a longer URL.
AnswerKenntnis:
WWW FAQs: What is the maximum length of a URL? has its own answer based on empirical testing and research. The short answer is that going over 2048 characters makes Internet Explorer unhappy and thus this is the limit you should use. See the page for a long answer.
AnswerKenntnis:
The longest URLs I came across are data URLs

Example image URL from Google image results (11747 characters)

data:image/jpeg;base64,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
AnswerKenntnis:
The URI RFC (of which URLs are a subset) doesn't define a maximum length, however, it does recommend that the hostname part of the URI (if applicable) not exceed 255 characters in length:


  URI producers should use names that
  conform to the DNS syntax, even when
  use of DNS is not immediately
  apparent, and should limit these names
  to no more than 255 characters in
  length.


As noted in other posts though, some browsers have a practical limitation on the length of a URL.
AnswerKenntnis:
There is really no universal maximum URL length. The max length is determined only by what the client browser chooses to support, which varies widely. The 2,083 limit is only present in Internet Explorer (all versions up to 7.0). The max length in Firefox and Safari seems to be unlimited, although instability occurs with URLs reaching around 65,000 characters.
Opera seems to have no max URL length whatsoever, and doesn't suffer instability at extremely long lengths.
AnswerKenntnis:
The HTTP 1.1 specification says:


  URIs in HTTP can be represented in
  absolute form or relative to some
  known base URI [11], depending upon
  the context of their use. The two
  forms are differentiated by the fact
  that absolute URIs always begin
  with a scheme name followed by a
  colon. For definitive information on
  URL syntax and semantics, see "Uniform
  Resource Identifiers (URI):    Generic
  Syntax and Semantics," RFC 2396 [42]
  (which replaces RFCs    1738 [4] and
  RFC 1808 [11]). This specification
  adopts the    definitions of
  "URI-reference", "absoluteURI",
  "relativeURI", "port",
  "host","abs_path", "rel_path", and
  "authority" from that
  specification.
  
  The HTTP protocol does not place
  any a priori limit on the length of
  a URI. Servers MUST be able to handle
  the URI of any resource they    serve,
  and SHOULD be able to handle URIs of
  unbounded length if they    provide
  GET-based forms that could generate
  such URIs.* A server    SHOULD return
  414 (Request-URI Too Long) status if a
  URI is longer    than the server can
  handle (see section 10.4.15).
  
  Note: Servers ought to be cautious about depending on URI
  lengths
        above 255 bytes, because some older client or proxy
        implementations might not properly support these lengths.


As mentioned by @Brian, the HTTP clients (e.g. browsers) may have their own limits, and HTTP servers will have different limits.
AnswerKenntnis:
Microsoft Support says "Maximum URL length is 2,083 characters in Internet Explorer".

IE has problems with URLs longer than that. Firefox seems to work fine with >4k chars.
AnswerKenntnis:
Sitemaps protocol which is a way for webmasters to inform search engines about pages on their sites (also used by Google in Webmaster Tools) supports URLs with less than 2048 characters. So if you are planning to use this feature for Search Engine Optimization take this into account.
AnswerKenntnis:
In URL as UI Jakob Nielsen recommends:


  the social interface to the Web relies on email when users want to recommend Web pages to each other, and email is the second-most common way users get to new sites (search engines being the most common): make sure that all URLs on your site are less than 78 characters long so that they will not wrap across a line feed.


This is not the maximum but I'd consider this a practical maximum if you want your URL to be shared.
AnswerKenntnis:
ASP.net 2 and SQL Server reporting services 2005 have a limit of 2028. I found this out the hard way, where my dynamic URL generator would not pass over some parameters to a report beyond that point. This was under IE8.
AnswerKenntnis:
Limit request line directive sets the maximum length of a URL. By default, it is set to 8190, which gives you a lot of room. However other servers and some browses, limit the length more.

Because all parameters are passed on the URL line,items that were in password of hidden fields will also be displayed in the URL of course. Neither mobile should be used for real security measures and should be considered cosmetic security at best.....
QuestionKenntnis:
Href attribute for JavaScript links: "#" or "javascript:void(0)"?
qn_description:
When building a link that has the sole purpose to run JavaScript code, there are 2 ways to write the code. Which is better, in terms of functionality, page load speed, validation purposes, etc?

<a href="#" onclick="myJsFunc();">Run JavaScript Code</a>


or

<a href="javascript:void(0)" onclick="myJsFunc();">Run JavaScript Code</a>
AnswersKenntnis
AnswerKenntnis:
I use javascript:void(0).

Three reasons. Encouraging the use of # amongst a team of developers inevitably leads to some using the return value of the function called like this:

function doSomething() {
    //Some code
    return false;
}


But then they forget to use return doSomething() in the onclick and just use doSomething().

A second reason for avoiding # is that the final return false; will not execute if the called function throws an error. Hence the developers have to also remember to handle any error appropriately in the called function.

A third reason is that there are cases where the onclick event property is assigned dynamically.  I prefer to be able to call a function or assign it dynamically without having to code the function specifically for one method of attachment or another. Hence my onclick (or on anything) in HTML markup look like this:

onclick="someFunc.call(this)"


OR

onclick="someFunc.apply(this, arguments)"


Using javascript:void(0) avoids all of the above headaches, and I haven't found any examples of a downside.

So if you're a lone developer then you can clearly make your own choice, but if you work as a team you have to either state:

Use href="#", make sure onclick always contains return false; at the end, that any called function does not throw an error and if you attach a function dynamically to the onclick property make sure that as well as not throwing an error it returns false.

OR

Use href="javascript:void(0)"

The second is clearly much easier to communicate.
AnswerKenntnis:
Neither.  

If you can have an actual URL that makes sense use that as the HREF.  The onclick won't fire if someone middle-clicks on your link to open a new tab or if they have JavaScript disabled.

If that is not possible, then you should at least inject the anchor tag into the document with JavaScript and the appropriate click event handlers.  

I realize this isn't always possible, but in my opinion it should be striven for in developing any public website.

Check out Unobtrusive JavaScript and Progressive enhancement (both Wikipedia).
AnswerKenntnis:
Doing <a href="#" onclick="myJsFunc();">Link</a> or <a href="javascript:void(0)" onclick="myJsFunc();">Link</a> or whatever else that contains an onclick attribute - was okay back five years ago, though now it can be a bad practice. Here's why:


It promotes the practice of obtrusive JavaScript - which has turned out to be difficult to maintain and difficult to scale. More on this in Unobtrusive JavaScript.
You're spending your time writing incredibly overly verbose code - which has very little (if any) benefit to your codebase.
There are now better, easier, and more maintainable and scalable ways of accomplishing the desired result.


The unobtrusive JavaScript way

Just don't have a href attribute at all! Any good CSS reset would take care of the missing default cursor style, so that is a non-issue. Then attach your JavaScript functionality using graceful and unobtrusive  best practices - which are more maintainable as your JavaScript logic stays in JavaScript, instead of in your markup - which is essential when you start developing large scale JavaScript applications which require your logic to be split up into blackboxed components and templates. More on this in Large-scale JavaScript Application Architecture)

Simple code example

Your HTML:

<a class="cancel-action">Cancel this action</a>


Your CSS:

a { cursor: pointer; color: blue; }
a:hover,a.hover { text-decoration: underline; }


Your JavaScript(using jQuery):

// Cancel click event
$('.cancel-action').click(function(){

});

// Hover shim for Internet Explorer 6 and Internet Explorer 7.
$(document.body).on('hover','a',function(){
    $(this).toggleClass('hover');
});


A blackboxed Backbone.js example

For a scalable, blackboxed, Backbone.js component example - see this working jsfiddle example here. Notice how we utilize unobtrusive JavaScript practices, and in a tiny amount of code have a component that can be repeated across the page multiple times without side-effects or conflicts between the different component instances. Amazing!

Notes


Omitting the href attribute on the a element will cause the element to not be accessible using tab key navigation. If you wish for those elements to be accessible via the tab key, you can set the tabindex attribute, or use button elements instead. You can easily style button elements to look like normal links as mentioned in Tracker1's answer.
Omitting the href attribute on the a element will cause Internet Explorer 6 and Internet Explorer 7 to not take on the a:hover styling, which is why we have added a simple JavaScript shim to accomplish this via a.hover instead. Which is perfectly okay, as if you don't have a href attribute and no graceful degradation then your link won't work anyway - and you'll have bigger issues to worry about.
If you want your action to still work with JavaScript disabled, then using an a element with a href attribute that goes to some URL that will perform the action manually instead of via an Ajax request or whatever should be the way to go. If you are doing this, then you want to ensure you do an event.preventDefault() on your click call to make sure when the button is clicked it does not follow the link. This option is called graceful degradation.
AnswerKenntnis:
'#' will take the user back to the top of the page, so I usually go with void(0).
AnswerKenntnis:
The first one, ideally with a real link to follow in case the user has JavaScript disabled. Just make sure to return false to prevent the click event from firing if the JavaScript executes.

<a href="#" onclick="myJsFunc(); return false;">Link</a>
AnswerKenntnis:
Ideally you'd do this:

<a href="javascriptlessDestination.html" onclick="myJSFunc(); return false;">Link text</a>


Or, even better, you'd have the default action link in the HTML, and you'd add the onclick event to the element unobtrusively via JavaScript after the DOM renders, thus ensuring that if JavaScript is not present/utilized you don't have useless event handlers riddling your code and potentially obfuscating (or at least distracting from) your actual content.
AnswerKenntnis:
Neither if you ask me;

If your "link" has the sole purpose of running some JavaScript code it doesn't qualify as a link; rather a piece of text with a JavaScript function coupled to it. I would recommend to use a <span> tag with an onclick handler attached to it and some basic CSS to immitate a link. Links are made for navigation, and if your JavaScript code isn't for navigation it should not be an <a> tag.

Example:

<style type="text/css">
    .jsAction {
        cursor: pointer;
        color: #00f;
        text-decoration: underline;
    }
</style>

<p>I want to call a JavaScript function <span class="jsAction" onclick="callFunction();">here</span>.</p>
AnswerKenntnis:
I would honestly suggest neither.  I would use a stylized <button></button> for that behavior.

button.link {
    display:inline-block;
    position:relative;
    background-color: transparent;
    cursor: pointer;
    border:0;
    padding:0;
    color:#00f;
    text-decoration:underline;
}


This way you can assign your onclick.  I also suggest binding via script, not using the onclick attribute on the element tag.

If you MUST use an A element, use javascript:void(0); for reasons already mentioned.


Will always intercept in case your onclick event fails.
Will not have errant load calls happen, or trigger other events based on a hash change


I've seen using the hash tag cause unexpected behavior, and it's best to avoid it unless you intend to use it, and are allowing the click to happen, then binding to the hash tag for interaction changes/history.
AnswerKenntnis:
I use the following

<a href="javascript:;" onclick="myJsFunc();">Link</a>


instead

<a href="javascript:void(0)" onclick="myJsFunc();">Link</a>
AnswerKenntnis:
I agree with suggestions elsewhere stating that you should use regular URL in href attribute, then call some JavaScript function in onclick. The flaw is, that they automaticaly add return false after the call.

The problem with this approach is, that if the function will not work or if there will be any problem, the link will become unclickable. Onclick event will always return false, so the normal URL will not be called.

There's very simple solution. Let function return true if it works correctly. Then use the returned value to determine if the click should be cancelled or not:

JavaScript

function doSomething() {
    alert( 'you clicked on the link' );
    return true;
}


HTML

<a href="path/to/some/url" onclick="return !doSomething();">link text</a>




Note, that I negate the result of the doSomething() function. If it works, it will return true, so it will be negated (false) and the path/to/some/URL will not be called. If the function will return false (for example, the browser doesn't support something used within the function or anything else goes wrong), it is negated to true and the path/to/some/URL is called.
AnswerKenntnis:
# is better than javascript:anything, but the following is even better:

HTML:

<a href="/gracefully/degrading/url/with/same/functionality.ext" class="some-selector">For great justice</a>


JavaScript:

$(function() {
    $(".some-selector").click(myJsFunc);
});


You should always strive for graceful degradation (in the event that the user doesn't have JavaScript enabled...and when it is with specs. and budget).  Also, it is considered bad form to use JavaScript attributes and protocol directly in HTML.
AnswerKenntnis:
Using just "#" makes some funny movements, so I would recommend to use "#self" if you would like to save on typing efforts of "JavaScript bla, bla,".
AnswerKenntnis:
Definitely hash (#) is better because in JavaScript it is a pseudoscheme:


pollutes history 
instantiates new copy of engine 
runs in global scope and doesn't respect event system. 


Of course "#" with an onclick handler which prevents default action is [much] better. Moreover, a link that has the sole purpose to run JavaScript is not really "a link" unless you are sending user to some sensible anchor on the page (just # will send to top) when something goes wrong. You can simply simulate look and feel of link with stylesheet and forget about href at all.

In addition, regarding cowgod's suggestion, particularly this: ...href="javascript_required.html" onclick="... This is good approach, but it doesn't distinguish between "JavaScript disabled" and "onclick fails" scenarios.
AnswerKenntnis:
Unless you're writing out the link using JavaScript (so that you know it's enabled in the browser), you should ideally be providing a proper link for people who are browsing with JavaScript disabled and then prevent the default action of the link in your onclick event handler. This way those with JavaScript enabled will run the function and those with JavaScript disabled will jump to an appropriate page (or location within the same page) rather than just clicking on the link and having nothing happen.
AnswerKenntnis:
I would use:

<a href="#" onclick="myJsFunc();return false;">Link</a>


Reasons:


This makes the href simple, search engines need it. If you use anything else ( such as a string), it may cause a 404 not found error.
When mouse hovers over the link, it doesn't show that it is a script.
By using return false;, the page doesn't jump to the top or break the back button.
AnswerKenntnis:
It would be better to use jQuery,

$(document).ready(function() {
    $("a").css("cursor", "pointer");
});


and omit both href="#" and href="javascript:void(0)".

The anchor tag markup will be like 

<a onclick="hello()">Hello</a>


Simple enough!
AnswerKenntnis:
I recommend using a <button> element instead, especially if the control is supposed to produce a change in the data. (Something like a POST.)

It's even better if you inject the elements unobtrusively, a type of progressive enhancement. (See this comment.)
AnswerKenntnis:
Depending on what you want to accomplish, you could forget the onclick and just use the href:

<a href="javascript:myJsFunc()">Link Text</a>


It gets around the need to return false. I don't like the # option because, as mentioned, it will take the user to the top of the page. If you have somewhere else to send the user if they don't have JavaScript enabled (which is rare where I work, but a very good idea), then Steve's proposed method works great. 

<a href="javascriptlessDestination.html" onclick="myJSFunc(); return false;">Link text</a>


Lastly, you can use javascript:void(0) if you do not want anyone to go anywhere and if you don't want to call a JavaScript function. It works great if you have an image you want a mouseover event to happen with, but there's not anything for the user to click on.
AnswerKenntnis:
If you happen to be using AngularJS, you can use the following:

<a href="">Do some fancy JavaScript</a>


Which will not do anything.

In addition


It will not take you to the top of the page, as with (#)

Therefore, you don't need to explicitly return false with JavaScript

It is short an concise
AnswerKenntnis:
Usually, you should always have a fall back link to make sure that clients with JavaScript disabled still has some functionality. This concept is called unobtrusive JavaScript. Example... Let's say you have the following search link:

<a href="search.php" id="searchLink">Search</a>


You can always do the following:

var link = document.getElementById('searchLink');

link.onclick = function() {
    try {
        // Do Stuff Here        
    } finally {
        return false;
    }
};


That way, people with JavaScript disabled are directed to search.php while your viewers with JavaScript view your enhanced functionality.
AnswerKenntnis:
If there is no href maybe there is no reason to use an anchor tag.

You can attach events (click, hover, etc) on almost every element, so why not just use a spanor a div?

And for users with javascript disabled: if there isn't a fallback (e.g. an alternative href), they should at least not be able to see and interact with that element at all, whatever it is an <a> or a <span> tag.
AnswerKenntnis:
You can also write a hint in an anchor like this:

<a href="javascript:void('open popup image')" onclick="return f()">...</a>


so the user will know what this link does.
AnswerKenntnis:
I'm basically paraphrasing from this practical article using progressive enhancement. The short answer is that you never use javascript:void(0); or # unless your user interface has already inferred that JavaScript is enabled, in which case you should use javascript:void(0);. Also, do not use span as links, since that is semantically false to begin with.

Using SEO friendly URL routes in your application, such as /Home/Action/Parameters is a good practice as well. If you have a link to a page that works without JavaScript first, you can enhance the experience afterward. Use a real link to a working page, then add an onlick event to enhance the presentation.

Here is a sample. Home/ChangePicture is a working link to a form on a page complete with user interface and standard HTML submit buttons, but it looks nicer injected into a modal dialog with jQueryUI buttons. Either way works, depending on the browser, which satisfies mobile first development.

<p><a href="Home/ChangePicture" onclick="return ChangePicture_onClick();" title="Change Picture">Change Picture</a></p>

<script type="text/javascript">
    function ChangePicture_onClick() {
        $.get('Home/ChangePicture',
              function (htmlResult) {
                  $("#ModalViewDiv").remove(); //Prevent duplicate dialogs
                  $("#modalContainer").append(htmlResult);
                  $("#ModalViewDiv").dialog({
                      width: 400,
                      modal: true,
                      buttons: {
                          "Upload": function () {
                              if(!ValidateUpload()) return false;
                              $("#ModalViewDiv").find("form").submit();
                          },
                          Cancel: function () { $(this).dialog("close"); }
                      },
                      close: function () { }
                  });
              }
        );
        return false;
    }
</script>
AnswerKenntnis:
What I understand from your words is that you want to create a link just to run JavaScript code.

Then you should consider that there are people who blocks JavaScript out there in their browsers.

So if you are really going to use that link only for running a JavaScript function then you should add it dynamically so it won't be even seen if the users didn't enable their JavaScript in the browser and you are using that link just to trigger a JavaScript function which makes no sense to use a link like that when JavaScript is disabled in the browser.

For that reason neither of them is good when JavaScript is disabled.

Aand if JavaScript is enabled and you only want to use that link to invoke a JavaScript function then

<a href="javascript:void(0)" onclick="myJsFunc();">Link</a>


is far better way than using

<a href="#" onclick="myJsFunc();">Link</a>


because href="#" is going to cause the page to do actions that are not needed.

Also, another reason why <a href="javascript:void(0)" onclick="myJsFunc();">Link</a> is better than <a href="#" onclick="myJsFunc();">Link</a> is that JavaScript is used as the default scripting language for most of the browsers. As an example Internet Explorer, uses an onclick attribute to define the type of scripting language that would be used. Unless another good scripting language pops up, JavaScript will be used by Internet Explorer as the default too, but if another scripting language used javascript:, it would let Internet Explorer to understand which scripting language is being used.

Considering this, I would prefer using and exercising on

<a href="javascript:void(0)" onclick="myJsFunc();">Link</a>


enough to make it a habit and to be more user friendly please add that kind of links within the JavaScript code:

$(document).ready(function(){
    $(".blabla").append('<a href="javascript:void(0)" onclick="myJsFunc();">Link</a>')
});
AnswerKenntnis:
Ideally you should have a real URL as fallback for non-JavaScript users.

If this doesn't make sense, use # as the href attribute.  I don't like using the onclick attribute since it embeds JavaScript directly in the HTML.  A better idea would be to use an external JS file and then add the event handler to that link.  You can then prevent the default event so that the URL doesn't change to append the # after the user clicks it.
AnswerKenntnis:
There is one more important thing to remember here. Section 508 compliance.
Because of it, I feel it's necessary to point out that you need the anchor tag for screen readers such as JAWS to be able to focus it through tabbing. So the solution "just use JavaScript and forget the anchor to begin with" is not an option for some of this. Firing the JavaScript inside the href is only necessary if you can't afford for the screen to jump back up to the top. You can use a settimeout for 0 seconds and have JavaScript fire to where you need focus but even the apage will jump to the top and then back.
AnswerKenntnis:
In total agreement with the overall sentiment, use void(0) when you need it, and use a valid URL when you need it.

Using URL rewriting you can make URLs that not only do what you want to do with JavaScript disabled, but also tell you exactly what its going to do.

<a href="./Readable/Text/URL/Pointing/To/Server-Side/Script" id="theLinkId">WhyClickHere</a>


On the server side, you just have to parse the URL and query string and do what you want. If you are clever, you can allow the server side script to respond to both Ajax and standard requests differently. Allowing you to have concise centralized code that handles all the links on your page.

URL rewriting tutorials

Pros


Shows up in status bar
Easily upgraded to Ajax via onclick handler in JavaScript
Practically comments itself
Keeps your directories from becoming littered with single use HTML files


Cons


Should still use event.preventDefault() in JavaScript
Fairly complex path handling and URL parsing on the server side.


I am sure there are tons more cons out there. Feel free to discuss them.
AnswerKenntnis:
I see a lot of answers by people who want to keep using # values for href, hence, here is an answer hopefully satisfying both camps:

A) I'm happy to have javascript:void(0) as my href value:

<a href="javascript:void(0)" onclick="someFunc.call(this)">Link Text</a>


B) I am using jQuery, and want # as my href value:

<a href="#" onclick="someFunc.call(this)">Link Text</a>

<script type="text/javascript">
    /* Stop page jumping when javascript links are clicked.
       Only select links where the href value is a #. */
    $('a[href="#"]').live("click", function(e) {
         return false; // prevent default click action from happening!
         e.preventDefault(); // same thing as above
    });
</script>


Note, if you know links won't be created dynamically, use the click function instead: 

$('a[href="#"]').click(function(e) {
AnswerKenntnis:
When I've got several faux-links, I prefer to give theme a class of 'no-link'

then in jQuery, I add the following code:

$(function(){
   $('.no-link').click(function(e){
       e.preventDefault();
   });
});


And for the HTML, the link is simply

<a href="/" class="no-link">Faux-Link</a>


I don't like using Hash-Tags unless they're used for anchors and I only do the above when I've got more than 2 faux-links, otherwise I go with javascript:void(0).

<a href="javascript:void(0)" class="no-link">Faux-Link</a>


Typically, I like to just avoid using a link at all and just wrap something around in a span and use that as a way to active some javascript like a pop-up or a content-reveal.
AnswerKenntnis:
You could use the href and remove all links that have only hashes:

HTML:

<a href="#" onclick="run_foo()"> foo </a>


JS:

$(document).ready(function(){         // on DOM ready or some other event

   $('a[href=#]').attr('href','');    // set all reference handles to blank strings
                                      //  for anchors that have only hashes

});
QuestionKenntnis:
Why does HTML think ΓÇ£chucknorrisΓÇ¥ is a color?
qn_description:
How come certain random strings produce various colors when entered as background colors in HTML? For example:

<body bgcolor="chucknorris"> test </body>


...produces a document with a red background across all browsers and platforms. Here is a Fiddle.

Interestingly, while chucknorri produces a red background as well, chucknorr produces a yellow background.

What's going on here?
AnswersKenntnis
AnswerKenntnis:
It's a holdover from the Netscape days:


  Missing digits are treated as 0[...]. An incorrect digit is simply interpreted as 0. For example the values #F0F0F0, F0F0F0, F0F0F, #FxFxFx and FxFxFx are all the same. 


From this blog post, which covers it in great detail, including varying lengths of color values, etc.

If we apply the rules in turn from the blog post, we get the following:


Replace all non valid hexadecimal characters with 0's


chucknorris becomes c00c0000000


Pad out to the next total number of characters divisible by 3 (11 -> 12)


c00c 0000 0000


Split into three equal groups, with each component representing the corresponding colour component of an RGB colour:


RGB (c00c, 0000, 0000)


Truncate each of the arguments from the right down to 2 characters


Which gives the result

RGB (c0, 00, 00) = #C00000 or RGB(192, 0, 0)

Here's a JSFiddle demonstrating the bgcolor attribute in action, to produce this "amazing" colour swatch:



This also answers the other part of the question; why does bgcolor="chucknorr" produce a yellow colour? Well, if we apply the rules, the string is:

c00c00000 => c00 c00 000 => c0 c0 00 [RGB(192, 192, 0)] 

Which gives a light yellow gold colour. As the string starts off as 9 characters, we keep the second C this time around hence it ends up in the final colour value.

I originally encountered this when someone pointed out you could do color="crap" and it, well, comes out brown.
AnswerKenntnis:
I'm sorry to disagree, but according to the rules for parsing a legacy color value posted by @Yuhong Bao, chucknorris DOES NOT equate to #CC0000, but rather to #C00000, a very similar but slightly different hue of red. I used the Firefox ColorZilla add-on to verify this.

The rules state:  


make the string a length that is a multiple of 3 by adding 0s: chucknorris0
separate the string into 3 equal length strings: chuc knor ris0
truncate each string to 2 characters: ch kn ri
keep the hex values, and add 0's where necessary: C0 00 00


I was able to use these rules to correctly interpret the following strings:


LuckyCharms
Luck
LuckBeALady
LuckBeALadyTonight
GangnamStyle


Unfortunately I have not yet been able to determine why this doesn't seem to work for adamlevine which should be ADE0E0 but it's actually AD0E0E.

UPDATE: The original answerers who said the color was #CC0000 have since edited their answers to include the correction.
AnswerKenntnis:
Most browsers will simply ignore any NON-hex values in your color string, substituting non-hex digits with zeros.

ChuCknorris translates to c00c0000000.  At this point, the browser will divide the string into three equal sections, indicating Red, Green and Blue values: c00c 0000 0000.  Extra bits in each section will be ignored, which makes the final result #c00000 which is a reddish color.

Note, this does not apply to CSS color parsing, which follow the CSS standard.

<p><font color='chucknorris'>Redish</font></p>
<p><font color='#c00000'>Same as above</font></p>
<p><span style="color: chucknorris">Black</span></p>


Fiddle
AnswerKenntnis:
The WHATWG HTML spec has the exact algorithm for parsing a legacy color value:
http://www.whatwg.org/specs/web-apps/current-work/multipage/common-microsyntaxes.html#rules-for-parsing-a-legacy-color-value

The code Netscape Classic used for parsing color strings is open source:
http://mxr.mozilla.org/classic/source/lib/layout/layimage.c#155

For example, notice that each character is parsed as a hex digit and then is shifted into a 32-bit integer without checking for overflow. Only eight hex digits fit into a 32-bit integer, which is why only the last 8 characters are considered. After parsing the hex digits into 32-bit integers, they are then truncated into 8-bit integers by dividing them by 16 until they fit into 8-bit, which is why leading zeros are ignored.
AnswerKenntnis:
Browser is trying to convert chucknorris into hex colour code coz it's not a valid value. 

in chucknorris everything except c is not a valid hex value. So it gets converted to    c00c00000000 

which becomes #c00000 a shade of red. 

This seems to be an issue primary with IE and Opera(12) as both chrome(31) and firefox(26) just ignore this.

ps: no in brackets are the browser i tested on
QuestionKenntnis:
What's the difference between String and string?
qn_description:
In C#, what is the difference between String and string? (note the case)

Example:

string s = "Hello, World";

String S = "Hello, World";


Also, what are the guidelines for the use of each?
AnswersKenntnis
AnswerKenntnis:
string is an alias for System.String.  So technically, there is no difference.  It's like int vs. System.Int32.

As far as guidelines, I think it's generally recommended to use string any time you're referring to an object.  

e.g. string place = "world";

Likewise, I think it's generally recommended to use String if you need to refer specifically to the class.

e.g.
    string greet = String.Format("Hello {0}!", place);

This is the style that Microsoft tends to use in their examples.



It appears that the guidance in this area may have changed, as StyleCop now enforces the use of the C#-specific aliases.
AnswerKenntnis:
Just for the sake of completeness, here's a brain dump of related information...

As others have noted, string is an alias for System.String. They compile to the same code, so at execution time there is no difference whatsoever. This is just one of the aliases in C#. The complete list is:

object:  System.Object
string:  System.String
bool:    System.Boolean
byte:    System.Byte
sbyte:   System.SByte
short:   System.Int16
ushort:  System.UInt16
int:     System.Int32
uint:    System.UInt32
long:    System.Int64
ulong:   System.UInt64
float:   System.Single
double:  System.Double
decimal: System.Decimal
char:    System.Char


Apart from string, object, the aliases are all to value types. decimal is a value type, but not a primitive type in the CLR. The only primitive type which doesn't have an alias is System.IntPtr.

In the spec, the value type aliases are known as "simple types". Literals can be used for constant values of every simple type; no other value types have literal forms available. (Compare this with VB, which allows DateTime literals, and has an alias for it too.)

There is one circumstance in which you have to use the aliases: when explicitly specifying an enum's underlying type. For instance:

public enum Foo : UInt32 {} // Invalid
public enum Bar : uint   {} // Valid


Finally, when it comes to which to use: personally I use the aliases everywhere for the implementation, but the CLR type for any APIs. It really doesn't matter too much which you use in terms of implementation - consistency among your team is nice, but no-one else is going to care. On the other hand, it's genuinely important that if you refer to a type in an API, you do so in a language neutral way. A method called ReadInt32 is unambiguous, whereas a method called ReadInt requires interpretation. The caller could be using a language which defines an int alias for Int16, for example. The .NET framework designers have followed this pattern, good examples being in the BitConverter, BinaryReader and Convert classes.
AnswerKenntnis:
String stands for System.String and it is a .NET Framework type. string is an alias in the C# language for  System.String. Both of them are compiled to System.String in IL (Intermediate Language), so there is no difference. Choose what you like and use that. If you code in C#, I'd prefer string as it's a C# type alias and well-known by C# programmers.

I can say the same about (int, System.Int32) etc..
AnswerKenntnis:
The best answer I have ever heard about using the provided type aliases in C# comes from Jeffrey Richter in his book CLR Via C#. Here are his 3 reasons:


  
  I've seen a number of developers confused, not knowing whether to use string or String in their code. Because in C# the string (a keyword) maps exactly to System.String (an FCL type), there is no difference and either can be used.
  In C#, long maps to System.Int64, but in a different programming language, long could map to an Int16 or Int32. In fact, C++/CLI does in fact treat long as an Int32. Someone reading source code in one language could easily misinterpret the code's intention if he or she were used to programming in a different programming language. In fact, most languages won't even treat long as a keyword and won't compile code that uses it.
  The FCL has many methods that have type names as part of their method names. For example, the BinaryReader type offers methods such as ReadBoolean, ReadInt32, ReadSingle, and so on, and the System.Convert type offers methods such as ToBoolean, ToInt32, ToSingle, and so on. Although it's legal to write the following code, the line with float feels very unnatural to me, and it's not obvious that the line is correct:
  


BinaryReader br = new BinaryReader(...);
float val  = br.ReadSingle(); // Ok, but feels unnatural
Single val = br.ReadSingle(); // OK and feels good


So there you have it. I think these are all really good points. I however, don't find myself using Jeffrey's advice in my own code. Maybe I am too stuck in my C# world but I end up trying to make my code look like the framework code.
AnswerKenntnis:
string is a reserved word, but String is just a class name. 
This means that 'string' cannot be used as a variable name by itself.

If for some reason you wanted a variable called string, you'd see only the first of these compiles:

StringBuilder String = new StringBuilder();  // compiles
StringBuilder string = new StringBuilder();  // doesn't compile 


If you really want a variable name called 'string' you can use @ as a prefix :

StringBuilder @string = new StringBuilder();


Another critical difference : Stackoverflow highlights them differently.
AnswerKenntnis:
It's been covered above; however, you can't use string in reflection; you must use String.
AnswerKenntnis:
There IS one difference - you can't use String without using System; beforehand.
AnswerKenntnis:
Valters, you cannot establish global aliases in the style of string, int, etc. so far as I know.  However, you can do more localized aliasing for types and namespaces with the using keyword.

e.g.

using str = System.String;
//...
str s = "Now you've got another alias for string!";


See here: using Directive (C# Reference)
AnswerKenntnis:
string and String are identical in all ways (except the uppercase "S").  There are no performance implications either way.

Lowercase string is preferred in most projects due to the syntax highlighting
AnswerKenntnis:
System.String is THE .net string class - in C# string is an alias for System.String - so in use they are the same.

As for guidelines I wouldn't get too bogged down and just use whichever you feel like - there are more important things in life and the code is going to be the same anyway.

If you find yourselves building systems where it is necessary to specify the size of the integers you are using and so tend to use Int16, Int32, UInt16, UInt32 etc. then it might look more natural to use String - and when moving around between different .net languages it might make things more understandable - otherwise I would use string and int.
AnswerKenntnis:
I prefer the capitalized .NET types (rather than the aliases) for formatting reasons. The .NET types are colored the same as other object types (the value types are proper objects, after all).

Conditional and control keywords (like if, switch, and return) are lowercase and colored dark blue (by default). And I would rather not have the disagreement in use and format.

Consider:

String someString; 
string anotherString;
AnswerKenntnis:
string is just an alias for System.String. The compiler will treat them identically.

The only practical difference is the syntax highlighting as you mention, and that you have to write using System if you use String.
AnswerKenntnis:
C# is a language which is used together with the CLR.

string is a type in C#.

System.String is a type in the CLR.

When you use C# together with the CLR string will be mapped to System.String.

Theoretically, you could implement a C#-compiler that generated Java bytecode. A sensible implementation of this compiler would probably map string to java.lang.String in order to interoperate with the Java runtime library.
AnswerKenntnis:
Both are same. But from coding guidelines perspective it's better to use string instead of String. This is what generally developers use. e.g. instead of using Int32 we use int as int is alias to Int32
FYI
ΓÇ£The keyword string is simply an alias for the predefined class System.String.ΓÇ¥ - C# Language Specification 4.2.3
http://msdn2.microsoft.com/En-US/library/aa691153.aspx
AnswerKenntnis:
As the others are saying, they're the same.  StyleCop rules, by default, will enforce you to use string as a C# code style best practice, except when referencing System.String static functions, such as String.Format, String.Join, String.Concat, etc...
AnswerKenntnis:
Lower case string is an alias for System.String.
They are the same in C#.

There's a debate over whether you should use the System types (System.Int32, System.String, etc.) types or the C# aliases (int, string, etc). I personally believe you should use the C# aliases, but that's just my personal preference.
AnswerKenntnis:
I just wrote a short article on this topic, including a response from Juval L├╢wy (of IDesign, creator of the C# coding standard). It says it all!

Check it out on this link.
AnswerKenntnis:
Against what seems to be common practice among other programmers, I prefer String over string, just to highlight the fact that String is a reference type, as Jon Skeet mentioned.
AnswerKenntnis:
Using System types makes it easier to port between C# and VB.Net, if you are into that sort of thing.
AnswerKenntnis:
ΓÇÿstringΓÇÖ is an alias (or shorthand) of System.String. That means, by typing ΓÇÿstringΓÇÖ we meant System.String. You can read more in think link: 'string' is an alias/shorthand of System.String.
AnswerKenntnis:
String (System.String) is a class in the base class library. string (lower case) is a reserved work in C# that is an alias for System.String. Int32 vs int is a similar situation as is Boolean vs. bool. These C# language specific keywords enable you to declare primitives in a style similar to C.
AnswerKenntnis:
String is not a keyword and it can be used as Identifier whereas string is a keyword and cannot be used as Identifier. And in function point of view both are same.
AnswerKenntnis:
I'd just like to add this to lfousts answer, from Ritchers book:


  The C# language specification states,
  ΓÇ£As a matter of style, use of the
  keyword is favored over use of the
  complete system type name.ΓÇ¥ I disagree
  with the language specification; I
  prefer to use the FCL type names and
  completely avoid the primitive type
  names. In fact, I wish that compilers
  didnΓÇÖt even offer the primitive type
  names and forced developers to use the
  FCL type names instead. Here are my
  reasons:


I didn't get his opinion before I read the complete paragraph.
AnswerKenntnis:
Coming late to the party: I use the CLR types 100% of the time (well, except if forced to use the C# type, but I don't remember when the last time that was). 

I originally started doing this years ago, as per the CLR books by Ritchie. It made sense to me that all CLR languages ultimately have to be able to support the set of CLR types, so using the CLR types yourself provided clearer, and possibly more "reusable" code.

Now that I've been doing it for years, it's a habit and I like the coloration that VS shows for the CLR types.

The only real downer is that auto-complete uses the C# type, so I end up re-typing automatically generated types to specify the CLR type instead.

Also, now, when I see "int" or "string", it just looks really wrong to me, like I'm looking at 1970's C code.
AnswerKenntnis:
There is no difference.

The C# keyword string maps to the .NET type System.String - it is an alias that keeps to the naming conventions of the language.

Similarly, int maps to System.Int32.
AnswerKenntnis:
There is no difference between the two - string, however, appears to be the preferred option when considering other developers' source code.
AnswerKenntnis:
It's a matter of convention, really.  "string" just looks more like C/C++ style.  The general convention is to use whatever shortcuts your chosen language has provided (int/Int for Int32).  This goes for "object" and "decimal" as well.

Theoretically this could help to port code into some future 64-bit standard in which "int" might mean Int64, but that's not the point, and I would expect any upgrade wizard to change any "int" references to "Int32" anyway just to be safe.
AnswerKenntnis:
Yes, that's no difference between them, just like the bool and Boolean.
AnswerKenntnis:
One argument not mentioned elsewhere to prefer the pascal case String:

System.String is a reference type, and reference types names are pascal case by convention.
AnswerKenntnis:
This youtube video demonstrates practically how they differ http://www.youtube.com/watch?v=ikqUUIg8gmk 

But now for a long textual answer.

When we talk about .NET there are two different things one there is .NET framework and the other there are languages ( C# , VB.NET etc) which use that framework.



"System.String" a.k.a "String" ( capital "S") is a .NET framework data type while "string" is a C# data type.



In short "String" is an alias ( the same thing called with different names) of "string". So technically both the below code statements will give the same output.

String s = "I am String";

or

string s = "I am String";

In the same way there are aliases for other c# data type as shown below:-

object: System.Object,string: System.String,bool: System.Boolean,byte: System.Byte,sbyte:,System.SByte,short: System.Int16 and so on

Now the million dollar question from programmer's point of view So when to use "String" and "string"?

First thing to avoid confusion use one of them consistently. But from best practices perspective when you do variable declaration it's good to use "string" ( small "s") and when you are using it as a class name then "String" ( capital "S") is preferred.

In the below code the left hand side is a variable declaration and it declared using "string". At the right hand side we are calling a method so "String" is more sensible.

string s = String.ToUpper() ;
QuestionKenntnis:
What is the most efficient way to clone an object?
qn_description:
What is the most efficient way to clone a JavaScript object? I've seen obj = eval(uneval(o)); being used, but that's currently Firefox-only. In Mootools 1.2, I've done things like obj = JSON.decode(JSON.encode(o)); but question the efficiency.

I've also seen recursive copying functions with various flaws. I'm surprised no canonical solution exists.
AnswersKenntnis
AnswerKenntnis:
Note: This is a reply to another answer, not a proper response to this question. If you wish to have fast object cloning please follow Corban's advice in his answer to this question.




I want to note that the .clone() method in jQuery only clones DOM elements. In order to clone JavaScript objects, you would do:

// Shallow copy
var newObject = jQuery.extend({}, oldObject);

// Deep copy
var newObject = jQuery.extend(true, {}, oldObject);


More information can be found in the jQuery documentation.

I also want to note that the deep copy is actually much smarter than what is shown above ΓÇô it's able to avoid many traps (trying to deep extend a DOM element, for example). It's used frequently in jQuery core and in plugins to great effect.
AnswerKenntnis:
Checkout this benchmark: http://jsperf.com/cloning-an-object/2

In my previous tests where speed was a main concern I found JSON.parse(JSON.stringify(obj)) to be the fastest way to Deep clone an object (it beats out JQuery.extend with deep flag set true by 10-20%).

JQuery.extend is pretty fast when deep flag is set to false (shallow clone). It is a good option because it includes some extra logic for type validation and doesnt copy over undefined properties, etc. but this will also slow you down a little.

If you know the structure of the objects you are trying to clone or can avoid deep nested arrays you can write a simple for (var i in obj) loop to clone your object while checking hasOwnProperty and it will be much much faster than JQuery. 

Lastly if you are attempting to clone a known object structure in a hot loop you can get MUCH MUCH MORE PERFORMANCE by simply in-lining the clone procedure and manually constructing the object. 
JS trace engines suck at optimizing for..in loops and checking hasOwnProperty will slow you down as well. Manual clone when speed is an absolute must.

var clonedObject = {
  knownProp: obj.knownProp,
  ..
}


I hope you found this helpful.
AnswerKenntnis:
There doesn't seem to be an in-built one, you could try:

function clone(obj){
    if(obj == null || typeof(obj) != 'object')
        return obj;

    var temp = obj.constructor(); // changed

    for(var key in obj)
        temp[key] = clone(obj[key]);
    return temp;
}


There's a lengthy post with many contributing comments on Keith Deven's blog.

If you want to stick to a framework, JQuery also has a clone() function:

// Clone current element
var cloned = $(this).clone();


There were reported issues previously with this not working in Internet Explorer, but these were resolved as of version 1.2.3.
AnswerKenntnis:
HTML5 includes a way to create deep clones of objects. It only works for some built-in types, but it's considerably more flexible than using JSON: the internal structured clone algorithm also supports Dates, RegExps, Files, Blobs, FileLists, ImageDatas, sparse Arrays, types defined in other specification such as Typed Arrays, and recursive/cyclical structures.

Sadly, this feature is not directly exposed through any API. Here are two ways that structured clones can be created. Both have drawbacks that make them unsuitable in many cases. Hopefully it will one day be possible to created structured clones without any side effects.



history.pushState() and history.replaceState() both create a structured clone of their first argument, and assign that value to history.state. You can use this to create a structured clone of any object like this:

function structuredClone(obj) {
    var oldState = history.state;
    history.replaceState(obj);
    var clonedObj = history.state;
    history.replaceState(oldState);
    return clonedObj;
}


Example Usage (jsfiddle)

var original = { date: new Date(), number: Math.random() };
original.self = original;

var clone = structuredClone(original);

// They're different objects:
console.log(original !== clone);
console.log(original.date !== clone.date);

// They're cyclical:
console.log(original.self === original);
console.log(clone.self === clone);

// They contain equivalent values:
console.log(original.number === clone.number);
console.log(Number(original.date) === Number(clone.date));


This shouldn't leave any trace in the browser's history, but it is very slow and may cause the browser to begin to briefly display its loading animation. If you attempt to clone many objects at once, it may cause the browser (including other tabs and windows) to become unresponsive until the cloning is complete. (The cloning itself isn't slow; that's a side-effect of manipulating the history.)



The alternative is to call the window.postMessage method to send a message containing the target object to window, then listen for the message event containing the cloned object. Unfortunately, this is inherently asynchronous.

If asynchronous cloning is acceptable, here's a asyncStructuredClone(original, callback(clone)) function which will produce a structured clone of a target object and pass it to your callback. An exception will be raised if you attempt to clone an unsupported object.

var pendingCallbacks = {};

window.addEventListener('message', function(e) {
    var cloneId = e.data.cloneId,
        clonedValue = e.data.value;

    if (e.source === window && cloneId && cloneId in pendingCallbacks) {
        var callback = pendingCallbacks[cloneId];
        delete pendingCallbacks[cloneId];
        callback(clonedValue);
    }
});

var asyncStructuredClone = function(o, callback) {
    var cloneId = String(Math.random());
    pendingCallbacks[cloneId] = callback;
    window.postMessage({ value: o, cloneId: cloneId }, '*');
};


Example Usage (jsfiddle)

var original = { date: new Date(), number: Math.random() };
original.self = original;

asyncStructuredClone(original, function(clone) {
    // They're different objects:
    console.log(original !== clone);
    console.log(original.date !== clone.date);

    // They're cyclical:
    console.log(original.self === original);
    console.log(clone.self === clone);

    // They contain equivalent values:
    console.log(original.number === clone.number);
    console.log(Number(original.date) === Number(clone.date));
});
AnswerKenntnis:
@David, assuming that you have only variables and not any functions in your object, you can  just use:

var newObject = JSON.parse(JSON.stringify(oldObject));
AnswerKenntnis:
This is what I'm using:

function cloneObject(obj) {
    var clone = {};
    for(var i in obj) {
        if(typeof(obj[i])=="object" && obj[i] != null)
            clone[i] = cloneObject(obj[i]);
        else
            clone[i] = obj[i];
    }
    return clone;
}
AnswerKenntnis:
Code:

// extends 'from' object with members from 'to'. If 'to' is null, a deep clone of 'from' is returned
function extend(from, to)
{
    if (from == null || typeof from != "object") return from;
    if (from.constructor != Object && from.constructor != Array) return from;
    if (from.constructor == Date || from.constructor == RegExp || from.constructor == Function ||
        from.constructor == String || from.constructor == Number || from.constructor == Boolean)
        return new from.constructor(from);

    to = to || new from.constructor();

    for (var name in from)
    {
        to[name] = typeof to[name] == "undefined" ? extend(from[name], null) : to[name];
    }

    return to;
}


Test:

var obj =
{
    date: new Date(),
    func: function(q) { return 1 + q; },
    num: 123,
    text: "asdasd",
    array: [1, "asd"],
    regex: new RegExp(/aaa/i),
    subobj:
    {
        num: 234,
        text: "asdsaD"
    }
}

var clone = extend(obj);
AnswerKenntnis:
var clone = function() {
    var newObj = (this instanceof Array) ? [] : {};
    for (var i in this) {
        if (this[i] && typeof this[i] == "object") {
            newObj[i] = this[i].clone();
        }
        else
        {
            newObj[i] = this[i];
        }
    }
    return newObj;
}; 

Object.defineProperty( Object.prototype, "clone", {value: clone, enumerable: false});
AnswerKenntnis:
I know this is an old post, but I thought this may be of some help to the next guy who stumbles along.

As long as you don't assign an object to anything it maintains no reference in memory.  So to make an object that you want to share among other objects, you'll have to create a factory like so:

var a = function(){
    return {
        father:'zacharias'
    };
},
b = a(),
c = a();
c.father = 'johndoe';
alert(b.father);
AnswerKenntnis:
ThereΓÇÖs a library (called ΓÇ£cloneΓÇ¥), that does this quite well. It provides the most complete recursive cloning/copying of arbitrary objects that I know of. It also supports circular references, which is not covered by the other answers, yet.

You can find it on npm, too. It can be used for the browser as well as Node.js.

Here is an example on how to use it:

Install it with

npm install clone


or package it with Ender.

ender build clone [...]


You can also download the source code manually.

Then you can use it in your source code.

var clone = require('clone');

var a = { foo: { bar: 'baz' } };  // inital value of a
var b = clone(a);                 // clone a -> b
a.foo.bar = 'foo';                // change a

console.log(a);                   // { foo: { bar: 'foo' } }
console.log(b);                   // { foo: { bar: 'baz' } }


(Disclaimer: IΓÇÖm the author of the library.)
AnswerKenntnis:
If you're using it, the underscore.js library has a clone method.

var newObject = _.clone(oldObject);
AnswerKenntnis:
dojo.clone apparently clones "anything". Certainly worth a look, perhaps?

http://api.dojotoolkit.org/jsdoc/1.5/dojo.clone
AnswerKenntnis:
Crockford suggests (and I prefer) using this function:

function object(o) {
    function F() {}
    F.prototype = o;
    return new F();
}

var newObject = object(oldObject);


It's terse, works as expected and you don't need a library.
AnswerKenntnis:
There seems to be no ideal deep clone operator yet for array-like objects.  As the code below illustrates, John Resig's jQuery cloner turns arrays with non-numeric properties into objects that are not arrays, and RegDwight's JSON cloner drops the non-numeric properties. The following tests illustrate these points on multiple browsers:

function jQueryClone(obj) {
   return jQuery.extend(true, {}, obj)
}

function JSONClone(obj) {
   return JSON.parse(JSON.stringify(obj))
}

var arrayLikeObj = [[1, "a", "b"], [2, "b", "a"]];
arrayLikeObj.names = ["m", "n", "o"];
var JSONCopy = JSONClone(arrayLikeObj);
var jQueryCopy = jQueryClone(arrayLikeObj);

alert("Is arrayLikeObj an array instance?" + (arrayLikeObj instanceof Array) +
      "\nIs the jQueryClone an array instance? " + (jQueryCopy instanceof Array) +
      "\nWhat are the arrayLikeObj names? " + arrayLikeObj.names +
      "\nAnd what are the JSONClone names? " + JSONCopy.names)
AnswerKenntnis:
Here's a version of ConroyP's answer above that works even if the constructor has required parameters:

//If Object.create isn't already defined, we just do the simple shim, without the second argument,
//since that's all we need here
var object_create = Object.create;
if (typeof object_create !== 'function') {
    object_create = function(o) {
        function F() {}
        F.prototype = o;
        return new F();
    };
}

function deepCopy(obj) {
    if(obj == null || typeof(obj) !== 'object'){
        return obj;
    }
    //make sure the returned object has the same prototype as the original
    var ret = object_create(obj.constructor.prototype);
    for(var key in obj){
        ret[key] = deepCopy(obj[key]);
    }
    return ret;
}


This function is also available in my simpleoo library.

Edit:

Here's a more robust version (thanks to Justin McCandless this now supports cyclic references as well):

/**
 * Deep copy an object (make copies of all its object properties, sub-properties, etc.)
 * An improved version of http://keithdevens.com/weblog/archive/2007/Jun/07/javascript.clone
 * that doesn't break if the constructor has required parameters
 * 
 * It also borrows some code from http://stackoverflow.com/a/11621004/560114
 */ 
function deepCopy(src, /* INTERNAL */ _visited) {
    if(src == null || typeof(src) !== 'object'){
        return src;
    }

    // Initialize the visited objects array if needed
    // This is used to detect cyclic references
    if (_visited == undefined){
        _visited = [];
    }
    // Otherwise, ensure src has not already been visited
    else {
        var i, len = _visited.length;
        for (i = 0; i < len; i++) {
            // If src was already visited, don't try to copy it, just return the reference
            if (src === _visited[i]) {
                return src;
            }
        }
    }

    // Add this object to the visited array
    _visited.push(src);

    //Honor native/custom clone methods
    if(typeof src.clone == 'function'){
        return src.clone(true);
    }

    //Special cases:
    //Array
    if (Object.prototype.toString.call(src) == '[object Array]') {
        //[].slice(0) would soft clone
        ret = src.slice();
        var i = ret.length;
        while (i--){
            ret[i] = deepCopy(ret[i], _visited);
        }
        return ret;
    }
    //Date
    if (src instanceof Date){
        return new Date(src.getTime());
    }
    //RegExp
    if(src instanceof RegExp){
        return new RegExp(src);
    }
    //DOM Elements
    if(src.nodeType && typeof src.cloneNode == 'function'){
        return src.cloneNode(true);
    }

    //If we've reached here, we have a regular object, array, or function

    //make sure the returned object has the same prototype as the original
    var proto = (Object.getPrototypeOf ? Object.getPrototypeOf(src): src.__proto__);
    if (!proto) {
        proto = src.constructor.prototype; //this line would probably only be reached by very old browsers 
    }
    var ret = object_create(proto);

    for(var key in src){
        //Note: this does NOT preserve ES5 property attributes like 'writable', 'enumerable', etc.
        //For an example of how this could be modified to do so, see the singleMixin() function
        ret[key] = deepCopy(src[key], _visited);
    }
    return ret;
}

//If Object.create isn't already defined, we just do the simple shim, without the second argument,
//since that's all we need here
var object_create = Object.create;
if (typeof object_create !== 'function') {
    object_create = function(o) {
        function F() {}
        F.prototype = o;
        return new F();
    };
}
AnswerKenntnis:
In AngularJS:

angular.copy(source[, destination]);


http://docs.angularjs.org/api/angular.copy
AnswerKenntnis:
In Prototype you would do something like

newObject = Object.clone(myObject);


The Prototype documentation notes that this makes a shallow copy.
AnswerKenntnis:
function clone(obj)
 { var clone = {};
   clone.prototype = obj.prototype;
   for (property in obj) clone[property] = obj[property];
   return clone;
 }
AnswerKenntnis:
in my FF3.6/IE8/Chrome4 works only this solution:

function cloneObject(obj){
  var newObj = (obj instanceof Array) ? [] : {};
  for (var i in obj) {
    if (obj[i] && typeof obj[i] == "object") 
      newObj[i] = obj[i].clone();
    else
      newObj[i] = obj[i];
  }
  return newObj;
}


I don't know why, but Object's prototype extension doesn't work well in FF ;(
AnswerKenntnis:
This isn't generally the most efficient solution, but it does what I need. Simple test cases below...

function clone(obj, clones) {
    // Makes a deep copy of 'obj'. Handles cyclic structures by
    // tracking cloned obj's in the 'clones' parameter. Functions 
    // are included, but not cloned. Functions members are cloned.
    var new_obj,
        already_cloned,
        t = typeof obj,
        i = 0,
        l,
        pair; 

    clones = clones || [];

    if (obj === null) {
        return obj;
    }

    if (t === "object" || t === "function") {

        // check to see if we've already cloned obj
        for (i = 0, l = clones.length; i < l; i++) {
            pair = clones[i];
            if (pair[0] === obj) {
                already_cloned = pair[1];
                break;
            }
        }

        if (already_cloned) {
            return already_cloned; 
        } else {
            if (t === "object") { // create new object
                new_obj = new obj.constructor();
            } else { // Just use functions as is
                new_obj = obj;
            }

            clones.push([obj, new_obj]); // keep track of objects we've cloned

            for (key in obj) { // clone object members
                if (obj.hasOwnProperty(key)) {
                    new_obj[key] = clone(obj[key], clones);
                }
            }
        }
    }
    return new_obj || obj;
}


Cyclic array test...

a = []
a.push("b", "c", a)
aa = clone(a)
aa === a //=> false
aa[2] === a //=> false
aa[2] === a[2] //=> false
aa[2] === aa //=> true


Function test...

f = new Function
f.a = a
ff = clone(f)
ff === f //=> true
ff.a === a //=> false
AnswerKenntnis:
Shallow copy one-liner (ECMAScript 5th edition) :

var origin = { foo : {} };
var copy = Object.keys(origin).reduce(function(c,k){c[k]=origin[k];return c;},{});

console.log(origin, copy);
console.log(origin == copy); // false
console.log(origin.foo == copy.foo); // true
AnswerKenntnis:
The way you are supposed to do it in Mootools.

var my_object = {one:1,two:2, subobject:{a:['a','A']}},three:'3'};
var my_object_clone = $merge({},my_object);
AnswerKenntnis:
Here is a comprehensive clone() method that can clone any js object. It handles almost all the cases:

function clone(src, deep) {

    var toString = Object.prototype.toString;
    if(!src && typeof src != "object"){
        //any non-object ( Boolean, String, Number ), null, undefined, NaN
        return src;
    }

    //Honor native/custom clone methods
    if(src.clone && toString.call(src.clone) == "[object Function]"){
        return src.clone(deep);
    }

    //DOM Elements
    if(src.nodeType && toString.call(src.cloneNode) == "[object Function]"){
        return src.cloneNode(deep);
    }

    //Date
    if(toString.call(src) == "[object Date]"){
        return new Date(src.getTime());
    }

    //RegExp
    if(toString.call(src) == "[object RegExp]"){
        return new RegExp(src);
    }

    //Function
    if(toString.call(src) == "[object Function]"){
        //Wrap in another method to make sure == is not true;
        //Note: Huge performance issue due to closures, comment this :)
        return (function(){
            src.apply(this, arguments);
        });

    }

    var ret, index;
    //Array
    if(toString.call(src) == "[object Array]"){
        //[].slice(0) would soft clone
        ret = src.slice();
        if(deep){
            index = ret.length;
            while(index--){
                ret[index] = clone(ret[index], true);
            }
        }
    }
    //Object
    else {
        ret = src.constructor ? new src.constructor() : {};
        for (var prop in src) {
            ret[prop] = deep
                ? clone(src[prop], true)
                : src[prop];
        }
    }

    return ret;
};
AnswerKenntnis:
I have two good answers depending on whether your objective is to clone a "plain old javascript object" or not.

Let's also assume that your intention is to create a complete clone with no prototype references back to the source object.  If you're not interested in a complete clone, then you can use many of the Object.clone() routines provided in some of the other answers (Crockford's pattern).

For plain old JavaScript objects, a tried and true good way to clone an object in modern runtimes is quite simply:

var clone = JSON.parse(JSON.stringify(obj));


Note that the source object must be a pure JSON object.  This is to say, all of its nested properties must be scalars (like boolean, string, array, object, etc).  Any functions or special objects like RegExp or Date will not be cloned.

Is it efficient?  Heck yes.  We've tried all kinds of cloning methods and this works best.  I'm sure some ninja could conjure up a faster method.  But I suspect we're talking about marginal gains.

This approach is just simple and easy to implement.  Wrap it into a convenience function and if you really need to squeeze out some gain, go for at a later time.

Now, for non-plain JavaScript objects, there isn't a really simple answer.  In fact, there can't be because of the dynamic nature of JavaScript functions and inner object state.  Deep cloning a JSON structure with functions inside requires you recreate those functions and their inner context.  And JavaScript simply doesn't have a standardized way of doing that.

The correct way to do this, once again, is via a convenience method that you declare and reuse within your code.  The convenience method can be endowed with some understanding of your own objects so you can make sure to properly recreate the graph within the new object.

We're written our own but the best general approach I've seen is covered here:

http://davidwalsh.name/javascript-clone

This is the right idea.  The author (David Walsh) has commented out the cloning of generalized functions.  This is something you might choose to do, depending on your use case.

The main idea is that you need to special handle the instantiation of your functions (or prototypal classes, so to speak) on a per-type basis.  Here, he's provided a few examples for RegExp and Date.

Not only is this code brief but it's also very readable.  It's pretty easy to extend.

Is this efficient?  Heck yes.  Given that the goal is to produce a true deep-copy clone, then you're going to have to walk the members of the source object graph.  With this approach, you can tweak exactly which child members to treat and how to manually handle custom types.

So there you go.  Two approaches.  Both efficient in my view.
AnswerKenntnis:
Lodash has a nice _.cloneDeep method: http://lodash.com/docs#cloneDeep

The usual _.clone method also accepts a second parameter to make a deep copy instead of the shallow one: http://lodash.com/docs#clone

_.clone(value [, deep=false, callback, thisArg])
AnswerKenntnis:
// obj target object, vals source object

var setVals = function (obj, vals) {
if (obj && vals) {
      for (var x in vals) {
        if (vals.hasOwnProperty(x)) {
          if (obj[x] && typeof vals[x] === 'object') {
            obj[x] = setVals(obj[x], vals[x]);
          } else {
            obj[x] = vals[x];
          }
        }
      }
    }
    return obj;
  };
AnswerKenntnis:
This is the fastest method I have created that doesn't use the prototype, so it will maintain hasOwnProperty in the new object. The solution is to iterate the top level properties of the original object, make 2 copies, delete each property from the original and then reset the original object and return the new copy. It only has to iterate as many times as top level properties. This saves all the if conditions to check if each property is a function/object/string etc, and doesn't have to iterate each descendant property. The only drawback is that the original object must be supplied with its original created namespace, in order to reset it.


copyDeleteAndReset:function(namespace,strObjName){
    var obj = namespace[strObjName],
    objNew = {},objOrig = {};
    for(i in obj){
        if(obj.hasOwnProperty(i)){
            objNew[i] = objOrig[i] = obj[i];
            delete obj[i];
        }
    }
    namespace[strObjName] = objOrig;
    return objNew;
}

var namespace = {};
namespace.objOrig = {
    '0':{
        innerObj:{a:0,b:1,c:2}
    }
}

var objNew = copyDeleteAndReset(namespace,'objOrig');
objNew['0'] = 'NEW VALUE';

console.log(objNew['0']) === 'NEW VALUE';
console.log(namespace.objOrig['0']) === innerObj:{a:0,b:1,c:2};
AnswerKenntnis:
for mootools in particular this severs the reference between the new obj and the old one:

var obj = {foo: 'bar'}; 
var bar = $unlink(obj);


you can also do

var obj = {foo: 'bar'};
var bar = $merge({}, $obj);


although $merge uses $unlink anyway.

p.s. for mootools 1.3 this becomes Object.clone
AnswerKenntnis:
In YUI you can do Deep object/array copy by 

//For Safe clone.. in case where you need to delete items on the cloned objects
clonedObj = Y.Clone(obj, true); 

//For Unsafe clone
clonedObj = Y.Clone(obj, false); 


More details
AnswerKenntnis:
I think that this is the best solution  if you want to generalize your object cloning algorithm.
It can be used with or without jQuery, although I recommend leaving jQuery's extend method out if you want you the cloned object to have the same "class" as the original one.

function clone(obj){
    if(typeof(obj) == 'function')//it's a simple function
        return obj;
    //of it's not an object (but could be an array...even if in javascript arrays are objects)
    if(typeof(obj) !=  'object' || obj.constructor.toString().indexOf('Array')!=-1)
        if(JSON != undefined)//if we have the JSON obj
            try{
                return JSON.parse(JSON.stringify(obj));
            }catch(err){
                return JSON.parse('"'+JSON.stringify(obj)+'"');
            }
        else
            try{
                return eval(uneval(obj));
            }catch(err){
                return eval('"'+uneval(obj)+'"');
            }
    // I used to rely on jQuery for this, but the "extend" function returns
    //an object similar to the one cloned,
    //but that was not an instance (instanceof) of the cloned class
    /*
    if(jQuery != undefined)//if we use the jQuery plugin
        return jQuery.extend(true,{},obj);
    else//we recursivley clone the object
    */
    return (function _clone(obj){
        if(obj == null || typeof(obj) != 'object')
            return obj;
        function temp () {};
        temp.prototype = obj;
        var F = new temp;
        for(var key in obj)
            F[key] = clone(obj[key]);
        return F;
    })(obj);            
}
QuestionKenntnis:
How do I remove local (untracked) files from my current Git branch?
qn_description:
How do you delete local files from your current branch?
AnswersKenntnis
AnswerKenntnis:
git clean -f


But beware... there's no going back. Use -n or --dry-run to preview the damage you'll do.

If you want to also remove directories, run git clean -f -d

If you just want to remove ignored files, run git clean -f -X 

If you want to remove ignored as well as non-ignored files, run git clean -f -x

Note the case difference on the X for the two latter commands.

If clean.requireForce is set to "true" (the default) in your configuration, then unless you specify -f nothing will actually happen, with a recent enough version of git.

Note that as of git-1.6.0, the dashed style of writing git commands (ie, git-clean instead of git clean) is obsoleted.

See the git-clean docs for more information.
AnswerKenntnis:
git-clean - Remove untracked files from the working tree
AnswerKenntnis:
git clean -f -d to be sure that also directories are gone!
you can check with git status if they are really gone.
AnswerKenntnis:
If untracked directory is a git repository of its own (e.g. submodule), you need to use -f twice:

git clean -d -f -f
AnswerKenntnis:
git-clean is what you are looking for. It is used to remove untracked files from the working tree.
AnswerKenntnis:
I am so surprised nobody mentioned this before, but I normally just type:

git clean -i


That stands for interactive and you will get a quick overview of what is going to be deleted and offer you the possibility to include/exclude the affected files. Overall, still faster than running the mandatory --dry-run before the real cleaning.
AnswerKenntnis:
If needed to remove untracked files from particular subdirectory,

git clean -f {dir_path}


And combined way to delete untracked dir/files and ignored files. 

git clean -fxd {dir_path}


after this you will have modified files only in git status.
AnswerKenntnis:
I like git stash save -u because it can undo them all with git stash pop

EDIT: 2013-02-12: use -u option instead of git add first.
AnswerKenntnis:
This is what I always use:

git clean -fdx


For a very large project you might want to run it a couple of time.
AnswerKenntnis:
This command is best for removing all types of untracked file

git clean -f -x
QuestionKenntnis:
PUT vs POST in REST
qn_description:
According to the HTTP/1.1 Spec: 


  The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line


In other words, POST is used to create.


  The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI."


That is, PUT is used to create or update.

So, which one should be used to create a resource? Or one needs to support both?
AnswersKenntnis
AnswerKenntnis:
Overall: 

Both PUT and POST can be used for creating. 

You have to ask "what are you performing the action to?" to distinguish what you should be using. If you want to use POST then you would do that to a list of questions. If you want to use PUT then you would do that to a particular question.

Great both can be used, so which one should I use in my RESTful design:

You do not need to support both PUT and POST.  

Which is used is left up to you.  But just remember to use the right one depending on what object you are referencing in the request. 

Some considerations:


Do you name your URL objects you create explicitly, or let the server decide? If you name them then use PUT.  If you let the server decide then use POST.
PUT is idempotent, so if you PUT an object twice, it has no effect.  This is a nice property, so I would use PUT when possible. 
You can update or create a resource with PUT with the same object URL
With POST you can have 2 requests coming in at the same time making modifications to a URL, and they may update different parts of the object. 


An example:

I wrote the following as part of another answer on SO regarding this:


  POST:
  
  Used to modify and update a resource

POST /questions/<existing_question> HTTP/1.1
Host: wahteverblahblah.com

  
  Note that the following is an error:

POST /questions/<new_question> HTTP/1.1
Host: wahteverblahblah.com

  
  If the URL is not yet created, you
  should not be using POST to create it
  while specifying the name.  This should
  result in a 'resource not found' error
  because <new_question> does not exist
  yet.  You should PUT the <new_question>
  resource on the server first.
  
  You could though do something like
  this to create a resources using POST:

POST /questions HTTP/1.1
Host: wahteverblahblah.com

  
  Note that in this case the resource
  name is not specified, the new objects
  URL path would be returned to you.
  
  PUT: 
  
  Used to create a resource, or
  overwrite it.  While you specify the
  resources new URL.
  
  For a new resource:

PUT /questions/<new_question> HTTP/1.1
Host: wahteverblahblah.com

  
  To overwrite an existing resource:

PUT /questions/<existing_question> HTTP/1.1
Host: wahteverblahblah.com
AnswerKenntnis:
You can find assertions on the web that say


POST should be used to create a resource, and PUT should be used to modify one
PUT should be used to create a resource, and POST should be used to modify one


Neither is quite right. 



Better is to choose between PUT and POST based on idempotence of the action. 

PUT implies putting a resource - completely replacing whatever is available at the given URL with a different thing.  By definition, a PUT is idempotent.  Do it as many times as you like, and the result is the same. x=5 is idempotent.  You can PUT a resource whether it previously exists, or not (eg, to Create, or to Update)! 

POST updates a resource, adds a subsidiary resource, or causes a change.  A POST is not idempotent, in the way that x++ is not idempotent. 



By this argument, PUT is for creating when you know the URL of the thing you will create. POST can be used to create when you know the URL of the "factory" or manager for the category of things you want to create. 

so: 

POST /expense-report


or:

PUT  /expense-report/10929
AnswerKenntnis:
POST to a URL creates a child resource at a server defined URL.
PUT to a URL creates/replaces the resource in it's entirety at the client defined URL. 
PATCH to a URL updates part of the resource at that client defined URL.


The relevant specification for PUT and POST is RFC 2616 ┬º9.5ff.

POST creates a child resource, so POST to /items creates a resources that lives under the /items resource. 
Eg. /items/1

PUT is for creating or replacing something with a URL known by the client. It's also idempotent as it replaces the resource in it's entirety.

Therefore PUT is only a candidate for CREATE where the client already knows the url before the resource is created. 
Eg. /blogs/nigel/entry/when_to_use_post_vs_put as the title is used as the resource key

The RFC reads like this:


  The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI,


Note: PUT has mostly been used to update resources (by replacing it in it's entirety), but recently there is movement towards using PATCH for updating existing resources, as PUT specifies that it replaces the whole resource. RFC 5789.
AnswerKenntnis:
I'd like to add my "pragmatic" advice.  Use PUT when you know the "id" by which the object you are saving can be retrieved.  Using PUT won't work too well if you need, say, a database generated id to be returned for you to do future lookups or updates.

So: To save an existing user, or one where the client generates the id and it's been verified that the id is unique:

PUT /user/12345 HTTP/1.1  <-- create the user providing the id 12345
Host: mydomain.com

GET /user/12345 HTTP/1.1  <-- return that user
Host: mydomain.com


Otherwise, use POST to initially create the object, and PUT to update the object:

POST /user HTTP/1.1   <--- create the user, server returns 12345
Host: mydomain.com

PUT /user/12345 HTTP/1.1  <--- update the user
Host: mydomain.com
AnswerKenntnis:
POST means "create new" as in "Here is the input for creating a user, create it for me".

PUT means "insert, replace if already exists" as in "Here is the data for user 5".

You POST to example.com/users since you don't know the URL of the user yet, you want the server to create it. 

You PUT to example.com/users/id since you want to replace/create a specific user.

POSTing twice with the same data means create two identical users. PUTing twice with the same data creates the user the first and updates him to the same state the second time (no changes). Since you end up with the same state after a PUT no matter how many times you perform it, it is said to be "equally potent" every time - idempotent. This is useful for automatically retrying requests. No more 'are you sure you want to resend' when you push the back button on the browser.

A general advice is to use POST when you need the server to be in control of URL generation of your resources. Use PUT otherwise.  Prefer PUT  over POST.
AnswerKenntnis:
Use POST to create, and PUT to update. That's how Rails is doing it, anyway.

PUT    /items/1      #=> update
POST   /items        #=> create
AnswerKenntnis:
REST is a very high-level concept. In fact, it doesn't even mention HTTP at all!

If you have any doubts about how to implement REST in HTTP, you can always take a look at the Atom Publication Protocol (AtomPub) specification. AtomPub is a standard for writing RESTful webservices with HTTP that was developed by many HTTP and REST luminaries, with some input from Roy Fielding, the inventor of REST and (co-)inventor of HTTP himself.

In fact, you might even be able to use AtomPub directly. While it came out of the blogging community, it is in no way restricted to blogging: it is a generic protocol for RESTfully interacting with arbitrary (nested) collections of arbitrary resources via HTTP. If you can represent your application as a nested collection of resources, then you can just use AtomPub and not worry about whether to use PUT or POST, what HTTP Status Codes to return and all those details.

This is what AtomPub has to say about resource creation:


  To add members to a Collection, clients send POST requests to the URI of the Collection.
AnswerKenntnis:
I like this advice, from RFC 2616's definition of PUT:


  The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource.


This jibes with the other advice here, that PUT is best applied to resources that already have a name, and POST is good for creating a new object under an existing resource (and letting the server name it).

I interpret this, and the idempotency requirements on PUT, to mean that:


POST is good for creating new objects under a collection (and create does not need to be idempotent)
PUT is good for updating existing objects (and update needs to be idempotent)
POST can also be used for non-idempotent updates to existing objects (especially, changing part of an object without specifying the whole thing -- if you think about it, creating a new member of a collection is actually a special case of this kind of update, from the collection's perspective)
PUT can also be used for create if and only if you allow the client to name the resource. But since REST clients aren't supposed to make assumptions about URL structure, this is less in the intended spirit of things.
AnswerKenntnis:
Summary:

Create:

Can be performed both with PUT or POST in the following way:


  PUT
  
  Create a new resource with newResourceId as the identifier, under the /resources URI, or collection.

PUT /resources/<newResourceId> HTTP/1.1 

  
  POST
  
  Create a new resource under the /resources URI, or collection. Usually the identifier is returned by the server.

POST /resources HTTP/1.1



Update:

Can only be performed with PUT in the following way:


  PUT
  
  Updates the resource with existingResourceId as the identifier, under the /resources URI, or collection.

PUT /resources/<existingResourceId> HTTP/1.1



Explanation:

When dealing with REST and URI as general, you have generic on the left and specific to the right. The generics are usually called collections and the more specific itens can be called resource. Note that a resource can contain a collection.


  Examples:
  
  <-- generic -- specific -->

URI: website.com/users/john
website.com  - whole site
users        - collection of users
john         - item of the collection, or a resource

URI:website.com/users/john/posts/23
website.com  - whole site
users        - collection of users
john         - item of the collection, or a resource
posts        - collection of posts from john
23           - post from john with identifier 23, also a resource



When you use POST you are always refering to a collection, so whenever you say:

POST /users HTTP/1.1


you are posting a new user to the users collection.

If you go on and try something like this:

POST /users/john HTTP/1.1


it will work, but semantically you are saying that you want to add a resource to the john collection under the users collection.

Once you are using PUT you are refering to a resource or single item, possibly inside a collection. So when you say:

PUT /users/john HTTP/1.1


you are telling to the server update, or create if it doesn't exist, the john resource under the users collection.

Spec:

Let me highlight some important parts of the spec:

POST


  The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line


Hence, creates a new resource on a collection.

PUT


  The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI."


Hence, create or update based on existence of the resource.

Reference:


HTTP/1.1 Spec
Wikipedia - REST
Uniform Resource Identifiers (URI): Generic Syntax and Semantics
AnswerKenntnis:
New answer (now that I understand REST better):

PUT is merely a statement of what content the service should, from now on, use to render representations of the resource identified by the client; POST is a statement of what content the service should, from now on, contain (possibly duplicated) but it's up to the server how to identify that content.

PUT x (if x identifies a resource): "Replace the content of the resource identified by x with my content."

PUT x (if x does not identify a resource): "Create a new resource containing my content and use x to identify it."

POST x: "Store my content and give me an identifier that I can use to identify a resource (old or new) containing said content (possibly mixed with other content). Said resource should be identical or subordinate to that which x identifies." "y's resource is subordinate to x's resource" is typically but not necessarily implemented by making y a subpath of x (e.g. x = /foo and y = /foo/bar) and modifying the representation(s) of x's resource to reflect the existence of a new resource, e.g. with a hyperlink to y's resource and some metadata. Only the latter is really essential to good design, as URLs are opaque in REST -- you're supposed to use hypermedia instead of client-side URL construction to traverse the service anyways.

In REST, there's no such thing as a resource containing "content". I refer as "content" to data that the service uses to render representations consistently. It typically consists of some related rows in a database or a file (e.g. an image file). It's up to the service to convert the user's content into something the service can use, e.g. converting a JSON payload into SQL statements.

Original answer (might be easier to read):

PUT /something (if /something already exists): "Take whatever you have at /something and replace it with what I give you."

PUT /something (if /something does not already exist): "Take what I give you and put it at /something."

POST /something: "Take what I give you and put it anywhere you want under /something as long as you give me its URL when you're done."
AnswerKenntnis:
Rails 4.0  will use the 'PATCH' method instead of PUT to do partial updates.

RFC 5789 says about PATCH (since 1995):


  A new method is necessary to improve interoperability and prevent
     errors.  The PUT method is already defined to overwrite a resource
     with a complete new body, and cannot be reused to do partial changes.
     Otherwise, proxies and caches, and even clients and servers, may get
     confused as to the result of the operation.  POST is already used but
     without broad interoperability (for one, there is no standard way to
     discover patch format support).  PATCH was mentioned in earlier HTTP
     specifications, but not completely defined.


"Edge Rails: PATCH is the new primary HTTP method for updates" explains it.
AnswerKenntnis:
POST is like posting a letter to a mailbox or posting an email to an email queue.
PUT is like when you put an object in a cubby hole or a place on a shelf (it has a known address).

With POST, you're posting to the address of the QUEUE or COLLECTION. With PUT, you're putting to the address of the ITEM.

PUT is idempotent. You can send the request 100 times and it will not matter. POST is not idempotent. If you send the request 100 times, you'll get 100 emails or 100 letters in your postal box.

A general rule: if you know the id or name of the item, use PUT. If you want the id or name of the item to be assigned by the receiving party, use POST.
AnswerKenntnis:
At the risk of restating what has already been said, it seems important to remember that PUT implies that the client controls what the URL is going to end up being, when creating a resource. So part of the choice between PUT and POST is going to be about how much you can trust the client to provide correct, normalized URLs that are coherent with whatever your URL scheme is. 

When you can't fully trust the client to do the right thing, it would be 
more appropriate to use POST to create a new item and then send the URL back to the client in the response.
AnswerKenntnis:
The most important consideration is reliability. If a POST message gets lost the state of the system is undefined. Automatic recovery is impossible. For PUT messages, the state is undefined only until the first successful retry.

For instance, it may not be a good idea to create credit card transactions with POST. 

If you happen to have auto generated  URI's on your resource you can still use PUT by passing a generated URI (pointing to an empty resource) to the client.

Some other considerations:


POST invalidates cached copies of the entire containing resource (better consistency)
PUT responses are not cacheable while POST ones are (Require Content-Location and expiration)
PUT is less supported by e.g. Java ME, older browsers, firewalls
AnswerKenntnis:
The decision of whether to use PUT or POST to create a resource on a server with an HTTP + REST API is based on who owns the URL structure. Having the client know, or participate in defining, the URL struct is an unnecessary coupling akin to the undesirable couplings that arose from SOA. Escaping types of couplings is the reason REST is so popular. Therefore, the proper method to use is POST. There are exceptions to this rule and they occur when the client wishes to retain control over the location structure of the resources it deploys. This is rare and likely means something else is wrong.

At this point some people will argue that if Restful-URL's are used, the client does knows the URL of the resource and therefore a PUT is acceptable. After all, this is why canonical, normalized, Ruby on Rails, Django URLs are important, look at the Twitter API ΓÇª blah blah blah. Those people need to understand there is no such thing as a Restful-URL and that Roy Fielding himself states that:


  A REST API must not define fixed resource names or hierarchies (an
  obvious coupling of client and server). Servers must have the freedom
  to control their own namespace. Instead, allow servers to instruct
  clients on how to construct appropriate URIs, such as is done in HTML
  forms and URI templates, by defining those instructions within media
  types and link relations. [Failure here implies that clients are
  assuming a resource structure due to out-of band information, such as
  a domain-specific standard, which is the data-oriented equivalent to
  RPC's functional coupling].
  
  http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven


The idea of a RESTful-URL is actually a violation of REST as the server is in charge of the URL structure and should be free to decide how to use it to avoid coupling. If this confuses you see a future article on the significance of self discovering on API design.

Using POST to create resources comes with a design consideration because POST is not idempotent. This means that repeating a POST several times does not guarantee the same behavior each time. This scares people into using PUT to create resources when they should not. They know it's wrong (POST is for CREATE) but they do it anyway because they don't know how to solve this problem.  This concern is demonstrated in the following situation:


The client POST a new resource to the server.
The server processes the request and sends a response.
The client never receives the response.
The server is unaware the client has not received the response.
The client does not have a URL for the resource (therefore PUT is not an option) and repeats the POST.
POST is not idempotent and the server ΓÇª


Step 6 is where people commonly get confused about what to do. However, there is no reason to create a kludge to solve this issue. Instead, HTTP can be used as specified in RFC 2616 and the server replies:


  10.4.10 409 Conflict
  
  The request could not be completed due to a conflict with the current
  state of the resource. This code is only allowed in situations where
  it is expected that the user might be able to resolve the conflict and
  resubmit the request. The response body SHOULD include enough
  
  information for the user to recognize the source of the conflict.
  Ideally, the response entity would include enough information for the
  user or user agent to fix the problem; however, that might not be
  possible and is not required.
  
  Conflicts are most likely to occur in response to a PUT request. For
  example, if versioning were being used and the entity being PUT
  included changes to a resource which conflict with those made by an
  earlier (third-party) request, the server might use the 409 response
  to indicate that it canΓÇÖt complete the request. In this case, the
  response entity would likely contain a list of the differences between
  the two versions in a format defined by the response Content-Type.


Replying with a status code of 409 Conflict is the correct recourse because:


Performing a POST of data which has an ID which matches a resource already in the system is ΓÇ£a conflict with the current state of the resource.ΓÇ¥
Since the important part is for the client to understand the server has the resource and to take appropriate action. This is a ΓÇ£situation(s) where it is expected that the user might be able to resolve the conflict and resubmit the request.ΓÇ¥
A response which contains the URL of the resource with the conflicting ID and the appropriate preconditions for the resource would provide ΓÇ£enough information for the user or user agent to fix the problemΓÇ¥ which is the ideal case per RFC 2616.


Update based on release of RFC 7231 to Replace 2616

RFC 7231 is designed to replace 2616 and in Section 4.3.3 describes the follow possible response for a POST


  If the result of processing a POST would be equivalent to a
  representation of an existing resource, an origin server MAY redirect
  the user agent to that resource by sending a 303 (See Other) response
  with the existing resource's identifier in the Location field.  This
  has the benefits of providing the user agent a resource identifier
  and transferring the representation via a method more amenable to
  shared caching, though at the cost of an extra request if the user
  agent does not already have the representation cached.


It now may be be tempting to simply return a 303 in the event that a POST is repeated. However, the opposite is true. Returning a 303 would only make sense if multiple create requests (creating different resources) return the same content. An example would be a "thank you for submitting your request message" that the client need not re-download each time. RFC 7231 still maintains in section 4.2.2 that POST is not be Idempotent and continues to maintain that POST should be used for create.

For more information about this read this article.
AnswerKenntnis:
The semantics are supposed be different, in that "PUT", like "GET" is supposed to be idempotent -- meaning, you can the same exact PUT request multiple times and the result will be as if you executed it only once. 

I will describe the conventions which I think are most widely used and are most useful:

When you PUT a resource at a particular URL what happens is that it should get saved at that URL, or something along those lines.

When you POST to a resource at a particular URL, often you are posting a related piece of information to that URL. This implies that the resource at the URL already exists.

For example, when you want to create a new stream, you can PUT it to some URL. But when you want to POST a message to an existing stream, you POST to its URL.

As for modifying the properties of the stream, you can do that with either PUT or POST. Basically, only use "PUT" when the operation is idempotent - otherwise use POST.

Note, however, that not all modern browsers support HTTP verbs other than GET or POST.
AnswerKenntnis:
There seems to always be some confusion as to when to use the HTTP POST versus the HTTP PUT method for REST services. Most developers will try to associate CRUD operations directly to HTTP methods. I will argue that this is not correct and one can not simply associate the CRUD concepts to the HTTP methods. That is:

Create => HTTP PUT
Retrieve => HTTP GET
Update => HTTP POST
Delete => HTTP DELETE


It is true that the R(etrieve) and D(elete) of the CRUD operations can be mapped directly to the HTTP methods GET and DELETE respectively. However, the confusion lies in the C(reate) and U(update) operations. In some cases, one can use the PUT for a create while in other cases a POST will be required. The ambiguity lies in the definition of an HTTP PUT method versus an HTTP POST method.

According to the HTTP 1.1 specifications the GET, HEAD, DELETE, and PUT methods must be idempotent, and the POST method is not idempotent. That is to say that an operation is idempotent if it can be performed on a resource once or many times and always return the same state of that resource. Whereas a non idempotent operation can return a modified state of the resource from one request to another. Hence, in a non idempotent operation, there is no guarantee that one will receive the same state of a resource.

Based on the above idempotent definition, my take on using the HTTP PUT method versus using the HTTP POST method for REST services is:
Use the HTTP PUT method when:

The client includes all aspect of the resource including the unique identifier to uniquely identify the resource. Example: creating a new employee.
The client provides all the information for a resource to be able to modify that resource.This implies that the server side does not update any aspect of the resource (such as an update date).


In both cases, these operations can be performed multiple times with the same results. That is the resource will not be changed by requesting the operation more than once. Hence, a true idempotent operation.
Use the HTTP POST method when:

The server will provide some information concerning the newly created resource. For example, take a logging system. A new entry in the log will most likely have a numbering scheme which is determined on the server side. Upon creating a new log entry, the new sequence number will be determined by the server and not by the client.
On a modification of a resource, the server will provide such information as a resource state or an update date. Again in this case not all information was provided by the client and the resource will be changing from one modification request to the next. Hence a non idempotent operation.


Conclusion

Do not directly correlate and map CRUD operations to HTTP methods for REST services. The use of an HTTP PUT method versus an HTTP POST method should be based on the idempotent aspect of that operation. That is, if the operation is idempotent, then use the HTTP PUT method. If the operation is non idempotent, then use the HTTP POST method.
AnswerKenntnis:
There are two possible scenarios for resource creation:


Adding a new item to a collection-resource by creating the new item with or without the help of the collection-resource.

For example: 


collection-resource: /users
item-resource: /users/1
create item-resource:

PUT /users/1 {...} -> 201 -
POST /users {...} -> 201 {id:1}

edit item-resource:

PUT /users/1 {...} -> 200 -



In this case if the new item is editable, then using PUT for create and edit item-resources is dangerous, because users can override accidentally already existing item-resources by creation simply by choosing the same id as the already existing resource has. There is no way to handle conflicts on server side if the same http method does two different things and the web service does not know which one the user meant. So by editable item-resources POST is the good solution for create and PUT is the good solution for edit. If the resource is not editable, then you can choose freely whether you use PUT or POST for its creation. I don't recommend to let users generate resource names, so I am against PUT by creation, but that's my opinion.
Adding a sub-resource to a sup-resource by editing the sup-resource.

For example:


sup-resource: /users/1
sub-resource: /users/1/address
create sub-resource:

POST /users {address: {locality: "New York"}} -> 201 {id:1}
PUT /users/1 {address: {...}} -> 200 -
POST /users/1/address {...} -> 201 -
PUT /users/1/address {...} -> 201 -

edit sub-resource:

PUT /users/1 {address: {...}} -> 200 -
PUT /users/1/address {...} -> 200 -



In this case the sub-resource is completely overridden by every modification of its sup-resource. So in this case it is safe to create the sub-resource by editing the sup-resource with a PUT request.


Summary:

My opinion is, that a single http method should not have two different meanings (create and edit) by the same resource. If the request body contains nested resources, then it can have different meanings by different resources (for example edit the sup-resource and create a sub-resource). Maybe I should mention the single responsibility principle, but :D
AnswerKenntnis:
I'm gonna land with the following:

PUT refers to a resource, identified by the URI.  In this case, you are updating it.  It is the part of the three verbs referring to resources -- delete and get being the other two.

POST is basically a free form message, with its meaning being defined 'out of band'.  If the message can be interpreted as adding a resource to a directory, that would be OK, but basically you need to understand the message you are sending (posting) to know what will happen with the resource.



Because put and get and delete refer to a resource, they are also by definition idempotent.

Post can perform the other three functions, but then the semantics of the request will be lost on the intermediaries such as caches and proxies.  This also applies to providing security on the resource, since a post's URI doesn't necessarily indicate the resource it is applying to (it can though).

A put doesn't need to be a create; the service could error if the resource isn't already created but otherwise update it.  Or vice versa -- it may create the resource, but not allow updates.  The only thing required about PUT is that it points to a specific resource, and its payload is the representation of that resource.  A successful put means (barring interference) that a get would retrieve the same resource.
AnswerKenntnis:
Here's a simple rule:

PUT to a URL should be used to update or create the resource that can be located at that URL.

POST to a URL should be used to update or create a resource which is located at some other ("subordinate") URL, or is not locatable via http.
AnswerKenntnis:
Post:Use for create New resource.It's like insert (sql statement) with auto-incremented ID .In Response part contain new generated Id.

Post is also use for Update record.

Put:Use for create new Resource but here i know the the identity key it's like Insert (sql statement) where i know in advance the identity key
.In response part it send nothing .

Put :is also use for Update resource
AnswerKenntnis:
For both (create and update) I would use POST since you are changing something in the data store (they are not idempotent function calls). For me this link was very helpful: Form methods
QuestionKenntnis:
Check checkbox checked property using jQuery
qn_description:
I need to check the checked property of a checkbox and perform an action based on the checked property using jQuery.

For example, if the age checkbox is checked, then I need to show a textbox to enter age, else hide the textbox.

But the following code returns false by default:

if($('#isAgeSelected').attr('checked')) {
    $("#txtAge").show();
} else {
    $("#txtAge").hide();
}


How do I successfully query the checked property?
AnswersKenntnis
AnswerKenntnis:
There's a much prettier way to do this, using toggle:

$('#isAgeSelected').click(function () {
    $("#txtAge").toggle(this.checked);
});

<input type="checkbox" id="isAgeSelected"/>
<div id="txtAge" style="display:none">Age is something</div>ΓÇï


Fiddle Demo
AnswerKenntnis:
Use jQuery's is() function:

if($("#isAgeSelected").is(':checked'))
    $("#txtAge").show();  // checked
else
    $("#txtAge").hide();  // unchecked
AnswerKenntnis:
Using jQuery > 1.6 

<input type="checkbox" value="1" name="checkMeOut" id="checkMeOut" checked="checked" />

// traditional attr
$('#checkMeOut').attr('checked'); // "checked"
// new property method
$('#checkMeOut').prop('checked'); // true


Using the new property method:

if($('#checkMeOut').prop('checked')) {
    // something when checked
} else {
    // something else when not
}
AnswerKenntnis:
I am using this and this is working absolutely fine:

$("#checkkBoxId").attr("checked") ? alert("Checked") : alert("Unchecked");


Note: If the checkbox is checked it will return true otherwise undefined, so better check for the "TRUE" value.
AnswerKenntnis:
Since jQuery 1.6, The behavior of jQuery.attr() has changed and users are encouraged  not to use it to retrieve an element's checked state. Instead, you should use jQuery.prop():

$("#txtAge").toggle(
    $("#isAgeSelected").prop("checked") // for checked attribute it returns true/false;
                                        // return value changes with checkbox state
);                           


Two other possibilities are:

$("#txtAge").get(0).checked
$("#txtAge").is(":checked")
AnswerKenntnis:
There are several ways of doing it:
.is()

$("#checkkBoxId").is(':checked') 


or you can try:
.prop()

$("#checkkBoxId").prop('checked')
AnswerKenntnis:
This worked for me:

$get("isAgeSelected ").checked == true


Where isAgeSelected is the id of the control.

Also, @karim79's answer works fine. I am not sure what I missed at the time I tested it.

Note, this is answer uses Microsoft Ajax, not jQuery
AnswerKenntnis:
I decided to post an answer on how to do that exact same thing without jQuery. Just because I'm a rebel.

var ageCheckbox = document.getElementById('isAgeSelected');
var ageInput = document.getElementById('txtAge');

// Just because of IE <333
ageCheckbox.onchange = function() {
    // Check if the checkbox is checked, and show/hide the text field.
    ageInput.hidden = this.checked ? false : true;
};


First you get both elements by their ID. Then you assign the checkboxe's onchange event a function that checks whether the checkbox got checked and sets the hidden property of the age text field appropriately. In that example using the ternary operator.

Here is a fiddle for you to test it.

Addendum

If cross-browser compatibility is an issue then I propose to set the CSS display property to none and inline.

elem.style.display = this.checked ? 'inline' : 'none';


Slower but cross-browser compatible.
AnswerKenntnis:
Using the Click event handler for the checkbox property is unreliable, as the checked property can change during the execution of the event handler itself! Ideally, you'd want to put your code into a change event handler such as it is fired every time the value of the check box is changed(independent of how it's done so).

$('#isAgeSelected').bind('change', function () {

   if ($(this).is(':checked'))
     $("#txtAge").show();
   else
     $("#txtAge").hide();

});
AnswerKenntnis:
$(selector).attr('checked') !== undefined


This returns true if the input is checked and false if it is not.
AnswerKenntnis:
Please consider that the best cross-browser & cross-HTML version way to check if a check box is checked or not is:

$("#checkkBoxId").is(':checked') 


or

$("#checkkBoxId").prop('checked') 


Other ways may working on some browsers but not all browsers and also HTML version is effecting on their behavior.

So the only 100% working way is using .is(':checked') or .prop('checked')
AnswerKenntnis:
I believe you could do this:

if ($('#isAgeSelected :checked').size() > 0)
{
    $("#txtAge").show(); 
} else { 
    $("#txtAge").hide();
}
AnswerKenntnis:
I ran in to the exact same issue. I have an ASP.NET checkbox

<asp:CheckBox ID="chkBox1" CssClass='cssChkBox1' runat="server" />


In the jQuery code I used the following selector to check if the checkbox was checked or not, and it seems to work like a charm.

if ($("'.cssChkBox1 input[type=checkbox]'").is(':checked'))
{ ... } else { ... }


I'm sure you can also use the ID instead of the CssClass,

if ($("'#cssChkBox1 input[type=checkbox]'").is(':checked'))
{ ... } else { ... }


I hope this helps you.
AnswerKenntnis:
Top answer didn't do it for me, this did though:

<script type="text/javascript">
$(document).ready(function(){

    $("#li_13").click(function(){
        if($("#agree").attr('checked')){
            $("#saveForm").fadeIn();
        }

        else
        {
            $("#saveForm").fadeOut();
        }
    });
    });

</script>


UPDATE 20121213: Basically when the element #li_13 is clicked, it checks if the element #agree (which is the checkbox) is checked by using the .attr('checked') function, if it is then fadeIn the #saveForm element, and if not fadeOut the saveForm element.
AnswerKenntnis:
My way of doing this is:

if ( $("#checkbox:checked").length ) {

    alert("checkbox is checked");

} else {

    alert("checkbox is not checked");

}
AnswerKenntnis:
if($("#checkkBoxId").is(':checked')){
  alert("Checked=true");
}


or

if($("#checkkBoxId").attr('checked') == true){
  alert("checked=true");
}
AnswerKenntnis:
this works for me,

/* isAgeSelected being id for checkbox */

$("#isAgeSelected").click(function(){
  $(this).is(':checked') ? $("#txtAge").show() : $("#txtAge").hide();
});
AnswerKenntnis:
I verified in Firefox 9.0.1 that the following works for catching the state of a checkbox post change:

$("#mycheckbox").change(function() {
    var value = $(this).prop("checked") ? 'true' : 'false';                     
    alert(value);
});
AnswerKenntnis:
1) If your HTML markup is: 

<input type="checkbox"  />


attr used:  

$(element).attr("checked");//will give you undefined as initial value of checkbox is not set


If prop is used :

$(element).prop("checked");//will give you false whether or not initial value is set 


2) If your HTML markup is: 

 <input type="checkbox"  checked="checked" />// may be like this also  checked="true"


attr used:

$(element).attr("checked")//will return checked whether it is checked="true"


prop used:

$(element).prop("checked")//will return true whether checked="checked"
AnswerKenntnis:
If you are using an updated version of jquery, you must go for .prop method to resolve your issue:

$('#isAgeSelected').prop('checked') will return true if checked and false if unchecked. I confirmed it and I came across this issue earlier. $('#isAgeSelected').attr('checked') and $('#isAgeSelected').is('checked') is returning undefined which is not a worthy answer for the situation. So do as given below.

if($('#isAgeSelected').prop('checked')) {
    $("#txtAge").show();
} else {
    $("#txtAge").hide();
}


Hope it helps someone.. :)- Thanks.
AnswerKenntnis:
check if not checked

if(!$("#your_id").is(':checked') ){
  //your code
}
AnswerKenntnis:
I was having the same problem and none of the posted solutions seemed to work and then I found out that it's because ASP.NET renders the CheckBox control as a SPAN with INPUT inside, so the CheckBox ID is actually an ID of a SPAN, not an INPUT, so you should use:

$('#isAgeSelected input')


rather than

$('#isAgeSelected')


and then all methods listed above should work.
AnswerKenntnis:
Here's an example that includes initialising the show/hide to match the state of the checkbox when the page loads; taking account of the fact that firefox remembers the state of checkboxes when you refresh the page, but won't remember the state of the shown/hidden elements.

$(function() {
    // initialise visibility when page is loaded
    $('tr.invoiceItemRow').toggle($('#showInvoiceItems').attr('checked'));
    // attach click handler to checkbox
    $('#showInvoiceItems').click(function(){ $('tr.invoiceItemRow').toggle(this.checked);})
});


(with help from other answers on this question)
AnswerKenntnis:
I am using this :

 <input type="checkbox" id="isAgeSelected" value="1" /> <br />
 <input type="textbox" id="txtAge" />

 $("#isAgeSelected").is(':checked') ? $("#txtAge").show() : $("#txtAge").hide();
AnswerKenntnis:
Im sure its not some revalation but didnt see it all in one example:
Selector for all checked checkboxes(on the page):

$('input[type=checkbox]:checked')
AnswerKenntnis:
jQuery 1.6+

$('#isAgeSelected').prop('checked')


jQuery 1.5 and below

$('#isAgeSelected').attr('checked')


Any version of jQuery

// Assuming an event handler on a checkbox
if (this.checked)


all credit to: http://stackoverflow.com/a/426276/443427
AnswerKenntnis:
$(this).toggle($("input:checkbox", $(this))[0].checked);


When you are selecting out of context, remember you need the [0] to access the checkbox
AnswerKenntnis:
Include jQuery from local file system. I used Google CDN there are also many CDNs to choose.

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>


The code will execute as soon as checkbox inside mycheck class is clicked. If the current clicked checkbox is checked then it will disable all other and enable the current one. If current one is unchecked, it will again enable all checkboxes for re checking.

<script type="text/javascript">
$(document).ready(function() {

    var checkbox_selector = '.mycheck input[type=checkbox]';

    $(checkbox_selector).click(function() {
        if ($($(this)).is(':checked')) {
            // disable all checkbox
            $(checkbox_selector).attr('disabled', 'disabled');
            // enable current one
            $($(this)).removeAttr('disabled');
        } else {
            // if unchecked open all checkbox
            $(checkbox_selector).removeAttr('disabled');
        }
    });
});
</script>


Simple form to test

<form method="post" action="">
    <div class="mycheck">
        <input type="checkbox" value="1" /> Television 
        <input type="checkbox" value="2" /> Computer 
        <input type="checkbox" value="3" /> Laptop 
        <input type="checkbox" value="4" /> Camera 
        <input type="checkbox" value="5" /> Music Systems 
    </div>
</form>


Output Screen
AnswerKenntnis:
The checked attribute of an input type="checkbox" is mapped with the defaultChecked property, not with the checked property. 

So when doing something in a page when a checkbox is checked on uncheked, use the prop() method instead. It fetches the property value and changes as the state of the checkbox changes. 

Using attr() or getAttribute(in pure js) in these cases are not the proper way of doing things. 

if elem is the concerned checkbox then do something like this to fetch the value:

elem.checked


or

$(elem).prop('checked')
AnswerKenntnis:
if( undefined == $('#isAgeSelected').attr('checked') ) {
    $("#txtAge").hide();
} else {
    $("#txtAge").show();
}
QuestionKenntnis:
Java += operator
qn_description:
Until today I thought that for example:

i += j;


is just a shortcut for:

i = i + j;


But what if we try this:

int i = 5;
long j = 8;


Then i = i + j; will not compile but i += j; will compile fine.

Does it mean that in fact i += j; is a shortcut for something like this
i = (type of i) (i + j)?

I've tried googling for it but couldn't find anything relevant.
AnswersKenntnis
AnswerKenntnis:
As always with these questions, the JLS holds the answer. In this case ┬º15.26.2 Compound Assignment Operators. An extract:


  A compound assignment expression of the form E1 op= E2 is equivalent to E1 = (T)((E1) op (E2)), where T is the type of E1, except that E1 is evaluated only once.


An example cited from ┬º15.26.2


  [...] the following code is correct:

short x = 3;
x += 4.6;

  
  and results in x having the value 7 because it is equivalent to:

short x = 3;
x = (short)(x + 4.6);



In other words, your assumption is correct.
AnswerKenntnis:
A good example of this casting is using *= or /=

byte b = 10;
b *= 5.7;
System.out.println(b); // prints 57


or

byte b = 100;
b /= 2.5;
System.out.println(b); // prints 40


or

char ch = '0';
ch *= 1.1;
System.out.println(ch); // prints '4'


or

char ch = 'A';
ch *= 1.5;
System.out.println(ch); // prints 'a'
AnswerKenntnis:
Very good question. The Java Language specification confirms your suggestion.

For example, the following code is correct:

 short x = 3;
 x += 4.6;


and results in x having the value 7 because it is equivalent to:

 short x = 3;
 x = (short)(x + 4.6);
AnswerKenntnis:
Yes,

basically when we are writing 

i += l; 


compiler is converting this to 

i = (int)(i + l);


I just checked the .class file code.

Really a good thing to know
AnswerKenntnis:
you need to cast from long to int explicitly in case of i = i + l  then it will compile and give correct output. like 

i = i + (int)l;


or

i = (int)((long)i + l); // this is what happens in case of += , dont need (long) casting since upper casting is done implicitly.


but in case of += it just works fine because the operator implicitly does the type casting from type of right variable to type of left variable so need not cast explicitly.
AnswerKenntnis:
The problem here is of type casting.

When you add int and long, 


The int object is casted to long & both are added and you get long object.
but long object cannot be implicitly casted to int. So, you have to that explicitly.


But += is coded in such a way that it does type casting. i=(int)(i+m)
AnswerKenntnis:
In Java type conversions are performed automatically when the type of the expression on the right hand side of an assignment operation can be safely promoted to the type of the variable on the left hand side of the assignment. Thus we can safely assign:  

 byte -> short -> int ->  float -> long -> double.   

The same will not work the other way round. For example we cannot automatically convert a long to an int because the first requires more storage than the second and consequently information may be lost. To force such a conversion we must carry out an explicit conversion.
Type - Conversion
QuestionKenntnis:
CSS rule to disable text selection highlighting
qn_description:
For anchors that act like buttons (for example, Questions, Tags, Users, etc. at the top of the Stack Overflow page) or tabs, is there a CSS standard way to disable the highlighting effect if the user accidentally selects the text?

I realize this could be done with JavaScript, and a little googling yielded the Mozilla-only -moz-user-select option.

Is there a standard-compliant way to accomplish this with CSS, and if not, what is the "best practice" approach?
AnswersKenntnis
AnswerKenntnis:
All of the correct CSS variations are:

-webkit-touch-callout: none;
-webkit-user-select: none;
-khtml-user-select: none;
-moz-user-select: none;
-ms-user-select: none;
user-select: none;




More information can be found here.
AnswerKenntnis:
In most browsers, this can be achieved using proprietary variations on the proposed-but-now-defunct CSS3 user-select property:

*.unselectable {
   -moz-user-select: none;
   -khtml-user-select: none;
   -webkit-user-select: none;

   /*
     Introduced in IE 10.
     See http://ie.microsoft.com/testdrive/HTML5/msUserSelect/
   */
   -ms-user-select: none;
   user-select: none;
}


For IE < 10 and Opera, you will need to use the unselectable attribute of the element you wish to be unselectable. You can set this using an attribute in HTML:

<div id="foo" unselectable="on" class="unselectable">...</div>


Sadly this property isn't inherited, meaning you have to put an attribute in the start tag of every element inside the <div>. If this is a problem, you could instead use JavaScript to do this recursively for an element's descendants:

function makeUnselectable(node) {
    if (node.nodeType == 1) {
        node.setAttribute("unselectable", "on");
    }
    var child = node.firstChild;
    while (child) {
        makeUnselectable(child);
        child = child.nextSibling;
    }
}

makeUnselectable(document.getElementById("foo"));




Update 30 April 2014: This tree traversal needs to be re-run whenever a new element is added to the tree, but it seems from a comment by @Han that it is possible to avoid this by
adding a mousedown event handler that sets unselectable on the target of the event. See http://jsbin.com/yagekiji/1 for details.



This still doesn't cover all possibilities. While it is impossible to initiate selections in unselectable elements, in some browsers (IE and Firefox, for example) it's still impossible to prevent selections that start before and end after the unselectable element without making the whole document unselectable.
AnswerKenntnis:
A JavaScript solution for IE is

onselectstart="return false;"
AnswerKenntnis:
If you want to disable text selection on everything except on <p> elements, you can do this in CSS (watch out for the -moz-none which allows override in sub-elements, which is allowed in other browsers with none):

* {
    -webkit-user-select: none;
    -khtml-user-select: none;
    -moz-user-select: -moz-none;
    -o-user-select: none;
    user-select: none;
}

p {
    -webkit-user-select: text;
    -khtml-user-select: text;
    -moz-user-select: text;
    -o-user-select: text;
    user-select: text;
}
AnswerKenntnis:
You can do so in Firefox and Safari (Chrome also?)

::selection { background: transparent; }
::-moz-selection { background: transparent; }
AnswerKenntnis:
Until CSS 3's user-select property becomes available only Gecko based browsers will support the -moz-user-select property you already found.

This of course is not supported in browsers that do not use the Gecko rendering engine.

There is no "standards" compliant quick and easy way to do it; using JavaScript is an option.

The real question is, why do you want users to not be able to highlight and presumably copy and paste certain elements? I have not come across a single time that I wanted to not let users highlight a certain portion of my website. Several of my friends, after spending many hours reading and writing code will use the highlight feature as a way to remember where on the page they were, or providing a marker so that their eyes know where to look next.

The only place I could see this being useful is if you have buttons for forms that should not be copy and pasted if a user copy and pasted the website.
AnswerKenntnis:
Workaround for WebKit:

/* Disable tap highlighting */
-webkit-tap-highlight-color: rgba(0,0,0,0);


I found it in a CardFlip example.
AnswerKenntnis:
I like the hybrid CSS + jQuery solution.

To make all elements inside <div class="draggable"></div> unselectable, use this CSS:

.draggable {
    -webkit-user-select: none;
    -khtml-user-select: none;
    -moz-user-select: none;
    -o-user-select: none;
    -ms-user-select: none;
    user-select: none;
}

.draggable input { 
    -webkit-user-select: text; 
    -khtml-user-select: text; 
    -moz-user-select: text; 
    -o-user-select: text; 
    user-select: text; 
 }


And then, if you're using jQuery, add this inside a $(document).ready() block:

if (($.browser.msie && $.browser.version < 10) || $.browser.opera) $('.draggable').find(':not(input)').attr('unselectable', 'on');


I figure you still want any input elements to be interactable, hence the :not() pseudo-selector. You could use '*' instead if you don't care.

Caveat: IE9 may not need this extra jQuery piece, so you may want to add a version check in there.
AnswerKenntnis:
Working 

css

 -khtml-user-select: none;
 -moz-user-select: none;
 -ms-user-select: none;
  user-select: none;
 -webkit-touch-callout: none;
 -webkit-user-select: none;


This should be working but won't work for the old browsers , there is a browser compatibility issue
AnswerKenntnis:
-webkit-user-select: none;
-khtml-user-select: none;
-moz-user-select: none;
-o-user-select: none;
user-select: none;

*.unselectable {
   -moz-user-select: -moz-none;
   -khtml-user-select: none;
   -webkit-user-select: none;
   user-select: none;
}


<div id="foo" unselectable="on" class="unselectable">...</div>


function makeUnselectable(node) {
    if (node.nodeType == 1) {
        node.unselectable = true;
    }
    var child = node.firstChild;
    while (child) {
        makeUnselectable(child);
        child = child.nextSibling;
    }
}

makeUnselectable(document.getElementById("foo"));


-webkit-user-select:none;
-moz-user-select:none;


onselectstart="return false;"


::selection { background: transparent; }
::-moz-selection { background: transparent; }

* {
-webkit-user-select: none;
-khtml-user-select: none;
-moz-user-select: -moz-none;
-o-user-select: none;
user-select: none;
}

p {
-webkit-user-select: text;
-khtml-user-select: text;
-moz-user-select: text;
-o-user-select: text;
user-select: text;
}


<div class="draggable"></div>


.draggable {
    -webkit-user-select: none;
    -khtml-user-select: none;
    -moz-user-select: none;
    -o-user-select: none;
    user-select: none;
}

.draggable input { 
    -webkit-user-select: text; 
    -khtml-user-select: text; 
    -moz-user-select: text; 
    -o-user-select: text; 
    user-select: text; 
 }


if ($.browser.msie) $('.draggable').find(':not(input)').attr('unselectable', 'on');
AnswerKenntnis:
Aside from the Mozilla-only property, no, there is no way to disable text selection with just standard CSS (as of now).

If you notice, Stack Overflow doesn't disable text selection for their navigation buttons, and I would recommend against doing so in most cases, since it modifies normal selection behavior and makes it conflict with a user's expectations.
AnswerKenntnis:
If you are using LESS and bootstrap you could write

.user-select(none);
AnswerKenntnis:
Add this to the first div in which you want to disable the selection for text:

onmousedown='return false;' 
onselectstart='return false;'
AnswerKenntnis:
For Internet Explorer in addition, you need to add pseudo class focus (.ClassName:focus) and outline-style:none.

.ClassName,
.ClassName:focus{
-webkit-touch-callout: none;
-webkit-user-select: none;
-khtml-user-select: none;
-moz-user-select: none;
-ms-user-select: none;
user-select: none;
outline-style:none;/*IE*/
}
AnswerKenntnis:
This is not CSS, but it is worth a mention:

jQuery UI Disable Selection:

$("your.selector").disableSelection();
AnswerKenntnis:
.hidden:after {
  content: attr(data-txt);
}


And in HTML:

<p class="hidden" data-txt="Some text you don't want to be selected"></p>


It's not the best way, though.
AnswerKenntnis:
This will be useful if color selection is also not needed.

::-moz-selection { background:none; color:none; }
::selection { background:none; color:none; }


..all other browser fixes. will work in ie9 +
AnswerKenntnis:
For those who have trouble achieving the same in android browser with the touch event,

html,body{
-webkit-touch-callout: none;
-webkit-user-select: none;
-webkit-tap-highlight-color: rgba(0,0,0,0);
-webkit-tap-highlight-color: transparent;
}
AnswerKenntnis:
Though this pseudo-element was in drafts of CSS Selectors Level 3, it was removed during the Candidate Recommendation phase,  as it appeared that its behavior was under-specified, especially with nested elements, and interoperability wasn't achieved.

its being discussed here
http://lists.w3.org/Archives/Public/www-style/2008Oct/0268.html

Despite it is being implemented in browser. You can make an illusion of text not being selected is to use the same color & background color on selection as of the tab design (in you case).

/* Normal CSS Design */

p { color: white;  background: black; }


/* on selection */

p::-moz-selection { color: white;  background: black; }
p::selection      { color: white;  background: black; }


Disallowing users to select the text will raise usability issues.
AnswerKenntnis:
this works in all browsers.

::selection{ background-color: transparent;}
::moz-selection{ background-color: transparent;}
::webkit-selection{ background-color: transparent;}


simply add your desired elements/ids in front of the selectors separated by commas without spaces, like so:

h1::selection,h2::selection,h3::selection,p::selection{ background-color: transparent;}
h1::moz-selection,h2::moz-selection,h3::moz-selection,p::moz-selection{ background-color: transparent;}
h1::webkit-selection,h2::webkit-selection,h3::webkit-selection,p::webkit-selection{ background-color: transparent;}
AnswerKenntnis:
Check my solution w/o javascript:

jsfiddle

html:

<ul>

    <li><a id="id1" href="www.w1.com"></a>
    <li><a id="id2" href="www.w2.com"></a>
    <li><a id="id3" href="www.w3.com"></a>


</ul>


css:

li:hover
{
    background-color:silver;
}

#id1:before
{content:"File";} 
#id2:before
{content:"Edit";} 
#id3:before
{content:"View";}
QuestionKenntnis:
How do I check a checkbox with jQuery?
qn_description:
I want to do something like this

$(".myCheckBox").checked(true);


or 

$(".myCheckBox").selected(true);


I wish to set the value.

Is such a thing built into jQuery?
AnswersKenntnis
AnswerKenntnis:
It certainly is in

jQuery 1.6+

Use the new .prop() function:

$('.myCheckbox').prop('checked', true);
$('.myCheckbox').prop('checked', false);


jQuery 1.5 and below

The .prop() function is not available, so you need to use .attr().

To check the checkbox (by setting the value of the checked attribute) do

$('.myCheckbox').attr('checked','checked');


and for un-checking (by removing the attribute entirely) do

$('.myCheckbox').removeAttr('checked');


Any version of jQuery

If you're working with just one element, it will always be fastest to use DOMElement.checked = true. The benefit to using the .prop() and .attr() functions is that they will operate on all matched elements.

// Assuming an event handler on a checkbox
if (this.checked)
AnswerKenntnis:
$(".myCheckbox").attr('checked', true); // deprecated
$(".myCheckbox").prop('checked', true);


and if you want to check if a checkbox is checked or not:

$('.myCheckbox').is(':checked');
AnswerKenntnis:
This is the correct way of checking and unchecking checkboxes with jQuery, as it is cross-platform standard, and will allow form reposts.

$('.myCheckBox').each(function(){ this.checked = true; });

$('.myCheckBox').each(function(){ this.checked = false; });


By doing this, you are using JavaScript standards for checking and unchecking checkboxes, so any browser that properly implements the "checked" property of the checkbox element will run this code flawlessly. This should be all major browsers, but I am unable to test previous to Internet Explorer 9.

The Problem (jQuery 1.6):

Once a user clicks on a checkbox, that checkbox stops responding to the "checked" attribute changes.

Here is an example of the checkbox attribute failing to do the job after someone has clicked the checkbox (this happens in Chrome).

http://jsfiddle.net/xixonia/zgcrB/

The Solution:

By using JavaScript's "checked" property on the DOM elements, we are able to solve the problem directly, instead of trying to manipulate the DOM into doing what we want it to do.

http://jsfiddle.net/xixonia/WnbNC/

This plugin will alter the checked property of any elements selected by jQuery, and successfully check and uncheck checkboxes under all circumstances. So, while this may seem like an over-bearing solution, it will make your site's user experience better, and help prevent user frustration.

(function( $ ) {
    $.fn.checked = function(value) {
        if(value === true || value === false) {
            // Set the value of the checkbox
            $(this).each(function(){ this.checked = value; });
        } 
        else if(value === undefined || value === 'toggle') {
            // Toggle the checkbox
            $(this).each(function(){ this.checked = !this.checked; });
        }

        return this;
    };
})( jQuery );


Alternatively, if you do not want to use a plugin, you can use the following code snippets:

// Check
$(':checkbox').prop('checked', true);

// Un-check
$(':checkbox').prop('checked', false);

// Toggle
$(':checkbox').prop('checked', function (i, value) {
    return !value;
});
AnswerKenntnis:
You can do

$('.myCheckbox').attr('checked',true) //Standards compliant


or

$("form #mycheckbox").attr('checked', true)


If you have custom code in the onclick event for the checkbox that you want to fire, use this one instead:

$("#mycheckbox").click();


You can uncheck by removing the attribute entirely:

$('.myCheckbox').removeAttr('checked')


You can check all checkboxes like this:

$(".myCheckbox").each(function(){
    $("#mycheckbox").click()
});
AnswerKenntnis:
You can also extend the $.fn object with new methods:

(function($)  {
   $.fn.extend({
      check : function()  {
         return this.filter(":radio, :checkbox").attr("checked", true);
      },
      uncheck : function()  {
         return this.filter(":radio, :checkbox").removeAttr("checked");
      }
   });
}(jQuery));


Then you can just do:

$(":checkbox").check();
$(":checkbox").uncheck();


Or you may want to give them more unique names like mycheck() and myuncheck()  in case you use some other library that uses those names.
AnswerKenntnis:
$("#mycheckbox")[0].checked = true;
$("#mycheckbox").attr('checked', true);
$("#mycheckbox").click();


The last one will fire the click event for the checkbox, the others will not.
So if you have custom code in the onclick event for the checkbox that you want to fire, use the last one.
AnswerKenntnis:
To check a checkbox you should use

 $('.myCheckbox').attr('checked',true);


or

 $('.myCheckbox').attr('checked','checked');


and to uncheck a check box you should always set it to false:

 $('.myCheckbox').attr('checked',false);


If you do

  $('.myCheckbox').removeAttr('checked')


it removes the attribute all together and therefore you will not be able to reset the form.

BAD DEMO jQuery 1.6. I think this is broken. For 1.6 I am going to make a new post on that.

NEW WORKING DEMO jQuery 1.5.2 works in Chrome.

Both demos use

$('#tc').click(function() {
    if ( $('#myCheckbox').attr('checked')) {
        $('#myCheckbox').attr('checked', false);
    } else {
        $('#myCheckbox').attr('checked', 'checked');
    }
});
AnswerKenntnis:
I'm missing the solution I'll always use:

if ($('#myCheckBox:checked').val() !== undefined)
{
    //Checked
}
else
{
    //Not checked
}
AnswerKenntnis:
Try this:

$('#checkboxid').get(0).checked = true;  //For checking

$('#checkboxid').get(0).checked = false; //For unchecking
AnswerKenntnis:
This selects elements that have the specified attribute with a value containing the given substring:

$('input[name *= ckbItem]').prop('checked', true);


It will select all elements that contain ckbItem in its name attribute.
AnswerKenntnis:
Here is a way to do it without jQuery - live sample:

JavaScript:

(function () {

    function addOrAttachListener(el, type, listener, useCapture) {
        if (el.addEventListener) {
            el.addEventListener(type, listener, useCapture);
        } else if (el.attachEvent) {
            el.attachEvent ("on" + type, listener);
        }
    };

    addOrAttachListener(window, "load", function () {
        var cbElem = document.getElementById("cb");
        var rcbElem = document.getElementById("rcb");
        addOrAttachListener(cbElem, "click", function() {
            rcbElem.checked = cbElem.checked;
        }, false);
    }, false);
})();


HTML:

<!DOCTYPE html>
<html>
<head>
    <title>setCheck Example</title>
</head>
<body>
    <label>Click Me! <input id="cb" type="checkbox" /></label>
    <label>Reflection: <input id="rcb" type="checkbox" /></label>
    <script type="text/javascript" src="setCheck.js"></script>
</body>
</html>
AnswerKenntnis:
Assuming that the question is...

How do I check a checkbox-set BY VALUE?

Remember that in a typical checkbox set, all input tags have the same name, they differ by the attribute value:  there are no ID for each input of the set.

Xian's answer can be extended with a more specific selector, using the following line of code:

$("input.myclass[name='myname'][value='the_value']").prop("checked", true);
AnswerKenntnis:
We can use elementObject with jQuery for getting the attribute checked:

$(objectElement).attr('checked');


We can use this for all jQuery versions without any error.
AnswerKenntnis:
Here is code for checked and unchecked with a button:

var set=1;
var unset=0;
jQuery( function() {
    $( '.checkAll' ).live('click', function() {
        $( '.cb-element' ).each(function () {
            if(set==1){ $( '.cb-element' ).attr('checked', true) unset=0; }
            if(set==0){ $( '.cb-element' ).attr('checked', false); unset=1; }
        });
        set=unset;
    });
});
AnswerKenntnis:
If you are using PhoneGap doing application development, and you have a value on the button that you want to show instantly, remember to do this

$('span.ui-[controlname]',$('[id]')).text("the value");


I found that without the span, the interface will not update no matter what you do.
AnswerKenntnis:
To check a checkbox using jQuery 1.9 just do this:

checkbox.prop('checked', true);


To uncheck, use:

checkbox.prop('checked', false);


Here' s what I like to use to toggle a checkbox using jQuery:

checkbox.prop('checked', !checkbox.prop('checked'));
AnswerKenntnis:
If using mobile and you want the interface to update and show the checkbox as unchecked, use the following:

$("#checkbox1").prop('checked', false).checkboxradio("refresh");
AnswerKenntnis:
To check and uncheck

$('.myCheckbox').prop('checked', true);
$('.myCheckbox').prop('checked', false);
AnswerKenntnis:
I couldn't get it working using:

$("#cb").prop('checked', true);
$("#cb").prop('checked', false);


Both true and false would check the checkbox. What worked for me was:

$("#cb").prop('checked', true); // For checking
$("#cb").prop('checked', '');     // For unchecking
AnswerKenntnis:
Be aware of memory leaks in IE prior to IE9, as the jQuery documentation states:


  In Internet Explorer prior to version 9, using .prop() to set a DOM
  element property to anything other than a simple primitive value
  (number, string, or boolean) can cause memory leaks if the property is
  not removed (using .removeProp()) before the DOM element is removed
  from the document. To safely set values on DOM objects without memory
  leaks, use .data().
AnswerKenntnis:
$('controlCheckBox').click(function(){
    var temp = $(this).prop('checked');
    $('controlledCheckBoxes').prop('checked', temp);
});
AnswerKenntnis:
Plain JavaScript is very simple and much less overhead:

var elements = document.getElementsByClassName('myCheckBox');
for(var i = 0; i < elements.length; i++)
{
    elements[i].checked = true;
}


Example here
AnswerKenntnis:
Here is the code and demo for how to check multiple check boxes...

http://jsfiddle.net/tamilmani/z8TTt/

$("#check").on("click", function () {

    var chk = document.getElementById('check').checked;

    if (chk) {
        var arr = document.getElementsByTagName("input");
        for (var i in arr) {
            if (arr[i].name == 'check') arr[i].checked = true;
        }
    } else {
        var arr = document.getElementsByTagName("input");
        for (var i in arr) {
            if (arr[i].name == 'check') arr[i].checked = false;
        }
    }
});
AnswerKenntnis:
Here's the complete answer
using jQuery

I test it and it works 100% :D

    // when the button (select_unit_button) is clicked it returns all the checed checkboxes values 
    $("#select_unit_button").on("click", function(e){

             var arr = [];

             $(':checkbox:checked').each(function(i){
                 arr[i] = $(this).val(); // u can get id or anything else
             });

              //console.log(arr); // u can test it using this in google chrome
    });
AnswerKenntnis:
$(".myCheckBox").attr("checked","checked");
AnswerKenntnis:
Another possible solution:

    var c = $("#checkboxid");
    if (c.is(":checked")) {
         $('#checkboxid').prop('checked', false);
    } else {
         $('#checkboxid').prop('checked', true);
    }
AnswerKenntnis:
In jQuery,

if($("#checkboxId").is(':checked')){
    alert("Checked");
}


or 

if($("#checkboxId").attr('checked')==true){
    alert("Checked");
}


In JavaScript,

if (document.getElementById("checkboxID").checked){
    alert("Checked");
}
AnswerKenntnis:
You can try this:

$('input[name="activity[task_state]"]').val("specify the value you want to check ")
AnswerKenntnis:
This is probably the shortest and easiest solution:

$(".myCheckBox")[0].checked = true;


or

$(".myCheckBox")[0].checked = false;




even shorter would be

$(".myCheckBox")[0].checked = !0;
$(".myCheckBox")[0].checked = !1;




Here is a jsfiddle as well.
AnswerKenntnis:
For overall:

$("#checkAll").click(function(){
    $(".somecheckBoxes").prop('checked',$(this).prop('checked')?true:false);
});
QuestionKenntnis:
How do I remove a Git submodule?
qn_description:
How do I remove a Git submodule?

By the way, is there a reason I can't simply do 

git submodule rm whatever


?
AnswersKenntnis
AnswerKenntnis:
Since git1.8.3 (April 22d, 2013):


  There was no Porcelain way to say "I no longer am interested in this submodule", once you express your interest in a submodule with "submodule init".
  "submodule deinit" is the way to do so.


The deletion process also uses git rm (since git1.8.5 October 2013).  

The all removal process would then be:

git submodule deinit asubmodule    
git rm asubmodule
# Note: asubmodule (no trailing slash)
# or, if you want to leave it in your working tree
git rm --cached asubmodule




git rm: See commit 95c16418:


  Currently using "git rm" on a submodule removes the submodule's work tree from that of the superproject and the gitlink from the index.
  But the submodule's section in .gitmodules is left untouched, which is a leftover of the now removed submodule and might irritate users (as opposed to the setting in .git/config, this must stay as a reminder that the user showed interest in this submodule so it will be repopulated later when an older commit is checked out).
  
  Let "git rm" help the user by not only removing the submodule from the work tree but by also removing the "submodule.<submodule name>" section from the .gitmodules file and stage both.




git submodule deinit: It stems from this patch:


  With "git submodule init" the user is able to tell git he cares about one or more submodules and wants to have it populated on the next call to "git submodule update".
  But currently there is no easy way he could tell git he does not care about a submodule anymore and wants to get rid of his local work tree (except he knows a lot about submodule internals and removes the "submodule.$name.url" setting from .git/config together with the work tree himself).
  
  Help those users by providing a 'deinit' command.
  This removes the whole submodule.<name> section from .git/config either for the given
  submodule(s) (or for all those which have been initialized if '.' is given).
  Fail if the current work tree contains modifications unless forced.
  Complain when for a submodule given on the command line the url setting can't be found in .git/config, but nonetheless don't fail. 


This takes care if the (de)initialization steps (.git/config and .git/modules/xxx)

Since git1.8.5, the git rm takes also care of the:


'add' step which records the url of a submodule in the .gitmodules file: it is need to removed for you.
the submodule special entry (as illustrated by this question): the git rm removes it from the index:
git rm --cached path_to_submodule (no trailing slash)
That will remove that directory stored in the index with a special mode "160000", marking it as a submodule root directory.


If you forget that last step, and try to add what was a submodule as a regular directory, you would get error message like:

git add mysubmodule/file.txt 
Path 'mysubmodule/file.txt' is in submodule 'mysubmodule'
AnswerKenntnis:
Via the page Git Submodule Tutorial:

To remove a submodule you need to:


Delete the relevant section from the .gitmodules file.
Stage the .gitmodules changes git add .gitmodules
Delete the relevant section from .git/config.
Run git rm --cached path_to_submodule (no trailing slash).
Run rm -rf .git/modules/path_to_submodule
Commit git commit -m "Removed submodule <name>"
Delete the now untracked submodule filesrm -rf path_to_submodule
AnswerKenntnis:
Simple steps


Remove config entries:git config -f .git/config --remove-section submodule.$submodulepath git config -f .gitmodules --remove-section submodule.$submodulepath
Remove directory from index:git rm --cached $submodulepath
Commit
Delete unused files:rm -rf $submodulepathrm -rf .git/modules/$submodulepath


Please note: $submodulepath doesn't contain leading or trailing slashes.

Background

When you do git submodule add, it only adds it to .gitmodules, but
once you did git submodule init, it added to .git/config.

So if you wish to remove the modules, but be able to restore it quickly,
then do just this:

git rm --cached $submodulepath
git config -f .git/config --remove-section submodule.$submodulepath


It is a good idea to do git rebase HEAD first and git commit
at the end, if you put this in a script.

Also have a look at an answer to Can I unpopulate a Git submodule?.
AnswerKenntnis:
In addition to the recommendations, I also had to rm -Rf .git/modules/path/to/submodule to be able to add a new submodule with the same name (in my case I was replacing a fork with the original)
AnswerKenntnis:
You must remove the entry in .gitmodules and .git/config, and remove the directory of the module from the history:

git rm --cached path/to/submodule


If you'll write on git's mailing list probably someone will do a shell script for you.
AnswerKenntnis:
You can use an alias to automate the solutions provided by others:

[alias]
  rms = "!f(){ git rm --cached \"$1\";rm -r \"$1\";git config -f .gitmodules --remove-section \"submodule.$1\";git config -f .git/config --remove-section \"submodule.$1\";git add .gitmodules; }; f"


Put that in your git config, and then you can do: git rms path/to/submodule
AnswerKenntnis:
To summarize, this is what you should do:


Set path_to_submodule var (no trailing slash): 

path_to_submodule=path/to/submodule
Delete the relevant line from the .gitmodules file:

git config -f .gitmodules --remove-section submodule.$path_to_submodule
Delete the relevant section from .git/config

git config -f .git/config --remove-section submodule.$path_to_submodule
Unstage and remove $path_to_submodule only from the index (to prevent losing information) 

git rm --cached $path_to_submodule
Track changes made to .gitmodules

git add .gitmodules
Commit the superproject

git commit -m "Remove submodule submodule_name"
Delete the now untracked submodule files

rm -rf $path_to_submodule

rm -rf .git/modules/$path_to_submodule
AnswerKenntnis:
If the submodule was accidentally added because you added, committed and pushed a folder that was already a Git repository (contained .git), you wonΓÇÖt have a .gitmodules file to edit, or anything in .git/config. In this case all you need is:

git rm --cached subfolder
git add subfolder
git commit -m "Enter message here"
git push


FWIW, I also removed the .git folder before doing the git add.
AnswerKenntnis:
What I'm currently doing Dec 2012 (combines most of these answers): 

oldPath="vendor/example"
git config -f .git/config --remove-section "submodule.${oldPath}"
git config -f .gitmodules --remove-section "submodule.${oldPath}"
git rm --cached "${oldPath}"
rm -rf "${oldPath}"              ## remove src (optional)
rm -rf ".git/modules/${oldPath}" ## cleanup gitdir (optional housekeeping)
git add .gitmodules
git commit -m "Removed ${oldPath}"
AnswerKenntnis:
To remove a submodule added using:

git submodule add blah@blah.com:repos/blah.git lib/blah


Run:

git submodule deinit lib/blah
git rm lib/blah
git config -f .gitmodules --remove-section submodule.lib/blah


(This solution is for git 1.8.4. In git 1.8.5 the final step will no longer be required)
AnswerKenntnis:
this is a bash script you can download an install

helpful for updating or removing submodules

http://artmees.github.io/gitsubmodule/
AnswerKenntnis:
I had to take John Douthat's steps one step further and cd into the submodule's directory, and then remove the Git repository: 

cd submodule
rm -fr .git


Then I could commit the files as a part of the parent Git repository without the old reference to a submodule.
AnswerKenntnis:
I recently find out a git project which include many useful git related command: https://github.com/visionmedia/git-extras

Install it and type:

git-delete-submodule submodule


Then things are done. The submodule directory will be removed from your repo and still exist in your filesystem. You can then commit the change like: git commit -am "Remove the submodule".
AnswerKenntnis:
Just a note. Since git 1.8.5.2, two commands will do:

git rm the_submodule
rm -rf .git/modules/the_submodule


As @Mark Cheverton's answer correctly pointed out, if the second line isn't used, even if you removed the submodule for now, the remnant .git/modules/the_submodule folder will prevent the same submodule from being added back or replaced in the future. Also, as @VonC mentioned, git rm will do most of the job on a submodule.
AnswerKenntnis:
I just found the .submodule (forgot exact name) hidden file, it has a list... you can erase them individually that way. I just had one, so I deleted it. Simple, but it might mess up Git, since I don't know if anything's attached to the submodule. Seems ok so far, aside from libetpan's usual upgrade issue, but that's (hopefully) unrelated.

Noticed nobody posted manual erasing, so added
QuestionKenntnis:
Hidden Features of C#?
qn_description:
This came to my mind after I learned the following from this question:

where T : struct


We, C# developers, all know the basics of C#. I mean declarations, conditionals, loops, operators, etc.

Some of us even mastered the stuff like Generics, anonymous types, lambdas, LINQ, ...

But what are the most hidden features or tricks of C# that even C# fans, addicts, experts barely know?

Here are the revealed features so far:



Keywords


yield by Michael Stum
var by Michael Stum
using() statement by kokos
readonly by kokos
as by Mike Stone
as / is by Ed Swangren
as / is (improved) by Rocketpants
default by deathofrats
global:: by pzycoman
using() blocks by AlexCuse
volatile by Jakub ┼áturc
extern alias by Jakub ┼áturc


Attributes


DefaultValueAttribute by Michael Stum
ObsoleteAttribute by DannySmurf
DebuggerDisplayAttribute by Stu
DebuggerBrowsable and DebuggerStepThrough by bdukes
ThreadStaticAttribute by marxidad
FlagsAttribute by Martin Clarke
ConditionalAttribute by AndrewBurns


Syntax


?? (coalesce nulls) operator by kokos
Number flaggings by Nick Berardi
where T:new by Lars M├ªhlum
Implicit generics by Keith
One-parameter lambdas by Keith
Auto properties by Keith
Namespace aliases by Keith
Verbatim string literals with @ by Patrick
enum values by lfoust
@variablenames by marxidad
event operators by marxidad
Format string brackets by Portman
Property accessor accessibility modifiers by xanadont
Conditional (ternary) operator (?:) by JasonS
checked and unchecked operators by Binoj Antony
implicit and explicit operators by Flory


Language Features


Nullable types by Brad Barker
Anonymous types by Keith
__makeref __reftype __refvalue by Judah Himango
Object initializers by lomaxx
Format strings by David in Dakota
Extension Methods by marxidad
partial methods by Jon Erickson
Preprocessor directives by John Asbeck
DEBUG pre-processor directive by Robert Durgin
Operator overloading by SefBkn
Type inferrence by chakrit
Boolean operators taken to next level by Rob Gough
Pass value-type variable as interface without boxing by Roman Boiko
Programmatically determine declared variable type by Roman Boiko
Static Constructors by Chris
Easier-on-the-eyes / condensed ORM-mapping using LINQ by roosteronacid
__arglist by Zac Bowling


Visual Studio Features


Select block of text in editor by Himadri
Snippets by DannySmurf  


Framework


TransactionScope by KiwiBastard
DependantTransaction by KiwiBastard
Nullable<T> by IainMH
Mutex by Diago
System.IO.Path by ageektrapped
WeakReference by Juan Manuel


Methods and Properties


String.IsNullOrEmpty() method by KiwiBastard
List.ForEach() method by KiwiBastard
BeginInvoke(), EndInvoke() methods by Will Dean
Nullable<T>.HasValue and Nullable<T>.Value properties by Rismo
GetValueOrDefault method by John Sheehan


Tips & Tricks


Nice method for event handlers by Andreas H.R. Nilsson
Uppercase comparisons by John
Access anonymous types without reflection by dp
A quick way to lazily instantiate collection properties by Will
JavaScript-like anonymous inline-functions by roosteronacid


Other


netmodules by kokos  
LINQBridge by Duncan Smart  
Parallel Extensions by Joel Coehoorn
AnswersKenntnis
AnswerKenntnis:
This isn't C# per se, but I haven't seen anyone who really uses System.IO.Path.Combine() to the extent that they should. In fact, the whole Path class is really useful, but no one uses it!

I'm willing to bet that every production app has the following code, even though it shouldn't:

string path = dir + "\\" + fileName;
AnswerKenntnis:
lambdas and type inferrence are underrated. Lambdas can have multiple statements and they double as a compatible delegate object automatically (just make sure the signature match) as in:

Console.CancelKeyPress +=
    (sender, e) => {
        Console.WriteLine("CTRL+C detected!\n");
        e.Cancel = true;
    };


Note that I don't have a new CancellationEventHandler nor do I have to specify types of sender and e, they're inferable from the event. Which is why this is less cumbersome to writing the whole delegate (blah blah) which also requires you to specify types of parameters.

Lambdas don't need to return anything and type inference is extremely powerful in context like this.

And BTW, you can always return Lambdas that make Lambdas in the functional programming sense. For example, here's a lambda that makes a lambda that handles a Button.Click event:

Func<int, int, EventHandler> makeHandler =
    (dx, dy) => (sender, e) => {
        var btn = (Button) sender;
        btn.Top += dy;
        btn.Left += dx;
    };

btnUp.Click += makeHandler(0, -1);
btnDown.Click += makeHandler(0, 1);
btnLeft.Click += makeHandler(-1, 0);
btnRight.Click += makeHandler(1, 0);


Note the chaining: (dx, dy) => (sender, e) =>

Now that's why I'm happy to have taken the functional programming class :-)

Other than the pointers in C, I think it's the other fundamental thing you should learn :-)
AnswerKenntnis:
From Rick Strahl:

You can chain the ?? operator so that you can do a bunch of null comparisons.

string result = value1 ?? value2 ?? value3 ?? String.Empty;
AnswerKenntnis:
Aliased generics:

using ASimpleName = Dictionary<string, Dictionary<string, List<string>>>;


It allows you to use ASimpleName, instead of Dictionary<string, Dictionary<string, List<string>>>.

Use it when you would use the same generic big long complex thing in a lot of places.
AnswerKenntnis:
From CLR via C#:
  
  When normalizing strings, it is highly
  recommended that you use
  ToUpperInvariant instead of
  ToLowerInvariant because Microsoft has
  optimized the code for performing
  uppercase comparisons.


I remember one time my coworker always changed strings to uppercase before comparing. I've always wondered why he does that because I feel it's more "natural" to convert to lowercase first. After reading the book now I know why.
AnswerKenntnis:
My favorite trick is using the null coalesce operator and parentheses to automagically instantiate collections for me.

private IList<Foo> _foo;

public IList<Foo> ListOfFoo 
    { get { return _foo ?? (_foo = new List<Foo>()); } }
AnswerKenntnis:
Avoid checking for null event handlers

Adding an empty delegate to events at declaration, suppressing the need to always check the event for null before calling it is awesome. Example:

public delegate void MyClickHandler(object sender, string myValue);
public event MyClickHandler Click = delegate {}; // add empty delegate!


Let you do this

public void DoSomething()
{
    Click(this, "foo");
}


Instead of this

public void DoSomething()
{
    // Unnecessary!
    MyClickHandler click = Click;
    if (click != null) // Unnecessary! 
    {
        click(this, "foo");
    }
}


Please also see this related discussion and this blog post by Eric Lippert on this topic (and possible downsides).
AnswerKenntnis:
Everything else, plus 

1) implicit generics (why only on methods and not on classes?)

void GenericMethod<T>( T input ) { ... }

//Infer type, so
GenericMethod<int>(23); //You don't need the <>.
GenericMethod(23);      //Is enough.


2) simple lambdas with one parameter:

x => x.ToString() //simplify so many calls


3) anonymous types and initialisers:

//Duck-typed: works with any .Add method.
var colours = new Dictionary<string, string> {
    { "red", "#ff0000" },
    { "green", "#00ff00" },
    { "blue", "#0000ff" }
};

int[] arrayOfInt = { 1, 2, 3, 4, 5 };




Another one:

4) Auto properties can have different scopes:

public int MyId { get; private set; }




Thanks @pzycoman for reminding me:

5) Namespace aliases (not that you're likely to need this particular distinction):

using web = System.Web.UI.WebControls;
using win = System.Windows.Forms;

web::Control aWebControl = new web::Control();
win::Control aFormControl = new win::Control();
AnswerKenntnis:
I didn't know the "as" keyword for quite a while.

MyClass myObject = (MyClass) obj;


vs

MyClass myObject = obj as MyClass;


The second will return null if obj isn't a MyClass, rather than throw a class cast exception.
AnswerKenntnis:
Two things I like are Automatic properties so you can collapse your code down even further:

private string _name;
public string Name
{
    get
    {
        return _name;
    }
    set
    {
        _name = value;
    }
}


becomes

public string Name { get; set;}


Also object initializers:

Employee emp = new Employee();
emp.Name = "John Smith";
emp.StartDate = DateTime.Now();


becomes

Employee emp = new Employee {Name="John Smith", StartDate=DateTime.Now()}
AnswerKenntnis:
The 'default' keyword in generic types:

T t = default(T);


results in a 'null' if T is a reference type, and 0 if it is an int, false if it is a boolean,
etcetera.
AnswerKenntnis:
Attributes in general, but most of all DebuggerDisplay. Saves you years.
AnswerKenntnis:
The @ tells the compiler to ignore any
  escape characters in a string.


Just wanted to clarify this one... it doesn't tell it to ignore the escape characters, it actually tells the compiler to interpret the string as a literal.

If you have 

string s = @"cat
             dog
             fish"


it will actually print out as (note that it even includes the whitespace used for indentation):

cat
             dog
             fish
AnswerKenntnis:
I think one of the most under-appreciated and lesser-known features of C# (.NET 3.5) are Expression Trees, especially when combined with Generics and Lambdas. This is an approach to API creation that newer libraries like NInject and Moq are using.

For example, let's say that I want to register a method with an API and that API needs to get the method name

Given this class:

public class MyClass
{
     public void SomeMethod() { /* Do Something */ }
}


Before, it was very common to see developers do this with strings and types (or something else largely string-based):

RegisterMethod(typeof(MyClass), "SomeMethod");


Well, that sucks because of the lack of strong-typing. What if I rename "SomeMethod"? Now, in 3.5 however, I can do this in a strongly-typed fashion:

RegisterMethod<MyClass>(cl => cl.SomeMethod());


In which the RegisterMethod class uses Expression<Action<T>> like this:

void RegisterMethod<T>(Expression<Action<T>> action) where T : class
{
    var expression = (action.Body as MethodCallExpression);

    if (expression != null)
    {
        // TODO: Register method
        Console.WriteLine(expression.Method.Name);
    }
}


This is one big reason that I'm in love with Lambdas and Expression Trees right now.
AnswerKenntnis:
"yield" would come to my mind. Some of the attributes like [DefaultValue()] are also among my favorites.

The "var" keyword is a bit more known, but that you can use it in .NET 2.0 applications as well (as long as you use the .NET 3.5 compiler and set it to output 2.0 code) does not seem to be known very well.

Edit: kokos, thanks for pointing out the ?? operator, that's indeed really useful. Since it's a bit hard to google for it (as ?? is just ignored), here is the MSDN documentation page for that operator: ?? Operator (C# Reference)
AnswerKenntnis:
I tend to find that most C# developers don't know about 'nullable' types. Basically, primitives that can have a null value.

double? num1 = null; 
double num2 = num1 ?? -100;


Set a nullable double, num1, to null, then set a regular double, num2, to num1 or -100 if num1 was null.

http://msdn.microsoft.com/en-us/library/1t3y8s4s(VS.80).aspx

one more thing about Nullable type:

DateTime? tmp = new DateTime();
tmp = null;
return tmp.ToString();


it is return String.Empty. Check this link for more details
AnswerKenntnis:
Here are some interesting hidden C# features, in the form of undocumented C# keywords:

__makeref

__reftype

__refvalue

__arglist


These are undocumented C# keywords (even Visual Studio recognizes them!) that were added to for a more efficient boxing/unboxing prior to generics. They work in coordination with the System.TypedReference struct.

There's also __arglist, which is used for variable length parameter lists.

One thing folks don't know much about is System.WeakReference -- a very useful class that keeps track of an object but still allows the garbage collector to collect it.

The most useful "hidden" feature would be the yield return keyword. It's not really hidden, but a lot of folks don't know about it. LINQ is built atop this; it allows for delay-executed queries by generating a state machine under the hood. Raymond Chen recently posted about the internal, gritty details.
AnswerKenntnis:
Unions (the C++ shared memory kind) in pure, safe C#

Without resorting to unsafe mode and pointers, you can have class members share memory space in a class/struct.  Given the following class:

[StructLayout(LayoutKind.Explicit)]
public class A
{
    [FieldOffset(0)]
    public byte One;

    [FieldOffset(1)]
    public byte Two;

    [FieldOffset(2)]
    public byte Three;

    [FieldOffset(3)]
    public byte Four;

    [FieldOffset(0)]
    public int Int32;
}


You can modify the values of the byte fields by manipulating the Int32 field and vice-versa.  For example, this program:

    static void Main(string[] args)
    {
        A a = new A { Int32 = int.MaxValue };

        Console.WriteLine(a.Int32);
        Console.WriteLine("{0:X} {1:X} {2:X} {3:X}", a.One, a.Two, a.Three, a.Four);

        a.Four = 0;
        a.Three = 0;
        Console.WriteLine(a.Int32);
    }


Outputs this:

2147483647
FF FF FF 7F
65535


just add
using System.Runtime.InteropServices;
AnswerKenntnis:
Using @ for variable names that are keywords.

var @object = new object();
var @string = "";
var @if = IpsoFacto();
AnswerKenntnis:
If you want to exit your program without calling any finally blocks or finalizers use FailFast:

Environment.FailFast()
AnswerKenntnis:
Returning anonymous types from a method and accessing members without reflection.

// Useful? probably not.
private void foo()
{
    var user = AnonCast(GetUserTuple(), new { Name = default(string), Badges = default(int) });
    Console.WriteLine("Name: {0} Badges: {1}", user.Name, user.Badges);
}

object GetUserTuple()
{
    return new { Name = "dp", Badges = 5 };
}    

// Using the magic of Type Inference...
static T AnonCast<T>(object obj, T t)
{
   return (T) obj;
}
AnswerKenntnis:
Here's a useful one for regular expressions and file paths:

"c:\\program files\\oldway"
@"c:\program file\newway"


The @ tells the compiler to ignore any escape characters in a string.
AnswerKenntnis:
Mixins. Basically, if you want to add a feature to several classes, but cannot use one base class for all of them, get each class to implement an interface (with no members). Then, write an extension method for the interface, i.e.

public static DeepCopy(this IPrototype p) { ... }


Of course, some clarity is sacrificed. But it works!
AnswerKenntnis:
Not sure why anyone would ever want to use Nullable<bool> though. :-)


True, False, FileNotFound?
AnswerKenntnis:
This one is not "hidden" so much as it is misnamed.

A lot of attention is paid to the algorithms "map", "reduce", and "filter". What most people don't realize is that .NET 3.5 added all three of these algorithms, but it gave them very SQL-ish names, based on the fact that they're part of LINQ.


  "map" => Select Transforms data
  from one form into another
  
  "reduce" => Aggregate Aggregates
  values into a single result
  
  "filter" => Where Filters data
  based on a criteria


The ability to use LINQ to do inline work on collections that used to take iteration and conditionals can be incredibly valuable. It's worth learning how all the LINQ extension methods can help make your code much more compact and maintainable.
AnswerKenntnis:
Environment.NewLine


for system independent newlines.
AnswerKenntnis:
If you're trying to use curly brackets inside a String.Format expression...

int foo = 3;
string bar = "blind mice";
String.Format("{{I am in brackets!}} {0} {1}", foo, bar);
//Outputs "{I am in brackets!} 3 blind mice"
AnswerKenntnis:
?? - coalescing operator  
using (statement / directive) - great keyword that can be used for more than just calling Dispose  
readonly - should be used more  
netmodules - too bad there's no support in Visual Studio
AnswerKenntnis:
@Ed, I'm a bit reticent about posting this as it's little more than nitpicking. However, I would point out that in your code sample:

MyClass c;
  if (obj is MyClass)
    c = obj as MyClass


If you're going to use 'is', why follow it up with a safe cast using 'as'? If you've ascertained that obj is indeed MyClass, a bog-standard cast:

c = (MyClass)obj


...is never going to fail.

Similarly, you could just say:

MyClass c = obj as MyClass;
if(c != null)
{
   ...
}


I don't know enough about .NET's innards to be sure, but my instincts tell me that this would cut a maximum of two type casts operations down to a maximum of one. It's hardly likely to break the processing bank either way; personally, I think the latter form looks cleaner too.
AnswerKenntnis:
Maybe not an advanced technique, but one I see all the time that drives me crazy:

if (x == 1)
{
   x = 2;
}
else
{
   x = 3;
}


can be condensed to:

x = (x==1) ? 2 : 3;
QuestionKenntnis:
Difference between INNER and OUTER joins
qn_description:
What is the difference between INNER JOIN and OUTER JOIN?

How do LEFT JOIN, RIGHT JOIN, and FULL JOIN fit in?
AnswersKenntnis
AnswerKenntnis:
Assuming you're joining on columns with no duplicates, which is by far the most common case:


An inner join of A and B gives the result of A intersect B, i.e. the inner part of a venn diagram intersection.
An outer join of A and B gives the results of A union B, i.e. the outer parts of a venn diagram union.


Examples

Suppose you have two Tables, with a single column each, and data as follows:

A    B
-    -
1    3
2    4
3    5
4    6


Note that (1,2) are unique to A, (3,4) are common, and (5,6) are unique to B.

Inner join

An inner join using either of the equivalent queries gives the intersection of the two tables, i.e. the two rows they have in common.

select * from a INNER JOIN b on a.a = b.b;
select a.*,b.*  from a,b where a.a = b.b;

a | b
--+--
3 | 3
4 | 4


Left outer join

A left outer join will give all rows in A, plus any common rows in B.

select * from a LEFT OUTER JOIN b on a.a = b.b;
select a.*,b.*  from a,b where a.a = b.b(+);

a |  b  
--+-----
1 | null
2 | null
3 |    3
4 |    4


Full outer join

A full outer join will give you the union of A and B, i.e. All the rows in A and all the rows in B.  If something in A doesn't have a corresponding datum in B, then the B portion is null, and vice versa.

select * from a FULL OUTER JOIN b on a.a = b.b;

 a   |  b  
-----+-----
   1 | null
   2 | null
   3 |    3
   4 |    4
null |    6
null |    5
AnswerKenntnis:
Also you can consider following schema for different join types;



Source: http://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins
AnswerKenntnis:
I recommend Jeff's blog article. The best description I've ever seen, plus there is a visualization, e.g.:

Inner Join:



Full Outer Join:
AnswerKenntnis:
The following was taken from the article "MySQL - LEFT JOIN and RIGHT JOIN, INNER JOIN and OUTER JOIN" by Graham Ellis on his blog Horse's Mouth.

In a database such as MySQL, data is divided into a number of tables which are then connected (Joined) together by JOIN in SELECT commands to read records from multiple tables. Read this example to see how it works.

First, some sample data:

people
    mysql> select * from people;
    +------------+--------------+------+
    | name       | phone        | pid  |
    +------------+--------------+------+
    | Mr Brown   | 01225 708225 |    1 |
    | Miss Smith | 01225 899360 |    2 |
    | Mr Pullen  | 01380 724040 |    3 |
    +------------+--------------+------+
    3 rows in set (0.00 sec)

property
    mysql> select * from property;
    +------+------+----------------------+
    | pid  | spid | selling              |
    +------+------+----------------------+
    |    1 |    1 | Old House Farm       |
    |    3 |    2 | The Willows          |
    |    3 |    3 | Tall Trees           |
    |    3 |    4 | The Melksham Florist |
    |    4 |    5 | Dun Roamin           |
    +------+------+----------------------+
    5 rows in set (0.00 sec)


REGULAR JOIN

If we do a regular JOIN (with none of the keywords INNER, OUTER, LEFT or RIGHT), then we get all records that match in the appropriate way in the two tables, and records in both incoming tables that do not match are not reported:

mysql> select name, phone, selling 
from people join property 
on people.pid = property.pid;
+-----------+--------------+----------------------+
| name      | phone        | selling              |
+-----------+--------------+----------------------+
| Mr Brown  | 01225 708225 | Old House Farm       |
| Mr Pullen | 01380 724040 | The Willows          |
| Mr Pullen | 01380 724040 | Tall Trees           |
| Mr Pullen | 01380 724040 | The Melksham Florist |
+-----------+--------------+----------------------+
4 rows in set (0.01 sec)


LEFT JOIN

If we do a LEFT JOIN, we get all records that match in the same way and IN ADDITION we get an extra record for each unmatched record in the left table of the join - thus ensuring (in this example) that every PERSON gets a mention:

   mysql> select name, phone, selling 
    from people left join property 
    on people.pid = property.pid; 
    +------------+--------------+----------------------+
    | name       | phone        | selling              |
    +------------+--------------+----------------------+
    | Mr Brown   | 01225 708225 | Old House Farm       |
    | Miss Smith | 01225 899360 | NULL <<-- unmatch    |
    | Mr Pullen  | 01380 724040 | The Willows          |
    | Mr Pullen  | 01380 724040 | Tall Trees           |
    | Mr Pullen  | 01380 724040 | The Melksham Florist |
    +------------+--------------+----------------------+
    5 rows in set (0.00 sec)


RIGHT JOIN

If we do a RIGHT JOIN, we get all the records that match and IN ADDITION  an extra record for each unmatched record in the right table of the join - in my example, that means that each property gets a mention even if we don't have seller details:

mysql> select name, phone, selling 
from people right join property 
on people.pid = property.pid;
+-----------+--------------+----------------------+
| name      | phone        | selling              |
+-----------+--------------+----------------------+
| Mr Brown  | 01225 708225 | Old House Farm       |
| Mr Pullen | 01380 724040 | The Willows          |
| Mr Pullen | 01380 724040 | Tall Trees           |
| Mr Pullen | 01380 724040 | The Melksham Florist |
| NULL      | NULL         | Dun Roamin           |
+-----------+--------------+----------------------+
5 rows in set (0.00 sec)


An INNER JOIN does a full join, just like the first example, and the word OUTER may be added after the word LEFT or RIGHT in the last two examples - it's provided for ODBC compatibility and doesn't add an extra capabilities.
AnswerKenntnis:
A (left) inner join only shows rows if there is a matching record on the other (right) side of the join.

A (left) outer join shows rows for each record on the left hand side, even if there are no matching rows on the other (right) side of the join. If there is no matching row, the columns for the other (right) side would show NULLs.
AnswerKenntnis:
Joins can be categorized as:

Inner joins (the typical join operation, which uses some comparison operator like = or <>). These include equi-joins and natural joins.

Inner joins use a comparison operator to match rows from two tables based on the values in common columns from each table. For example, retrieving all rows where the student identification number is the same in both the students and courses tables.

Outer joins. Outer joins can be a left, a right, or full outer join.

Outer joins are specified with one of the following sets of keywords when they are specified in the FROM clause:

LEFT JOIN or LEFT OUTER JOIN -The result set of a left outer join includes all the rows from the left table specified in the LEFT OUTER clause, not just the ones in which the joined columns match. When a row in the left table has no matching rows in the right table, the associated result set row contains null values for all select list columns coming from the right table.

RIGHT JOIN or RIGHT OUTER JOIN - A right outer join is the reverse of a left outer join. All rows from the right table are returned. Null values are returned for the left table any time a right table row has no matching row in the left table.

FULL JOIN or FULL OUTER JOIN - A full outer join returns all rows in both the left and right tables. Any time a row has no match in the other table, the select list columns from the other table contain null values. When there is a match between the tables, the entire result set row contains data values from the base tables.

Cross joins - Cross joins return all rows from the left table, each row from the left table is combined with all rows from the right table. Cross joins are also called Cartesian products. (A Cartesian join will get you a Cartesian product. A Cartesian join is when you join every row of one table to every row of another table. You can also get one by joining every row of a table to every row of itself.)
AnswerKenntnis:
in simple words 

inner join retrieve the matched rows only

where as

outer join retrieve the matched rows from one table and all rows in other table ....the result depend on which one you are using

LEFT ( MATCHED ROWS IN RIGHT TABLE AND ALL ROWS IN LEFT TABLE ) 

RIGHT ( MATCHED ROWS IN LEFT TABLE AND ALL ROWS IN RIGHT TABLE ) or 

FULL ( ALL ROWS IN ALL TABLES IT DOESN'T MATTERS EVEN MATCH IS THERE OR NOT )
AnswerKenntnis:
Inner joins require that a record with a related ID exist in the joined table.

Outer joins will return records for the left side even if nothing exists for the right side.

For instance, you have an Orders and an OrderDetails table. They are related by an "OrderID".

Orders


OrderID
CustomerName


OrderDetails


OrderDetailID
OrderID
ProductName
Qty
Price


The request

SELECT Orders.OrderID, Orders.CustomerName FROM Orders 
INNER JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID


will only return Orders that also have something in the OrderDetails table.

If you change it to OUTER LEFT JOIN

SELECT Orders.OrderID, Orders.CustomerName FROM Orders 
LEFT JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID


then it will return records from the Orders table even if they have no OrderDetails records.

You can use this to find Orders that do not have any OrderDetails indicating a possible orphaned order by adding a where clause like WHERE OrderDetails.OrderID IS NULL.
AnswerKenntnis:
Inner Join


  Retrieve the matched rows only i.e  A intersect B




SELECT *
FROM dbo.Students S
INNER JOIN dbo.Advisors A
    ON S.Advisor_ID = A.Advisor_ID



Left Outer Join


  select all records from the first table, and any records in the second
  table that match the joined keys.




SELECT *
FROM dbo.Students S
LEFT JOIN dbo.Advisors A
    ON S.Advisor_ID = A.Advisor_ID



Full Outer Join


  select all records from the second table, and any records in the first
  table that match the joined keys.




SELECT *
FROM dbo.Students S
FULL JOIN dbo.Advisors A
    ON S.Advisor_ID = A.Advisor_ID



Reference 

Inner and outer joins SQL examples and the Join block

SQL: JOINS
AnswerKenntnis:
You use INNER JOIN to return all rows from both tables where there is a match. ie. in the resulting table all the rows and colums will have values.

In OUTER JOIN the relulting table may have empty colums. Outer join may be either LEFT or RIGHT

LEFT OUTER JOIN returns all the rows from the first table, even if there are no matches in the second table.

RIGHT OUTER JOIN returns all the rows from the second table, even if there are no matches in the first table..
AnswerKenntnis:
INNER JOIN requires there is at least a match in comparing the two tables. For example, table A and table B which implies A ┘¿ B (A intersection B).

LEFT OUTER JOIN and LEFT JOIN are the same. It gives all the records matching in both tables and all possibilities of the left table.

Similarly, RIGHT OUTER JOIN and RIGHT JOIN are the same. It gives all the records matching in both tables and all possibilities of the right table.

FULL JOIN is the combination of LEFT OUTER JOIN and RIGHT OUTER JOIN without duplication.
AnswerKenntnis:
The difference is in the way tables are joined if there are no common records.


JOIN is same as INNER JOIN and means to only show records common to both tables. Whether the records are common is determined by the fields in join clause. For example:

FROM t1
JOIN t2 on t1.ID = t2.ID


means show only records where the same ID value exists in both tables.
LEFT JOIN is same as LEFT OUTER JOIN and means to show all records from left table (i.e. the one that precedes in SQL statement) regardless of the existance of matching records in the right table.
RIGHT JOIN is same as RIGHT OUTER JOIN and means opposite of LEFT JOIN, i.e. shows all records from the second (right) table and only matching records from first (left) table. 


Source: What's the difference between LEFT, RIGHT, INNER, OUTER, JOIN?
AnswerKenntnis:
I don't see much details about performance and optimizer in the other answers.

Sometimes it is good to know that only inner join is associative which means the optimizer has the most option to play with it. It can reorder the join order to make it faster keeping the same result. The optimizer can use the most join modes.

Generally it is a good practice to try to use inner joins instead of the different kind of outers.
AnswerKenntnis:
Inner join. 
A join is combining the rows from two tables. An inner join attempts to match up the two tables based on the criteria you specify in the query, and only returns the rows that match. If a row from the first table in the join matches two rows in the second table, then two rows will be returned in the results. If thereΓÇÖs a row in the first table that doesnΓÇÖt match a row in the second, itΓÇÖs not returned; likewise, if thereΓÇÖs a row in the second table that doesnΓÇÖt match a row in the first, itΓÇÖs not returned.

Outer Join. 
A left join attempts to find match up the rows from the first table to rows in the second table. If it canΓÇÖt find a match, it will return the columns from the first table and leave the columns from the second table blank (null).
QuestionKenntnis:
What is the single most influential book every programmer should read?
qn_description:
If you could go back in time and tell yourself to read a specific book at the beginning of your career as a developer, which book would it be?

I expect this list to be varied and to cover a wide range of things.

To search: Use the search box in the upper-right corner. To search the answers of the current question, use inquestion:this.  For example:

inquestion:this "Code Complete"
AnswersKenntnis
AnswerKenntnis:
Code Complete (2nd edition) by Steve McConnell
The Pragmatic Programmer
Structure and Interpretation of Computer Programs
The C Programming Language by Kernighan and Ritchie
Introduction to Algorithms by Cormen, Leiserson, Rivest & Stein
Design Patterns by the Gang of Four
Refactoring: Improving the Design of Existing Code
The Mythical Man Month
The Art of Computer Programming by Donald Knuth
Compilers: Principles, Techniques and Tools by Alfred V. Aho, Ravi Sethi and Jeffrey D. Ullman
G├╢del, Escher, Bach by Douglas Hofstadter
Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin
Effective C++
More Effective C++
CODE by Charles Petzold
Programming Pearls by Jon Bentley
Working Effectively with Legacy Code by Michael C. Feathers
Peopleware by Demarco and Lister
Coders at Work by Peter Seibel
Surely You're Joking, Mr. Feynman!
Effective Java 2nd edition
Patterns of Enterprise Application Architecture by Martin Fowler
The Little Schemer
The Seasoned Schemer
Why's (Poignant) Guide to Ruby
The Inmates Are Running The Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity
The Art of Unix Programming
Test-Driven Development: By Example by Kent Beck
Practices of an Agile Developer
Don't Make Me Think
Agile Software Development, Principles, Patterns, and Practices by Robert C. Martin
Domain Driven Designs by Eric Evans
The Design of Everyday Things by Donald Norman
Modern C++ Design by Andrei Alexandrescu
Best Software Writing I by Joel Spolsky
The Practice of Programming by Kernighan and Pike
Pragmatic Thinking and Learning: Refactor Your Wetware by Andy Hunt
Software Estimation: Demystifying the Black Art by Steve McConnel
The Passionate Programmer (My Job Went To India) by Chad Fowler
Hackers: Heroes of the Computer Revolution
Algorithms + Data Structures = Programs
Writing Solid Code
JavaScript - The Good Parts
Getting Real by 37 Signals
Foundations of Programming by Karl Seguin
Computer Graphics: Principles and Practice in C (2nd Edition)
Thinking in Java by Bruce Eckel
The Elements of Computing Systems
Refactoring to Patterns by Joshua Kerievsky
Modern Operating Systems by Andrew S. Tanenbaum
The Annotated Turing
Things That Make Us Smart by Donald Norman
The Timeless Way of Building by Christopher Alexander
The Deadline: A Novel About Project Management by Tom DeMarco
The C++ Programming Language (3rd edition) by Stroustrup
Patterns of Enterprise Application Architecture
Computer Systems - A Programmer's Perspective
Agile Principles, Patterns, and Practices in C# by Robert C. Martin
Growing Object-Oriented Software, Guided by Tests
Framework Design Guidelines by Brad Abrams
Object Thinking by Dr. David West
Advanced Programming in the UNIX Environment by W. Richard Stevens
Hackers and Painters: Big Ideas from the Computer Age
The Soul of a New Machine by Tracy Kidder
CLR via C# by Jeffrey Richter
The Timeless Way of Building by Christopher Alexander
Design Patterns in C# by Steve Metsker
Alice in Wonderland by Lewis Carol
Zen and the Art of Motorcycle Maintenance by Robert M. Pirsig
About Face - The Essentials of Interaction Design
Here Comes Everybody: The Power of Organizing Without Organizations by Clay Shirky
The Tao of Programming
Computational Beauty of Nature
Writing Solid Code by Steve Maguire
Philip and Alex's Guide to Web Publishing
Object-Oriented Analysis and Design with Applications by Grady Booch
Effective Java by Joshua Bloch
Computability by N. J. Cutland
Masterminds of Programming
The Tao Te Ching
The Productive Programmer
The Art of Deception by Kevin Mitnick
The Career Programmer: Guerilla Tactics for an Imperfect World by Christopher Duncan
Paradigms of Artificial Intelligence Programming: Case studies in Common Lisp
Masters of Doom
Pragmatic Unit Testing in C# with NUnit by Andy Hunt and Dave Thomas with Matt Hargett
How To Solve It by George Polya
The Alchemist by Paulo Coelho
Smalltalk-80: The Language and its Implementation
Writing Secure Code (2nd Edition) by Michael Howard
Introduction to Functional Programming by Philip Wadler and Richard Bird
No Bugs! by David Thielen 
Rework by Jason Freid and DHH
JUnit in Action
AnswerKenntnis:
K&R

@Juan: I know Juan, I know - but there are some things that can only be learned by actually getting down to the task at hand. Speaking in abstract ideals all day simply makes you into an academic. It's in the application of the abstract that we truly grok the reason for their existence. :P

@Keith: Great mention of "The Inmates are Running the Asylum" by Alan Cooper - an eye opener for certain, any developer that has worked with me since I read that book has heard me mention the ideas it espouses. +1
AnswerKenntnis:
Discrete Mathematics For Computer Scientists by J.K. Truss.

While this doesn't teach you programming, it teaches you fundamental mathematics that every programmer should know. You may remember this stuff from university, but really, doing predicate logic will improve you programming skills, you need to learn Set Theory if you want to program using collections.

There really is a lot of interesting information in here that can get you thinking about problems in different ways. It's handy to have, just to pick up once in a while to learn something new.
AnswerKenntnis:
Systemantics: How Systems Work and Especially How They Fail. Get it used cheap. But you might not get the humor until you've worked on a few failed projects.

The beauty of the book is the copyright year.

Probably the most profound takeaway "law" presented in the book:

The Fundamental Failure-Mode Theorem (F.F.T.): Complex systems usually operate in failure mode.

The idea being that there are failing parts in any given piece of software that are masked by failures in other parts or by validations in other parts. See a real-world example at the Therac-25 radiation machine, whose software flaws were masked by hardware failsafes. When the hardware failsafes were removed, the software race condition that had gone undetected all those years resulted in the machine killing 3 people.
AnswerKenntnis:
One of my personal favorites is Hacker's Delight, because it was as much fun to read as it was educational.

I hope the second edition will be released soon!
AnswerKenntnis:
Concepts, Techniques, and Models of Computer Programming.
AnswerKenntnis:
Extreme Programming Explained: Embrace Change by Kent Beck. While I don't advocate a hardcore XP-or-the-highway take on software development, I wish I had been introduced to the principles in this book much earlier in my career. Unit testing, refactoring, simplicity, continuous integration, cost/time/quality/scope - these changed the way I looked at development. Before Agile, it was all about the debugger and fear of change requests. After Agile, those demons did not loom as large.
AnswerKenntnis:
Types and Programming Languages by Benjamin C Pierce for a thorough understanding of the underpinnings of programming languages.
AnswerKenntnis:

AnswerKenntnis:
Database System Concepts is one of the best books you can read on understanding good database design principles.
AnswerKenntnis:
The practice of programming.  By Brian W. Kernighan, Rob Pike.

The style shown here is excellent - the code just speaks for itself, and the whole book follows the KISS principle.  Personally not my languages of choice, but still influential to me.
AnswerKenntnis:
Programming from the ground up. It's free on the internet. This book taught me AT&T asm. It is very easy to read.
AnswerKenntnis:
Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp by Peter Norvig



I started reading it because I wanted to learn Common Lisp. When I was halfway, I realized this was the greatest book about programming I had read so far.
AnswerKenntnis:
Definitively Software Craftsmanship                                                                                



This book explains a lot of things about software engineering, system development. It's also extremly useful to understand the difference between different kind of product developement: web VS shrinkwrap VS IBM framework. What people had in mind when they conceived waterfall model? Read this and all we'll become clear (hopefully)
AnswerKenntnis:
@Peter Coulton -- you don't read Knuth, you study it.

For me, and my work... Purely Functional Data Structures is great for thinking and developing with functional languages in mind.
AnswerKenntnis:
"The World is Flat" by Thomas Friedman. 

Excellence in programming demands an investment of mental energy and a dedication to continued learning comparable to the professions of medicine or law. It pays a fraction of what those professions pay, much less the wages paid to the mathematically savvy who head into the finance sector. And wages for constructing code are eroding because it's a profession that is relatively easy for the intelligent and self-disciplined in most economies to enter.

Programming has already eroded to the point of paying less than, say, plumbing. Plumbing can't be "offshored." You don't need to pay $2395 to attend the Professional Plumber's Conference every other year for the privilege of receiving an entirely new set of plumbing technologies that will take you a year to learn. 

If you live in North America or Europe, are young, and are smart, programming is not a rational career choice. Businesses that involve programming, absolutely. Study business, know enough about programming to refine your BS detector: brilliant. But dedicating the lion's share of your mental energy to the mastery of libraries, data structures, and algorithms? That only makes sense if programming is something more to you than an economic choice.

If you love programming and for that reason intend to make it your career, then it behooves you to develop a cold-eyed understanding of the forces that are, and will continue, to make it a harder and harder profession in which to make a living. "The World is Flat" won't teach you what to name your variables, but it will immerse you for 6 or 8 hours in economic realities that have already arrived. If you can read it, and not get scared, then go out and buy "Code Complete."
AnswerKenntnis:
This last year I took a number of classes. I read

The Innovator's Dilemma (disruptive tech)
The Mythical Man Month (managing software)
Crossing the Chasm (startup)
Database Management Systems, The COW Book
Programming C#, The OSTRICH Book
Beginning iPhone Developmen, The GRAPEFRUIT Book

Each book was amazing but the Innovator's Dilemma by Clayton Christensen (1997!!!) is really a fantastic book, and it got me really thinking about the modern software world. The challenge addressed is disruptive technology, and how disk drive companies and non-technical companies are always disrupted by new, game changing technology. It gives one a new perspective when thinking about Google, probably the biggest 'web' company. Why do they have their hands in EVERYTHING? It's because they don't want to have their position disrupted by something new. The preview on google is plenty to get the idea. Read it!
AnswerKenntnis:
hackers, by Steven Levy.

The personality and way of life must come first. Everything else can be learned.
AnswerKenntnis:
The Practice of Programming



and

How to solve it by computer
AnswerKenntnis:
The Python language was very influential to me, I wish I would have read these book years ago. The beauty and simplicity of the Python language really affected how I wrote code in other languages.
AnswerKenntnis:
Really good book. Has a high-level taste of the most important areas of computer science. Yes, CS != programming, but this is still useful to every programmer.
AnswerKenntnis:
Object Oriented Analysis and Design with Applications by Brady Booch
AnswerKenntnis:
The Mythical Man-Month by Fred Brooks
http://en.wikipedia.org/wiki/The_Mythical_Man-Month
AnswerKenntnis:
I think that "The Art of Unix Programming" is an excellent book, by an excellent hacker/brilliant mind as Eric S. Raymond, who tries to make us understand a few principles of software design (simplicity mainly). This book is a must for every programming who is about to start a project under Unix platform.
AnswerKenntnis:
While I agree that many of the books above are must-reads (Pragmatic Programmer, Mythical Man-Month, Art of Computer Programming, and SICP come to mind immediately), I'd like to go in a slightly different direction and recommend A Discipline of Programming by Edsger Dijkstra. Even though it's 32 years old, the emphasis on "design for verifiability" is highly relevant (even if "verifiability" means "proof" instead "unit tests").
AnswerKenntnis:
Code Craft by Pete Goodliffe is a good read!
AnswerKenntnis:
Martin Fowler's Refactoring: Improving the Design of Existing Code has already been listed. But I will detail why it has impacted me.

The essence of the whole book is about structuring code so that it is simpler to read and understand by humans. It teaches me strongly that the code that I write is meant for my colleagues and successors to consume and possibly learn something good out of it. It inspires me to consciously program in a manner that leaves people praising my name, and not cursing me to damnation for all eternity.
AnswerKenntnis:
C++ How to Program It is good for beginner.This is excellent book that full complete with 1500 pages.
AnswerKenntnis:
Here's an excellent book that is not as widely applauded, but is full of deep insight: Agile Software Development: The Cooperative Game, by Alistair Cockburn. 

What's so special about it? Well, clearly everyone has heard the term "Agile", and it seems most are believers these days. Whether you believe or not, though, there are some deep principles behind why the Agile movement exists. This book uncovers and articulates these principles in a precise, scientific way. Some of the principles are (btw, these are my words, not Alistair's):


The hardest thing about team software development is getting everyone's brains to have the same understanding. We are building huge, elaborate, complex systems which are invisible in the tangible world. The better you are at getting more peoples' brains to share deeper understanding, the more effective your team will be at software development. This is the underlying reason that pair programming makes sense. Most people dismiss it (and I did too initially), but with this principle in mind I highly recommend that you give it another shot. You wind up with TWO people who deeply understand the subsystem you just built ... there aren't many other ways to get such a deep information transfer so quickly. It is like a Vulcan mind meld.
You don't always need words to communicate deep understanding quickly. And a corollary: too many words, and you exceed the listener/reader's capacity, meaning the understanding transfer you're attempting does not happen. Consider that children learn how to speak language by being "immersed" and "absorbing". Not just language either ... he gives the example of some kids playing with trains on the floor. Along comes another kid who has never even SEEN a train before ... but by watching the other kids, he picks up the gist of the game and plays right along. This happens all the time between humans. This along with the corollary about too many words helps you see how misguided it was in the old "waterfall" days to try to write 700 page detailed requirements specifications.


There is so much more in there too. I'll shut up now, but I HIGHLY recommend this book!
AnswerKenntnis:
Masters of doom. As far as motivation and love for your profession go: it won't get any better than what's been described in this book, truthfully inspiring story!
QuestionKenntnis:
Avoiding "!= null" statements in Java?
qn_description:
I work with Java all day long. The most used idiom (code snippet) I'm programming in Java, is to test if an object != null before I use it. This is to avoid a NullPointerException. I find the code very ugly and it becomes unreadable.

Is there a good alternative to avoid this code snippet? 

Update:
Pan, I was not clear with my question. I want to address the necessity to test every object if you want to access a field or method of this object. For example:

...
if (someobject != null) {
    someobject.doCalc();
}
...


In this case I will avoid a NullPointerException, and I don't know exactly if the object is null or not. So my code get splattered with these tests.

Nevertheless thanks a lot for your answers, I got a bunch of new insight.
AnswersKenntnis
AnswerKenntnis:
This to me sounds like a reasonably common problem that junior to intermediate developers tend to face at some point: they either don't know or don't trust the contracts they are participating in and defensively overcheck for nulls.  Additionally, when writing their own code, they tend to rely on returning nulls to indicate something thus requiring the caller to check for nulls.

To put this another way, there are two instances where null checking comes up:


Where null is a valid response in terms of the contract; and
Where it isn't a valid response.


(2) is easy.  Either use assert statements (assertions) or allow failure (for example,  NullPointerException).  Assertions are a highly-underused Java feature that was added in 1.4.  The syntax is:

assert *<condition>*


or

assert *<condition>* : *<object>*


where <object>'s toString() output will be included in the error.

An assert statement throws an Error (AssertionError) if the condition is not true.  By default, Java ignores assertions.  You can enable assertions by passing the option -ea to the JVM.  You can enable and disable assertions for individual classes and packages.  This means that you can validate code with the assertions while developing and testing, and disable them in a production environment, although my testing has shown next to no performance impact from assertions.

Not using assertions in this case is OK because the code will just fail, which is what will happen if you use assertions.  The only difference is that with assertions it might happen sooner, in a more-meaningful way and possibly with extra information, which may help you to figure out why it happened if you weren't expecting it.

(1) is a little harder.  If you have no control over the code you're calling then you're stuck.  If null is a valid response, you have to check for it.

If it's code that you do control, however (and this is often the case), then it's a different story.  Avoid using nulls as a response.  With methods that return collections, it's easy: return empty collections (or arrays) instead of nulls pretty much all the time.

With non-collections it might be harder.  Consider this as an example: if you have these interfaces:

public interface Action {
  void doSomething();
}

public interface Parser {
  Action findAction(String userInput);
}


where Parser takes raw user input and finds something to do, perhaps if you're implementing a command line interface for something.  Now you might make the contract that it returns null if there's no appropriate action.  That leads the null checking you're talking about.

An alternative solution is to never return null and instead use the Null Object pattern:

public class MyParser implements Parser {
  private static Action DO_NOTHING = new Action() {
    public void doSomething() { /* do nothing */ }
  };

  public Action findAction(String userInput) {
    // ...
    if ( /* we can't find any actions */ ) {
      return DO_NOTHING;
    }
  }
}


Compare:

Parser parser = ParserFactory.getParser();
if (parser == null) {
  // now what?
  // this would be an example of where null isn't (or shouldn't be) a valid response
}
Action action = parser.findAction(someInput);
if (action == null) {
  // do nothing
} else {
  action.doSomething();
}


to

ParserFactory.getParser().findAction(someInput).doSomething();


which is a much better design because it leads to more concise code.

That said, perhaps it is entirely appropriate for the findAction() method to throw an Exception with a meaningful error message -- especially in this case where you are relying on user input.  It would be much better for the findAction method to throw an Exception than for the calling method to blow up with a simple NullPointerException with no explanation.

try{
    ParserFactory.getParser().findAction(someInput).doSomething();
} catch(ActionNotFoundException anfe) {
    userConsole.err(anfe.getMessage());
}


Or if you think the try/catch mechanism is too ugly, rather than Do Nothing your default action should provide feedback to the user.

public Action findAction(final String userInput){
    /* Code to return requested Action if found */
    return new Action(){
        public void doSomething(){
            userConsole.err("Action not found: "+userInput);
        }
    }
}
AnswerKenntnis:
If you use (or planning to use) JetBrains Idea, a Java ide, you can use some particular annotations developed by them. 

Basically, you've got @Nullable and @NotNull.

You can use in method and parameters, like this:

@NotNull public static String helloWorld() {
    return "Hello World";
}


or

@Nullable public static String helloWorld() {
    return "Hello World";
}


The second example won't compile (in Idea). 

When you use the first HelloWorld() function in another piece of code:

public static void main(String[] args)
{
    if(helloWorld() != null) {
        System.out.println(helloWorld());
    } 
}


Now the idea compiler will tell you that the check is useless, since the HelloWorld() function won't return null, ever.

Using parameter

void someMethod(@NotNull someParameter) { }


if you write something like:

someMethod(null);


This won't compile.

Last example using @Nullable

@Nullable iWantToDestroyEverything() { return null; }


Doing this

iWantToDestroyEverything().something();


And you can be sure that this won't happen. :)

It's a nice way to let the compiler check something more than it usually does and to enforce your contracts to be stronger. Unfortunately, it's not supported by all the compilers.

In IntelliJ 10.5 and on, they added support for any other @Nullable @NotNull implementations.

http://blogs.jetbrains.com/idea/2011/03/more-flexible-and-configurable-nullublenotnull-annotations/
AnswerKenntnis:
If null-values is not allowed

If your method called externally, start with something like this:

public void method(Object object) {
  if (object == null) {
    throw new IllegalArgumentException("...");
  }


In the rest of that method, you know that it's not null.

If it is an internal method (not part of an api), just document that it cannot be null, and that's it. Example:

public String getFirst3Chars(String text) {
  return text.subString(0, 3);
}


However, if your method just passes the value on, and the next method passes on etc. it could get problematic. In that case you may want to check the argument as above.

If null is allowed

This really depends. If find that I often do something like this:

if (object == null) {
  // something
} else {
  // something else
}


So I branch, and do two completely different things. There is no ugly code snippet, because I really need to do two different things depending on the data. For example, should I work on the input, or should I calculate a good default value?



It's actually rare for me to use the idiom "if (object != null && ...".

It may be easier to give you examples, if you show examples of where you typically use the idiom.
AnswerKenntnis:
Wow, I almost hate to add another answer when we have 57 different ways to recommend the NullObject pattern, but I think that some people interested in this question may like to know that there is a proposal on the table for Java 7 to add "null-safe handling"—a streamlined syntax for if-not-equal-null logic.

The example given by Alex Miller looks like this:

public String getPostcode(Person person) {  
  return person?.getAddress()?.getPostcode();  
}  


The ?. means only de-reference the left identifier if it is not null, otherwise evaluate the remainder of the expression as null. Some people, like Java Posse member Dick Wall and the voters at Devoxx really love this proposal, but there is opposition too, on the grounds that it will actually encourage more use of null as a sentinel value.



Update: An official proposal for a null-safe operator in Java 7 has been submitted under Project Coin. The syntax is a little different than the example above, but it's the same notion.



Update: The null-safe operator proposal didn't make it into Project Coin. So, you won't be seeing this syntax in Java 7.
AnswerKenntnis:
If undefined values are not permitted:

You might configure your IDE to warn you about potential null dereferencing. E.g. in Eclipse, see Preferences > Java > Compiler > Errors/Warnings/Null analysis.

If undefined values are permitted:

If you want to define a new API where undefined values make sense, use the Option Pattern (may be familiar from functional languages). It has the following advantages:


It is stated explicitly in the API whether an input or output exists or not.
The compiler forces you to handle the "undefined" case.
Option is a monad, so there is no need for verbose null checking, just use map/foreach/getOrElse or a similar combinator to safely use the value (example).


Java 8 has a built-in Optional class (recommended); for earlier versions, there are library alternatives, for example Guava's Optional or FunctionalJava's Option. But like many functional-style patterns, using Option in Java (even 8) results in quite some boilerplate, which you can reduce using a less verbose JVM language, e.g. Scala or Xtend.

If you have to deal with an API which might return nulls, you can't do much in Java. Xtend and Groovy have the Elvis operator ?: and the null-safe dereference operator ?., but note that this returns null in case of a null reference, so it just "defers" the proper handling of null.
AnswerKenntnis:
Only for this situation - 
Avoiding checking for null before a string compare:

if ( foo.equals("bar") ) {
 // ...
}


will result in a NullPointerException if foo doesn't exist.

You can avoid that if you compare your Strings like this:

if ( "bar".equals(foo) ) {
 // ...
}
AnswerKenntnis:
Depending on what kind of objects you are checking you may be able to use some of the classes in the apache commons such as: apache commons lang and apache commons collections

Example: 

String foo;
...
if( StringUtils.isBlank( foo ) ) {
   ///do something
}


or (depending on what you need to check):

String foo;
...
if( StringUtils.isEmpty( foo ) ) {
   ///do something
}


The StringUtils class is only one of many; there are quite a few good classes in the commons that do null safe manipulation.

Here follows an example of how you can use null vallidation in JAVA when you include apache library(commons-lang-2.4.jar)

public DOCUMENT read(String xml, ValidationEventHandler validationEventHandler) {
    **Validate.notNull(validationEventHandler,"ValidationHandler not Injected");**
    return read(new StringReader(xml), true, validationEventHandler);
}


And if you are using Spring, Spring also has the same functionality in its package, see library(spring-2.4.6.jar) 

Example on how to use this static classf from spring(org.springframework.util.Assert)

Assert.notNull(validationEventHandler,"ValidationHandler not Injected");
AnswerKenntnis:
If you consider an object should not be null (or it is a bug) use an assert.
If your method doesn't accept null params say it in the javadoc and use an assert.


You have to check for object != null only if you want to handle the case where the object may be null...

There is a proposal to add new annotations in Java7 to help with null / notnull params:
http://tech.puredanger.com/java7/#jsr308
AnswerKenntnis:
I'm a fan of "fail fast" code. Ask yourself - are you doing something useful in the case where the parameter is null? If you don't have a clear answer for what your code should do in that case... I.e. it should never be null in the first place, then ignore it and allow a NullPointerException to be thrown. The calling code will make just as much sense of an NPE as it would an IllegalArgumentException, but it'll be easier for the developer to debug and understand what went wrong if an NPE is thrown rather than your code attempting to execute some other unexpected contingency logic - which ultimately results in the application failing anyway.
AnswerKenntnis:
The google collections framework offers a good and elegant way to achieve the null check.

There is a method in a library class like this:

static <T> T checkNotNull(T e){
   if(e == null){
      throw new NullPointerException();
   }
   return e;
}


And the usage is (with import static):

...
void foo(int a, Person p){
   if(checkNotNull(p).getAge() > a){
      ...
   }else{
      ...
   }
}
...


or in your example:

checkNotNull(someobject).doCalc();
AnswerKenntnis:
Rather than Null Object Pattern -- which has its uses -- you might consider situations where the null object is a bug.

When the exception is thrown, examine the stack trace and work through the bug.
AnswerKenntnis:
Sometimes, you have methods that operate on its parameters that define a symmetric operation:
a.f(b); <-> b.f(a);

If you know b can never be null, you can just swap it. most useful for equals:
instead of foo.equals("bar"); better do "bar".equals(foo);,
AnswerKenntnis:
Ultimately, the only way to completely solve this problem is by using a different programming language:


In Objective-C, you can do the equivalent of invoking a method on nil, and absolutely nothing will happen. This makes most null checks unnecessary but can make errors much harder to diagnose.
In Nice, a Java-derivated language, there are two versions of all types: a potentially-null version and a not-null version. You can only invoke methods on not-null types. Potentially-null types can be converted to not-null types through explicit checking for null. This makes it much easier to know where null checks are necessary and where they aren't.
AnswerKenntnis:
In addition to using assert you can use the following :

if (someobject == null) {
    // handle null here then move on.
}


This is slightly better than :

if (someobject != null) {
    .....
    .....



    .....
}
AnswerKenntnis:
Asking that question points out that you may be interested in error handling strategies.  Your team's architect should decide how to work errors.  There are several ways to do this:


allow the Exceptions to ripple through - catch them at the 'main loop' or in some other managing routine.
check for error conditions and handle them appropriately


Sure do have a look at Aspect Oriented Programming, too - they have neat ways to insert if( o == null ) handleNull() into your bytecode.
AnswerKenntnis:
You can use the Null Object design pattern.
AnswerKenntnis:
Common "problem" in Java indeed.

First, my thoughts on this:

I consider that it is bad to "eat" something when NULL was passed where NULL isn't a valid value. If you're not exiting the method with some sort of error then it means nothing went wrong in your method which is not true. Then you probably return null in this case, and in the receiving method you again check for null, and it never ends, and you end up with "if != null", etc..

So, imho, null must be a critical error which prevents further execution (i.e. where null is not a valid value).

The way I solve this problem is this:

First, I follow this convetion:


all public methods / API always check for null its arguments
all private methods do not check for null since they are controlled methods (just let die with NPE in case it wasn't handled above)
the only other methods which do not check for null are utility methods. They are public, but if you call them for some reason, you know what parameters you pass. This is like trying to boil water in the kettle without providing water...


And finally, in the code, the first line of the public method goes like this:

ValidationUtils.getNullValidator().addParam(plans, "plans").addParam(persons, "persons").validate();


note that addParam() returns self, so that you can add more params to check.

method validate() will throw checked ValidationException if any of the params is null (checked or unchecked is more a design/taste issue, but my ValidationException is checked).

void validate() throws ValidationException;


The message will contain the following text if, for example, "plans" is null:

"Illegal argument value null is encountered for parameter [plans]"

As you can see, 2nd value in the addParam() method (string) is needed for the user message, because you cannot easily detect passed-in variable name, even with reflection (not subject of this post anyway...).

And yes, we know that beyond this line we will no longer encounter a null value so we just safely invoke methods on those objects.

This way, code is clean, easy maintainable and readable.
AnswerKenntnis:
Look at "Null Object" pattern.
AnswerKenntnis:
Java 7 has a new java.util.Objects utility class on which there is a requireNonNull() method. All this does is throw a NullPointerException if its argument is null, but it cleans up the code a bit. Example:

Objects.requireNonNull(someObject);
someObject.doCalc();




The method is most useful for checking just before an assignment in a constructor, where each use of it can save three lines of code:

Parent(Child child) {
   if (child == null) {
      throw new NullPointerException("child");
   }
   this.child = child;
}


becomes

Parent(Child child) {
   this.child = Objects.requireNonNull(child, "child");
}
AnswerKenntnis:
Null is not a 'problem'. It is integral part of complete modeling tool set. Software aims to model complexity of the world and null bears its burden. Some programming languages are designed to be concurrent-oriented some other use empty collections and have no nulls. They are like different tool sets designed for specific 'application areas'. Null indicates 'No data' or 'Unknown' in java and the like. So it is appropriate to use nulls for these purposes. I don't prefer 'Null object' pattern, I think it rise Who will guard the guards problem.
If you ask me what is the name of my girlfriend I'll tell you that I have no girlfriend. In 'java language' I'll return null. Some other 'business logic' could consider it necessary to throw a meaningful exception to indicate some problem that can't be (or don't want to be) solved right there and delegate it somewhere higher in the stack.
For 'unknown question' give 'unknown answer'. Checking arguments for null once inside method before usage relieves multiple callers from checking them before call.  

public Photo getPhotoOfThePerson(Person person) {
    if (person == null)
        return null;
    // grabbing some resources or intensive calculation
    // using person object anyhow
}


previous leads to normal logic flow to get no photo of non-existent girlfriend from my photo library.  

getPhotoOfThePerson(me.getGirlfriend())  


and fits with new coming java api (looking forward)  

getPhotoByName(me.getGirlfriend()?.getName())

NullPointerException is subclass of an Exception. Thus it is a form of Throwable that indicates conditions that a reasonable application might want to catch (javadoc)! To utilize The first most advantage of Exceptions and separate Error-Handling Code from 'Regular' Code (according to creators of java) it is appropriate, as for me, to catch NullPointerException.  

public Photo getGirlfriendPhoto() {
    try {
        return appContext.getPhotoDataSource().getPhotoByName(me.getGirlfriend().getName());
    } catch (NullPointerException e) {
        return null;
    }
}


Questions could arise:  

q. What if getPhotoDataSource() return null?
a. It is up to business logic. If I fail to find photo album I'll show you no photos. What if appContext is not initialized?.. This method's business logic put up with this. If some logic should be more strict then throwing an exception it is part of business logic and explicit check for null should be used (case 4). New Java Null-safe API fits better here to specify selectively what implies and what does not imply to be initialized to be fail-fast in case of programmer errors.

q. Redundant code could be executed and unnecessary resources could be grabbed.
a. It could take place if getPhotoByName() would try to open DB connection, create PreparedStatement and use person name as SQL parameter at last. Approach for unknown question give unknown answer (case 2) works here. Before grabbing resources method should check parameters and return 'unknown' result if need.

q. This approach have performance penalty due to try closure opening.
a. Software should be easy to understand and modify firstly. Only after this one could think about performance only if needed! and where needed! (source, and many others).  

PS. This approach will be as reasonable to use as Separate Error-Handling Code from "Regular" Code principle is reasonable to use in some place. Consider next example:

public SomeValue calculateSomeValueUsingSophisticatedLogic(Predicate predicate) {
    try {
        Result1 result1 = performSomeCalculation(predicate);
        Result2 result2 = performSomeOtherCalculation(result1.getSomeProperty());
        Result3 result3 = performThirdCalculation(result2.getSomeProperty());
        Result4 result4 = performLastCalculation(result3.getSomeProperty());
        return result4.getSomeProperty();
    } catch (NullPointerException e) {
        return null;
    }
}

public SomeValue calculateSomeValueUsingSophisticatedLogic(Predicate predicate) {
    SomeValue result = null;
    if (predicate != null) {
        Result1 result1 = performSomeCalculation(predicate);
        if (result1 != null && result1.getSomeProperty() != null) {
            Result2 result2 = performSomeOtherCalculation(result1.getSomeProperty());
            if (result2 != null && result2.getSomeProperty() != null) {
                Result3 result3 = performThirdCalculation(result2.getSomeProperty());
                if (result3 != null && result3.getSomeProperty() != null) {
                    Result4 result4 = performLastCalculation(result3.getSomeProperty());
                    if (result4 != null) {
                        result = result4.getSomeProperty();
                    }
                }
            }
        }
    }
    return result;
}

Check for 'No data' only if business logic implies.  

public void updatePersonPhoneNumber(Long personId, String phoneNumber) {
    if (personId == null)
        return;
    DataSource dataSource = appContext.getStuffDataSource();
    Person person = dataSource.getPersonById(personId);
    if (person != null) {
        person.setPhoneNumber(phoneNumber);
        dataSource.updatePerson(person);
    } else {
        Person = new Person(personId);
        person.setPhoneNumber(phoneNumber);
        dataSource.insertPerson(person);
    }
}


and

public void updatePersonPhoneNumber(Long personId, String phoneNumber) {
    if (personId == null)
        return;
    DataSource dataSource = appContext.getStuffDataSource();
    Person person = dataSource.getPersonById(personId);
    if (person == null)
        throw new SomeReasonableUserException("What are you thinking about ???");
    person.setPhoneNumber(phoneNumber);
    dataSource.updatePerson(person);
}


If appContext or dataSource are not initialized NullPointerException reveals programmer error somewhere in initialization stuff and it should be fixed exactly there. Current methods business logic implies these to be ready for use.
AnswerKenntnis:
With Java 8 come the new java.util.Optional class that arguably solves some of the problem. One can at least say that it improves the readability of the code, and in the case of public API's make the API's contract clearer to the client developer.

They work like that :

An optional object for a given type (Fruit) is created as the return type of a method. It can be empty or contain a Fruit object :

public static Optional<Fruit> find(String name, List<Fruit> fruits) {
   for(Fruit fruit : fruits) {
      if(fruit.getName().equals(name)) {
         return Optional.of(fruit);
      }
   }
   return Optional.empty();
}


No look at this code where we search a list of Fruit (fruits) for a given Fruit instance :

Optional<Fruit> found = find("lemon", fruits);
if(found.isPresent()) {
   Fruit fruit = found.get();
   String name = fruit.getName();
}


Of course, the check for null/empty value is still necessary, but at least the developer is conscious that the value might be empty and the risk of forgetting to check is limited.

In an API built from scratch using Optional whenever a return value might be empty, and returning a plain object only when it cannot be null (convention), the client code might abandon null checks on simple object return values...

Of course Optional could also be used as method argument, perhaps a better way to indicate optional arguments than 5 or 10 overloading methods in some cases.

Optional offers other convenient methods, such as orElse that allow the use of a default value, and ifPresent that woks with lamba expressions.

I invite you te read this article (my main source for writing this answer) in which the NullPointerException (and in general null pointer) problematic as well as the (partial) solution brought by Optional are well explained : http://java.dzone.com/articles/java-optional-objects
AnswerKenntnis:
I like articles from Nat Pryce, here are the links: 


Avoiding Nulls with Polymorphic Dispatch 
Avoiding Nulls with "Tell, Don't Ask" Style 


In the articles there is also a link to a git repository for a java Maybe Type
which i find interesting, but I don't think it alone could decrease the
checking code bloat. After doing some research on the Internet, I think != null code bloat could be decreased mainly by carefull design
AnswerKenntnis:
just don't ever use null, don't allow it

in my classes, most fields and local vars have a non-null default values, and I add contract statements (always-on asserts) everywhere in the code to make sure this is being enforced (since it's more succinct, and more expressive than letting it come up as a NPE and then having to resolve the line number, etc).

once i adopted this practice, I noticed that the problems seemed to fix themselves, you'd catch things much earlier in the development process just by accident and realize you had a weak spot..  and more importantly.. it helps encapsulate different modules' concerns, different modules can 'trust' each other, and no more littering the code with if = null else constructs!

this is defensive program and results in much cleaner code in the long run. always sanitize the data, e.g. here by enforcing rigid standards, and the problems go away.

class C {
    private final MyType mustBeSet;
    public C(MyType mything) { 
       mustBeSet=Contract.notNull(mything);
    }
   private String name = "<unknown>";
   public void setName(String s) {
      name = Contract.notNull(s);
   }
}


class Contract {
    public static <T> T notNull(T t) { if (t == null) { throw new ContractException("argument must be non-null"); return t; }  
}


the contracts are like mini-unit tests which are always running, even in production, and when things fail, you know why, rather than a random NPE you have to somehow figure out
AnswerKenntnis:
Guava, a very useful core library by Google, has a nice and useful API to avoid nulls. I find UsingAndAvoidingNullExplained very helpful.
AnswerKenntnis:
I've tried the NullObjectPattern but for me is not always the best way to go. There are sometimes when a "no action" is not appropiate.

NullPointerException is a Runtime exception that means it's developers fault and with enough experience it tells you exactly where is the error.

Now to the answer:

Try to make all your attributes and its accessors as private as possible or avoid to  expose them to the clients at all. You can have the argument values in the constructor of course, but by reducing the scope you don't let the client class pass an invalid value. If you need to modify the values, you can always create a new object.  You check the values in the constructor only once and in the rest of the methods you can be almost sure that the values are not null.

Of course, experience is the better way to understand and apply this suggestion.

Byte!
AnswerKenntnis:
Never initialise variables to null.
If (1) is not possible, initialise all collections and arrays to empty collections/arrays.


Doing this in your own code and you can avoid != null checks.

Most of the time null checks seem to guard loops over collections or arrays, so just initialise them empty, you won't need any null checks.

// Bad
ArrayList<String> lemmings;
String[] names;

void checkLemmings() {
    if (lemmings != null) for(lemming: lemmings) {
        // do something
    }
}



// Good
ArrayList<String> lemmings = new ArrayList<String>();
String[] names = {};

void checkLemmings() {
    for(lemming: lemmings) {
        // do something
    }
}


There is a tiny overhead in this, but it's worth it for cleaner code and less NullPointerExceptions.
AnswerKenntnis:
One more alternative:

The following simple function helps to hide the null-check (I don't know why, but I haven't found it as part of the some common library):

public static <T> boolean isNull(T argument) {
    return (argument == null);
}


You could now write

if (!isNull(someobject)) {
    someobject.doCalc();
}


which is IMO a better way of expressing != null.
AnswerKenntnis:
public static <T> T ifNull(T toCheck, T ifNull){
       if(toCheck == null){
           return ifNull;
       }
       return toCheck;
}
AnswerKenntnis:
Wherever you pass an array or a Vector, initialise these to empty ones, instead of null.  - This way you can avoid lots of checking for null and all is good :)

public class NonNullThing {

   Vector vectorField = new Vector();

   int[] arrayField = new int[0];

   public NonNullThing() {

      // etc

   }

}
AnswerKenntnis:
If ( "bar".equals(foo) ) { // ... }
I do agree with this, because you get the string not the parameter which is we do not know about the value.
QuestionKenntnis:
How to include a JavaScript file in another JavaScript file?
qn_description:
Is there something similar to @import in CSS in JavaScript that allows you to include a JavaScript file inside another JavaScript file?
AnswersKenntnis
AnswerKenntnis:
There isn't any import, include or require in JavaScript, but there are two main ways to achieve what you want:

1 - You can load it with an Ajax call and then use eval.

This is the most straightforward way, but it's limited to your domain because of the JavaScript safety settings, and using eval is opening the door to bugs and hacks.

2 - Add a script tag with the script URL in the HTML.

This is definitely the best way to go. You can load the script even from a foreign server, and it's clean as you use the browser parser to evaluate the code. You can put the <script /> tag in the head of the web page, or at the bottom of the body.

Both of these solutions are discussed and illustrated in JavaScript Madness: Dynamic Script Loading.

Now, there is a big issue you must know about. Doing that implies that you remotely load the code. Modern web browsers will load the file and keep executing your current script because they load everything asynchronously to improve performance.

It means that if you use these tricks directly, you won't be able to use your newly loaded code the next line after you asked it to be loaded, because it will be still loading.

For example: my_lovely_script.js contains MySuperObject:

var js = document.createElement("script");

js.type = "text/javascript";
js.src = jsFilePath;

document.body.appendChild(js);

var s = new MySuperObject();

Error : MySuperObject is undefined


Then you reload the page hitting F5. And it works! Confusing...

So what to do about it ?

Well, you can use the hack the author suggests in the link I gave you. In summary, for people in a hurry, he uses en event to run a callback function when the script is loaded. So you can put all the code using the remote library in the callback function. For example:

function loadScript(url, callback)
{
    // Adding the script tag to the head as suggested before
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    script.type = 'text/javascript';
    script.src = url;

    // Then bind the event to the callback function.
    // There are several events for cross browser compatibility.
    script.onreadystatechange = callback;
    script.onload = callback;

    // Fire the loading
    head.appendChild(script);
}


Then you write the code you want to use AFTER the script is loaded in a lambda function:

var myPrettyCode = function() {

   // Here, do what ever you want
};


Then you run all that:

loadScript("my_lovely_script.js", myPrettyCode);


OK, I got it. But it's a pain to write all this stuff.

Well, in that case, you can use as always the fantastic free jQuery library, which let you do the very same thing in one line:

$.getScript("my_lovely_script.js", function(){

   alert("Script loaded and executed.");
   // Here you can use anything you defined in the loaded script
});
AnswerKenntnis:
If anyone is looking for something more advanced, try out RequireJS. You'll get added benefits such as dependency management, better concurrency, and avoid duplication (that is, retrieving a script more than once).

You can write your JavaScript files in "modules" and then reference them as dependencies in other scripts. Or you can use RequireJS as a simple "go get this script" solution.

Example:

Define dependencies as modules:

some-dependency.js

define(['lib/dependency1', 'lib/dependency2'], function (d1, d2) {

     //Your actual script goes here.   
     //The dependent scripts will be fetched if necessary.

     return libraryObject;  //For example, jQuery object
});


implementation.js is your "main" JavaScript file that depends on some-dependency.js

require(['some-dependency'], function(dependency) {

    //Your script goes here
    //some-dependency.js is fetched.   
    //Then your script is executed
});


Excerpt from the GitHub README:


  RequireJS loads plain JavaScript files as well as more defined
  modules. It is optimized for in-browser use, including in a Web
  Worker, but it can be used in other JavaScript environments, like
  Rhino and Node. It implements the Asynchronous Module API.
  
  RequireJS uses plain script tags to load modules/files, so it should
  allow for easy debugging. It can be used simply to load existing
  JavaScript files, so you can add it to your existing project without
  having to re-write your JavaScript files.
  
  ...
AnswerKenntnis:
There actually is a way to load a JavaScript file not asynchronously, so you could use the functions included in your newly loaded file right after loading it, and I think it works in all browsers.

You need to use jQuery.append() on the <head> element of your page, that is:

$("head").append('<script type="text/javascript" src="' + script + '"></script>');


However, this method also has a problem: if an error happens in the imported JavaScript file, Firebug (and also Firefox Error Console and Chrome Developer Tools as well) will report its place incorrectly, which is a big problem if you use Firebug to track JavaScript errors down a lot (I do). Firebug simply doesn't know about the newly loaded file for some reason, so if an error occurs in that file, it reports that it occurred in your main HTML file, and you will have trouble finding out the real reason for the error.

But if that is not a problem for you, then this method should work.

I have actually written a jQuery plugin called *$.import_js()* which uses this method:

(function($)
{
    /*
     * $.import_js() helper (for JavaScript importing within JavaScript code).
     */
    var import_js_imported = [];

    $.extend(true,
    {
        import_js : function(script)
        {
            var found = false;
            for (var i = 0; i < import_js_imported.length; i++)
                if (import_js_imported[i] == script) {
                    found = true;
                    break;
                }

            if (found == false) {
                $("head").append('<script type="text/javascript" src="' + script + '"></script>');
                import_js_imported.push(script);
            }
        }
    });

})(jQuery);


So all you would need to do to import JavaScript is:


  $.import_js('/path_to_project/scripts/somefunctions.js');


I also made a simple test for this at http://www.kipras.com/dev/import_js_test/.

It includes a main.js file in the main HTML and then the script in main.js uses $.import_js() to import an additional file called included.js, which defines this function:

function hello()
{
    alert("Hello world!");
}


And right after including included.js, the hello() function is called, and you get the alert.

(This answer is in response to e-satis' comment.)
AnswerKenntnis:
Another way, that in my opinion is much cleaner, is to make a synchronous Ajax request instead of using a <script> tag. Which is also how Node.js handles includes.

Here's an example using jQuery:

function require(script) {
    $.ajax({
        url: script,
        dataType: "script",
        async: false,           // <-- This is the key
        success: function () {
            // all good...
        },
        error: function () {
            throw new Error("Could not load script " + script);
        }
    });
}


You can then use it in your code as you'd usually use an include:

require("/scripts/subscript.js");


And be able to call a function from the required script in the next line:

subscript.doSomethingCool();
AnswerKenntnis:
There is a good news for you. Very soon you will be able to load javascript easily. It will become a standard way of importing modules of javascript and will be part of core javascript itself. 

You simply have to write 
import cond from 'cond.js';
to load a macro named cond from a file cond.js

So you don't have to rely upon any javascript framework nor you have to explicitly make AJAX calls
Refer to the links below

Source 1

Source 2
AnswerKenntnis:
Maybe you can use this function that I found page How do I include a JavaScript file in a JavaScript file?:

function include(filename)
{
    var head = document.getElementsByTagName('head')[0];

    var script = document.createElement('script');
    script.src = filename;
    script.type = 'text/javascript';

    head.appendChild(script)
}
AnswerKenntnis:
It is possible to dynamically generate a JavaScript tag and append it to HTML document from inside other JavaScript code. This will load targeted JavaScript file.

function includeJs(jsFilePath) {
    var js = document.createElement("script");

    js.type = "text/javascript";
    js.src = jsFilePath;

    document.body.appendChild(js);
}

includeJs("/path/to/some/file.js");
AnswerKenntnis:
You can also assemble your scripts using PHP:

File main.js.php:

<?php
    header('Content-type:text/javascript; charset=utf-8');
    include_once("foo.js.php");
    include_once("bar.js.php");
?>

// Main JavaScript code goes here
AnswerKenntnis:
Or rather than including at run time, use a script to concatenate prior to upload.

I use Sprockets (I don't know if their are others). You build your JS in separate files and include comments that are processed by the Sprockets engine as includes. For Dev you can include files sequentially, then for production merge them...

http://37signals.com/svn/posts/1587-introducing-sprockets-javascript-dependency-management-and-concatenation

https://github.com/sstephenson/sprockets
AnswerKenntnis:
This is perhaps the biggest weakness of Javascript in my opinion. Its caused me no end of problems over the years with dependency tracing. Anyhow it does appear that the only practical solution is to use script includes in the HTML file and thus horribly making your JS dependent upon the user including the source you need and making reuse unfriendly.   

Sorry if this comes across as a lecture ;) Its a bad habit of mine, but I want to make a point. 

The problem comes back to the same as everything else with the web, the history of Javascript. It really wasn't designed to be used in the widespread manner its used in today. Netscape made a language that would allow you to control a few things but didn't envisage its widespread use for so many things as it is put to now and for one reason or another its expanded from there, without addressing some of the fundamental weaknesses of he original strategy. Its not alone of course, HTML wasn't designed for the modern webpage, it was designed to be a way of expressing the logic of a document, so that readers (browsers in the modern world) could display this in an applicable form that was within the capabilities of the system and it took years for a solution (other than the hacks of MS and Netscape) to come along. CSS solves this problem but it was a long time coming and even longer to get people to use it rather than the established BAD techniques, it happened though, praise be.

Hopefully Javascript (especially now its part of the standard) will develop to take on board the concept of proper modularity (as well as some other things) as every other (extant) programming language in the world does and this stupidity will go away, until then you just have to not like it and lump it I'm afraid.
AnswerKenntnis:
I just wrote this JavaScript code (using Prototype for DOM manipulation):

var require = (function () {
var _required = {};
return (function (url, callback) {
    if (typeof url == 'object') {
        // We've (hopefully) got an array: time to chain!
        if (url.length > 1) {
            // Load the nth file as soon as everything up to the
            // n-1th one is done.
            require(url.slice(0,url.length-1), function () {
                require(url[url.length-1], callback);
            });
        } else if (url.length == 1) {
            require(url[0], callback);
        }
        return;
    }
    if (typeof _required[url] == 'undefined') {
        // Haven't loaded this URL yet; gogogo!
        _required[url] = [];

        var script = new Element('script', {src:url, type:'text/javascript'});
        script.observe('load', function () {
            console.log("script " + url + " loaded.");
            _required[url].each(function (cb) {
                cb.call(); // TODO: does this execute in the right context?
            });
            _required[url] = true;
        });

        $$('head')[0].insert(script);
    } else if (typeof _required[url] == 'boolean') {
        // We already loaded the thing, so go ahead.
        if (callback) { callback.call(); }
        return;
    }

    if (callback) { _required[url].push(callback); }
});
})();


Usage:

<script src="prototype.js"></script>
<script src="require.js"></script>
<script>
    require(['foo.js','bar.js'], function () {
        /* Use foo.js and bar.js here */
    });
</script>


Gist: http://gist.github.com/284442
AnswerKenntnis:
Most of solutions shown here imply dynamical loading. I was searching instead for a compiler which assemble all the depended files into a single output file. The same as less/sass preprocessors deal with CSS @import at-rule. Since I didn't find anything decent of this sort, I wrote a simple tool solving the issue. 
So here is the compiler https://github.com/dsheiko/jsic which replace $import("file-path") with the requested file content securely. Here is the corresponding GRUNT plugin https://github.com/dsheiko/grunt-jsic

On jQuery master branch they simply concatenate atomic source files into a single one starting with intro.js and ending with outtro.js. That doesn't suite me as it provides no flexibility on the source code design. Check out how it works with jsic:

src/main.js   

var foo = $import("./Form/Input/Tel");


src/Form/Input/Tel.js

function() {
    return {
          prop: "",
          method: function(){}
    }
}


Now we can run the compiler:

node jsic.js src/main.js build/mail.js


And get the combined file

build/main.js

var foo = function() {
    return {
          prop: "",
          method: function(){}
    }
};
AnswerKenntnis:
What's wrong with something like the following?

xhr = new XMLHttpRequest();
xhr.open("GET", "/soap/ajax/11.0/connection.js", false);
xhr.send();
eval(xhr.responseText);
AnswerKenntnis:
I have created a function that will allow you to use similar verbiage to C#/Java to include a Javascript file.  I've tested it a little bit even from inside of another Javascript file and it seems to work.  It does require jQuery though for a bit of "magic" at the end.  I put this code in a file at the root of my script directory (I named it global.js but you can use whatever you want.  Unless I'm mistaken this and jQuery should be the only required scripts on a given page.  Keep in mind this is largely untested beyond some basic usage, so there may or may not be any issues with the way I've done it; use at your own risk yadda yadda I am not responsible if you screw anything up yadda yadda:

/**
* @fileoverview This file stores global functions that are required by other libraries.
*/

if (typeof(jQuery) === 'undefined') {
    throw 'jQuery is required.';
}

/** Defines the base script directory that all .js files are assumed to be organized under. */
var BASE_DIR = 'js/';

/**
* Loads the specified file, outputting it to the <head> HTMLElement.
*
* This method mimics the use of using in C# or import in Java, allowing
* JavaScript files to "load" other JavaScript files that they depend on
* using a familiar syntax.
*
* This method assumes all scripts are under a directory at the root and will
* append the .js file extension automatically.
*
* @param {string} file A file path to load using C#/Java "dot" syntax.
* 
* Example Usage:
* imports('core.utils.extensions'); 
* This will output: <script type="text/javascript" src="/js/core/utils/extensions.js"></script>
*/
function imports(file) {
    var fileName = file.substr(file.lastIndexOf('.') + 1, file.length);

    // convert PascalCase name to underscore_separated_name
    var regex = new RegExp(/([A-Z])/g);
    if (regex.test(fileName)) {
        var separated = fileName.replace(regex, ",$1").replace(',', '');
        fileName = separated.replace(/[,]/g, '_');
    }

    // remove the original js file name to replace with underscore version
    file = file.substr(0, file.lastIndexOf('.'));

    // convert the dot syntax to directory syntax to actually load the file
    if (file.indexOf('.') > 0) {
        file = file.replace(/[.]/g, '/');
    }

    var src = BASE_DIR + file + '/' + fileName.toLowerCase() + '.js';
    var script = document.createElement('script');
    script.type = 'text/javascript';
    script.src = src;

    $('head').find('script:last').append(script);
}
AnswerKenntnis:
I came to this question because I was looking for a simple way to maintain a collection of useful JavaScript plugins. After seeing some of the solutions here, I came up with this:

1) Set up a file called "plugins.js" (or extentions.js or what have you). Keep your plugin files together with that one master file.

2) plugins.js will have an array called "pluginNames[]" that we will iterate over each(),
then append a  tag to the head for each plugin

//set array to be updated when we add or remove plugin files
 var pluginNames = ["lettering", "fittext", "butterjam", etc.];
//one script tag for each plugin
 $.each(pluginNames, function(){
   $('head').append('<script src="js/plugins/' + this + '.js"></script>');
 });


3) manually call just the one file in your head:
<script src="js/plugins/plugins.js"></script>

UPDATE: I found that even though all of the plugins were getting dropped into the head tag the way they ought to, they weren't always being run by the browser when you click into the page or refresh.

I found it's more reliable to just write the script tags in a PHP include. You only have to write it once and that's just as much work as calling the plugin using JavaScript.
AnswerKenntnis:
var js = document.createElement("script");

js.type = "text/javascript";
js.src = jsFilePath;

document.body.appendChild(js);
AnswerKenntnis:
var s=['Hscript.js','checkRobert.js','Hscript.js'];
for(i=0;i<s.length;i++){
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src = s[i];
  document.getElementsByTagName("head")[0].appendChild(script);
}
AnswerKenntnis:
If you want in pure JavaScript, you can use document.write.

document.write('<script src="myscript.js" type="text/javascript"></script>');


If you use the jQuery library, you can use the $.getScript method.

$.getScript("another_script.js");
AnswerKenntnis:
Here is my synchronous version without jQuery:

function myRequire( url ) {
    var ajax = new XMLHttpRequest();
    ajax.open( 'GET', url, false ); // <-- the 'false' makes it synchronous
    ajax.onreadystatechange = function () {
        var script = ajax.response || ajax.responseText;
        if (ajax.readyState === 4) {
            switch( ajax.status) {
                case 200:
                    eval.apply( window, [script] );
                    console.log("script loaded: ", url);
                    break;
                default:
                    console.log("ERROR: script not loaded: ", url);
            }
        }
    };
    ajax.send(null);
}
AnswerKenntnis:
Better use JQuery way. To delay the ready event, first call $.holdReady(true).
Example (source):

$.holdReady(true);
$.getScript("myplugin.js", function() {
    $.holdReady(false);
});
AnswerKenntnis:
var s=["Hscript.js","checkRobert.js","Hscript.js"];
for(i=0;i<s.length;i++){
  var script=document.createElement("script");
  script.type="text/javascript";
  script.src=s[i];
  document.getElementsByTagName("head")[0].appendChild(script)
};
AnswerKenntnis:
Surprisingly no one mentioned Head.js. It is very easy to deal with:

head.load("js/jquery.min.js",
          "js/jquery.someplugin.js",
          "js/jquery.someplugin.css", function() {
  alert("Everything is ok!");
});


As you see, it's easier than Require.js and as convenient as JQuery's $.getScript method. It also has some advanced features, like conditional loading, feature detection and much more.
AnswerKenntnis:
In a past project I had quite a bit of success using ajile to do imports of reusable JavaScript files. I always wished there was a feature for this built into JavaScript itself.
AnswerKenntnis:
Don't forget to check out LAB.js!

<script type="text/javascript">
       $LAB
       .script("jquery-1.8.3.js").wait()
       .script("scripts/clientscript.js");      
</script>
AnswerKenntnis:
Now, I may be totally misguided, but here's what I've recently started doing... 
Start and end your JavaScript files with a carriage return, place in the PHP script, followed by one more carriage return.
The JavaScript comment "//" is ignored by PHP so the inclusion happens anyways. The purpose for the carriage returns, is so that the first line of your included JavaScript isn't commented out.

Technically you don't need the comment, but it posts errors in Dreamweaver that annoy me. If you're scripting in an IDE that doesn't post errors, you shouldn't need the comment or the carriage returns.

\n
//<?php require_once("path/to/javascript/dependency.js"); ?>

function myFunction(){
    // stuff
}
\n
AnswerKenntnis:
I wrote a simple module that automatizes the job of importing/including module scripts in JavaScript. Give it a try and please spare some feedback! :) For detailed explanation of the code refer to this blog post: http://stamat.wordpress.com/2013/04/12/javascript-require-import-include-modules/

// ----- USAGE -----

require('ivar.util.string');
require('ivar.net.*');
require('ivar/util/array.js');
require('http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js');

ready(function(){
    //do something when required scripts are loaded
});

    //--------------------

var _rmod = _rmod || {}; //require module namespace
_rmod.LOADED = false;
_rmod.on_ready_fn_stack = [];
_rmod.libpath = '';
_rmod.imported = {};
_rmod.loading = {
    scripts: {},
    length: 0
};

_rmod.findScriptPath = function(script_name) {
    var script_elems = document.getElementsByTagName('script');
    for (var i = 0; i < script_elems.length; i++) {
        if (script_elems[i].src.endsWith(script_name)) {
            var href = window.location.href;
            href = href.substring(0, href.lastIndexOf('/'));
            var url = script_elems[i].src.substring(0, script_elems[i].length - script_name.length);
            return url.substring(href.length+1, url.length);
        }
    }
    return '';
};

_rmod.libpath = _rmod.findScriptPath('script.js'); //Path of your main script used to mark the root directory of your library, any library


_rmod.injectScript = function(script_name, uri, callback, prepare) {

    if(!prepare)
        prepare(script_name, uri);

    var script_elem = document.createElement('script');
    script_elem.type = 'text/javascript';
    script_elem.title = script_name;
    script_elem.src = uri;
    script_elem.async = true;
    script_elem.defer = false;

    if(!callback)
        script_elem.onload = function() {
            callback(script_name, uri);
        };

    document.getElementsByTagName('head')[0].appendChild(script_elem);
};

_rmod.requirePrepare = function(script_name, uri) {
    _rmod.loading.scripts[script_name] = uri;
    _rmod.loading.length++;
};

_rmod.requireCallback = function(script_name, uri) {
    _rmod.loading.length--;
    delete _rmod.loading.scripts[script_name];
    _rmod.imported[script_name] = uri;

    if(_rmod.loading.length == 0)
        _rmod.onReady();
};

_rmod.onReady = function() {
    if (!_rmod.LOADED) {
        for (var i = 0; i < _rmod.on_ready_fn_stack.length; i++){
            _rmod.on_ready_fn_stack[i]();
        });
        _rmod.LOADED = true;
    }
};

_.rmod = namespaceToUri = function(script_name, url) {
    var np = script_name.split('.');
    if (np.getLast() === '*') {
        np.pop();
        np.push('_all');
    }

    if(!url)
        url = '';

    script_name = np.join('.');
    return  url + np.join('/')+'.js';
};

//you can rename based on your liking. I chose require, but it can be called include or anything else that is easy for you to remember or write, except import because it is reserved for future use.
var require = function(script_name) {
    var uri = '';
    if (script_name.indexOf('/') > -1) {
        uri = script_name;
        var lastSlash = uri.lastIndexOf('/');
        script_name = uri.substring(lastSlash+1, uri.length);
    } else {
        uri = _rmod.namespaceToUri(script_name, ivar._private.libpath);
    }

    if (!_rmod.loading.scripts.hasOwnProperty(script_name) 
     && !_rmod.imported.hasOwnProperty(script_name)) {
        _rmod.injectScript(script_name, uri, 
            _rmod.requireCallback, 
                _rmod.requirePrepare);
    }
};

var ready = function(fn) {
    _rmod.on_ready_fn_stack.push(fn);
};
AnswerKenntnis:
I did:

var require = function (src, cb) {
    cb = cb || function () {};

    var newScriptTag = document.createElement('script'),
        firstScriptTag = document.getElementsByTagName('script')[0];
    newScriptTag.src = src;
    newScriptTag.async = true;
    console.log(newScriptTag)
    newScriptTag.onload = newScriptTag.onreadystatechange = function () {
        (!this.readyState || this.readyState === 'loaded' || this.readyState === 'complete') && (cb());
    };
    firstScriptTag.parentNode.insertBefore(newScriptTag, firstScriptTag);
}


Works great and uses no page-reloads for me. Tried that AJAX thing. It doesn't really work.
AnswerKenntnis:
Here is a Grunt plugin allowing you to use @import "path/to/file.js"; syntax in any file including JavaSript files. It can be paired with uglify or watch or any other plugin.

Can be installed with npm install: https://npmjs.org/package/grunt-import

Let me know if any questions.
AnswerKenntnis:
The @import syntax for achieving "css-like" js importing is possible using a tool such as Mixture via their special .mix file type (see here). I imagine the application simply uses one of the aforementioned methods "under the hood," though I don't know. 

From the Mixture documentation on .mix files: 


  Mix files are simply .js or .css files with .mix. in the file name. A
  mix file simply     extends the functionality of a normal style or
  script file and allows you to import and combine.


Here's an example .mix file that combines multiple .js files into one:

// scripts-global.mix.js
// Plugins - Global

@import "global-plugins/headroom.js";
@import "global-plugins/retina-1.1.0.js";
@import "global-plugins/isotope.js";
@import "global-plugins/jquery.fitvids.js";


Mixture outputs this as scripts-global.js and also as a minified version (scripts-global.min.js).

Note: I'm not in any way affiliated with Mixture, other than using it as a front-end development tool. I came across this question upon seeing a .mix javascript file in action (in one of the Mixture boilerplates) and being a bit confused by it ("you can do this?" I thought to myself). Then I realized that it was an app-specific file type (somewhat disappointing, agreed). Nevertheless, figured the knowledge might be helpful for others.
AnswerKenntnis:
function include(js)
{
    document.writeln("<script src=" + js + "><" + "/script>");
}
QuestionKenntnis:
Hidden features of Python
qn_description:
What are the lesser-known but useful features of the Python programming language?


Try to limit answers to Python core.
One feature per answer.
Give an example and short description of the feature, not just a link to documentation.
Label the feature using a title as the first line.


Quick links to answers:


Argument Unpacking
Braces
Chaining Comparison Operators
Decorators
Default Argument Gotchas / Dangers of Mutable Default arguments
Descriptors
Dictionary default .get value
Docstring Tests
Ellipsis Slicing Syntax
Enumeration
For/else
Function as iter() argument
Generator expressions
import this
In Place Value Swapping
List stepping
__missing__ items
Multi-line Regex
Named string formatting
Nested list/generator comprehensions
New types at runtime
.pth files
ROT13 Encoding
Regex Debugging
Sending to Generators
Tab Completion in Interactive Interpreter
Ternary Expression
try/except/else
Unpacking+print() function
with statement
AnswersKenntnis
AnswerKenntnis:
Chaining comparison operators:

>>> x = 5
>>> 1 < x < 10
True
>>> 10 < x < 20 
False
>>> x < 10 < x*10 < 100
True
>>> 10 > x <= 9
True
>>> 5 == x > 4
True


In case you're thinking it's doing 1 < x, which comes out as True, and then comparing True < 10, which is also True, then no, that's really not what happens (see the last example.) It's really translating into 1 < x and x < 10, and x < 10 and 10 < x * 10 and x*10 < 100, but with less typing and each term is only evaluated once.
AnswerKenntnis:
Get the python regex parse tree to debug your regex.

Regular expressions are a great feature of python, but debugging them can be a pain, and it's all too easy to get a regex wrong.

Fortunately, python can print the regex parse tree, by passing the undocumented, experimental, hidden flag re.DEBUG (actually, 128) to re.compile.

>>> re.compile("^\[font(?:=(?P<size>[-+][0-9]{1,2}))?\](.*?)[/font]",
    re.DEBUG)
at at_beginning
literal 91
literal 102
literal 111
literal 110
literal 116
max_repeat 0 1
  subpattern None
    literal 61
    subpattern 1
      in
        literal 45
        literal 43
      max_repeat 1 2
        in
          range (48, 57)
literal 93
subpattern 2
  min_repeat 0 65535
    any None
in
  literal 47
  literal 102
  literal 111
  literal 110
  literal 116


Once you understand the syntax, you can spot your errors.  There we can see that I forgot to escape the [] in [/font].

Of course you can combine it with whatever flags you want, like commented regexes:

>>> re.compile("""
 ^              # start of a line
 \[font         # the font tag
 (?:=(?P<size>  # optional [font=+size]
 [-+][0-9]{1,2} # size specification
 ))?
 \]             # end of tag
 (.*?)          # text between the tags
 \[/font\]      # end of the tag
 """, re.DEBUG|re.VERBOSE|re.DOTALL)
AnswerKenntnis:
enumerate

Wrap an iterable with enumerate and it will yield the item along with its index.

For example:


>>> a = ['a', 'b', 'c', 'd', 'e']
>>> for index, item in enumerate(a): print index, item
...
0 a
1 b
2 c
3 d
4 e
>>>


References:


Python tutorialΓÇölooping techniques
Python docsΓÇöbuilt-in functionsΓÇöenumerate
PEP 279
AnswerKenntnis:
Creating generators objects

If you write 

x=(n for n in foo if bar(n))


you can get out the generator and assign it to x. Now it means you can do

for n in x:


The advantage of this is that you don't need intermediate storage, which you would need if you did

x = [n for n in foo if bar(n)]


In some cases this can lead to significant speed up.

You can append many if statements to the end of the generator, basically replicating nested for loops:

>>> n = ((a,b) for a in range(0,2) for b in range(4,6))
>>> for i in n:
...   print i 

(0, 4)
(0, 5)
(1, 4)
(1, 5)
AnswerKenntnis:
iter() can take a callable argument

For instance:

def seek_next_line(f):
    for c in iter(lambda: f.read(1),'\n'):
        pass


The iter(callable, until_value) function repeatedly calls callable and yields its result until until_value is returned.
AnswerKenntnis:
Be careful with mutable default arguments

>>> def foo(x=[]):
...     x.append(1)
...     print x
... 
>>> foo()
[1]
>>> foo()
[1, 1]
>>> foo()
[1, 1, 1]


Instead, you should use a sentinel value denoting "not given" and replace with the mutable you'd like as default:

>>> def foo(x=None):
...     if x is None:
...         x = []
...     x.append(1)
...     print x
>>> foo()
[1]
>>> foo()
[1]
AnswerKenntnis:
Sending values into generator functions. For example having this function:

def mygen():
    """Yield 5 until something else is passed back via send()"""
    a = 5
    while True:
        f = (yield a) #yield a and possibly get f in return
        if f is not None: 
            a = f  #store the new value


You can:

>>> g = mygen()
>>> g.next()
5
>>> g.next()
5
>>> g.send(7)  #we send this back to the generator
7
>>> g.next() #now it will yield 7 until we send something else
7
AnswerKenntnis:
If you don't like using whitespace to denote scopes, you can use the C-style {} by issuing:

from __future__ import braces
AnswerKenntnis:
The step argument in slice operators. For example:

a = [1,2,3,4,5]
>>> a[::2]  # iterate over the whole list in 2-increments
[1,3,5]


The special case x[::-1] is a useful idiom for 'x reversed'.

>>> a[::-1]
[5,4,3,2,1]
AnswerKenntnis:
Decorators

Decorators allow to wrap a function or method in another function that can add functionality, modify arguments or results, etc. You write decorators one line above the function definition, beginning with an "at" sign (@).

Example shows a print_args decorator that prints the decorated function's arguments before calling it:

>>> def print_args(function):
>>>     def wrapper(*args, **kwargs):
>>>         print 'Arguments:', args, kwargs
>>>         return function(*args, **kwargs)
>>>     return wrapper

>>> @print_args
>>> def write(text):
>>>     print text

>>> write('foo')
Arguments: ('foo',) {}
foo
AnswerKenntnis:
The for...else syntax (see http://docs.python.org/ref/for.html )

for i in foo:
    if i == 0:
        break
else:
    print("i was never 0")


The "else" block will be normally executed at the end of the for loop, unless the break is called.

The above code could be emulated as follows:

found = False
for i in foo:
    if i == 0:
        found = True
        break
if not found: 
    print("i was never 0")
AnswerKenntnis:
From 2.5 onwards dicts have a special method __missing__ that is invoked for missing items:

>>> class MyDict(dict):
...  def __missing__(self, key):
...   self[key] = rv = []
...   return rv
... 
>>> m = MyDict()
>>> m["foo"].append(1)
>>> m["foo"].append(2)
>>> dict(m)
{'foo': [1, 2]}


There is also a dict subclass in collections called defaultdict that does pretty much the same but calls a function without arguments for not existing items:

>>> from collections import defaultdict
>>> m = defaultdict(list)
>>> m["foo"].append(1)
>>> m["foo"].append(2)
>>> dict(m)
{'foo': [1, 2]}


I recommend converting such dicts to regular dicts before passing them to functions that don't expect such subclasses.  A lot of code uses d[a_key] and catches KeyErrors to check if an item exists which would add a new item to the dict.
AnswerKenntnis:
In-place value swapping

>>> a = 10
>>> b = 5
>>> a, b
(10, 5)

>>> a, b = b, a
>>> a, b
(5, 10)


The right-hand side of the assignment is an expression that creates a new tuple. The left-hand side of the assignment immediately unpacks that (unreferenced) tuple to the names a and b.

After the assignment, the new tuple is unreferenced and marked for garbage collection, and the values bound to a and b have been swapped.

As noted in the Python tutorial section on data structures,


  Note that multiple assignment is really just a combination of tuple packing and sequence unpacking.
AnswerKenntnis:
Readable regular expressions

In Python you can split a regular expression over multiple lines, name your matches and insert comments.

Example verbose syntax (from Dive into Python):

>>> pattern = """
... ^                   # beginning of string
... M{0,4}              # thousands - 0 to 4 M's
... (CM|CD|D?C{0,3})    # hundreds - 900 (CM), 400 (CD), 0-300 (0 to 3 C's),
...                     #            or 500-800 (D, followed by 0 to 3 C's)
... (XC|XL|L?X{0,3})    # tens - 90 (XC), 40 (XL), 0-30 (0 to 3 X's),
...                     #        or 50-80 (L, followed by 0 to 3 X's)
... (IX|IV|V?I{0,3})    # ones - 9 (IX), 4 (IV), 0-3 (0 to 3 I's),
...                     #        or 5-8 (V, followed by 0 to 3 I's)
... $                   # end of string
... """
>>> re.search(pattern, 'M', re.VERBOSE)


Example naming matches (from Regular Expression HOWTO)

>>> p = re.compile(r'(?P<word>\b\w+\b)')
>>> m = p.search( '(((( Lots of punctuation )))' )
>>> m.group('word')
'Lots'


You can also verbosely write a regex without using re.VERBOSE thanks to string literal concatenation.

>>> pattern = (
...     "^"                 # beginning of string
...     "M{0,4}"            # thousands - 0 to 4 M's
...     "(CM|CD|D?C{0,3})"  # hundreds - 900 (CM), 400 (CD), 0-300 (0 to 3 C's),
...                         #            or 500-800 (D, followed by 0 to 3 C's)
...     "(XC|XL|L?X{0,3})"  # tens - 90 (XC), 40 (XL), 0-30 (0 to 3 X's),
...                         #        or 50-80 (L, followed by 0 to 3 X's)
...     "(IX|IV|V?I{0,3})"  # ones - 9 (IX), 4 (IV), 0-3 (0 to 3 I's),
...                         #        or 5-8 (V, followed by 0 to 3 I's)
...     "$"                 # end of string
... )
>>> print pattern
"^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$"
AnswerKenntnis:
Function argument unpacking

You can unpack a list or a dictionary as function arguments using * and **.

For example:

def draw_point(x, y):
    # do some magic

point_foo = (3, 4)
point_bar = {'y': 3, 'x': 2}

draw_point(*point_foo)
draw_point(**point_bar)


Very useful shortcut since lists, tuples and dicts are widely used as containers.
AnswerKenntnis:
ROT13 is a valid encoding for source code, when you use the right coding declaration at the top of the code file:

#!/usr/bin/env python
# -*- coding: rot13 -*-

cevag "Uryyb fgnpxbiresybj!".rapbqr("rot13")
AnswerKenntnis:
Creating new types in a fully dynamic manner

>>> NewType = type("NewType", (object,), {"x": "hello"})
>>> n = NewType()
>>> n.x
"hello"


which is exactly the same as

>>> class NewType(object):
>>>     x = "hello"
>>> n = NewType()
>>> n.x
"hello"


Probably not the most useful thing, but nice to know.

Edit: Fixed name of new type, should be NewType to be the exact same thing as with class statement.

Edit: Adjusted the title to more accurately describe the feature.
AnswerKenntnis:
Context managers and the "with" Statement

Introduced in PEP 343, a context manager is an object that acts as a run-time context for a suite of statements.

Since the feature makes use of new keywords, it is introduced gradually: it is available in Python 2.5 via the __future__ directive. Python 2.6 and above (including Python 3) has it available by default.

I have used the "with" statement a lot because I think it's a very useful construct, here is a quick demo:

from __future__ import with_statement

with open('foo.txt', 'w') as f:
    f.write('hello!')


What's happening here behind the scenes, is that the "with" statement calls the special __enter__ and __exit__ methods on the file object. Exception details are also passed to __exit__ if any exception was raised from the with statement body, allowing for exception handling to happen there.

What this does for you in this particular case is that it guarantees that the file is closed when execution falls out of scope of the with suite, regardless if that occurs normally or whether an exception was thrown. It is basically a way of abstracting away common exception-handling code.

Other common use cases for this include locking with threads and database transactions.
AnswerKenntnis:
Dictionaries have a get() method

Dictionaries have a 'get()' method. If you do d['key'] and key isn't there, you get an exception. If you do d.get('key'), you get back None if 'key' isn't there. You can add a second argument to get that item back instead of None, eg: d.get('key', 0).

It's great for things like adding up numbers:

sum[value] = sum.get(value, 0) + 1
AnswerKenntnis:
Descriptors

They're the magic behind a whole bunch of core Python features. 

When you use dotted access to look up a member (eg, x.y), Python first looks for the member in the instance dictionary. If it's not found, it looks for it in the class dictionary. If it finds it in the class dictionary, and the object implements the descriptor protocol, instead of just returning it, Python executes it. A descriptor is any class that implements the __get__, __set__, or __delete__ methods.

Here's how you'd implement your own (read-only) version of property using descriptors:

class Property(object):
    def __init__(self, fget):
        self.fget = fget

    def __get__(self, obj, type):
        if obj is None:
            return self
        return self.fget(obj)


and you'd use it just like the built-in property():

class MyClass(object):
    @Property
    def foo(self):
        return "Foo!"


Descriptors are used in Python to implement properties, bound methods, static methods, class methods and slots, amongst other things. Understanding them makes it easy to see why a lot of things that previously looked like Python 'quirks' are the way they are.

Raymond Hettinger has an excellent tutorial that does a much better job of describing them than I do.
AnswerKenntnis:
Conditional Assignment

x = 3 if (y == 1) else 2


It does exactly what it sounds like: "assign 3 to x if y is 1, otherwise assign 2 to x". Note that the parens are not necessary, but I like them for readability. You can also chain it if you have something more complicated:

x = 3 if (y == 1) else 2 if (y == -1) else 1


Though at a certain point, it goes a little too far.

Note that you can use if ... else in any expression. For example:

(func1 if y == 1 else func2)(arg1, arg2) 


Here func1 will be called if y is 1 and func2, otherwise. In both cases the corresponding function will be called with arguments arg1 and arg2.

Analogously, the following is also valid:

x = (class1 if y == 1 else class2)(arg1, arg2)


where class1 and class2 are two classes.
AnswerKenntnis:
Doctest: documentation and unit-testing at the same time.

Example extracted from the Python documentation:

def factorial(n):
    """Return the factorial of n, an exact integer >= 0.

    If the result is small enough to fit in an int, return an int.
    Else return a long.

    >>> [factorial(n) for n in range(6)]
    [1, 1, 2, 6, 24, 120]
    >>> factorial(-1)
    Traceback (most recent call last):
        ...
    ValueError: n must be >= 0

    Factorials of floats are OK, but the float must be an exact integer:
    """

    import math
    if not n >= 0:
        raise ValueError("n must be >= 0")
    if math.floor(n) != n:
        raise ValueError("n must be exact integer")
    if n+1 == n:  # catch a value like 1e300
        raise OverflowError("n too large")
    result = 1
    factor = 2
    while factor <= n:
        result *= factor
        factor += 1
    return result

def _test():
    import doctest
    doctest.testmod()    

if __name__ == "__main__":
    _test()
AnswerKenntnis:
Named formatting

% -formatting takes a dictionary (also applies %i/%s etc. validation).

>>> print "The %(foo)s is %(bar)i." % {'foo': 'answer', 'bar':42}
The answer is 42.

>>> foo, bar = 'question', 123

>>> print "The %(foo)s is %(bar)i." % locals()
The question is 123.


And since locals() is also a dictionary, you can simply pass that as a dict and have % -substitions from your local variables. I think this is frowned upon, but simplifies things..

New Style Formatting

>>> print("The {foo} is {bar}".format(foo='answer', bar=42))
AnswerKenntnis:
To add more python modules (espcially 3rd party ones), most people seem to use PYTHONPATH environment variables or they add symlinks or directories in their site-packages directories. Another way, is to use *.pth files. Here's the official python doc's explanation:


  "The most convenient way [to modify
  python's search path] is to add a path
  configuration file to a directory
  that's already on Python's path,
  usually to the .../site-packages/
  directory. Path configuration files
  have an extension of .pth, and each
  line must contain a single path that
  will be appended to sys.path. (Because
  the new paths are appended to
  sys.path, modules in the added
  directories will not override standard
  modules. This means you can't use this
  mechanism for installing fixed
  versions of standard modules.)"
AnswerKenntnis:
Exception else clause:

try:
  put_4000000000_volts_through_it(parrot)
except Voom:
  print "'E's pining!"
else:
  print "This parrot is no more!"
finally:
  end_sketch()


The use of the else clause is better than adding additional code to the try clause because it avoids accidentally catching an exception that wasnΓÇÖt raised by the code being protected by the try ... except statement.

See http://docs.python.org/tut/node10.html
AnswerKenntnis:
Re-raising exceptions:

# Python 2 syntax
try:
    some_operation()
except SomeError, e:
    if is_fatal(e):
        raise
    handle_nonfatal(e)

# Python 3 syntax
try:
    some_operation()
except SomeError as e:
    if is_fatal(e):
        raise
    handle_nonfatal(e)


The 'raise' statement with no arguments inside an error handler tells Python to re-raise the exception with the original traceback intact, allowing you to say "oh, sorry, sorry, I didn't mean to catch that, sorry, sorry."

If you wish to print, store or fiddle with the original traceback, you can get it with sys.exc_info(), and printing it like Python would is done with the 'traceback' module.
AnswerKenntnis:
Main messages :)

import this
# btw look at this module's source :)




De-cyphered:


  The Zen of Python, by Tim Peters    
  
  Beautiful is better than ugly.
  Explicit is better than implicit.
  Simple is better than complex.
  Complex is better than complicated.
  Flat is better than nested.
  Sparse is better than dense.
  Readability counts.
  Special cases aren't special enough to break the rules.
  Although practicality beats purity.
  Errors should never pass silently.
  Unless explicitly silenced.
  In the face of ambiguity, refuse the temptation to guess.
  There should be one-- and preferably only one --obvious way to do it.
  Although that way may not be obvious at first unless you're Dutch.
  Now is better than never.
  Although never is often better than right now.
  If the implementation is hard to explain, it's a bad idea.
  If the implementation is easy to explain, it may be a good idea.
  Namespaces are one honking great idea -- let's do more of those!
AnswerKenntnis:
Interactive Interpreter Tab Completion

try:
    import readline
except ImportError:
    print "Unable to load readline module."
else:
    import rlcompleter
    readline.parse_and_bind("tab: complete")


>>> class myclass:
...    def function(self):
...       print "my function"
... 
>>> class_instance = myclass()
>>> class_instance.<TAB>
class_instance.__class__   class_instance.__module__
class_instance.__doc__     class_instance.function
>>> class_instance.f<TAB>unction()


You will also have to set a PYTHONSTARTUP environment variable.
AnswerKenntnis:
Nested list comprehensions and generator expressions:

[(i,j) for i in range(3) for j in range(i) ]    
((i,j) for i in range(4) for j in range(i) )


These can replace huge chunks of nested-loop code.
AnswerKenntnis:
Operator overloading for the set builtin:

>>> a = set([1,2,3,4])
>>> b = set([3,4,5,6])
>>> a | b # Union
{1, 2, 3, 4, 5, 6}
>>> a & b # Intersection
{3, 4}
>>> a < b # Subset
False
>>> a - b # Difference
{1, 2}
>>> a ^ b # Symmetric Difference
{1, 2, 5, 6}


More detail from the standard library reference: Set Types
