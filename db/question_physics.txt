QuestionKenntnis:
Cooling a cup of coffee with help of a spoon
qn_description:
During the breakfast with my colleagues, a question popped into my head:

What is the fastest method to cool a cup of coffee, if your only available instrument is a spoon?

A qualitative answer would be nice, but if we could find a mathematical model or even better make the experiment (we don't have the means here :-s) for this it would be great! :-D

So far, the options that we have considered are (any other creative methods are also welcome):

Stir the coffee with the spoon

Pros:


The whirlpool has greater surface than the flat coffee, so it is better for heat exchange with the air.
Due to the difference of speed between the liquid and the surrounding air, the Bernoulli effect should lower the pressure and that would cool it too to keep the atmospheric pressure constant.


Cons:


Joule effect should heat the coffee.


Leave the spoon inside the cup

As the metal is a good heat conductor (and we are not talking about a wooden spoon!), and there is some part inside the liquid and other outside, it should help with the heat transfer, right?

A side question about this is what is better, to put it like normal or reversed, with the handle inside the cup? (I think it is better reversed, as there is more surface in contact with the air, as in the CPU heat sinks).

Insert and remove the spoon repeatedly

The reasoning about this is that the spoon cools off faster when it's outside.

(I personally think it doesn't pay off the difference between keeping it always inside, as as it gets cooler, the lesser the temperature gradient and the worse for the heat transfer).
AnswersKenntnis
AnswerKenntnis:
I did the experiment. (dipping wins)


H2O ice bath  
canning jar  
thermometer 
pot of boiling water
stop watch


There were four trials, each lasting 10 minutes.  Boiling water was poured into the canning jar, and the spoon was taken from the ice bath and placed into the jar.  A temperature reading was taken once a minute.  After each trial the water was poured back into the boiling pot and the spoon was placed back into the ice bath.




 Method:                  Final Temp.
 1. No Spoon              151 F 
 2. Spoon in, no motion   149 F
 3. Spoon stirring        147 F
 4. Spoon dipping         143 F


Temperature Readings have an error $\pm$ 1 F, 



 Red line, no Spoon
 Green line, Spoon in, no motion
 Aqua line, Stirring
 Blue line, Dipping




Data:

No Spoon       

Min Temp (F)
0   182
1   180
2   174
3   171
4   168
5   164
6   161
7   158
8   155
9   153
10  151

Spoon, no stir, no dip       

Min Temp (F)
0   180
1   175
2   172
3   168
4   165
5   162
6   160
7   156
8   153
9   151
10  149

Stirring

Min Temp (F)
0   180
1   175
2   171
3   167
4   164
5   161
6   158
7   155
8   152
9   150
10  147

Dipping

Min Temp (F)
0   180
1   177
2   173
3   168
4   164
5   160
6   156
7   152
8   149
9   146
10  143
AnswerKenntnis:
Stirring will win, hands down, every time.

This is why physicists need to talk to chemists once in a while.

As Georg correctly remarks, the latent heat of vaporization of water is enormous - but he's wrong about waving the spoon; stirring is the champion here.

Why? Temperature is really the average kinetic energy of the molecules in the bulk substance, which actually have a variety of individual kinetic energies. Stirring is the fastest way to bring high-kinetic-energy outlier water molecules to the surface, where they will overcome the electrostatic bonding mechanisms that keep them in the liquid phase, and jump into the air (vapor phase). This rapid decrease in the high-energy outliers is the quickest way to cool a hot aqueous solution.

It's similar to stirring iced tea. If you just plop ice cubes into a glass of warm tea, it will take quite a while for the warmer tea to cool; if you stir it vigorously, it will reach a cold equilibrium within seconds; the latent heat of fusion absorbed by the ice melting is similarly enormous.

This kind of thing has a lot of applications to laboratory and industrial chemical processes, surface catalysis, petroleum cracking, yadda yadda. You learn a lot about it in third-year university physical chemistry, and really must master it before or during graduate work as a chemist.

If you want an even faster way to cool a cup of coffee, here's a tip from my Granddad Parker: forget the spoon and saucer your coffee. In other words, pour the top part of it from the cup into a saucer, and then back again a few times. The large and constantly changing surface area during this process will cause extremely rapid evaporation of those high-energy outliers, much faster than stirring. Saucering was very common up through the Great Depression, which is one of the reasons older coffee sets always included saucers. You also get deep-ish saucers at many restaurants as a holdover from this practice, although I doubt many people do it any more.
AnswerKenntnis:
How to cool a cup of coffee with the help of a spoon. Hmm...


Empty the cup using the spoon, discarding the hot coffee.
Strike the cup with the spoon, shattering it and forcing it to release the hot coffee.
Drink the coffee with the spoon.
Use the spoon to carve a cup-shaped hole in a large block of ice, put the cup in there.
Put the spoon in the coffee, attach it to the cold side of a small thermoelectric-effect element.
Use the spoon to catapult the cup 200km up. Measure temperature before it comes back down.
Use the spoon to add a spoonful of liquid helium to the coffee. Slowly.


I'm sure you can come up with some good ones (edit away!)
AnswerKenntnis:
Take the spoon.  Use it to push the coffee over.  It will spill out into a large puddle.  This has maximal surface area and will cool VERY quickly.
AnswerKenntnis:
With respect to the content in the cup, all Your hampering 
with the spoon is irrelevant. Cooling of a hot coffee is 
achieved by vaporisation of water. At temperatures between 
100 and say 50 ┬░C the vapor pressure is so big, that the heat 
carried away by convection of the hot (and much less dense 
than air!) vapor dominates all other heat transfer mechanisms. 
(ca 540 cal/g heat of vaporisation!) 

So, because You do not "allow/accept" blowing the surface of the coffee, 
the second best is to wave the spoon over the surface .
Blowing/waving will enhance the vaporisation due to quick replacement 
of the vapour on top of the surface with fresh, cool, dry air.
AnswerKenntnis:
I can't believe the density or material of the spoon hasn't been considered. If the spoon is very dense you can take it and wave it in the air in a 15┬░ arc and say, "Dear waiter, if you don't put some cold milk in my coffee I will hit you between the eyes with this abnormally dense spoon". 

On the other hand, if its made of gold or silver you hold it horizontal to the ground in a north south direction such that it refracts the light and say, "Dear waiter, here I have a brilliant and valuable spoon which I will give you in exchange for placing my coffee cup in a bath of ice til it cools"
AnswerKenntnis:
Well, if you are only allowed to use a spoon, the fastest way to cool the coffee for drinking is to get a spoonful, blow on it, drink it from the spoon, take a next spoonful. Convection does wonders.

If you are allowed a saucer instead of a spoon, pour a bit of coffee in the saucer, blow on it and drink it.
AnswerKenntnis:
The answer may depend slightly on the humidity in the room (as that will determine the evaporative cooling rate), but basically your best bet is to increase the surface area of your coffee as much as possible and increase the rate of airflow over the coffee as much as possible (so that the local gradient of partial pressure of water vapor is as steep as possible).  I suspect that your best bet would be to pick up spoonfuls of coffee and pour them back into the cup from as high as you can manage without splattering the coffee all over the place--the stream of falling liquid has a high surface to volume ratio and is traveling quickly.
AnswerKenntnis:
The fastest and coolest way to cool the coffee, with only a cup and spoon, that is also theoretically possible, is to throw all the coffee up in the air, and with somewhat well-coordinated movement catch it all in the cup as it falls down. This maximises the total surface area of the coffee with the air per time, and thus also the total heat transfer.
AnswerKenntnis:
The fastest method for cooling coffee (or hot chocolate, as I'm more likely to drink) I've discovered when I don't have a saucer or second cup is to ignore the spoon altogether.

Without a saucer, spoon, or blowing, I first place my hands around the container without any insulation device and let my hands absorb as much heat as they can stand. I then move my hands to the table to transfer the heat to the table. I then repeat the cupping the cup and transferring to the table as necessary. Within 1-2 minutes I can take a 200+ degree cup down to pleasantly drinkable levels.

This method is all about conducting the heat out of the container to the environment, and has the convenient secondary benefit of warming your hands (and the rest of your body) very quickly while waiting for your drink to cool. Combine it with a swishing of the cup to bring down the overall temperature of the coffee (not just the sides), as well as blowing on the surface (if you are allowed to cheat), and the cup cools even faster.

This works best with glass or metal tables, but wood tables also work well.
AnswerKenntnis:
Pre-cool the spoon first (in the freezer, or in your kitchen thermos of liquid nitrogen :) and put it in the cup. Periodically repeat the process with new spoons. Use a silver spoon (some of us were born with one in our mouths.)

Note: If you allow someone to take the spoon in and out of the coffee, the question allows much too much freedom of action--there is nothing in the bounds of the question to rule out cooling the spoon either then---hence, my answer, which was meant ironically, not seriously, for those who have trouble determining the difference...
AnswerKenntnis:
Using the spoon lift some coffee into the air, and let it pour back into the cup. A nice long slow pour is ideal (as high as you can without splashing).

The motion of the water through the air will cool it fastest.

The bigger the spoon the faster it works.

In the real world, to cool coffee fast, get a second cup and pour from one to the other (again, as high as you can). Three pours should be enough to get it from burning hot to drinkable.
AnswerKenntnis:
There's also a significant boundry layer on the liquid side, similar to the Sea surface microlayer

The organization of the surface of water is an impediment to the diffusion of gases. For example, an unstirred (~5 ml) oximeter cell will take about 10 minutes to equilibrate with atmospheric oxygen, while a stirred cell will equilibrate in under a minute. 

Surface foam, or a monolayer of fatty molecules, such as might be formed by coffee oils or creamer, likely has insulative value. Stirring would minimize that by disrupting the structure.
AnswerKenntnis:
This question is a classic. On related topics, you can read:


Factors influencing cooling of coffee and tea, 1988
Wolfram simulator


I am pretty sure I read a paper in Scientific American about this, but I cannot find it back
AnswerKenntnis:
Strange question :)

I would say use the room temperature, lift the coffee with the soon and drop it back to the cup repeatedly, this will make a part of the coffee be in contact with the cold room hence getting colder and mixing with the hot one bringing down the overall temperature.

I have (obviously??) no mathematical / physical knowledge about how effective this technique might be, and besides I drink the coffee as hot as I can ;P

Good luck!
AnswerKenntnis:
Move the cup.  One of the main ways to cool something down is to ensure hot particles that have risen off the surface of the drink don't fall back on it again.
AnswerKenntnis:
Stir the coffee with the spoon and blow air into the cup with your mouth. Every few seconds change the direction of the stir (clockwise to counterclockwise and vice-versa). This should cool it quickly than just stirring the coffee with the spoon.
AnswerKenntnis:
By leaving the spoon stationary in the coffee, you are only benefiting from the thermal conduction and dissipation of the spoon.

By stirring the coffee, you are not only helping to keep the hottest parts of the coffee in contact with the air & cup, increasing the surface area by introducing abnormalities in the top surface, and helping to move the warm, moist air atop the coffee, but you are also getting all of the benefits of the thermal conduction and dissipation of the spoon.

By dunking the spoon repeatedly, you are not only introducing air into the liquid, which will grab lots of heat and be whisked away as soon as it hits the surface, but you are also getting all of the benefits of the thermal conduction and dissipation of the spoon and all of the benefits of stirring the coffee.

Ignoring the fact that you will probably splash the coffee and make a mess of the counter, the correct answer is repeatedly dunking the spoon.  Also, you and your coworkers are nerds!
AnswerKenntnis:
I stir the coffee in a side to side motion with the spoon bowl parallel to the side of the cup. Due to the angle bernoulli's principle moves the coffee through the center of the cup towards the top. This circulates the liquid much more quickly than you'd expect in a controllable fashion that avoids splashing vigorous stirring can cause. It also has the side effect of removing the dry creamer from the bowl of the spoon far more efficiently than any other possible motion. The liquid coming up from the center and being forced across the top to the sides smoothly and without cavitation gaps stirring causes. The liquid also conducts with the sides better and releases heat both via the wave that forms at the top increasing the surface area contact with air and transferring the heat to the body of the cup/heat radiator.
Actually I do stir that way. It cleans the spoon far better and requires less effort. Other than that the above is merely serendipitous inference.
QuestionKenntnis:
How does light bend around my finger tip?
qn_description:
When I close one eye and put the tip of my finger near my open eye, it seems as if the light from the background image bends around my finger slightly, warping the image near the edges of my blurry finger tip.

What causes this? Is it the heat from my finger that bends the light? Or the minuscule gravity that the mass in my finger exerts? (I don't think so.) Is this some kind of diffraction?



To reproduce: put your finger about 5 cm from your open eye, look through the fuzzy edge of your finger and focus at something farther away. Move your finger gradually through your view and you'll see the background image shift as your finger moves.



For all the people asking, I made another photo. This time the backdrop is a grid I have on my screen (due to lack of grid paper). You see the grid deform ever so slightly near the top of my finger. Here's the setup:



Note that these distances are arbitrary. It worked just as well with my finger closer to the camera, but this happens to be the situation that I measured.



Here are some photos of the side of a 2 mm thick flat opaque plastic object, at different aperture sizes. Especially notice hoe the grid fails to line up in the bottom two photos.
AnswersKenntnis
AnswerKenntnis:
OK, it seems that user21820 is right; this effect is caused by both the foreground and the background objects being out of focus, and occurs in areas where the foreground object (your finger) partially occludes the background, so that only some of the light rays reaching your eye from the background are blocked by the foreground obstacle.

To see why this happens, take a look at this diagram:



The black dot is a distant object, and the dashed lines depict light rays emerging from it and hitting the lens, which refocuses them to form an image on a receptor surface (the retina in your eye, or the sensor in your camera).  However, since the lens is slightly out of focus, the light rays don't converge exactly on the receptor plane, and so the image appears blurred.

What's important to realize is that each part of the blurred image is formed by a separate light ray passing through a different part of the lens (and of the intervening space).  If we insert an obstacle between the object and the lens that blocks only some of those rays, those parts of the image disappear!



This has two effects: first, the image of the background object appears sharper, because the obstacle effectively reduces the aperture of the lens.  However, it also shifts the center of the aperture, and thus of the resulting image, to one side.

The direction in which the blurred image shifts depends on whether the lens is focused a little bit too close or a little bit too far.  If the focus is too close, as in the diagrams above, the image will appear shifted away from the obstacle.  (Remember that the lens inverts the image, so the image of the obstacle itself would appear above the image of the dot in the diagram!)  Conversely, if the focus is too far, the background object will appear to shift closer to the obstacle.

Once you know the cause, it's not hard to recreate this effect in any 3D rendering program that supports realistic focal blur.  I used POV-Ray, because I happen to be familiar with it:




Above, you can see two renderings of a classic computer graphics scene: a yellow sphere in front of a grid plane.  The image on the left is rendered with a narrow aperture, showing both the grid and the sphere in sharp detail, while the one on the right is rendered with a wide aperture, but with the grid still perfectly in focus.  In neither case does the effect occur, since the background is in focus.

Things change, however, once the focus is moved slightly.  In the image on the left below, the camera is focused slightly in front of the background plane, while in the image on the right, it is focused slightly behind the plane:




You can clearly see that, with the focus between the grid and the sphere, the grid lines close to the sphere appear shifted away from it, while with the focus behind the grid plane, the grid lines shift towards the sphere.

Moving the camera focus further away from the background plane makes the effect even stronger:




You can also clearly see the lines getting sharper near the sphere, as well as bending, because part of the blurred image is blocked by the sphere.

I can even recreate the broken line effect in your photos by replacing the sphere with a narrow cylinder:




To recap: This effect is caused by the background being (slightly) out of focus, and by the foreground object effectively occluding part of the camera / eye aperture, causing the effective aperture (and thus the resulting image) to be shifted.  It is not caused by:


Diffraction: As shown by the computer renderings above (which are created using ray tracing, and therefore do not model any diffraction effects), this effect is fully explained by classical ray optics.  In any case, diffraction cannot explain the background images shifting towards the obstacle when the focus is behind the background plane.
Reflection: Again, no reflection of the background from the obstacle surface is required to explain this effect. In fact, in the computer renderings above, the yellow sphere / cylinder does not reflect the background grid at all.  (The surfaces have no specular reflection component, and no indirect diffuse illumination effects are included in the lighting model.)
Optical illusion: The fact that this is not a perceptual illusion should be obvious from the fact that the effect can be photographed, and the distortion measured from the photos, but the fact that it can also be reproduced by computer rendering further confirms this.




Addendum: Just to check, I went and replicated the renderings above using my old dSLR camera (and an LCD monitor, a yellow plastic spice jar cap and some thread to hang it from):




The photo above on the left has the camera focus behind the screen, the one on the right has it in front of the screen.  The photo below on the left shows what the scene looks like with the screen in focus (or as close as I could get it with manual focus adjustment).  Finally, the crappy cellphone camera picture below on the right shows the setup used to take the other three photos.






Addendum 2: Before the comments below were cleaned out, there was some discussion there about the usefulness of this phenomenon as a quick self-diagnostic test for myopia (nearsightedness).

While I Am Not An Opthalmologist, it does appear that, if you experience this effect with your naked eye, while trying to keep the background in focus, then you may have some degree of myopia or some other visual defect, and may want to get an eye exam.

(Of course, even if you don't, getting one every few years or so isn't a bad idea, anyway.  Mild myopia, up to the point where it becomes severe enough to substantially interfere with your daily life, can be surprisingly hard to self-diagnose otherwise, since it typically appears slowly and, with nothing to compare your vision to, you just get used to distant objects looking a bit blurry.  After all, to some extent that's true for everyone; only the distance varies.)

In fact, with my mild (about −1 dpt) myopia, I can personally confirm that, without my glasses, I can easily see both the bending effect and the sharpening of background features when I move my finger in front of my eye.  I can even see a hint of astigmatism (which I know I have; my glasses have some cylindrical correction to fix it) in the fact that, in some orientations, I can see the background features bending not just away from my finger, but also slightly sideways.  With my glasses on, these effects almost but not quite disappear, suggesting that my current prescription may be just a little bit off.
AnswerKenntnis:
Contrary to some of the answers people have posted on Yahoo Answers (like here and here) and other places, this is not caused by diffraction.

To show this, note that the bending effect can roughly be modeled as the diffraction pattern due to light incident on the edges of an opaque object. As explained by Rod Vance, the intensity profile on a screen at height $x$ due to a flat object a distance $d$ from the screen is given by 
$$I(x)\propto\left| C\left(\sqrt{\frac{k}{2d}}
   x\right)+i S\left(\sqrt{\frac{k}{2d}}
   x\right)+\left(\frac{1}{2}+\frac{i}{2}\right)\right|^2$$
where $C$ and $S$ are the FresnelC and FresnelS functions, and $k=2\pi/\lambda$ is the wavenumber of the light. 

Using $d=5\text{cm},\lambda=600\text{nm}$, this gives



This indicates that there's a spread of roughly $0.05\text{mm}$ to $0.1\text{mm}$. This is a very small distance, roughly the same as the thickness of a sheet of paper, and far smaller than the quite visible sag towards the finger present in the $2^\text{nd}$ blue line on the paper background. So while diffraction may play a small role, it seems doubtful that it's the dominant role.

Additional evidence against it being due to diffraction come from considering chromatic effects. The bending is strongly $\lambda$-dependent, with red light bent more strongly than blue light. If diffraction were the primary phenomenon responsible, you'd expect to see a rainbow-effect at the edges of your finger, in which the light from the paper gets bent by different angles depending on wavelength. However, this is not observed.

Also (and probably the most important point!), as rob pointed out in his answer, diffraction would cause the blue lines behind the finger to appear to bend upwards, but instead they appear to bend downwards.

I'd guess that some sort of geometric factors (perhaps with the camera, lenses, etc?) play the primary role here, but I'll await the judgement of people who know more about optics than I do.
AnswerKenntnis:
Haha when I was young I thought that this effect was due to gravity, which is of course too weak for tiny objects to be observable. But it turns out that it is neither refraction nor diffraction nor parallax error. Instead it is due to having the wrong focus. If you are short-sighted like I am, then when looking at a far object each point will generate a circular disk image on your retina instead of a sharp point. When you move the edge of any object near your eye and it blocks part of the pupil, then the image generated on your retina will no longer be a full disk, hence the image appears to shift away from the edge. This explains your four later grid images. Notice that the rest of the grid is never in sharp focus, but the region near the edge of the occluding object is sharper, which is because those points generated less than a full disk on the camera sensor plane. As for your earlier images, they were due to the focus being beyond the object you were looking at. As before, each point would result in a disk image on your retina, but inverted. Thus when another object blocks part of your pupil, the image appears to shift towards the edge instead of away from it.

To prove that this explanation is the correct one, put your face close to the screen without focusing on it, and move your finger in front of your eye. The image on the screen will appear to move towards your finger and also become sharper in the direction perpendicular to your finger's edge. Now repeat this experiment with the screen at a comfortable reading distance and make sure you focus exactly on the screen. Blocking your view now should have no effect on the apparent position of each pixel on the screen. If it still seems to move, it is because the blur (non-focused) image of your finger is interfering in an optical illusion. To prevent that, you can use a black thread instead. And if you focus somewhere in front of the screen, the image on the screen would appear to move away from your finger. If you use a black thread at an oblique angle to a grid on the screen, there will not be mismatching grid lines on the two sides of the thread if you focus exactly on the screen, but if you don't focus exactly on the screen the grid lines will indeed mismatch.
AnswerKenntnis:
It seems to me this is actually a case of parallax, for the most part. Your retina and camera CCD are not point sinks. They are an array of point sinks. If you sum the point sinks (photo receptors) over the surface of each of these imaging sensors you will achieve this exact effect. You can demonstrate this by holding your finger farther from your face (to account for the larger distance between sensors), focus on something in the background (as required with your one eye example), and mentally (or digitally with a camera) overlaying those two images. You will get a solid overlap where both eyes see the same image and a fuzzy edge where the view is different.
Try holding your finger even closer to you face, the blur around it gets larger because it's parallax and the effect is increased with smaller distances.
AnswerKenntnis:
Edited:  Ilmari Karonen has posted a complete and convincing answer to this question.  I'm leaving this answer up, despite downvotes, because it contained useful hints: the effect was inconsistent with diffraction of light around obstructions, and had something squishy to do with imperfect focus in a non-ideal optical system.

I think it's interesting that distorted part of the line is nearer to your fingertip than the undistorted part.  That suggests it isn't diffraction.  If diffraction were permitting light from the blue line to travel over the top of your fingertip, those rays would enter your eye with a slight downward angle, compared to the undeflected light on either side.  That would make the image of the distorted part of the line appear slightly higher.  If you have light reflecting from your fingertip, on the other hand, you'll see the line bend down.  Here's a cartoon:



This raises the question of why you don't see both a direct and reflected image; I suspect that the angular separation is small enough that the camera's focus can merge the two images, but I don't have a good model for that yet.
AnswerKenntnis:
There are many things happening which could have an influence on what you see (what kinds of distortion happen). Some have been touched on (or more fully discussed) in other answers, so I won't go into detail:


Aperture and focus effects
Optical system aberrations (low quality lens in the eye or camera)
Diffraction around an object
Your skin (in fact, your whole finger except for the bone) isn't quite opaque -- a little light gets through it, being bent in the process. Using an object like a piece of sheet metal or pencil would avoid this.
Your finger is likely a bit warmer than the surrounding air. Warmer air is less dense, and so would slightly bend light away from your finger. There may also be convective currents affecting the light further away from the finger, depending on orientation. Using an object that is at the same temperature as the air would help here.
Light being bent by the mass of your finger (a very, very tiny effect that will probably never be measurable. Read about the measurement of a star's light being bent by the Sun, during a 1919 solar eclipse, that was one of the first confirmations of Einstein's General Theory of Relativity. It was a quite small angle.)


All of these things (and possibly more that I can't think of at the moment), in roughly descending order, are affecting the image and have to be accounted for.
AnswerKenntnis:
That is diffraction and arises from the wave nature of light. Light does bend around objects very slightly (depending on the wavelength, which is pretty tiny for visible light) but because the diffraction effect is very small you only see this when your finger tip is very close to your eyes.
QuestionKenntnis:
A mirror flips left and right, but not up and down
qn_description:
Why is it that when you look in the mirror left and right directions appear flipped, but not the up and down?
AnswersKenntnis
AnswerKenntnis:
Here's a video of physicist Richard Feynman discussing this question.

Imagine a blue dot and a red dot.  They are in front of you, and the blue dot is on the right.  Behind them is a mirror, and you can see their image in the mirror.  The image of the blue dot is still on the right in the mirror.

What's different is that in the mirror, there's also a reflection of you.  From that reflection's point of view, the blue dot is on the left.

What the mirror really does is flip the order of things in the direction perpendicular to its surface.  Going on a line from behind you to in front of you, the order in real space is 


Your back
Your front
Dots
Mirror


The order in the image space is


Mirror
Dots
Your front
Your back


Although left and right are not reversed, the blue dot, which in reality is lined up with your right eye, is lined up with your left eye in the image.  

The key is that you are roughly left/right symmetric.  The eye the blue dot is lined up with is still your right eye, even in the image.  Imagine instead that Two-Face was looking in the mirror. (This is a fictional character whose left and right side of his face look different.  His image on Wikipedia looks like this:)



If two-face looked in the mirror, he would instantly see that it was not himself looking back!  If he had an identical twin and looked right at the identical twin, the "normal" sides of their face would be opposite each other.  Two-face's good side is the right.  When he looked at his twin, the twin's good side would be to the original two-face's left.

Instead, the mirror Two-face's good side is also to the right.  Here is an illustration:





So two-face would not be confused by the dots.  If the blue dot is lined up with Two-Face's good side, it is still lined up with his good side in the mirror.  Here it is with the dots:



Two-face would recognize that left and right haven't been flipped so much as forward and backward, creating a different version of himself that cannot be rotated around to fit on top the original.
AnswerKenntnis:
Because they don't flip left with right (or up with down), they flip the 3D space you're standing in "inside out", so far from the mirror becomes far away inside the mirror and vice versa. A hand 1 meter from the mirror seems like it's 1 meter on the other side of the mirror but in the same spot with regards to left/right so nothing is flipped.

Wiggle your left hand - you'll see the hand which is to the left in the mirror wiggle. Wiggle your toes and the toes in the mirror image wiggle etc.
AnswerKenntnis:
This common confusion stems from our familiarity with photographs. We forget that we rotate them to face ourselves.

Take a picture of yourself and hold it up in front of you. Probably you are holding it so that you can see your image. If so, you "flipped" the image of yourself when you rotated it 180 degrees around the vertical axis. When you look to the left side of the photo, you are looking over the right shoulder of your image. These directions are flipped!

Now look in a mirror. When you look to the left, you are looking over the left shoulder of your image. These directions are not flipped!

Now pick up the picture again and turn it so it's facing the same direction you are facing. You have removed the 180 degree rotation so that you and your image are "looking" in the same direction. The left side of your image is again to your left. If the picture is transparent enough that you can see your image, you'll see not the back of your head, but your eyes, giving you the impression that you're looking back at yourself. A mirror image! But again, left and right are not flipped.

When you say the mirror "flips" left and right, you are speaking from the frame of reference of one who is used to the 180 degree rotation that you apply to view an opaque photograph. But that's what we all do because we consider photographs, rotated 180 degrees to face ourselves, as being the "correct" left-right orientation.

What a mirror really flips is the depth dimension. That which is behind you appears to be in front of you.
AnswerKenntnis:
Take a picture and look at it. Now turn the picture to face the mirror. Question one: who flipped the picture? Answer: you did. Now, face the picture back to you, and walk to the nearest refrigerator. Turn the picture to face the refrigerator. Wow! Refrigerators flip images too! Don't believe me? Take your flipped page and hold it up to a bright light. The image is flipped; no mirror required.

Now, most people will turn a page around the vertial axis when they want to face it away from themselves. However, you could flip the page around any axis you choose, as long as it's in the plane of the page. You could easily, for example, flip the page around the horizontal axis. If you still believe that mirrors flip images, you'll notice that you've now tricked the mirror into flipping the image top-to-bottom, not left-to-right. Flip the page around a diagonal axis, and you'll get a very different result.

Bottom line: mirrors don't flip images; people do.
AnswerKenntnis:
It's easier with images... The mirror doesn't flip left and right as you can see in the upper image. The so-called flip
occurs when somebody in the real world rotates 180 degrees about the vertical axis to see you face to face, as can be seen in the lower image.


Regards Hans
AnswerKenntnis:
First, lets separate the concepts; there is nothing that is "flipped" in the mirror image regarding one orientation more than others. the full group of transformations $O(3)$ includes transformations where $det(R) = -1$. You can consider the following transformations examples of this:

1) they have one random direction flipped in sign, or

2) for the special case where $D= 2n+1$, you can flip all direction signs

moreover, all these rotations are equivalent to each other, that is, any of them is equivalent to any other by a normal rotation $det(R) = 1$

explained in other way, the mirror image is also equivalent to your image with up and down flipped. You only think in the equivalent with the right-left flipped version because its easier to build in your mind. That might be related with the fact that our bodies are almost right-left symmetrical, but very poorly up-down symmetrical
AnswerKenntnis:
Here's how I explain it (this is pretty similar to most of the other answers).

Assume the mirror is hanging vertically on a wall, and you're standing upright and facing it, looking at your own reflection.  (Just to make the assumptions explicit.)  And let's assume that you're facing north, and wearing a watch on your left wrist.

The mirror doesn't flip left and right; it flips forward and back.  Your front is farther north than your back, but your reflection's front is farther south than its back.  Your feet and the reflection's feet both face down.  The wrist with the watch is on the west side, both for you and your reflection.

So why do we think that the mirror flips left and right?  I think it's because we mentally map the (mathematically simple but physically awkward) transformation to something that makes physical sense.  The "simple" front-to-back reversal is mentally mapped to (a) a 180-degree rotation (something that people do all the time), and (b) a left-to-right reversal (which people don't do, but since we're pretty much bilaterally symmetric, it doesn't seem as strange).  So the person you see in the mirror looks like a normal person who happens to wear his watch on his "right" wrist and part his hair on the other side.

If we weren't bilaterally symmetric, we might see it differently.
AnswerKenntnis:
Imagine a mirror placed on the floor. Your face, looking at it, will be pointed downwards; but the face "in the mirror" is "looking" up.

The other answers here are great ΓÇö┬áI just wanted to make sure we discard the assumption that the mirror is placed on a wall!
AnswerKenntnis:
Think about where a point above, below, left, and right of your point of view are in the reflection.  Your head is still on top, your feet still on the bottom in the mirror.  Likewise, your left hand is still to the left and your right hand to the right.  It seems flipped because, to look behind you, you are used to turning around (which swaps left/right), rather than flipping (which swaps top/bottom).  If we flipped upside down instead of turning around to see behind us, you would experience the opposite effect.
AnswerKenntnis:
Because your eyes are set left and right and how you relate to the mirror.  

When you turn a picture to reflect in a mirror, you usually turn it left or right.  If you turned it vertically, the up and down reflection will be opposite, but left and right will still be the same.
AnswerKenntnis:
Left and right is relative to you only but not to the rest of your environment (including the mirror), but up and down is static for the whole environment including you and the mirror.
AnswerKenntnis:
All you have to do is imagine the line y=x+1 on the cartesian plane. If you reflect the line across a mirror in the vertical direction (across the y-axis), then the line becomes y=-x+1. Therefore, a vertical mirror flips the line's horizontal coordinates.

However, if you reflect the lines across a mirror is in the horizontal direction (across the x-axis), then the line becomes y=-x-1. A horizontal mirror flips the line's vertical coordinates.

The mirror just flips the axis perpendicular to its orientation!
AnswerKenntnis:
I can't resist: Here are two real experiments I came up with to explore the assertion that mirrors invert left-to-right but not up-to-down. Fans of the scientific method (and of Douglas Adams) know that the first step in any good experiment is to figure out exactly what the question (or hypothesis) really is. So, if you like having "obvious" assumptions shaken up a bit, please read on!


On an index card, write your name or some text in large block letters. Next, use one hand to hold it just in front of you as if it were a name badge at a meeting. With your other hand, hold a mirror in front of you so that you can read your "name badge." The text will look inverted left-to-right, as expected, while up-and-down looks normal, also as expected. Next, move the mirror upwards until it is straight above you, while tilting the bottom of the card out just enough so you can read it in the mirror. Result: You will still see the card text reversed left-to-right, as before. But think for a second: You are also seeing yourself standing on your head. That's top-to-bottom inversion, so you are getting both types of inversion at once! So, is the lack of vertical inversion really inherent in the mirror question, or have you just been holding the mirror in the wrong place all these years? And if so, why does the choice of where to place the mirror give one inversion in one case, and two in the other?
In the second experiment, hold the same card at arms length in front of you. Have it facing towards you this time so you an easily read it. Next, use your other arm to position a small mirror to the left or right of your line of sight of the card, and adjust the mirror until you can see the card in it. You will see the card text reversed left-to-right, just as with a face-on mirror. Now start moving the mirror in a circular arc until it is above the line of sight instead of to the left or right, keeping the reflected image of the card visible as you do so. When the mirror reaches the top, what do you see? Probably not what you expected: The text now has normal left-to-right order, but each letter is flipped upside down! In other words, the missing "vertical inversion" that everyone worries about in mirror experiments is right there in front of you. If you don't believe me, try it; there's no ambiguity in the effect. And if you want something more to ponder, try to figure out exactly when and how the vertical-is-normal, left-to-right-is-flipped image of the text transforms into the vertical-is-flipped, left-to-right-is-normal image during the rotation of the mirror. You can watch every step of the process as it occurs!


So, with these two experiments, it's worth asking: Is it really true that mirrors always reverse left-to-right? The earlier answer that references Feynman's video discusses this same point nicely, so be sure to look at that if you have not already. I'll likely comment more myself at a later time. But for now, I just wanted to provide a couple of easy, fast experiments you can try yourself, ones that may make you ponder just what the question really is. (The answer of course is 42 -- that much we already know!)
AnswerKenntnis:
The mirror is not inverting the image in either direction but rather is acting like a rubber stamp doing a 1:1 transfer from surface to surface. If you wrote a word on a rubber stamp with fresh ink, then pressed the stamp to paper, the word on the paper would be backwards but not upside down. It's just a 1:1 transfer.
AnswerKenntnis:
Try this:


Make a paper cut-out of the letter F (or any easy-to-cut-out letter which is different from its horizontally and vertically flipped images). Shade or mark one side of the cut-out to distinguish it from the other. Get a second piece of paper and write the same letter on it several times.
Hold the cut-out in front of you while standing in front of a mirror.  The real cut-out and the image will both appear in the normal orientation, but (due to the shading/mark on one side) it will be obvious that the direct view and the mirror view show opposite sides of the same object.
Hold the paper in front of you so that you can read it normally.  You won't be able to read it in the mirror - only the backside will be visible.  Flip it so that you can read it in the mirror.  Which way did you turn it?  Chances are that you flipped the paper horizontally, and the letter now appears to be flipped horizontally.  You could also flip it vertically, and the letter would appear to be flipped vertically.  


The flipping came from you.  All the mirror is doing is allowing you to see the opposite side of an object which is between you and the mirror.  For the cutout, the opposite side has the same shape, so you don't do any flipping and the image appears normal.  For the paper, the opposite side isn't interesting, and you have to turn the interesting side to face the mirror. 

A more real-world situation is seeing a license plate in a mirror.  I you want to instead view the license plate directly, either you must turn to face it (if it's behind you) or it must turn to face you (if it's in front of you). Both of these turns would typically be about the vertical axis, because it's hard to rotate your head or car about the horizontal axis.
AnswerKenntnis:
A Mirror flips the image front to back, not left to right. In the mirror your nose is in front of your face, but in reality it should be behind your face. Because your image is flipped front-to back it just so happens to appear from our common sense perspective that left and right are actually flipped. The L-R flip is an illusion symptomatic of the front to back flip.

This makes sense from the perspective of symmetry groups however the wiki article on molecular symmetry groups is much more complete. 


  Rotation-reflection axis: an axis around which a rotation by  $\tfrac{360^\circ} {n}$ , followed by a reflection in a plane perpendicular to it, leaves the molecule unchanged. Also called an n-fold improper rotation axis, it is abbreviated Sn. Examples are present in tetrahedral silicon tetrafluoride, with three S4 axes, and the staggered conformation of ethane with one S6 axis.


Essentially the front to back reflection is indistinguishable from a rotation.
AnswerKenntnis:
The simplest, but probably the only correct unswer is as follows:

"Left" and "Right" is determined WITH RESPECT TO "Up" and "Down".


Once you don't know what is "Up" (and/or "Down") you don't know what
is "Right" and "Left". For instance, when you see only wheels of a car, but not it's top and bottom, you cannot say which tire is left and which is right.
Alternatively, one can identify Up (U) and Down (D) if he/she knows
Right (R) and Left (L).


Hence, when you say that mirror "flips" R and L, you ASSUME that it does not "flip" U and D.

If you will assume that mirror does not "flip" R and L, you will come to conclusion that it flips U and D.
QuestionKenntnis:
Why don't metals bond when touched together?
qn_description:
It is my understanding that metals are a crystal lattice of ions, held together by delocalized electrons, which move freely through the lattice (and conduct electricity, heat, etc.). 

If two pieces of the same metal are touched together, why don't they bond? 

It seems to me the delocalized electrons would move from one metal to the other, and extend the bond, holding the two pieces together. If the electrons don't move freely from one piece to the other, why would this not happen when a current is applied (through the two pieces)?
AnswersKenntnis
AnswerKenntnis:
I think that mere touching does not bring the surfaces close enough. The surface of a metal is not perfect usually. Maybe it has an oxide layer that resists any kind of reaction. If the metal is extremely pure and if you bring two pieces of it extremely close together, then they will join together. It's also called cold welding.

For more information:


What prevents two pieces of metal from bonding?
Cold Welding
AnswerKenntnis:
They do, as Feynman said. If you have two copper pieces perfectly polished and you put them in contact, they will weld automatically (the copper atoms won't know what piece they belonged to). 

But in real life, oils, oxides and other impurities don't allow this process.

Found it! Read Feynman's own words:


  If
  we try to get absolutely pure
  copper, if we clean and polish the
  surfaces, outgas the materials in a
  vacuum, and take every conceivable
  precaution, we still do not get $\mu$.
  For if we tilt the apparatus even to
  a vertical position, the slider will
  not fall offΓÇöthe two pieces of
  copper stick together! The
  coefficient , which is ordinarily
  less than unity for reasonably hard
  surfaces, becomes several times
  unity! The reason for this
  unexpected behavior is that when
  the atoms in contact are all of the
  same kind, there is no way for the
  atoms to ΓÇ£knowΓÇ¥ that they are in
  different pieces of copper. When
  there are other atoms, in the oxides
  and greases and more complicated
  thin surface layers of contaminants
  in between, the atoms ΓÇ£knowΓÇ¥
  when they are not on the same
  part. When we consider that it is
  forces between atoms that hold the
  copper together as a solid, it should
  become clear that it is impossible to
  get the right coefficient of friction
  for pure metals.
  
  The same phenomenon can be
  observed in a simple home-made
  experiment with a flat glass plate
  and a glass tumbler. If the tumbler
  is placed on the plate and pulled
  along with a loop of string, it slides
  fairly well and one can feel the
  coefficient of friction; it is a little
  irregular, but it is a coefficient. If
  we now wet the glass plate and the
  bottom of the tumbler and pull
  again, we find that it binds, and if
  we look closely we shall find
  scratches, because the water is able
  to lift the grease and the other
  contaminants off the surface, and
  then we really have a glass-to-glass
  contact; this contact is so good that
  it holds tight and resists separation
  so much that the glass is torn apart;
  that is, it makes scratches.


Source: http://www.feynmanlectures.caltech.edu/I_12.html
AnswerKenntnis:
I believe this is essentially what happens in gilding, owing to the special properties of gold (malleability and lack of corrosion).

Extremely flat surfaces can get stuck together due to Van der Waals forces as well as air pressure. I once accidentially stuck two quartz optical windows together, and had a hell of a time separating them.
AnswerKenntnis:
Two reasons:


Oxides
The roughness of the surface


If the surface is rough, then the majority of the surface is touching the air gap between the two, not the opposite surface. A bond may form at the touching "peaks", but it will be weak compared to the rest of the metal because a very small fraction of the surface has actually bonded.

In addition, metal surfaces adsorb oxygen and form oxides/oxygen monolayers on the surface. This is actually a visible process with metals like sodium and potassium (the color changes in a short time period). But for all metals, there still is oxide formation to a sufficient extent, because the edge metals have not completely fulfilled their valencies. Even a monolayer of adsorbed oxygen is enough to stop the surfaces from welding.

If two clean, flat metal surfaces are brought together (usually in a vacuum), they do indeed cold weld. This is hard to achieve for macroscopic objects because of the perfect flatness requirement, but is still possible. In practice, it is more commonly used for welding small things.
AnswerKenntnis:
Metals with perfectly clean surfaces WILL bond together just like you explained, but that isn't the case in real life because there is a thin layer of oxygen blocking the metal's surface. 

Much like how rust forms, thin layers of oxygen coat every metallic surface upon contact.
AnswerKenntnis:
I can't comment since I don't have the reputation for it, but I do have some relevant knowledge from my research in materials science.

To add to what DumpsterDoofus said, it is very easy for two pieces of glass or polymer to bond if you clean them extremely well and ionize the surface. Look up plasma polymerisation.

Moreover, you'd be surprised how much "gunk" is actually on the surface of any given piece of material in standard atmospheric conditions. There is a reason why a lot of surfacial materials characterization techniques require ultra-high vacuum, as I recall it's about 1 atomic layer/second of deposition under $10^{-6}\, \mathrm{torr}$ (source). If you want two metals to bond without applying heat or force you'd need to get a vacuum better than that and then clean off the oxide layer.

You'd also be surprised at how much organic material is covering the surface of everything around you. Your fingers produce oil and they stick to the surface of everything you touch, and your dead skin flakes off all the time and covers the stuff around you. You can notice it if you take a metallic sample to an SEM, and shoot electrons at it to get an image of the surface. If it's got organic material on the surface, after a while the area where you shot electrons will turn dark, you can notice it if you zoom out or pan around. This is due to hydrocarbon contamination, usually from the oils on your fingers.
AnswerKenntnis:
There is much more to bonding than exchange of electrons. Especially because electrons that are part of electron cloud doesn't take part in crystal bonds. 

All metals are basically crystals --- they have proper lattice, to weld two parts you'd have them to build common lattice (at least in the are where they are welded). When you weld properly you create liquid phase between (by heat or current) that crystallizes. 

I might be mistaken, but this welding can be observed. For example if you have some old iron screws, that are screwed into element also made of iron (and not oiled properly), after time they can be very hard to unscrew, it was supposedly because of diffusion of atoms between both parts which was easy because of the same lattice structure. Once again: I heard this as an anecdote on some Physics lecture, but did not search any further proofs.
AnswerKenntnis:
To add to the other's ideas on this topic, I think the concept of "Surface Potential" also plays a large role in this. 
Roughness interferes with the surface potential of the material because it creates gaps where the two metals cannot bond. This lowers the surface potential of the material.

Materials such as oxides, oils or other residues that can be found on metals also lower the surface potential of the material. This surface potential can be reduced by Van Der Waal's interactions, ionic interactions and other polar non polar molecular level interactions. Every molecule that comes in contact with the metal, whether it's air found on the surface or between gaps due to roughness or residues, has the potential to reduce it's surface potential. 

Ex:
Consider bonding in a metal of it's easier for you (in a semiconductor) where you are used to drawing Si-Si bonds to every neighboring atom, however, ON THE SURFACE of the atom the electrons cannot bond because there are no more available electrons. This causes surfaces of metal or other materials like semiconductors to be extremely reactive: thus forming oxide layers, or reducing their surface potential via the aforementioned atom-atom, atom-molecule interactions. (It's easier to consider Silicon as it has electrons that are "considered" attached to it's nucleus rather than a pure transition metal that has "free electrons" that people don't consider attached to the nucleus.
AnswerKenntnis:
While simple contact between metals isn't enough for most metals to bond, relative motion will achieve the fusion between the metals (at small contacts). A common occurrence is seizing up of mechanical devices due to insufficient lubrication. 

I don't think screws stick due to metal-metal bonding- its mostly simple distortion particularly of the threads and body of the screw. Damage a screw and insert in a tight space and you won't that screw out again.
AnswerKenntnis:
I believe the success or failure of a passive (zero activation energy bond) will be dependent on the dielectric potential differential and/or a fero-magnetic factor.  Just a guess.  http://en.wikipedia.org/wiki/Dielectric
AnswerKenntnis:
It depends upon the purity of the metal. If the surface is well polished it is indeed possible to make bonds with the adjacent metal pieces. However, if there are oxides and other impurities present at the surface then bonding is not possible. This can be explained by the surface energy of the metal. Well, you can see that metals are bonded in powder metallurgy.
AnswerKenntnis:
I think in true what happens is that when you "touch" the two together, you actually aren't - on a macromollecular scale, yes they are touching, but if you consider it on a more mollecular scale, there will be other things coming into play - namely the fact that firstly, there are still going to be large air particles in between them most of the time (I mean, air like 70% nitrogen, and that is a pretty big diatomic mollecule).  Even if the air is not in the way, you would still end up with numerous impurities on the surface, which would stop it from working (I think is the case with Aluminium, which forms an oxide layers just a few atoms thick, but which is strong enough to stop it from reacting with many other substances).

Lastly, even if you did manage to get them close enough - well, the electrons are delocalised because of something called orbital overlap, which is how you get any electron sharing. In things with delocalisation, you get a large orbital which shares the electron density throughout. Now, this, when it forms, is stable and has a certain energy level that makes it stable. However, trying to attach another couple of atoms of metal would involve breaking these orbitals, at least on the surface, and reforming them. You can do the reforming thing when you twist or otherwise mess around with metals, because the delocalisation isn't attacked as badly, and hence does not constitute a high energy barrier. But in terms of attaching another molecule, I think the activation energy i.e. energy required to break the previous orbital, is too high to be overcome under normal circumstances.
QuestionKenntnis:
How does gravity escape a black hole?
qn_description:
My understanding is that light can not escape from within a black hole (within the event horizon).  I've also heard that information cannot propagate faster than the speed of light.  It would seem to me that the gravitational attraction caused by a black hole carries information about the amount of mass within the black hole.  So, how does this information escape?  Looking at it from a particle point of view: do the gravitons (should they exist) travel faster than the photons?
AnswersKenntnis
AnswerKenntnis:
Well, the information doesn't have to escape from inside the horizon, because it is not inside. The information is on the horizon. 

One way to see that is from the fact that from the perspective of an observer outside the horizon of a black hole, nothing ever crosses the horizon. It asymptotically gets to the horizon in infinite time (as it is measured from the perspective of an observer at infinity).

An other way to see that is the fact that from the boundary conditions on the horizon you can get all the information you need to describe the space-time outside but that is something more technical.

Finally, since classical GR is a geometrical theory and not a quantum field theory*, gravitons is not the appropriate way to describe it.

*To clarify this point, GR can admit a description in the framework of gauge theories like the theory of electromagnetism. But even though electromagnetism can admit a second quantization (and be described as a QFT), GR can't.
AnswerKenntnis:
There are some good answers here already but I hope this is a nice short summary:

Electromagnetic radiation cannot escape a black hole, because it travels at the speed of light. Similarly, gravitational radiation cannot escape a black hole either, because it too travels at the speed of light. If gravitational radiation could escape, you could theoretically use it to send a signal from the inside of the black hole to the outside, which is forbidden.

A black hole, however, can have an electric charge, which means there is an electric field around it. This is not a paradox because a static electric field is different from electromagnetic radiation. Similarly, a black hole has a mass, so it has a gravitational field around it. This is not a paradox either because a gravitational field is different from gravitational radiation.

You say the gravitational field carries information about the amount of mass (actually energy) inside, but that does not give a way for someone inside to send a signal to the outside, because to do so they would have to create or destroy energy, which is impossible. Thus there is no paradox.
AnswerKenntnis:
Let's get something out of the way: let's agree not to bring gravitons into this answer. The rationale is simple: when you talk about gravitons you imply a whole lot of things about quantum phenomena, none of which is really necessary to answer your main question. In any case, gravitons propagate with the very same speed as photons: the speed of light, $c$. This way we can focus simply in Classical GR, ie, the Differential Geometry of Spacetime: this is more than enough to address your question.

In this setting, GR is a theory that says how much curvature a space "suffers" given a certain amount of mass (or energy, cf Stress-Energy Tensor).

A Black Hole is a region of spacetime that has such an intense curvature that it "pinches out" a certain region of spacetime.

In this sense, it's not too bad to understand what's going on: if you can measure the curvature of spacetime, you can definitely tell whether or not you're moving towards a region of increasing curvature (ie, towards a block hole).

This is exactly what's done: one measures the curvature of spacetime and that's enough: at some point, the curvature is so intense that the light-cones are "flipped". At that exact point, you define the Event Horizon, ie, that region of spacetime where causality is affected by the curvature of spacetime.

This is how you make a map of spacetime and can chart black holes. Given that curvature is proportional to gravitational attraction, this sequence of ideas completely addresses your doubt: you don't have anything coming out of the black hole, nor anything like that. All you need is to chart the curvature of spacetime, measuring what happens to your light-cone structure. Then, you find your Event Horizon and, thus, your black hole. This way you got all the information you need, without having anything coming out of the black hole.
AnswerKenntnis:
The problem here is a misunderstanding of what a particle is in QFT.

A particle is an excitation of a field, not the field itself.  In QED, if you set up a static central charge, and leave it there a very long time, it sets up a field $E=k{q \over r^2}$.  No photons.  When another charge enters that region, it feels that force.  Now, that second charge will scatter and accelerate, and there, you will have a $e^{-}->e^{-}+\gamma$ reaction due to that acceleration, (classically, the waves created by having a disturbance in the EM field) but you will not have a photon exchange with the central charge, at least not until it feels the field set up by our first charge, which will happen at some later time.

Now, consider the black hole.  It is a static solution of Einstein's equations, sitting there happily.  When it is intruded upon by a test mass, it already has set up its field.  So, when something scatters off of it, it moves along the field set up by the black hole.  Now, it will accelerate, and perhaps, "radiate a graviton", but the black hole will only feel that after the test particle's radiation field enters the black hole horizon, which it may do freely.  But nowhere in this process, does a particle leave the black hole horizon. 

Another example of why the na├»ve notion of all forces coming from a Feynman diagram with two pairs of legs is the Higgs boson—the entire universe is immersed in a nonzero Higgs field.  But we only talk about the 'creation' of Higgs 'particles' when we disturb the Higgs field enough to create ripples in the Higgs field—Higgs waves.  Those are the Higgs particles we're looking for in the LHC.  You don't need ripples in the gravitational field to explain why a planet orbits a black hole.  You just need the field to have a certain distribution.
AnswerKenntnis:
I think it's helpful to think about the related question of how the electric field gets out of a charged black hole.  That question came up in the (now-defunct) Q&A section of the American Journal of Physics back in the 1990s.  Matt McIrvin and I wrote up an answer that was published in the journal.  You can see it at https://facultystaff.richmond.edu/~ebunn/ajpans/ajpans.html . 

As others have pointed out, it's easier to think about the question in purely classical terms (avoiding any mention of photons or gravitons), although in the case of the electric field of a charged black hole the question is perfectly well-posed even in quantum terms: we don't have a theory of quantum gravity at the moment, but we do think we understand quantum electrodynamics in curved spacetime.
AnswerKenntnis:
While in many ways the question was already answered, I think it should be empasized that on the classical level, the question is in some sense backwards. The prior discussion of static and dynamic properties especially comes very close.

Let's first examine a toy model of a spherically-symmetric thin shell of dust particles collapsing into a Schwarzschild black hole. The spacetime outside of the the shell will then also be Schwarzschild, but with a larger mass parameter than the original black hole (if the shell starts at rest at infinity, then just the sum of the two). Intuitively, the situation is analogous to Newton's shell theorem, which a more limited analogue in GTR. At some point, it crosses the horizon and eventually gets crushed out of existence at the singularity, the black hole now gaining mass.

So we have the following picture: as the shell collapses, the external graviational field takes on some value, and as it crossed the horizon, the information about what it's doing can't get out the horizon. Therefore, he gravitational field can't change in response to shell's further behavior, for this would send a signal across the horizon, e.g., a person riding along with the shell would be able to communicate across it by manipulating the shell.

Therefore, rather than gravity having a special property that enables it to cross the horizon, in a certain sense gravity can't cross the horizon, and it is that very property that forces gravity outside of it to remain the same.

Although the above answer assumed a black hole already, that doesn't matter at all, as for a spherically collapsing star the event horizon begins at the center and stretches out during the collapse (for the prior situation, it also expands to meet the shell). It also assumes that the situation has spherical symmetry, but this also turns out to not be conceptually important, although for far more complicated and unobvious reasons. Most notably, the theorems of Penrose and Hawking, as it was initially thought by some (or perhaps I should say hoped) that any perturbation from spherical symmetry would prevent black hole formation.

You may also be wondering about a related question: if the Schwarzschild solution of GTR is a vacuum, does it make sense for a vacuum to bend spacetime? The situation is somewhat analogous to a simpler one from classical electromagnetism. Maxwell's equations dictate how the electric and magnetic fields change in response to the presence and motion of electric charges, but the charges alone do not determine the field, as you can always have a wave come in from infinity without any contradictions (or something more exotic, like an everywhere-constant magnetic field), and in practice these things are dictated by boundary conditions. The situation is similar in GTR, where the Einstein field equation that dictates how geometry are connected only fixes half of the twenty degrees of freedom of spacetime curvature.
AnswerKenntnis:
Gravitation doesn't work the way light does (which is why quantum gravity is hard).

A massive body "dents" space and time, so that, figuratively speaking, light has a hard time running uphill. But the hill itself (i.e. the curved spacetime) has to be there in the first place.
AnswerKenntnis:
I think the best explanation that can be given is this: you have to discern between statical and dynamical properties of the space-time. What do I mean by that?

Well, there are certain space-times that are static. This is for example the case of the prototypical black-hole solution of GTR. Now, this space-time exists a priori (by definition of static: it always was there and always will be), so the gravity doesn't really need to propagate. As GTR tells us gravity is only an illusion left on us by the curved space-time. So there is no paradox here: black holes appear to be gravitating (as in producing some force and being dynamical) but in fact they are completely static and no propagation of information is needed. In reality we know that black-holes are not completely static but this is a correct first approximation to that picture.

Now, to address the dynamical part, two different things can be meant by this:


Actual global change of space-time as can be seen e.g. in the expansion of the universe. This expansion need not obey the speed of light but this is in no contradiction with any known law. In particular you cannot send any superluminal signals. In fact, opposite is true: by too quick an expansion parts of universe might go too far away for even their light to ever reach us. They will get causally disconnected from our sector of space-time and to us it will appear as if it never existed. So it shouldn't be surprising that no information can be communicated.
Gravitational waves, which is a just fancy name for the disturbances in the underlying space-time. They obey the speed of light and the corresponding quantum particles are called gravitons. Now these waves/particles indeed wouldn't be able escape from underneath the horizon (in the precisely the same way as any other particle, except for Hawking radiation, but this is a special quantum effect).
AnswerKenntnis:
In my opinion this is an excellent question, which manages to puzzle also some accomplished physicists. So I do not hesitate to provide another, a bit more detailed, answer, even though several good answers exist already.

I think that at least part of this question is based upon an incomplete understanding on what it means to mediate a static force from a particle physics point of view. As others have mentioned in their answers already, you encounter a similar issue in the Coulomb problem in electrodynamics. 



Let me answer your question from a field theory point of view, since I believe this concurs best with your intuition about particles being exchanged (as apparent from the way you phrased the question). 

First, no gravitational waves can escape from inside the black hole, as you hinted already in your question.

Second, no gravitational waves have to escape from inside the black hole (or from the horizon) in order to mediate a static gravitational force.

Gravity waves do not mediate the static gravitational force, but only quadrupole or higher moments.

If you want to think about forces in terms of particles being exchanged you can view the static gravitational force (the monopole moment, if you wish) as being mediated by "Coulomb-gravitons" (see below for the analogy with electrodynamics). Coulomb-gravitons are gauge degrees of freedom (so one may hesitate to call them "particles"), and thus no information is mediated by their "escape" from the black hole. 



This is quite analog to what happens in electrodynamics: photon exchange is responsible for the electromagnetic force, but photon waves are not responsible for the Coulomb force.

Photon waves do not mediate the static electromagnetic force, but only dipole or higher moments.

You can view the static electromagnetic force (the monopole moment, if you wish) as being mediated by Coulomb-photons. Coulomb-photons are gauge degrees of freedom (so one may hesitate to call them "particles"), and thus no information is mediated by their "instantaneous" transmission.

Actually, this is precisely how you deal with the Coulomb force in the QFT context. In so-called Bethe-Salpeter perturbation theory you sum all ladder graphs with Coulomb-photon exchanges and obtain in this way the 1/r potential to leading order and various quantum corrections (Lamb shift etc.) to sub-leading order in the electromagnetic fine structure constant.



In summary, it is possible to think about the Schwarzschild and Coulomb force in terms of some (virtual) particles (Coulomb-gravitons or -photons) being exchanged, but as these "particles" are actually gauge degrees of freedom no conflict arises with their "escape" from the black hole or their instantaneous transmission in electrodynamics.

An elegant (but perhaps less intuitive) way to arrive at the same answer is to observe that (given some conditions) the ADM mass - for stationary black hole space-times this is what you would call the "black hole mass" - is conserved. Thus, this information is provided by boundary conditions "from the very beginning", i.e., even before a black hole is formed. Therefore, this information never has to "escape" from the black hole. 



On a side-note, in one of his lectures Roberto Emparan posed your question (phrased a bit differently) as an exercise for his students, and we discussed it for at least an hour before everyone was satisfied with the answer - or gave up ;-)
AnswerKenntnis:
The various theories - QED, GTR, classical electromagnetism, quantum loop gravity, etc - are all different ways to describe nature.  Nature is what it is; theories all have defects.  So far as saying whether gravity resembles electromagnetism in some way or not, is just blowing warm air  about how humans think and not saying anything substantial about physical reality.

So what if we don't have a full grasp of quantum gravity?  Gravitons are a sensible concept, and a key part in some unified (or semi-unified) field theories.  It might get tricky because unlike other quantum particles, gravitons are a part of the curvature of spacetime and the relations of nearby lightcones, as they fly through said spacetime. We can sort of ignorer that for now.   The question is good, and can be answered in terms of quantum theory and gravitons.  We just don't know, given the existing state  of physics knowledge, how far we can push the idea.  

When charged particle attract or repel, the force is due to  virtual photons.  Photons like to travel at  the universal speed c, but they  don't have to.  Heisenberg says so! You can break the laws of conservation of energy and  momentum as much as you like, but the more you deviate, the shorter the time span and smaller the bit of space in which you violate these laws.  For the virtual photons connecting two charged particles, they've got the room  between the two particles, and a time span matching that at lightspeed.  These not running waves with a well-defined wavelength, period or phase velocity.  This ill-defined velocity can be faster than c or less equally well.  In QED, the photon propagator - the wavefunction giving the probability amplitude of a virtual photon connecting (x1, t1) to (x2, t2) is nonzero everywhere - inside and outside the past and future light cones, though becoming unlimited in magnitude on the light cones.

So gravitons, if they are that  much like photons, can exist just fine outside the horizon and inside.   They are, in a rough sense, as big as the space between the black hole and whatever is orbiting or falling into it.  Don't  picture them as little energy  pellets flying from the black  hole center (singularity or whatever) - even with Heisenberg's indulgence, it's just not a matter of small particles  trying to get  through the horizon the wrong way.   A graviton is probably already on both sides!  

For a more satisfying answer, I suspect it takes knowing the math,Fourier transforms, Riemann tensors and all that.
AnswerKenntnis:
The holographic principle gives a clue, as pointed out by David Zaslavsky.  The Schwarzschild metric element $g_{tt}~=~1 ΓÇô r_0/r$, for $r_0~=~2GM/c^2$ gives a proper distance called the delay coordinate
$$
r^*~=~r~+~r_0 ln[(r-r_00)/r_0]
$$
which diverges $r^*~\rightarrow~-\infty$ as you approach the horizon.  What this means is that all the stuff which makes up the black hole is never seen to cross the horizon from the perspective of a distant outside observer.  The clock on anything falling into a black hole is observed to slow to a near stop and never cross the horizon.  This means nothing goes in or out of the black hole, at least classically.  So there really is not problem of gravity escaping from a black hole, for as observed from the exterior nothing actually ever went in.
AnswerKenntnis:
The black hole does "leak" information, but it is not due to "gravitions, but in the form of the Hawking radiation. It has its basis in quantum mechanics, and is a thermal sort of radiation with extremely low rate. This also means that the black hole is slowly evaporates, but on a time scale that is comparable to the age of the universe.

The origin of this radiation can be described in a little bit hand-waving way as such: due to quantum fluctuations, there's particle-antiparticle pair creation going on in the vacuum. If such a pair-creating happens on the horizon, one of the pair can fall into the black hole while the other can escape. To preserve the total energy (since the vacuum fluctuations are around 0) with a particle now flying away, its fallen pair has to have a negative energy from the black hole's point of view, thus it is effectively losing mass. The outside observer perceives this whole process as "evaporation".

This radiation has a distribution as described by a "temperature", which is inversely proportional to the black hole's mass.

Might want to check out http://en.wikipedia.org/wiki/Hawking_radiation and other sources for more details...
AnswerKenntnis:
No escape is necessary (a slightly diifferent perspective).

A lot of nice answers so far but a couple of things need mentioning. It's not clear where, exactly, the mass of the black hole is supposed to be. Where does the mass reside? That's one thing. The other thing is, how does the mass/energy in the gravitational field, itself, fit into this picture?

I think (and I'll no doubt get hammered mercilessly for this) that the mass of a black hole resides spread out through its external gravitational field and nowhere else. The mass of a black hole resides, wholly and solely, in the gravitational field outside the hole. Fortunately for me, I'm not completely alone here.

The calculation of the total gravitational field energy of a black hole (or any spherical object) was made in 1985 by the Cambridge astrophysicist Donald Lynden-Bell and Professor Emeritus J. Katz of the Racah Institute Of Physics. http://adsabs.harvard.edu/full/1985MNRAS.213P..21L, Their conclusion was that the total energy in the field is ... (drum-roll here) ... mc^2 !!!

The total mass of the BH must reside, completely, and only, in the self-energy of the curvature of spacetime around the hole!

Here are a couple of quotes from the paper: "... the field energy outside a Schwarzschild black hole totals Mc^2." and, " ... all these formulae lead to all the black hole's mass being accounted for by field energy outside the hole."

The answer to your question, then, is this: information about the mass of a black hole doesn't have to escape from within the black hole because there is no mass inside the black hole. All the mass is distributed in the field outside the hole. Therefore, no information needs to escape from inside.
QuestionKenntnis:
Surviving under water in air bubble
qn_description:
An incredible news story today is about a man who survived for two days at the bottom of the sea (~30 m deep) in a capsized boat, in an air bubble that formed in a corner of the boat. He was eventually rescued by divers who came to retrieve dead bodies. Details here. Since gases diffuse through water (and are dissolved in it) the composition of air in the bubble should be close to the atmosphere outside, if the surface of the bubble is large enough; so the excessive carbon dioxide is removed and oxygen is brought in to support life of a human.

Question: How large does the bubble have to be so that a person in it can have indefinite supply of breathable air?
AnswersKenntnis
AnswerKenntnis:
Summary: I find a formula for the diameter of a bubble large enough to support one human and plug in known values to get $d=400m$.

I'll have a quantitative stab at the answer to the question of how large an air bubble has to be for the carbon dioxide concentration to be in a breathable steady state, whilst a human is continuously producing carbon dioxide inside the bubble.

Fick's law of diffusion is that the flux of a quantity through a surface (amount per unit time per unit area) is proportional to the concentration gradient at that surface, 

$\vec{J} = - D \nabla \phi$

where $\phi$ is concentration and $D$ is the diffusivity of the species. We want to find the net flux out of the bubble at the surface, or $\vec{J} = -D_{surface} \nabla \phi$.

$D_{surface}$ is going to be some funny combination of the diffusivity of $CO_2$ in air and in water, but since the coefficient in water is so much lower, really diffusion is going to be dominated by this coefficient: it can't diffuse rapidly out of the surface and very slowly immediately outside the surface, because the concentration would then pile up in a thin layer immediately outside until it was high enough to start diffusing back in again. So I'm going to assume $D_{surface} = D_{water}$ here.

To estimate $\nabla \phi$, we can first assume $\phi(surface)=\phi(inside)$, fixing $\phi(inside)$ from the maximum nonlethal concentration of CO2 in air and the molar density of air ($=P/RT$); then assuming the bubble is a sphere of radius $a$, because in a steady state the concentration outside is a harmonic function, we can find

$\phi(r) = \phi(far) + \frac{(\phi(inside)-\phi(far))a}{r} $

where $\phi(far)$ is the concentration far from the bubble, assumed to be constant. Then

$\nabla \phi(a) = -\frac{(\phi(inside)-\phi(far))a}{a^2} = -\frac{\phi(inside)-\phi(far)}{a}$

yielding

$J = D \frac{\phi(inside)-\phi(far)}{a}$

Next we integrate this over the surface of the bubble to get the net amount leaving the bubble, and set this $=$ the amount at which carbon dioxide is exhaled by the human, $\dot{N}$. Since for the above simplifications $J$ is constant over the surface (area $A$), this is just $JA$.

So we have $\dot{N} = D_{water} A \frac{\phi(inside)-\phi(far)}{a} = D_{water} 4 \pi a (\phi(inside)-\phi(far))$.

Finally assuming $\phi(far)=0$ for convenience, and rearranging for diameter $d=2a$ 

$d = \frac{\dot{N}}{2 \pi D_{water} \phi(inside)}$

and substituting

$D = 1.6\times 10^{-9} m^2/s$ (from wiki)

$\phi \approx 1.2 mol/m^3$ (from OSHA maximum safe level of 3% at STP)

$\dot{N}= 4\times 10^{-6} m^3/s = 4.8\times 10^{-6} mol/s$ (from %$CO_2 \approx 4%$, lung capacity $\approx 500 mL$ and breath rate $\approx 1/5 s^{-1}$)

I get $d \approx 400 m$.

EDIT: It's interesting to note that this is independent of pressure: I've neglected pressure dependence of $D$ and human resilience to carbon dioxide, and the maximum safe concentration of carbon dioxide is independent of pressure, just derived from measurements at STP.

Finally, a bubble this large will probably rapidly break up due to buoyancy and Plateau-Rayleigh instabilities.
AnswerKenntnis:
Maybe I should turn the comment to an answer. 

The physics of the situation is the same as when one can upturn a water glass with the water not falling out. The atmospheric pressure keeps it in.

There exist the diving bells with open bottoms . As they are lowered the pressure in the air goes up to balance the water pressure, because the lower in the water the higher the pressure. I think one atmosphere is ten meters of water. The person in the link was at 30 meters and that is why he had to be decompressed in order not to get the bends.

The oxygen versus CO2 and volume to survive one day is solved in this link:


  The atmosphere is about 20 percent oxygen. People breathe this in, and breathe 15 percent oxygen out, making the air that's left lower in Good Ol' Oh Two, but still quite breathable. Each minute a person at rest takes in roughly seven to eight liters of air, which adds up to about 11,000 liters of air a day. That sounds like a lot, volume-wise it's only 388 cubic feet. A ten by ten by ten foot room has 1000 cubic feet of air.Add that to the fact that you'll be breathing out a lot of oxygen, and you only need about 19 cubic feet of pure oxygen a day. Your breathing may get labored by the end of the second day, but a relatively small room should be fine for about three days. right?


wrong because there is CO2  exhaled which at large concentrations is poisonous:


  Once the carbon dioxide levels in the room rise to two percent, carbon dioxide poisoning starts to happen. That occurs when the overall oxygen level falls to 19 percent, about half way through the first day.


For the person to survive three days the compressed air he was breathing must have been from a lot of volume. From these numbers an order of magnitude estimate for non renewable volume to survive for 3 days is  6000 cubic feet. An enclosure of 10*25*25 in feet is a reasonable one on a tugboat. He was just lucky the air siphoned where he was trapped.

..........
AnswerKenntnis:
This is to summarize some of the excellent comments made
previously by participants of this discussion, and to emphasize
a couple of important points.

1) The original question implied that gas exchange between the bubble
   and surrounding water may be enough to sustain indefinitely
   breathing organisms inside. However this does not seem to be
   possible if the bubble is below the sea level because the gas in
   the bubble will eventually dissolve in the water and the bubble will
   cease to exist. The reason is that the concentration of dissolved
   gas near the bubble surface has to be higher than the ambient
   concentration. The ambient concentration is not higher than its
   value at the ocean surface (it is convective mixing rather than
   diffusion what transports dissolved air into the ocean). However
   the bubble is at the pressure $\sim$4 times the atmospheric
   pressure, and according to the Henry law the dissolved gas
   concentration next to the bubble should be $\sim$4 times higher
   than the ambient (the possible variation of temperature does not
   change the overall conclusion). So eventually the dissolved gas
   will diffuse away and the bubble will disappear. Thus, for any size
   of the bubble, with or without breathing organisms inside, there is
   no steady-state solution, if the bubble is below the sea
   level. This is illustrated in the figure: for either regular or
   upside-down siphon, the equilibrium state has water level equal in
   both legs. If one leg end in Fig. A is below the equilibrium level then
   eventually there will be no gas on this side of the siphon; this is similar
   to having no water in one leg in Fig. B if the end of this leg is above the
   equilibrium level.



2) The good news is that it would take a long time for the bubble to
   diffuse away. Estimate the diffusion time as 

$
   \tau \sim \frac{n_{g}}{c} \frac{a^2}{D}, 
   $

where $c/n_g$ is the ratio of concentrations of dissolved gas molecules
   and gas molecules in the
   bubble, $a$ is the size of the bubble, $D$ is the diffusion
   coefficient of dissolved gas.

According to Wikipedia, at 25 C, $c/n_g$ values are:
   for N$_2$ $1.492\times10^{-2}$; for O$_2$ $3.181\times10^{-2}$;
   for CO$_2$ $0.8317$.

Also, according to Wikipedia, the values of $D_g$ are: for
   C0$_2$ at 5-25 C $1.07-1.91 \times 10^{-5}$ cm$^2$/s; for
   N$_2$ at 25 C $1.88 \times 10^{-5}$ cm$^2$/s; for
   O$_2$ at 25 C $2.1\times 10^{-5}$ cm$^2$/s.

So, with these numbers, for an air bubble of the size $a\sim$ 1 m
   the lifetime is $\sim 10^{11}$ s $\sim$10,000 years! This is just
   unrealistic (if such things were possible we would've known about
   that!). The problem with this estimate is that the gas diffusion
   coefficient in water,
   D$_g\sim$10$^{-5}$cm$^2$/s = 10$^{-9}$m$^2$/s applies only to
   phenomena on small spatial scales, e.g., transport in porous
   media. We know that we can make carbonated water at home in a
   matter of minutes rather than months! On macroscopic scales
   transport is dominated by turbulence and convective flows, and,
   rather than diffusion, macroscopic transport models use the notion
   of effective ``exchange velocity'' $V_{ex}$, such that the flux of
   dissolved gas is

$
   \Gamma = A c V_{ex},
   $

where $A$ is the area, $c$ the concentration of dissolved gas.

The idea is that dissolved gas is removed by background convection
   and/or turbulence at the rate $V_{ex}$. Typical magnitude of
   $V_{ex}$ in the ocean, near the surface, can be 1 m/day
   [reference needed].

The lifetime of an air pocket is then

$\tau \sim \frac{a^3}{a^2 (c/n_g) V_{ex}} = \frac{n_g}{c} \frac{a}{V_{ex}},$

which, for $V_{ex}$=1 m/day, yields $\tau\sim$100 days, much more
   reasonable that 10,000 years!

3) As pointed out earlier, probably one does not need to make complex 
   assumptions about  water flows,
   turbulent diffusion etc if the available bubble of
   air contained the equivalent of $\sim$25 $m^3$ at atmospheric
   pressure, now compressed into $25/4\sim$6 m$^3$ volume,
   theoretically that would be enough for 2.5 days, without degassing
   excessive CO$_2$ into the water, or importing O$_2$ from
   surrounding water (the latter is impossible anyway as discussed in
   (1) above). The way it is described in the media, the man 
   ``...survived, breathing inside a four-foot high bubble of air as
   it slowly shrank from the waters rising from the ceiling of the
   tiny toilet and adjoining bedroom'', but probably that four-foot
   bubble communicated with a larger volume or air under the hull of
   the boat - and that's the most reasonable explanation of this
   miraculous survival.

4) One can note that due to different solubility of CO$_2$, N$_2$, and
   O$_2$ these gases will be leaving the bubble at different rates,
   and this is perhaps good news. Due to the breathing human present
   inside, the CO$_2$ level in the bubble will be increasing and the
   O$_2$ level will be accordingly decreasing. Once this goes to the
   dangerous level, say 5$\%$ CO$_2$ and 15$\%$ O$_2$, it will be
   mostly CO$_2$ that will be leaving the bubble, transport of other gases will
   be much slower. As pointed out by other people in this discussion, with
   the ``classical'' diffusion the gas removal rate would not be
   enough to counterbalance the buildup of CO$_2$ (unrealistic surface area
   needed), but background flows and turbulence in water can significantly increase the
   rate of transport, and perhaps the available surface area as well
   (foam formation). 

To make a quantitative estimate, let's take that the human rate
   of O$_2$ consumption and CO$_2$ production is $\sim 10^{25}$
   molecules/day; and for the concentration of dissolved carbon
   dioxide use

$
   c_{CO2} \sim 0.8 \times 0.05 \times 4 \times 6.02 \times 
   10^{23} / (22.4 \times 10^{-3} m^3) \sim 4 \times 10^{24} m^{-3}
   $

For the rate of removal of carbon dioxide take

$
   \Gamma_{CO2} \sim  a^2 V_{ex} c_{CO2},
   $

assuming transport of dissolved CO$_2$ with the exchange velocity
   $V_{ex}$=1 m/day, and one can see that just a couple of square meters of
   surface area would be enough to balance the buildup of exhaled CO$_2$
   concentration. This effect can probably extend the survival time by
   a factor of $\sim$2. Normally only a quarter of available oxygen in
   the air can be used, reducing it from 20$\%$ on inhale to 15$\%$ on
   exhale. With excessive CO$_2$ removed from the air by water, however, probably surviving on the fraction of oxygen in the air as low as 10$\%$ is possible. 
   For example, the diving gas mix Heliox uses 10$\%$ oxygen (and the rest helium).

5) And last, it is interesting to note that in principle in this kind
   of situation one could possibly survive free ascent on a single breath from 
   this depth; in the
   history of submarine accidents some people managed to escape from
   sunken submarines to the surface even from greater depths by free
   ascent. Of course the risk of doing so would be enormous; but
   probably not greater than the risk of not to be found in time.
QuestionKenntnis:
What experiment would disprove string theory?
qn_description:
I know that there's big controversy between two groups of physicists:


those who support string theory (most of them, I think)
and those who oppose it.


One of the arguments of the second group is that there's no way to disprove the correctness of the string theory.

So my question is if there's any defined experiment that would disprove string theory?
AnswersKenntnis
AnswerKenntnis:
String theory should come with a proposal for an experiment, and make some predictions about the results of the experiment; then we could check against the real results.

If a theory cannot come with any predictions, then it will disprove itself little by little ...

The problem is that, with string theory, this is extremely difficult to do, and string theorists have year in front of them to go in that direction; but if in 100 years we are still at the same status, then it would be a proof that string theory is unfruitful...
AnswerKenntnis:
One can disprove string theory by many observations that will almost certain not occur, for example:


By detecting Lorentz violation at high energies: string theory predicts that the Lorentz symmetry is exact at any energy scale; recent experiments by the Fermi satellite and others have showed that the Lorentz symmetry works even at the Planck scale with a precision much better than 100% and the accuracy may improve in the near future; for example, if an experiment ever claimed that a particle is moving faster than light, string theory predicts that an error will be found in that experiment
By detecting a violation of the equivalence principle; it's been tested with the relative accuracy of $10^{-16}$ and it's unlikely that a violation will occur; string theory predicts that the law is exact
By detecting a mathematical inconsistency in our world, for example that $2+2$ can be equal both to $4$ as well as $5$; such an observation would make the existing alternatives of string theory conceivable alternatives because all of them are mathematically inconsistent as theories of gravity; clearly, nothing of the sort will occur; also, one could find out a previously unknown mathematical inconsistency of string theory - even this seems extremely unlikely after the neverending successful tests
By experimentally proving that the information is lost in the black holes, or anything else that contradicts general properties of quantum gravity as predicted by string theory, e.g. that the high center-of-mass-energy regime is dominated by black hole production and/or that the black holes have the right entropy; string theory implies that the information is preserved in any processes in the asymptotical Minkowski space, including the Hawking radiation, and confirms the Hawking-Bekenstein claims as the right semiclassical approximation; obviously, you also disprove string theory by proving that gravitons don't exist; if you could prove that gravity is an entropic force, it would therefore rule out string theory as well
By experimentally proving that the world doesn't contain gravity, fermions, or isn't described by quantum field theories at low energies; or that the general postulates of quantum mechanics don't work; string theory predicts that these approximations work and the postulates of quantum mechanics are exactly valid while the alternatives of string theory predict that nothing like the Standard Model etc. is possible
By experimentally showing that the real world contradicts some of the general features predicted by all string vacua which are not satisfied by the "Swampland" QFTs as explained by Cumrun Vafa; if we lived in the swampland, our world couldn't be described by anything inside the landscape of string theory; the generic predictions of string theory probably include the fact that gravity is the weakest force, moduli spaces have finite volume, and similar predictions that seem to be satisfied so far
By mapping the whole landscape, calculating the accurate predictions of each vacuum for the particle physics (masses, couplings, mixings), and by showing that none of them is compatible with the experimentally measured parameters of particle physics within the known error margins; this route to disprove string theory is hard but possible in principle, too (although the full mathematical machinery to calculate the properties of any vacuum at any accuracy isn't quite available today, even in principle)
By analyzing physics experimentally up to the Planck scale and showing that our world contains neither supersymmetry nor extra dimensions at any scale. If you check that there is no SUSY up to a certain higher scale, you will increase the probability that string theory is not relevant for our Universe but it won't be a full proof
A convincing observation of varying fundamental constants such as the fine-structure constant would disprove string theory unless some other unlikely predictions of some string models that allow such a variability would be observed at the same time


The reason why it's hard if not impossible to disprove string theory in practice is that string theory - as a qualitative framework that must replace quantum field theory if one wants to include both successes of QFT as well as GR - has already been established. There's nothing wrong with it; the fact that a theory is hard to exclude in practice is just another way of saying that it is already shown to be "probably true" according to the observations that have shaped our expectations of future observations. Science requires that hypotheses have to be disprovable in principle, and the list above surely shows that string theory is. The "criticism" is usually directed against string theory but not quantum field theory; but this is a reflection of a deep misunderstanding of what string theory predicts; or a deep misunderstanding of the processes of the scientific method; or both.

In science, one can only exclude a theory that contradicts the observations. However, the landscape of string theory predicts the same set of possible observations at low energies as quantum field theories. At long distances, string theory and QFT as the frameworks are indistinguishable; they just have different methods to parameterize the detailed possibilities. In QFT, one chooses the particle content and determines the continuous values of the couplings and masses; in string theory, one only chooses some discrete information about the topology of the compact manifold and the discrete fluxes and branes. Although the number of discrete possibilities is large, all the continuous numbers follow from these discrete choices, at any accuracy. 

So the validity of QFT and string theory is equivalent from the viewpoint of doable experiments at low energies. The difference is that QFT can't include consistent gravity, in a quantum framework, while string theory also automatically predicts a consistent quantum gravity. That's an advantage of string theory, not a disadvantage. There is no known disadvantage of string theory relatively to QFT. For this reason, it is at least as established as QFT. It can't realistically go away.

In particular, it's been showed in the AdS/CFT correspondence that string theory is automatically the full framework describing the dynamics of theories such as gauge theories; it's equivalent to their behavior in the limit when the number of colors is large, and in related limits. This proof can't be "unproved" again: string theory has attached itself to the gauge theories as the more complete description. The latter, older theory - gauge theory - has been experimentally established, so string theory can never be removed from physics anymore. It's a part of physics to stay with us much like QCD or anything else in physics. The question is only what is the right vacuum or background to describe the world around us. Of course, this remains a question with a lot of unknowns. But that doesn't mean that everything, including the need for string theory, remains unknown.

What could happen - although it is extremely, extremely unlikely - is that a consistent, non-stringy competitor to string theory that is also able to predict the same features of the Universe as string theory can emerges in the future. (I am carefully watching all new ideas.) If this competitor began to look even more consistent with the observed details of the Universe, it could supersede or even replace string theory. It seems almost obvious that there exists no "competing" theory because the landscape of possible unifying theories has been pretty much mapped, it is very diverse, and whenever all consistency conditions are carefully imposed, one finds out that he returns back to the full-fledged string/M-theory in one of its diverse descriptions.

Even in the absence of string theory, it could hypothetically happen that new experiments will discover new phenomena that are impossible - at least unnatural - according to string theory. Obviously, people would have to find a proper description of these phenomena. For example, if there were preons inside electrons, they would need some explanation. They seem incompatible with the string model building as we know it today.

But even if such a new surprising observation were made, a significant fraction of the theorists would obviously try to find an explanation within the framework of string theory, and that's obviously the right strategy. Others could try to find an explanation elsewhere. But neverending attempts to "get rid of string theory" are almost as unreasonable as attempts to "get rid of relativity" or "get rid of quantum mechanics" or "get rid of mathematics" within physics. You simply can't do it because those things have already been showed to work at some level. Physics hasn't yet reached the very final end point - the complete understanding of everything - but that doesn't mean that it's plausible that physics may easily return to the pre-string, pre-quantum, pre-relativistic, or pre-mathematical era again. It almost certainly won't.
AnswerKenntnis:
Since many people seem to have very odd ideas about this, let's address this from a much simpler point of view.  

Let's suppose you have a friend who only knows math at the level of arithmetic of positive integers.  You try to tell him about the existence of negative numbers, and he tells you,


  That's stupid, there's obviously no such thing as "negative" numbers, how can I possibly measure something so stupid?  Can you have negative one apple?  No, you can't.  I can owe you positive one apple, but there's clearly no such thing as negative apples.


How can you start to argue that there is such a thing as negative numbers?  

A very powerful first step is mathematical consistency.  You can list all of the abstract properties you believe to characterize everything about positive integer arithmetic:


For all a,b,c, a(b+c) = ab+ac
For all a,b, a+b=b+a, ab=ba
There exists a number, called 0, such that, for all a, a+0=0+a=a, a0=0a=0
There exists a number, called 1, such that, for all a, 1a=a1=a


(note that, in sharp contrast to the case of real numbers, the first property can be proved with induction, and need not be an axiom.  Similarly, other listed properties can be proved from other ones designated to be more basic if one wishes, which can not be done in the case of the reals.)

So, once you both agree that these axioms characterize the positive integers completely, you can show that these hypothetical negative numbers, based on their formal properties, are consistent with the above axioms.  What does this show?

The positive integers, with the addition of the negative integers, can do at least as much as the positive integers by themselves.

(STOP  At this point, pause to realize how powerful this constraint is!!  How many other ways could one generalize arithmetic, at this level, to something else that is consistent with the properties you want?  Zero.  There is absolutely no other way to do it.  This is incredibly suggestive, and you should keep this in mind for the rest of the cartoon argument, and see how every argument that follows is secretly an aspect of this one!)

Your friend responds:


  Sure, you can write down toy models like that, and they may be consistent, but they don't correspond to reality.


Now, what else do you need to demonstrate to your friend to convince him of the validity of the negative numbers?

You find something else they can do that you can't do with the positive numbers alone.  Simply, you can state that every positive-integer-valued algebraic equation does not have a solution:

x + 1 = 0

does not have a solution.

But, it is a trivial fact that extending to the negative numbers allows you to solve such equations.  Then, all that's left to convince your friend of the validity of negative numbers is to show that this is equivalent to solving an ("a priori") different problem which only involved arithmetic of positive integers:

x+1=0 <=> y + 1 = 1 

So, y = 0, and y=x+1 is equivalent to the other problem.

To be complete, we also have to consider problems that are "unique" to the negatives, such as (-1)(-1) = 1, but in the realm of integers, these are trivial matters that are reducible to the above.  Even in the case of reals, given the other things we've shown, these consequences are almost "guaranteed" to work out intuitively obviously.

Now, assuming your friend is a reasonable, logical, person, he must now believe in the validity of negative numbers.

What have we shown?


Consistency, both with previous models and with itself
The ability to solve new problems
The reduction of some problems in the new language to problems in the old language


Now, to decide if this a good model for a particular system, you must look at the subset of problems that did not have a solution before, and see if the new properties characterize that system.  In this case, that's trivial, because the properties of negative numbers are so obvious.  In the case of applying more complicated things to describe the details of physical situations, it's less obvious, because the structure of the theory, and the experiments, is not so simple.

How does this apply to string theory?  What must we show to convince a reasonable person of its validity?  Following the above argument, I claim:


String theory reproduces (by construction) general relativity
String theory reproduces (by construction) quantum mechanics (and by the above, quantum field theory)


So string theory is at least as good as the rest of the foundations of physics.  Stop again to marvel at how powerful this statement is!  Realistically, how many ways are there to consistently and non-trivially write a theory that reduces to GR and QFT?  Maybe more than one, but surely not many!

Now the question is--what new do we learn?  What additional constraints do we get out of string theory?  What problems in GR and QFT can be usefully written as equivalent problems in string theory?  What problems can string theory solve that are totally outside of the realm of GR and QFT?

Only the last of these is beyond the reach of current experiments.  The "natural" realm where string theory dominates the behavior of an experiment is is at very high energies, or equivalently, very short distances.  Simple calculations show that these naive regions are well outside of direct detection by current experiments.  (Note that in the above example of negative numbers, the validity of the "theory" strictly in the corresponding realm didn't need to be directly addressed to make a very convincing argument; pause to reflect on why!)

However, theoretical "problems" with the previous theories, such as black hole information loss, can be solved with string theory.  Though these can't be experimentally verified, it's very suggestive that they admit the expected solution in addition to reproducing the right theories in the right limits.

There are two major successes of string theory that satisfy the other two requirements.

AdS/CFT allows us to solve purely field theory problems in terms of string theory.  In other words, we have solved a problem in the new language that we could already solve in the old language.  A bonus here is that it allows us to solve the problem precisely in a domain where the old language was difficult to deal with.

String theory also constrains, and specifies, the spectrum and properties of particles at low energies.  In principle (and in toy calculations), it tells us all of the couplings, generations of particles, species of particles, etc.  We don't yet know a description in string theory that gives us exactly the Standard Model, but the fact that it does constrain the low-energy phenomenology is a pretty powerful statement.

Really, all that's left to consider to convince a very skeptical reader is that one of the following things is true:


It is possible for string theory to reproduce the Standard Model (e.g., it admits solutions with the correct gauge groups, chiral fermions, etc.)
It is not possible for string theory to reproduce the Standard Model (e.g., there is no way to write down chiral theories, it does not admit the correct gauge groups, etc.   This is the case in, e.g., Kaluza-Klein models.)


I claim, and it is generally believed (for very good reasons), that the first of these is true.  There is no formal, complete, mathematical proof that this is the case, but there is absolutely no hint of anything going wrong, and we can get models very similar to the standard model.  Additionally, one can show that all of the basic features of the Standard Model, such as chiral fermions, the right number of generations, etc, are consistent with string theory.

We can also ask, what would it mean if string theory was wrong?  Really, this would signal that, 


The theory was mathematically inconsistent (there is no reason to believe this)
At a fundamental level, either quantum mechanics or relativity failed in some fairly pathological way, such as a violation of Lorentz invariance, or unitarity.  This would indicate that a theory of everything would look radically different than anything written down so far; this is a very precarious claim--consider what would happen in the example of arithmetic in the above if there were something "wrong" with addition.
The theory is consistent, and a generalization of GR and QFT, but is somehow not a generalization in the right "limit" in some sense.  This happens in, e.g., Kaluza-Klein theory, where chiral fermions can't be properly written down.  In that case, a solution is also suggested by a sufficiently careful analysis (and is one potential way to get to string theory).


Of these three possibilities, the first two are extremely unlikely.  The third is more likely, but given that it is known that all the basic features can show up, it would seem very strange if we could almost reproduce what we want, but not quite.  This would be like, in the arithmetic example, being able to reproduce all the properties we want, except for 1+ (-1) = 0.

If you're careful, you can phrase my argument in a more formal way, in terms of what it precisely means to have a consistent generalization, in the sense of formal symbolic logic, if you like, and see what must "fail" in order for the contrapositive of the argument to be true.  (That is, (stuff) => strings are true, so ~strings => ~(stuff), and then unpack the possibilities for what ~(stuff) could mean in terms of its components!)
AnswerKenntnis:
String theory, was constructed with the idea that at low energies it should reduce to the quantum mechanical and particle world that we see every day. This is analogous to the correspondence principle in quantum mechanics.

In some sense, any experiment that discredits quantum mechanics will cause a serious re-examination of string theory, however, as many experimentalists will joke, a theorist will always find a way to fix his theory to match the observations. In any case, quantum theory seems very well supported and is unlikely that it will be discredited any time soon

Direct tests of string theory, however,  will have to wait until we can probe much higher energies.
AnswerKenntnis:
The only way to 'test' string theory, is to actually figure out what it predicts first, which is ambigous at the moment.  

Contrary to most claims, String theory is actually a unique theory in that there are no adjustable free parameters.  However it has a large or possibly infinite amount of classical solutions or 'vacua'.  Most of these vacua look nothing like the real world, some (by some I mean it could be ~ 10^500) are very similar (in that they seem to correspond to low energy physics like the standard model), and at most one corresponds to the real world.

When you have specified a vacua, you then have fixed the predictions identically -- for everything (thats what it means to be a theory of everything).  So for instance, you could compute the mass of the electron to however many decimal places and then test it in the lab and also the vacua might make an unambigous prediction for cosmological objects (eg the existence of cosmic strings or domain walls).  We don't know exactly. 

Of course absent a way to figure out which vacua corresponds to reality, we are left with the same problem that quantum field theory has.  Namely you actually have to go out and measure certain things first in order to make predictions (so for instance in the standard model we need to pin down the 26 adjustable free parameters, like the Yukawa couplings, masses of elementary particles and so forth).. But once you have done that, you can then predict infinitely many other things, like scattering cross sections. 

So in string theory, that would mean some way of wittling down 10^500 vacua to a more human number, like 50, 10 or better 1 or 0 (0 means the theory is falsified).  Which requires doing scattering experiments at the Planck scale, which means a particle accelerator roughly on the scale of the Milky Way.

Of course, it may turn out that string theory makes unambigous falsifiable predictions, but that really requires a great deal of theoretical work b/c it implies knowing something about not just the vacua that corresponds to the real world, but also the 10^500 other ones if you get my meaning.  Still, we know that it does make some predictions.  For instance, the existence of the quanta of gravity is universal across all solutions.  Likewise, the fact that quantum mechanics and special relativity must hold is another robust prediction.
AnswerKenntnis:
It is very hard to probe the scales of quantum gravity experimentally so there are very few possibilities for testing if string theory is incorrect as a unified theory encompassing quantum gravity. One of the few observations that has probed the Planck scale was the Fermi gamma ray telescope observation which showed that photons of different energy travel very close to the same speed over cosmic distances. If the result had shown a dispersion of speed it would have disproved string theory and encouraged physicist to look at other ideas.

Of course any observation that disproves quantum theory or general relativity will also disprove string theory, but the real interest is in observations that relate directly to quantum gravity because the separate theories are already well established in their own regimes.

Although it is hard to get direct experimental input for any theory of quantum gravity the constraints on any theory from the logical requirement to combine quantum theory and gravity into one consistent theory for physics are already tremendously strong. In particular there should be some perturbative low energy limit that describes gravity in terms of gravitons which interact with matter. Despite much effort string theory is the only approach that can accomplish this and it is very hard to conceive of a second way. In fact it is very surprising that there is this one way because it requires almost miraculous cancellations of anomalies to make it work. This gives many people the confidence that string theory is the correct path to follow.

Ultimately there needs to be some definitive observation of a quantum gravitational effect that supports string theory. As I said, there are not many possibilities for such observations at present, but this would be a problem for any alternative theory of quantum gravity too.

It is possible that we might get lucky and observe large extra dimensions at the LHC, but there is no reason to expect that. Another possibility that I think is a little more plausible is that supersymmetry would be observed and found to take a form that supports a supergravity origin. Still we have no moral right to expect the universe to hand us such an easy clue and string theory does not promise one.

Such difficulties do not mean that string theory is wrong as some opponents say. It just means that it is going to be difficult to explore quantum gravity empirically.

There has still been constant progress in understanding string theory from the theoretical side and that is continuing. More work needs to be done on the non-perturbative side of string theory so that we can better understand its implications for cosmology. There is some hope that an observation of relic gravitational waves or even low frequency radio waves left over from the big bang may have a characteristic signature dependent on quantum gravitational effects. Again we have no moral rights to demand that such an observation will be forthcoming but it might if we are lucky.
AnswerKenntnis:
If understood correctly (?) something Lubos once said, string theory requires torsion (in GR) to be zero.  There are experiments underway/planned right now to measure torsion.  Therefore not only is there an experimental disproof of string theory, but we should get the data soon.
AnswerKenntnis:
String excitations, rspt lack thereof. Problem is however that unless you believe in a theory with a lowered Planck scale, you'll have to go to energies we'll never be able to reach in the lab to test this regime. But at least in principle it's falsifiable.
AnswerKenntnis:
As suggested by other posters, the key question is energy.

At very high energy levels, approaching some of the quantum gravity 'limits', the string-like  nature of fundamental particles would become increasingly apparent. (In terms of an experiment, at a high enough energy level, for instance, there would likely be new specific 'resonances' of the material that could be identified.)

You might also find this article interesting.
AnswerKenntnis:
To disprove the string theory I would say is not very likely; but there are two ways that would cause trouble in string theory: one theoretical and one by experiment. Both unlikely to happen soon, (an both unlikely to ever disprove it, because the string theory seems to be in the right way, at least until now by far.)
- If one would like to "dissprove it" at least in an initial approximation to cause some scepticism, should find a theory which solves at least as many problens as the string theory solves, and deals with at least some of the remaining problems of the string theory. (Even if this other theory exists, it will take several decades after its discovery to evolve anyway..)

-Experimentally as everyone else said we need to achieve higher energies on earth, or some better way to  locate and detect these higher energies in universe possibly. To achieve or locate somewhere, the energies where string theory effects are detectable will take many many years from now-at least that is the common belief.

-The "argument" that sometimes I listen the string theory is not a theory or is wrong because it can not be verified or disproved is competely wrong. Simply because this is not an argument. 
If we have to reach higher energies to see the string theory, then we just have to; is very possible that this is the physics. In any case even simpler physics stories like the one of neutrino which critised (with the critics of that time to be:'not even wrong' or the most optimistic "there is no practical way of observing the neutrino"); with Pauli and Fermi doing the theoretical prediction in ~1930 and the discovery of the Cowan and Reines in 1956 can teach us some things...

In one sentence the answer to your last line is:
Until now there is no experiment or detector that can verify the string theory. In the future it is a belief that this will happen when we will be able to achieve or locate somewhere these higher energies that the stringy effects become detectable.
AnswerKenntnis:
Disprove string theory by empirically falsifying a string theory postulate.  No postulate can be defended or it need not be postulated.  Falsify BRST invariance and string theories collapse.  Falsify the Equivalence Principle (EP) and the whole of physics needs a rewrite. No measurable observable violates the EP.  All continuous and most approximately continuous symmetries bow to Noether's theorems.  

You need an observable (so you know it is there) that is calculable (so you know how much) but not measurable, that is an absolutely discontinuous symmetry - and a test for its divergent consequences.  Are shoes different from socks when given a left foot?  By how much, yours versus mine? Quantitative chirality is calculated in any number of dimensions, J. Math. Phys. 40, 4587 (1999) and

http://petitjeanmichel.free.fr/itoweb.petitjean.freeware.html#QCM 
(http://www.mazepath.com/uncleal/norbors.gif
Solution optical rotations ignore atomic mass distribution)

Chop a pair of shoes into mm^2 pieces. Sort them left versus right.  Chirality is an emergent property.  It depends on scale.  To test the EP against a pair of shoes, one would need a shoe built on the smallest possible scale - a few atoms - and rather a lot of shoes to sum to a measurable divergence.  Physics cannot do that, but chemistry can.

If anything can break string theory (we cannot be smarter than Lubo┼í, but we can be orthogonal) then

http://www.mazepath.com/uncleal/erotor1.jpg 
   Two geometric E├╢tv├╢s experiments.  0.113 nm^3 volume/╬▒-quartz unit cell.  40 grams net as 8 single crystal test masses compare 6.68├ù10^22 pairs of opposite shoes (pairs of 9-atom enantiomorphic unit cells, the test mass array cube's opposite vertical sides).

DO NOT bet your grade on that!  Betting a December Swedish dinner is acceptable, especially if its main course is surstr├╢mming,

http://www.youtube.com/watch?v=xgV2imaOCao
QuestionKenntnis:
Book recommendations
qn_description:
Every once in a while, we get a question asking for a book or other educational reference on a particular topic at a particular level. This is a meta-question that collects all those links together. If you're looking for book recommendations, this is probably the place to start.

All the questions linked below, as well as others which deal with more specialized books, can be found under the tag resource-recommendation (formerly books).

If you find a question that should be added to the list, edit it into the existing list answer rather than creating your own answer for it.  See our policy on resource recommendations for details.

Related Meta: Do we need/want an overarching books question?
AnswersKenntnis
AnswerKenntnis:
This list is pretty rough, but I figured it would be better to post something rather than keep putting it off. If anyone wants to organize the links a little better, that would help. I'm not sure all the questions currently linked actually need to be here, anyway.

If you have a question to add, please edit it in. However, make sure of a few things first:


The question should be tagged resource-recommendations
It should be of the form "What are good books to learn/study [subject] at [level]?"
It shouldn't duplicate a topic and level that's already in the list




General Physics


Basic: Please recommend a good book about physics for young child (elementary school aged)


Newtonian Mechanics


General: Recommendations for good Newtonian mechanics and kinematics books
Foundations: Book suggestions for foundation of Newtonian Mechanics


Classical Mechanics


Introductory (math): Which Mechanics book is the best for beginner in math major?
Advanced: Book about classical mechanics
Coordinate-free: Classical mechanics without coordinates book


Electromagnetism


Introductory: Electrodynamics textbook that emphasizes applications


Continuum Mechanics


Good books on elasticity
Modern references for continuum mechanics


Classical Field Theory


Texts on field theory in classical physics


Thermodynamics and Statistical Mechanics


Short: Crash course in classical thermodynamics
Advanced: Recommendations for Statistical Mechanics book
Mathematical: References about rigorous thermodynamics


Waves


What's a good textbook to learn about waves and oscillations?


Optics


Where is a good place to learn (classical) optics?


Quantum Mechanics


Popular: Popular books on QM
Introductory: What is a good introductory book on quantum mechanics?
Advanced: Learn algebra and interpretation of QM
Historical: Good book on the history of Quantum Mechanics?
Mathematical : A book on quantum mechanics supported by the high-level mathematics
Path Integral : Path integral formulation of quantum mechanics


Quantum Information theory


Advanced: Quantum information science references


Quantum Field Theory


What is a complete book for quantum field theory?
Are there books on Regularization and Renormalization in QFT at an Introductory level? (maybe this should go under "Math"?)
Textbook on group theory to be able to start QFT
A No-Nonsense Introduction to Quantum Field Theory


Particle Physics


Popular: Good book about elementary particles for high school students?
Introductory: Particle physics getting started


Experimental Particle Physics


Enlightening experimental physics books/resources
Reference for solid state particle detector


General Relativity


Introductory: Getting started general relativity
Mathematical: Mathematically-oriented Treatment of General Relativity


Theories of Everything


Popular: What is a good non-technical introduction to theories of everything?


String Theory


Introductory: Introduction to string theory
Advanced: Advanced topics in string theory
Matrix-string-theory: Good introductory text for matrix string theory
AdS-CFT: Introduction to AdS/CFT


Cosmology


Dark matter references


Astronomy/Astrophysics


What is a good introductory text to astronomy
What are good books for graduates/undergraduates in Astrophysics?


Condensed Matter and Solid State Physics


Introductory: Intro to Solid State Physics
Advanced: Books for Condensed Matter Physics
Intermolecular forces: Soft Condensed Matter
Mathematically Rigorous: Mathematical rigorous introduction to solid state physics
Anyons: References on the physics of anyons
Iron-based superconductors: Reference needed for Iron-based superconductors


Material Science


Introductory: Best Materials Science Introduction Book?


Fluid Dynamics


Introductory: Book recommendations for fluid dynamics self study
Boundary layer theory: Boundary layer theory in fluids learning resources


Complex Systems


Introductory: What are some of the best books on complex systems?


Math


Best books for mathematical background?
Group Theory: Comprehensive book on group theory for physicists? 
Spectral Theory: Books for linear operator and spectral theory
Variational Calculus: Introductory texts for functionals and calculus of variation
Topology: Book covering Topology required for physics and applications
Algebraic Geometry: Crash course on algebraic geometry with view to applications in physics
Fractals: http://physics.stackexchange.com/q/12784


Problem books


Graduate Physics Problems Books


Miscellaneous


Books that every physicist should read
List of freely available physics books
Books that every layman should read
Books that develop interest & critical thinking among high school students
What is a good introduction to integrable models in physics?
QuestionKenntnis:
How do I explain to a 6 year old why people on the other side of the Earth don't fall off?
qn_description:
Today a friend's 6 year old sister asked me the question "why don't people on the other side of the earth fall off?". I tried to explain that the Earth is a huge sphere and there's a special force called "gravity" that tries to attract everything to the center of the Earth, but she doesn't seem to understand it. I also made some attempts using a globe, saying that "Up" and "Down" are all local perspective and people on the other side of the Earth feel they're on the top, but she still doesn't get it.

How can I explain the concept of "gravity" to a 6 year old in a simple and meaningful way?
AnswersKenntnis
AnswerKenntnis:
Having my own 6-year-old and having successfully explained this, here's my advice from experience:


Don't try to explain gravity as a mysterious force. It doesn't make sense to most adults (sad, but true! talk to non-physicists about it and you'll see), it won't make sense to a 6yo.

The reason this won't work is that it requires inference from general principles to specific applications, plus it requires advanced abstract thinking to even grasp the concept of invisible forces. Those are not skills a 6-year-old has at their fingertips. Most things they're figuring out right now is piecemeal and they won't start fitting their experiences to best-fit conscious models of reality for a few years yet.
Do exploit 6-year-old's tendency to take descriptions of actions-that-happen at face value as simple piecemeal facts.


  Stuff pulls other stuff to itself. When you have a lot of stuff, it pulls other things a lot. The bigger things pull the smaller things to them.


Them having previously understood the shape of the solar system and a loose grasp of the fact of orbits (not how they work—that's a different piece—just that planets and moons move in "circular" tracks around heavier things like the Sun and Earth) may be useful before embarking on these parts of the conversation. I'm not sure, but that was a thing my 6yo already had started to grasp at this point.

These conversations were also mixed in with our conversations about how Earth formed from debris, and how the pull was involved in making that happen, and how it made the pull more and more. So, I can't really separate out that background; it may also help/be necessary.
Don't try to correct a 6-year-old's confusion about up and down being relative, but use it instead.


  There's a lot of Earth under us, and it pulls us down when we jump. If we jumped off the side, it would pull us back sideways. If we fell off the bottom, it would pull us back up.


You can follow this up later with a Socratic dialogue about the relative nature of up and down, but don't muddy the waters with that immediately. That won't have any purchase until they accept the fact that Earth will pull you "back up" if you fall off.
Build it up over a series of conversations. They won't get it the first time, or the tenth, but pieces of it will stick.
Don't try to instill a grasp of the overall working model. If you can successfully give them some single, disconnected facts that they actually believe, putting them together will happen as they age and mature and get more exposure to this stuff.


All this is assuming a decently smart but not prodigious child, of course. (A 6-year-old prodigy can probably grasp a lay adult's model of gravity, but if that's who you're dealing with then you don't need to adjust your teaching.)

For some more context, this was also after my child's class started experimenting with magnets at school. I was inspired to attempt to explain gravity when my kid told me that trees didn't float off into space because the Earth was a giant magnet. (True! But not why trees don't float away.) Comparing gravity and magnetism might help, to give them an example of invisible pull that they can feel, but it might just confuse the subject a lot too since I had a lot of work (over multiple conversations) to convince my own that trees aren't sticking to the ground because of magnetism, even if the Earth is a giant magnet.

And, a final piece of advice that's incidental, but can help:


Once you've had a few of these conversations, play Kerbal Space Program while they watch. (Again, this comes from experience. My kid loves to watch KSP.) Seeing a practical example of gravity at work in it natural environment will go a long way to cementing the previous conversations. It may sound like a sign-off joke, but seeing a system moving and being manipulated makes a huge difference to a young child's comprehension, because it is no longer abstract or requires building mental abstractions to grasp, like showing them a globe does.
AnswerKenntnis:
The misconception likely comes from a misunderstanding of "down".  Making a 2-D drawing of the earth with buildings, people, and trees might help.  For example,
AnswerKenntnis:
Wrap a ball (like a tennis ball) with a rubber band. Tell her to put her finger between the ball and the rubber band and try to move her finger away from the ball. Have her do this on all sides of the ball. Now explain to her how the rubber band is like gravity.
AnswerKenntnis:
Rub a balloon with a cloth to induce a charge which will demonstrate "static cling", then attract small pieces of paper with the balloon.  Once they see that static electricity attracts the small pieces of paper to the bottom of the balloon, then you can start explaining to them about 'the force'.
AnswerKenntnis:
Ask the child what "down" would mean to the people on the "other side" of the Earth.
AnswerKenntnis:
Here are some ideas:

a) Try to make her understand the concept of "force" : tie two balls on an elastic band. Make her pull them apart so that she gets a feel  of force. Take an apple and drop it. Let her understand that if a force ( her hand) does not hold an object it is pulled by the earth, the way the elastic pulls the balls.

b) Then show her on a globe where you are. Show the vertical where the apple drops. You could then go to the force pulling towards the center, the way the elastic pulls along the line, and that it is the line that defines which way  the force pulls, both for the elastic and the earth.  Make the analogy that each point on the earth pulls along the line as if there is a band, towards the center.

good luck
AnswerKenntnis:
Take a magnet and hold it vertically, sprinkle iron fillings on either side of the pole. Iron fillings on the bottom side will be hanging. Then the child will get a feel that, it is possible for things to stay without falling down. You need not explain the concept of gravity for the child now. The child will create the explanation for itself (might also become next newton by creating a new concept).
AnswerKenntnis:
After carefully considering the OP's situation, I believe the focus falls on the rhetorical process.  Following from this, my argumentative approach would be the following: stand firm about the fact that Earth is round, get the child to reconcile the inconsistencies.

The reason for this kind of approach is the child's behavior.  The following quote is a script that I have repeated over and over in physics tutoring:


  I also made some attempts using a globe, saying that "Up" and "Down" are all local perspective and people on the other side of the earth feel they're on the top, but she still doesn't get it.


Here, you presented an argument.  But what followed after that argument?  There's no apparent commentary on the argument by the child.  I have no expectation that a kid would retort with consistent or event coherent counter-arguments, but the response seems to be missing altogether.

This is familiar to those of us with physics teaching experience.  Wrong answers are always easy to deal with, and almost universally constructive.  It's the lack of any model formation that blockades progress, and often leads them to switch majors to something non-technical because of the bad experience.

Consider the rhetoric to be like a chess game (with formally established rules for movement).  As an educated adult, you probably don't have trouble responding to any move the child makes.  If you do, ask another question here.

No rhetorical approach will be helpful 100% of the time.  Counter-examples aren't always helpful either, but they appeal to a very particular example of inconsistent logic.  If you ascribed to the bad logic before you see the counter-example, then it will accomplish it's purpose - demonstrating that you're wrong.  I think the best response I've seen here was the following image (posted as a comment):

http://www.caloi.com.ar/caloidoscopio_new/byn/byn53.gif

This is physics perfection.

The absurd illustration makes the viewer abandon a view.  You could reject that Earth is a sphere, or you could reject that gravity is always in the same direction.  I suppose the option remains that people are presently, at this moment, falling off the side of the planet.  I think that's what makes physics fun.  For each model we build, there is a story that goes along with it.  Most individual propositions can have a model built around it, changing everything else in the universe to accommodate.  But once you are forced to explain multiple facts simultaneously, then you are in the process of building physics.  Each model is a story.  If you can start enjoying the process of telling those models/stories, then you are on your way to grad school.

I say physics involves two things: rationalism and evidence.  Your model's consistency is dictated by reason, but which model applies is determined by evidence we get by the world around us (for which there is no mental shortcut).  Here are some stories that do have consistency:


Earth is flat, gravity is always down.  The validity can more-or-less only be determined by measuring the Earth's shape.
Earth is round, gravity is always the same direction.  A privileged "top of the Earth" location exists, and everything else slopes down.  You could verify this by observing a ball to roll off the edge of the world.


Maybe the best thing to do isn't to be correct, but show how you, yourself, enjoy being wrong.  If they emulate that behavior, they will be fantastic scientists.
AnswerKenntnis:
There's a good chance you won't be able to make her understand how it works.

According to developmental psychologist Jean Piaget, children at her age tend to see themselves as the "center of the world", and are incapable of reasoning about it from any other viewpoint. The process is called egocentrism, and is the same reason why a kid gets confused when its mother calls its (the mother's) parents mother/father instead of grandmother/grandfather. 

It won't hurt trying to explain, and she may gain some insight, but don't get frustrated if she doesn't.
AnswerKenntnis:
Why don't you try to explain that every object attracts other objects, but that in order for it to be felt, at least one of the objects must be very big? The earth is a very big object, so it attracts things towards it.
AnswerKenntnis:
Explain that since the earth is round, and people do not fall down (as there is no absolute "down"), they fall towards the center of the earth.
AnswerKenntnis:
Point out the fact that a ball falls straight down, and not sidewards.  This is because it wants to go to the center of the Earth.

This is true everywhere.  What we think is down is towards the center of the Earth, and this is true whereever we go.

(and then have a look at the globe and see what direction towards the center is)
AnswerKenntnis:
Tell him that down means "in towards the center of the Earth, no matter where you are."
AnswerKenntnis:
What helped me to understand gravity is to model spacetime as a trampoline or a stretched bed sheet. Now put a melon on it, saying "This is earth which bends space." Now take a marble and say "This is you, also bending space, yet not as much". And no matter where you put the marble, it always goes "down" towards earth, as earth bends spacetime the most.

Yet what I think is important to convey that not only does stuff fall towards earth but that every mass has its own gravity field not entirely unlike a magnetic field.

You can go crazy with this experiment, saying the melon is the sun, a larger marble is the earth. Instead of letting the earth marble fall give it a sideway push and it will orbit the sun for a while.

You can even add a smaller marble as the moon and if you throw the earth and the moon marbles well enough, you will see the moon marble orbiting the earth marble while both orbit the melon sun.

There is a real nice example of this experiment on youtube.
AnswerKenntnis:
This question is in fact a very challenging one and was first satisfactorily solved by Albert Einstein in the course of developing general theory of relativity from special relativity:

His first step toward a relativistic theory of gravitation was the proposition of the principle of equivalence. Equipped with this principle, he could explain some gravitational effects like gravitational frequency shift of light (or time dilation) and gravitational deflection of light:



but he couldn't account for the gravitational effects near gravitational sources like the Earth: he couldn't explain why the people on the opposite side of the Earth experience a gravitational pull in the opposite direction of the Earth. This effects are called Tidal effects:



He solved this problem by proposing a deep analogy between tidal forces and a property of surfaces called curvature. Basically, he proposed that space(time) can be represented by some curved geometrical objects called (pseudo-riemannian) manifolds. Thus, all you have to explain to him is this concept. I think the following picture, (although, with having the time dimension omitted, it's not precise at all) can help you/him a lot:



This pictures helps him to grasp the concept in a simple, geometrical and yet as scientifically precise as possible: all objects around the Earth fall toward the surface (center, of course) of the Earth. I think this explanation is in accordance with Einstein's quote everything should be as simple as possible, but not simpler.

To complete this answer, I explain (very) briefly the remaining steps to general relativity:

After this point, the only remaining step toward his theory was finding a relationship between this curvature and presence of gravity sources like matter and radiation. He arrived at his famous equation:
$$\mathbf{G}=\frac{8\pi G}{c^4}\mathbf{T}$$.

in which the quantity $\mathbf{G}$ measures curvature of space(time) and the quantity $\mathbf{T}$ measures matter content.

Thus, a complete solution to the question why objects placed at different locations around the Earth experience forces in different directions (all toward the center of the Earth) necessarily needs a general-relativistic argument.
QuestionKenntnis:
What really allows airplanes to fly?
qn_description:
What aerodynamic effects actually contribute to producing the lift on an airplane?

I know there's a common belief that lift comes from the Bernoulli effect, where air moving over the wings is at reduced pressure because it's forced to travel further than air flowing under the wings. But I also know that this is wrong, or at best a minor contribution to the actual lift. The thing is, none of the many sources I've seen that discredit the Bernoulli effect explain what's actually going on, so I'm left wondering. Why do airplanes actually fly? Is this something that can be explained or summarized at a level appropriate for someone who isn't trained in fluid dynamics?

(Links to further reading for more detail would also be much appreciated)
AnswersKenntnis
AnswerKenntnis:
A short summary of the paper mentioned in another answer and another good site.

Basically planes fly because they push enough air downwards and receive an upwards lift thanks to Newton's third law.

They do so in a variety of manners, but the most significant contributions are:


The angle of attack of the wings, which uses drag to push the air down. This is typical during take off (think of airplanes going upwards with the nose up) and landing (flaps). This is also how planes fly upside down.
The asymmetrical shape of the wings that directs the air passing over them downwards instead of straight behind. This allows planes to fly level to the ground without having a permanent angle on the wings.
AnswerKenntnis:
From Stick and Rudder by Wolfgang Langewiesche, page 9, published 1944:


  The main fact of all heavier-than-air
  flight is this: the wing keeps the
  airplane up by pushing the air down.
  
  It shoves the air down with its bottom
  surface, and it pulls the air down
  with its top surface; the latter
  action is the more important. But the
  really important thing to understand
  is that the wing, in whatever fashion,
  makes the air go down. In exerting a
  downward force upon the air, the wing
  receives an upward counterforce--by
  the same principle, known as Newton's
  law of action and reaction, which
  makes a gun recoil as it shoves the
  bullet out forward; and which makes
  the nozzle of a fire hose press
  backward heavily against the fireman
  as it shoots out a stream of water
  forward. Air is heavy; sea-level air
  weights about 2 pounds per cubic yard;
  thus, as your wings give a downward
  push to a cubic yard after cubic yard
  of that heavy stuff, they get upward
  reactions that are equally hefty.
  
  That's what keeps an airplane up.
  Newton's law says that, if the wing
  pushes the air down, the air must push
  the wing up. It also puts the same
  thing the other way 'round: if the
  wing is to hold the airplane up in the
  fluid, ever-yielding air, it can do so
  only by pushing the air down. All the
  fancy physics of Bernoulli's Theorem,
  all the highbrow math of the
  circulation theory, all the diagrams
  showing the airflow on a wing--all
  that is only an elaboration and more
  detailed description of just how
  Newton's law fulfills itself--for
  instance, the rather interesting but
  (for the pilot) really quite useless
  observation that the wing does most of
  its downwashing work by suction, with
  its top surface. ...
  
  Thus, if you will forget some of this
  excessive erudition, a wing becomes
  much easier to understand; it is in
  the last analysis nothing but an air
  deflector. It is an inclined plane,
  cleverly curved, to be sure, and
  elaborately streamlined, but still
  essentially an inclined plane. That's,
  after all, why that whole fascinating
  contraption of ours is called an
  air-plane.
AnswerKenntnis:
Since you asked for an explanation appropriate to an non-specialized audience, maybe this will do: "A Physical Description of Flight; Revisited" by David Anderson & Scott Eberhardt. It is a revision of the earlier "A Physical Description of Flight" (HTML version).
AnswerKenntnis:
This answer is nothing more than a variation of Sklivv's answer. I simply wish to discuss some quantitative ideas following from Sklivv's answer and discuss what I understand (from an aerospace engineering friend) to be a common conceptual mistake - that the application of "mere surface effects" and "application of Bernoulli's principle" is wrong. These "mere surface effects and Bernoulli's principle" follow from Sklivv's idea as I hope to make clear. Everything in aeroplane physics begins and ends with "aeroplanes thrust air downwards, so the air thrusts aeroplanes up". This answer is written to be understandable to someone like I who know nothing about fluid dynamics - aside from:


The mathematically elegant and thoroughly enjoyable 2D problems tackled with complex variable theory (see Finding Stagnation Points from the complex potential );
That I know that there is Clay Mathematics prize up for grabs for anyone who can prove existence of, or give counterexample against, the existence of smooth, globally well defined solutions to the Navier-Stokes equations;
That aerospace engineering colleagues and friends tell me experimental proof is still queen in this field: most real fluid dynamics involving aeroplane flight heavily leans on phenomenological models tuned by experiment. 


I shall answer by taking these points up in turn.

Experiment is Queen

From a particular experimental point of view, there is no mystery why aeroplanes fly. Rather, the better question, in my opinion, is "how do they control the inevitable huge lift forces on them to make the latter stably lift in a constant, vertical direction?"

This experimental view is as follows: think of the Beaufort Scale and other scales used by meteorologists to get across the practical meaning of their wind and other warnings: for example the Fujita Scale for tornadoes and Tropical Cyclone Category Systems, which describe in practical terms the effects of storms of various intensities. 

Now I understand flying regulations forbid commercial jetliners from flying at slower than $300\mathrm{km\,h^{-1}}$ before their final approach to the runway. Think about $300\mathrm{km\,h^{-1}}$ airspeed in terms of the scales I have just spoken of: this is an F4 tornado, category 5 cyclone and is well off the 12-class Beaufort scale. Buildings and structures of any shape the size and weight of fully laden aeroplanes are torn up and borne into the sky or utterly torn down and destroyed. There is NO shortage of lift from a $300\mathrm{km\,h^{-1}}$ relative airspeed to hold almost anything the size and weight of fully laden commercial jetliner up: at these airspeeds, almost anything of this size and weight and lighter flies. At least it does so fleetingly: if it isn't designed like an aeroplane, as it moves its attitude changes and so does the direction of ram pressure: it is then likely to be flipped over and dashed catastrophically onto the ground. Put simply: almost everything flies at this airspeed, but only very special things do so stably.

Simple Mathematical Models

We can do a back of the envelope estimation of ram pressure in this case: see my drawing below of a simple aerofoil with significant angle of attack being held stationary in a wind tunnel. I'm going to put some numbers to Sklivvz's description:



Lets suppose the airflow is deflected through some angle $\theta$ radians to model an aeroplane's attitude (not altitude!) on its last approach to landing or as it takes off, flying at $300\mathrm{km\,h^{-1}}$ airspeed or roughly $80\mathrm{m\,s^{-1}}$. I have drawn it with a steep angle of attack. Air near sea-level atmospheric pressure has a density of about $1.25\mathrm{kg\,m^{-3}}$ (molar volume of $0.0224\mathrm{m^{-3}})$. The change in momentum diagram is shown, whence the change in vertical and horizontal momentum components are (assuming the speed of flow stays roughly constant):

$$\Delta p_v = p_b \sin\theta;\quad\quad\Delta p_h = p_b \,(1-\cos\theta)$$

At the same time, the deflecting wing presents an effective blocking area to the fluid of $\alpha\,A\,\sin\theta$ where $A$ is the wing's actual area and $\alpha$ a scale factor to account for the fact that in the steady state not only fluid right next to the wing is distrubed so that the wing's effective area will be bigger than its actual area. Therefore, the mass of air deflected each second is $\rho\,\alpha\,A\,v\,\sin\theta$ and the lift $L$ and drag $D$ (which force the engines must afford on takeoff) must be:

$$L = \rho\,\alpha\,A\,v^2\,(\sin\theta)^2;\quad\quad D = \rho\,\alpha\,A\,v^2\,(1-\cos\theta)\, \sin\theta$$

If we plug in an angle of attack of 30 degrees, assume $\alpha = 1$ and use $A = 1000\mathrm{m^3}$ (roughly the figure for an Airbus A380 wing area), we get a lifting force $L$ for $\rho = 1.25\mathrm{kg\,m^{-3}}$ and $v = 80\mathrm{m\,s^{-1}}$ of 200 tonne weight. This is rather less than the takeoff weight of a fully laden A380 Airbus (which is 592 tonnes, according to the A380 Wikipedia page) but it is an astonishingly high weight just the same and within the right order of magnitude. As I said, experiment is Queen here. We see that the wing's effective vertical cross section is bigger than the actual wing by a factor of 2 to 3. This is not surprising at steady state, well below speed of sound flow: the fluid bunches up and the disturbance is much bigger than just around the wing's neighbourhood. So, plugging in an $\alpha = 3$ (given the experimental fact that the A380 can lift off at 592 tonnes gross laden weight), we get a drag $D$ of 
54 tonne weight (538kN) - about half of the Airbus's full thrust of 1.2MN, so this ties in well with the Airbus's actual specifications, given there must be a comfortable margin to lift the aeroplane out of difficulty when needed.

In these F4 / C5 grade winds (and up to three times faster in normal flight), we see therefore there simply isn't any shortage of lift. The aeronautical engineering problem is more about keeping this plentiful lift stably directed upwards and allowing the aeroplane to hold a steady attitude and keep any torques arising from lift nonuniformity from flipping the plane over.

As the aeroplane picks up speed, the ram pressure calculated above is proportional to the square of the airspeed (see my answer to Drag force at high speeds ), so that at full speed the effect more than accounts for the drop in air density and the shallower angle of attack - we cannot make this downwards ram pressure without overcoming the much greater horizontal hindwards component - drag - so that is important to fly with low angle of attack for good fuel efficiency.

Refining the Mathematical Model

It is important to heed that the above description in terms of momentum difference between incoming air and the downwash begotten by the wing is exactly the same physics as the "more popular" descriptions given in terms of the Bernoulli equation and the integration of pressure around the wing. This is easy to see: the Navier-Stokes equation(See the Wikipedia page for the derivation of the Navier-Stokes equation ), is a very simple application of nothing more than Newton's second and third laws to infinitessimal volumes of fluid, notwithstanding the lack of knowledge about its fundamental mathematical properties (as bespoken by the Clay Mathematics Millenium Prize's unclaimed status: I love the Navier-Stokes equation- such a simple, readily grasped idea so baldly just an embodiment of Newton's laws, yet throwing up profound mysteries that show us scientists how little we yet know about the World). The steady state Navier Stokes equation for a perfect, incompressible fluid is (here $\vec{v}$ is the steady state velocity field and $p$ the scalar pressure field):

$$(\vec{v}\cdot \nabla) \vec{v} = \nabla \left(\frac{|\vec{v}|^2}{2}\right) + \nabla\wedge(\nabla\wedge\vec{v}) = -\nabla p$$

which gives $\nabla\left(p + \frac{|\vec{v}|^2}{2}\right) = 0$ or $p + \frac{|\vec{v}|^2}{2} = const$ for an irrotational flow ($\nabla\wedge\vec{v} = \vec{0}$) when integrated along the integral curve of $\vec{v}$, i.e. a streamline. Or, alternatively, we can argue in a more first principles way in this simple case: the force on a infinitessimal volume is $-\nabla p$ and the acceleration of a particle on the streamline is, by application of the Serret-Frenet formulas (here $s$ is the arc length along the streamline through the particle and $\kappa$ the path's curvature):

$$\mathrm{d}_t (v \hat{\mathbf{t}}) = \mathrm{d}_s v \times \mathrm{d}_t s\, \hat{\mathbf{t}} + v\,\mathrm{d}_s(\hat{\mathbf{t}})\,\mathrm{d}_t s=v\,\mathrm{d}_s v, \hat{\mathbf{t}} - \kappa\,v^2\,\hat{\mathbf{n}}=\mathrm{d}_s \left(\frac{v^2}{2}\right)\, \hat{\mathbf{t}} - \kappa\,v^2\,\hat{\mathbf{n}}$$

whence, on applying $\vec{F} = m \vec{a} \Rightarrow -\nabla p \,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z = \rho\,\vec{a}\,\mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z$, we get:

$$-\nabla p = \rho \left(\mathrm{d}_s \left(\frac{v^2}{2}\right)\, \hat{\mathbf{t}} - \kappa\,v^2\,\hat{\mathbf{n}}\right)$$

which again yields $p + \frac{|\vec{v}|^2}{2} = const$ when integrated along a streamline (here we can see the sideways (normal to streamline) centripetal force $-v^2\,\hat{\mathbf{n}} / R$ given by the wonted $v^2/R$ formula). So we can (and will, below), for example, apply the Theorem of Blasius to calculate lift, and be assured it is no more than a quantification of Sklivv's idea that "aeroplanes thrust air downwards, so the air thrusts aeroplanes up". The pressure difference between the upper and lower surface of a wing exists because the wing is pushing the air down, not a separate phenomenon. Often one hears that the Bernoulli principle applied to wings is wrong: this is not true. There is a fallacy (to be discussed below) as shown by experiment (and, hand-wavingly, by theory) in the wonted demonstration of lift using Bernoulli's principle, but the idea is basically sound, as it must be from its derivation from the Navier-Stokes equation and Newton's laws shown above.

A Joukowsky Aerofoil Calculation and Errors in Wonted Application of Bernoulli's Principle to Wings

We look at a 2D calculation of lift by Bernoulli's principle, or, equivalently, by application of the Theorem of Blasius. The common misconception here is that airflows split at the wing's leading edge and two neighbouring particles will reach the wing's lagging edge at the same time, so that the upper particles must fare the curved surface at higher speeds and therefore the pressure on the upper wing surface is less. Actually, the upper path particles are sped up much more than this explanation implies and reach the wing's lagging edge well before their lower-path-faring neighbours. See this wonderful video from the University of Cambridge, particularly at about 50 seconds in. This fact shows that the circulation $\oint_\Gamma \vec{v}\cdot\mathrm{d}\vec{r}$ around the wing's surface $\Gamma$ is nonzero, a fact which we intuitively expect from simple theory (as shown below) and which is amply confirmed in experiment: see the video, or go to the end of a runway of a large airport on a damp day so that you can let big commercial jetliners fly over you at about 50m height (take your earmuffs). On a damp day, you will see vortices breaking off the wings' outer edges, will see them swirling in the damp air for many seconds in the aeroplane's wake and, if you take you hearing protection off after the aeroplane has pass, you will hear the vortices crackling in the air, sounding a little like waves washing on the beach. This is much more fun than it sounds when your children are badgering you to do such a thing and, from the sights and sounds, I learnt heaps more from doing it that I thought I would. Even though the following calculation has an air of theoretical soundness and "first principles" to it, it is important to understand that it too is an experimental model: the circulation is forced into our description, motivated by the confirmation of the former's existence by experiment. The Kutta-Joukowski Condition (see Wikipedia page for Kutta Condition) as well as the Wikipedia page for the Kutta-Joukowski Theorem  is little more than an ad-hoc experimentally motivated fix: it is simply this. When we model the flow with a Joukowski aerofoil (described below), there is a sharp, lagging edge on the wing. This begets a singularity with unphysical, infinite velocties. However, by postulating and choosing the right circulation in the flow, we can put a  stagnation point at the lagging edge, thus cancelling the singularity, regularising our solution and also forcing the experimentally observed condition that there is only ever one stagnation point at the wing's leading edge, never elsewhere.

Another way to look at this experimentally motivated condition is well explained in this answer to the Physics SE question Does a wing in a potential flow have lift?. An irrotational, inviscid, incompressible flow cannot alone lift a wing. We add circulation to "fudge" a compensation for this theoretical lack: viscosity is "nature's way of enforcing the Kutta-Joukowsski condition".

So we begin with the complex variable method (see the Wikipedia page for "Potential flow" in the section "Analysis for two-dimensional flow" to study a potential flow i.e. irrotational ($\nabla \wedge = \vec{0}$) velocity field $\vec{v}$ with a potential $\psi$ such that $\vec{v} = -\nabla \psi$ that is also incompressible (continuity equation $\nabla\cdot \vec{v} = \nabla^2 \psi = 0$). See also the Physics SE questions Finding Stagnation Points from the complex potential ).

The main method here is to use the Joukowski transform:

$$\omega(z,\,s_z,\,s_\omega) = \frac{ s_\omega }{2}\left(\frac{z}{ s_z } + \frac{ s_z }{z}\right)$$

to map the potential flow corresponding to a spinning, offset cylinder  (see the NASA page ΓÇ£Lift of a Rotating Cylinder" ) into the flow around the image of this cylinder under the Joukowsky transform.  The truly weird Flettner Aeroplane actually used spinning cylinders rather than wings to fly successfully. The Joukowsky transform maps the circle $|z| = s_z$ onto the real axis between the points $\omega = \pm s_\omega$ in the $\omega$-plane; this section of the real axis between $\omega = \pm s_\omega $ is then the branch cut for the inverse Joukowski transform. The Joukowsky transform is a two  to one mapping, and the branches of the inverse Joukowski transform map the whole $\omega$-Riemann sphere (if we define the stereographic projection so that $|z| = s_\omega $ is the $\omega$-Riemann sphereΓÇÖs equator) separately to the inside and outside of the circle $|z| = s_z$ in the $z$-plane (which outside and inside can be thought of the Northern and Southern hemispheres of the $z$-Riemann sphere, if the stereographic projection is chosen so that the circle $|z| = s_z$ is the $z$-Riemann sphereΓÇÖs equator). The $\omega$-Riemann surface is made by slitting two copies of the Riemann sphere along the branch cut and stitching the edges together, to get a double cover of genus nought for the $\omega$-Riemann sphere. For this problem, I define the branch cut as slightly differently from the real axis section between the $\pm s_\omega$, I define it as the path:

$$\operatorname{Im}(\omega) = h \cos\left(\frac{\pi}{2} \operatorname{Re}(\omega)\right)$$

between the two branch points with an adjustable height parameter $h$, for reasons that will become clear.

The radius $r$ of the spinning cylinder radius is chosen so that cylinder surface passes through the point $z=+s_z$, which is the image of one of the branch points in the $\omega$ plane. This achieves the sharp edge that becomes the lagging edge of our aerofoil.

The complex potential for the spinning cylinder is:

$$\Omega(z) = v \,e^{-i\alpha}\,\left(z- \delta\right) + \frac{r^2 \,v\, e^{+i\alpha }}{z- \delta } + i\,a\,\log\left(z - \delta \right)$$

where $\alpha$ is the angle of attack, $\delta = \delta_r + i\,\delta_i$ is the offset and $r$ is the radius of the cylinder steeped in a uniform flow which converges to $v$ metres per second along the positive real axis, as $z\to\infty$. The logarithm and dipole terms put a branch point and pole at the cylinderΓÇÖs centre, so the flow is perfectly valid outside and on the cylinder. $a$ is the circulation. If we let $\phi$ stand for the angular co-ordinate labelling the cylinderΓÇÖs edge, there are two stagnation points on the cylinder with angular co-ordinates $\phi_\pm$ where $\mathrm{d}_z \Omega(z) = 0$, i.e. when:

$$e^{i\,(\phi_\pm - \alpha)} =  -i\frac{a}{2\,v\,r}\pm\sqrt{1-\left(\frac{a}{2\,v\,r }\right)^2} = \exp\left(-\arcsin\frac{a}{2\,v\,r }\right)$$

Now, we map this flow to the $\omega$ plane and apply the Theorem of Blasius to the image of the offset circle so as to work out the lift on this image. The image can be plotted with the Mathematica command:

$$\small{\mathrm{P[\delta_r\_, \delta_i\_] := \\
 ParametricPlot[\{Re[\omega[\delta_r + i \delta_i + 
      \sqrt{(1 - \delta_r)^2 + \delta_i^2} Exp[i \theta]], 
   Im[\omega[\delta_r + i \delta_i + 
      \sqrt{(1 - \delta_r)^2 + \delta_i^2} Exp[i \theta]]\}, \{\theta, 0, 2 \pi\}]}}$$

and the result is drawn below in the $\omega$-plane for $s_z = s_\omega = 1$, $\delta_r = -0.1$, $\delta_i = 0.3$ (i.e. the spinning circle offset so that its centre is at $-0.1+i\,0.2$ and with a radius $r = \sqrt{(1 - \delta_r)^2 + \delta_i^2}$ so that its image passes through the branch point $\omega = +s_\omega = 1$ in the $\omega$-plane:



Now we come to the crucial Kutta-Joukowski postulate, an experimental "fudge". The sharp edge on the aerofoil above would normally map the flow in the $z$-plane so that there were an unphysical infinite velocity at this sharp point. In practice, it is seen in wind tunnel tests that the streamlines stay tangent to the upper surface, and that there is one stagnation point at the wing's leading edge (intuitively the air "crashes" here) and no other stagnation points on either the top of bottom of the wing. Sometimes there is a small region of turbulence around the wing's lagging edge (as in the University of Cambridge video) (i.e. the incompressible potential flow model fails here) or the flow peels smoothly off the lagging edge. The way we achieve effects similar to experiment and "renormalise" our solution is to add the right amount of circulation $a$ to the flow so that one of the stagnation points on the spinning cylinder is mapped to the sharp edge (the branch point at $\omega = +s_\omega$) in the $\omega$-plane: the stagnation thus cancels the otherwise unphysical infinite velocities there and "regularises" our solution. With the radius of the cylinder chosen as  $r = \sqrt{(1 - \delta_r)^2 + \delta_i^2}$, it can readily be shown from the equation above for the stagnation point positions that the circulation needed is:

$$a = 2 v\,\delta_i \cos\alpha + 2\,v\,(1-\delta_r) \sin\alpha$$

This then is the wholly experimentally motivated Kutta-Joukowski condition. It is motivated by the knowledge that circulation is observed aroung wings, there is experimentally only one stagnation point on the leading edge of the wing and the fact that the right amount of circulation can reproduce these experimentally seen results.

When this is done, the Blasius theorem lift calculation done around the transformed Joukowski aerofoil in the $\omega$-plane is:

$$\begin{array}{lcl}D_\ell - i\,L_\ell &=& \frac{i\,\rho}{2}\oint_{\Gamma_\omega} (\mathrm{d}_\omega \Omega)^2 \,\mathrm{d} \omega\\
&=& \frac{i\,\rho}{2}\oint_{\Gamma_z} (\mathrm{d}_z \Omega)^2 \frac{1}{\mathrm{d}_z \omega}\,\mathrm{d} z\\
 &=& -\pi\,\rho \Sigma[\,\mathrm{residues\,of\,}\,(\mathrm{d}_z \Omega)^2 \frac{1}{\mathrm{d}_z \omega}\,\mathrm{at\,poles\,within\,}\Gamma]\\
&=& -4\,\pi\,i\,\rho\,a\,v\,e^{-i\,\alpha}\end{array}$$

where $\Gamma_\omega$ is the Joukowski aerofoil and $\Gamma_z$ the transformed aerofoil (i.e. the spinning cylinder). So there is no lift without circulation. It's worth stating again:


  An irrotational, inviscid, incompressible flow cannot alone lift a wing. We add circulation to "fudge" a compensation for this theoretical lack: viscosity is "nature's way of enforcing the Kutta-Joukowsski condition".


Now we substitute the Kutta-Joukowski condition to get:

$$D_\ell + i\,L_\ell = 8\,\pi\,i\,\rho\,v^2\,\left(\delta_i\,\cos\alpha + (1-\delta_r)\,\sin\alpha\right) \frac{s_z^2}{s_\omega} e^{+i\alpha}$$

We now need to scale the velocities so that the relative airspeeds are equal in the $\omega$- and $z$-planes.

The above is the force per unit length (in direction normal to the page) on the wing and its direction is the direction in the $\omega$-plane. We have:

$$\lim\limits_{\omega\to\infty} \left(\mathrm{d}_\omega \Omega(\omega(z))\right) = \lim\limits_{z\to\infty} \left(\mathrm{d}_z\Omega(\omega(z))\right) \lim\limits_{\omega\to\infty} \left(\mathrm{d}_\omega z\right) = 2 \,e^{-i\alpha} v \frac{s_z}{s_\omega}$$

so we need $s_\omega = 2$ and $s_z = 1$, then $\delta$ will be a dimensionless parameter defining the offset of the $z$-plane cylinder as a fraction of its radius. But now the $\omega$-plane planform width of the wing is 4 units. Moreover, the above calculation yields the force per unit length (normal to the 2D flow). So we divide the result for $s_\omega = 2$ and $s_z = 1$ by 4 and then scale up by the total wing area to get the total force on the wing. Furthermore, we need to we rotate the flow in the sketch below so that the incoming flow is horizontal (i.e. in the direction of the aeroplane's relative air velocity) in the $\omega$-total force on the wing above becomes:

$$D + i\,L = \pi\,i\,\rho\,v^2\,A\,\left(\delta_i\,\cos\alpha + (1-\delta_r)\,\sin\alpha\right)$$

We witness the d'Alembert paradox: the perfect flow cannot model the drag. Now let's put some numbers in. If we put $\delta = 0$, then the wing is simply the straight branch cut between $\omega = \pm 1$, so we have a version of the calculation I began with but now refined to take account of the full flow pattern. With $\alpha = 0.3$ (a little less than 20 degrees), $\rho = 1.25\mathrm{kg\,m^{-3}}$, $v=80\mathrm{m\,s^{-1}}$ and $A = 850\mathrm{m^2}$, we get $L=643\mathrm{tonne}$, pretty near to the Airbus's fully laden takeoff weight.  If we chose the parameters $\delta_i = 0.2$, $\delta_r =-0.1$ to give a wing shape that does not seem too fanciful for a jetliner wing with the lagging edge flaps wound fully out for takeoff and landing (see the plot below) we get about 1200 tonnes lift for our $300\mathrm{km\,h^{-1}}$ airspeed. Clearly this is optimistic and the overreckonning arises from the assumption of equal effectiveness of whole wingspan, whereas the tips will clearly not be well modelled by 2D flow. Not all of the wings will work as modelled, thus the $A$ in this formula is somewhat less than the planform area. What the flow model does show (see below), however, is that the effective vertical cross section presented to the incoming air is much greater than the tilted area $A \,\sin\theta$ assumed in the very simple model at the beginning of my answer. At steady state, a considerable cross section of air both above and below the vertical cross section is bent downwards and contributes to the effect "aeroplanes thrust air downwards, so the air thrusts aeroplanes up" described in Sklivv's answer.

Now, to plot the complete transformed flow in the $\omega$-plane, we must use the inverse Joukowski transform. To do this successfully, one must use the right branches of the inverse transform in the right co-ordinate patches. For Mathematica, which puts the branch cut for the square root function along the negative real axis (the namespace std::sqrt in Microsoft Visual C++ puts it along the positive real axis), we define the following chart functions, which are particular branches of the inverse transform:

$$\zeta_1(\omega) = \frac{s_z}{s_\omega}\left(\omega- i \sqrt{\omega-s_\omega}\,\sqrt{-\left(\omega+s_\omega\right)}\right)$$
$$\zeta_2(\omega) = \frac{s_z}{s_\omega}\left(\omega+ i \sqrt{\omega-s_\omega}\,\sqrt{-\left(\omega+s_\omega\right)}\right)$$
$$\zeta_3(\omega) = \frac{s_z}{s_\omega}\left(\omega- \sqrt{\omega^2-s_\omega^2}\right)$$
$$\zeta_4(\omega) = \frac{s_z}{s_\omega}\left(\omega+ \sqrt{\omega^2-s_\omega^2}\right)$$

and then the following Mathematica commands will plot the full flow:

$$\small{\mathrm{\Omega[z\_,\,\delta\_,\,v\_,\,r\_,\,a\_,\,\alpha\_,\,s\_]:= v\,e^{-i\,\alpha}\left(\frac{z}{s}-\delta\right) + \frac{r^2\,v\,e^{i\,\alpha}}{\frac{z}{s}-\delta} + i\,a\,Log\left[\frac{z}{s}-\delta\right]}}$$
$$\small{\mathrm{G[z\_,\,\delta_r\_,\,\delta_i\_,\,\alpha\_]:=\Omega\left[z,\,\delta_r+i\,\delta_i,\,1,\,\sqrt{(1-\delta_r)^2 + \delta_i^2},2\,\delta_i Cos[\alpha] + 2\,(1-\delta_r)\,Sin[\alpha],\,\alpha,\,1\right]}}$$

$$\small{\mathrm{S[\delta_r\_, \delta_i\_, \alpha\_, h\_, c\_] := \\
 Show[ContourPlot[
   Im[If[(Abs[x] < 1 ) \wedge (y > 0) \wedge (y < h\, Cos[\pi x/2]), 
     G[\zeta_1[x + i y], \delta_r, \delta_i, \alpha]], 
     If[x < 0, G[\zeta_3[x + i y], \delta_r, \delta_i, \alpha]], 
      G[\zeta_4[x + i y], \delta_r, \delta_i, \alpha]]]]], \{x, -2, 2\}, \{y, -2, 
    2\}, Contours \to c, MaxRecursion\to 2, PlotPoints \to 300, AspectRatio \to 1], 
  P[\delta_r, \delta_i, \{Black, Thick\}]]}}$$

where $\mathrm{P}[]$ is the parametric plot command above used to plot the aerofoil. The above use of the branch functions works for $\delta_r < 0$: other branches are needed for correct results when $\delta_r > 0$. The parameter $h$ bends the branch cut so that it bows upwards and stays inside the aerofoil, thus allowing the branches of the inverse Joukowsky transform to plot the mapped cylinder flow properly. Drawn below is the outcome from the command $\mathrm{S[-0.1, 0.2, 0.2, 0.2, 100]}$, i.e. the flow around the wing for an angle of attack of 0.2 radians, the circle offset parameters of $-0.1 + 0.2\,i$, a bow in the branch cut so that $h=0.2$. Witness the branch cut inside the aerofoil below and also how far from the wing's surface its effect stretches. The effective vertical component of the wing's area that is presented to the flow is clearly much greater than the actual vertical component of the wing's area, so the factor of 2 to 3 scaling in the A380 Airbus lift as reckoned by the simple fluid deflexion calculation seems highly plausible and unsurprising.



Lastly, to come the full circle, here is an animation to be found on web pages "Irrotational plane flows of an inviscid fluid" at the University of Genoa's environmental engineering department; see http://www.diam.unige.it/~irro/. The animation shows the progress of fluid particles for the Joukowski aerofoil flow, illustrates the assertion that the flow above the wing traverses the wing much more quickly than the flow underneath and lastly, shows very well the main thesis that "aeroplanes thrust air downwards".
AnswerKenntnis:
Without going into the excellent and detailed mechanics explaining reaction lift that others have provided for this answer, I just want to say that contrary to popular belief/high school physics textbooks, airplanes do not fly solely on account of Bernoulli's principle. According to Walter Lewin's excellent "For the Love of Physics":

"Bernoulli's principle accounts for 20% of an airplane's lift, the rest is provided by reaction lift."

Walter Lewin also poses an insightful question if planes really fly due to the equal transit theory and Bernoulli's principle (they do not!). 

"...then how do planes fly upside down?"
AnswerKenntnis:
Consider the velocity field of the particles in the air mass in a 2D projection of the X(forward) and Z(up) axes. For each particle, Integrate over area and time, to derive the center of air-mass momentum (p) before and after the passage of the airplane : dp/dt. (On a very calm morning, with no wind or turbulence, the center of air-mass and its momentum is stationary in Z(assume level un-accelerated flight), and equal to the True Airspeed in X pointing in the aft -X direction. Integrate over the area and you will find that the center and momentum of the particle and vector field has changed, with passage of the plane. This center of air mass and center of momentum will move forward(+X) and downward(-Z) relative to its original state. The equal and opposite momentum change with time dp/dt of the airplane is a force. We might label the -X component "drag" and the +Z component "lift"(careful: the airplane coordinate system is different from the stationary airmass). This is a dissipative system, so don't wait too long after the plane passes to record the vector field. We can observe this process in contrails on clear days when the high altitude air is cold and relatively moist. Sadly since we mostly view them from below with a projection along the Z, we miss the downward component of the momentum field. You can see this as a test pilot, flying as chase wing-man, in formation (projection in the Y-Z plane from behind or X-Z from the side).
 Expand this model to 3D to include lateral or Y axis flow and effects!
 I suggest this "p-dot"(dp/dt) of momentum-change explanation is better, than "pushing" or "pulling" the air downwards, because the later may confuse position and momentum in the view of the reader. This is also the first term(LHS) in the beautiful Euler-LaGrange equation, which would lead to an even more elegant analysis of this question!

As a new user, I will need to figure out how to attach the appropriate Figures and Equations to this post...-thanks

Note: The drag equation is really the ideal gas law, except density replaces m/V.

P/rho = R T :
QuestionKenntnis:
Why does NASA use gold foil on equipment and gold-coated visors?
qn_description:
I've read several websites about equipment covered with gold foil and astronaut helmet visors are coated with gold. However, their explanations are devoid of almost all physics content. Can someone explain the basic concept of why gold foil is so popular with NASA as a coating on visors?
AnswersKenntnis
AnswerKenntnis:
In space, the sun transfers heat via radiation to equipment and astronauts. Although the sunΓÇÖs peak emission is in the visible region (about 500 nm), you can see that there is also a fair amount IR (infrared) and UV (ultraviolet) emitted as well at the top of the atmosphere.



To control the surface temperature of an object that is exposed to IR (heat waves), NASA wraps its equipment with a metallic reflector that reflects IR to keep it from getting ΓÇ£hot.ΓÇ¥ The common reflectors are aluminum, silver, copper and gold. Below, the plot of reflectance vs. wavelength shows that all four metals are good IR-reflectors since the reflectance is close to 100% for wavelengths greater than 700 nm (╬╗ ΓëÑ  700 nm).



So why use gold? ItΓÇÖs most likely the same reason why they use gold extensively in circuit boards. (i) Gold does not corrode or rust while silver and copper does, which would reduce reflectance (by the way this happens before takeoff) and (ii) itΓÇÖs a lot easier to work with gold than aluminum. 

The outer sun visor is made from a polycarbonate plastic and coated with a thin layer of gold. This combination gives complete protection to the astronaut. Why? Your eyes can focus both visible and near IR light onto your retina equally well. Your eye has visible receptors but not IR ones. When intense visible light hits these receptors, the receptors transmit information letting you know that this is painful and will cause damage if you donΓÇÖt either close them or look away. On the other hand, without IR receptors,  you wouldnΓÇÖt realize that your eye was being ΓÇ£burnedΓÇ¥ with an intense IR source. Therefore, astronauts need IR protection from intense sunlight above the earth atmosphere. From the plot above, using a gold-coated visor reflects almost all IR but gold will also transmit about 60% of visible as well as UV light for about ╬╗ Γëñ 500 nm. According to the plot above, with the visor down you would see a blue-green hue to objects). On the other hand, about 60% of UV is transmitted through the gold but a polycarbonate plastic visor has excellent visible transmittance but absorbs/reflects almost all UV as shown below. 



PMMA (Polymethylmetacrylate, Lexan, Plexyglass..) and PC (Polycarbonate, the DVD material)
QuestionKenntnis:
What software programs are used to draw physics diagrams, and what are their relative merits?
qn_description:
Undoubtedly, people use a variety of programs to draw diagrams for physics, but I am not familiar with many of them.  I usually hand-draw things in GIMP which is powerful in some regards, but it is time consuming to do things like draw circles or arrows because I make them from more primitive tools.  It is also difficult to be precise.  

I know some people use LaTeX, but I am not quite sure how versatile or easy it is.  The only other tools I know are Microsoft Paint and the tools built into Microsoft Office.

So, which tools are commonly used by physicists?  What are their good and bad points (features, ease of use, portability, etc.)?

I am looking for a tool with high flexibility and minimal learning curve/development time.  While I would like to hand-draw and drag-and-drop pre-made shapes, I also want to specify the exact locations of curves and shapes with equations when I need better precision. Moreover, minimal programming functionality would be nice additional feature (i.e. the ability to run through a loop that draws a series of lines with a varying parameter).

Please recommend few pieces of softwares if they are good for different situations.
AnswersKenntnis
AnswerKenntnis:
I've had good experiences with Inkscape.  It has a GUI interface, but allows you to enter coordinates directly if you want, and it's scriptable.  There is a plug-in that allows you to enter LaTeX directly (for labels and such).  The downside is that it is very much still in development, so sometimes you find that a feature you want is not completely implemented yet.  

As an example, here is a poster I made last week, entirely within Inkscape:


Inkscape now also has the "JessyInk" plug-in which allows you to use it to make presentations (├á la Powerpoint).  The presentation can be viewed in a web browser as SVG, or exported to PDF.

If you have a Mac and don't mind spending some money ($100), I've heard good things about OmniGraffle.
AnswerKenntnis:
I'm learning TikZ (a drawing package for LaTeX) as we speak. It's good for two-dimensional line drawings, the syntax for specifying shapes and curves is extremely versatile, but the learning curve is steeper than LaTeX even.

There is a superb gallery of TikZ examples.

Here is another collection of neat TikZ examples on SE.tex.
AnswerKenntnis:
I'll interpret your term diagram as "any fancy image that captures some physics".

For this I can hardly recommend anything else then MetaPost. It's on par with TeX in being a little hard to learn but once you do master the basics you won't believe you could have ever used anything else (in particular, GIMP and Inkscape; good analogy here would be to TeX vs. MS Word).

Basic properties


it's a (simple) programming language
it's vectorial (this should probably go without saying but still)
it's primitives are things like points, lines, paths, splines
it contains excellent image manipulation facilities; you can say things like "take this image, scale it up by two and rotate it by 60 degrees"
you can insert TeX labels
it can solve equations; This is a real killer that no one else offers. You can draw two curves $X(t)$ and $Y(t)$ (defined most comfortably as splines) and tell MP to compute their intersection, draw a point there and label it with some text


Success story

I used MP to create some polygons on a hexagonal lattice in the context of cluster expansions. There was a huge number of those polygons to draw, so I quickly abandoned all hope of trying to draw them by hand in Inkscape or something similar. True, it would probably be quicker in the end, but I hate manual work; I rather spend much longer learning some programming language and then just code all the work in few minutes. So I put together simple MP program that has converted my input data (vertices and edges of the polygons as just numbers) into beautiful images. For a one night's work and my first time with MP I was more than satisfied.

Goodies



Credit for this amazing picture goes to Johan K├Ñhrstr├╢m (go also see more stuff under illustrations there).


http://www.cs.toronto.edu/~mackay/metapost/
http://www.ursoswald.ch/metapost/tutorial.html
http://commons.wikimedia.org/wiki/File:Contravariant_Coordinates.png
AnswerKenntnis:
First of all do not use a raster graphics software like Gimp to draw pictures. This has serious disadvantages when you want to make screen readable documents (the picture pixelizes). For this purpose always use vector graphics. Wikipedia has a nice list of vector graphics software. Among them, I'd recommend the following:


Inkscape (Cross-platform): Although the learning curve is a bit steep, its worth it. Note that Inkscape was not meant for making scientific diagram. Still you'll find a LaTeX plugin, export to LaTeX/PSTricks etc.
Dia (Cross-platform): Very simple and easy, but not very powerful. Can't process LaTeX.
Ipe (Cross-platform): Ipe is something in between Inkscape and Dia. Also Ipe was designed for scientific drawing. Can process LaTeX source code and import PDF figures.
WinFIG (Cross-platform): Although commercial, its quite popular in the scientific community.


EDIT: Wikipedia has a nice page which discusses various software that can be used to create figures. See Wikipedia:How to create graphs for Wikipedia articles
AnswerKenntnis:
I would try matplotlib, but first check here and decide if these pictures satisfies your needs. Also click some picture and inspect source code.
AnswerKenntnis:
Related post in SO.

My personal favorite is Asymtpote which is like MetaPost on steroids. A gallery is here.
AnswerKenntnis:
Sometimes raster graphics is also necessary. I often used POV-Ray to make some illustrations. 


It may be prepared with any size using the same script with description of the picture, that avoid scaling problem. But it is not a graphical editor and fast only for preparation of simple pictures.
AnswerKenntnis:
Just for completeness, I'll leave this here:  

It's always possible to compose your illustrations in raw postscript!  Postscript is itself a Forth-like programming language.  It's particularly useful for illustrations that lend themselves to being generated procedurally.  If postscript itself is too low-level, one can often write a script in some other language that outputs a postscript program.

One superb introduction to producing drawings in raw postscript is this free book: 


Mathematical Illustrations by Bill Casselman.
AnswerKenntnis:
For drawing Feynman diagrams with SVG, I have developed jQuery.Feyn to make it easier (see the screenshot below).
AnswerKenntnis:
I've recently been introduced to GeoGebra, and while I haven't yet had the opportunity to use for any work, I love the interface. For geometrical diagrams it looks spectacular.

I've also used XFig and gnuplot extensively. Particularly if you use $\LaTeX$, these tools serve their purpose very very well.
AnswerKenntnis:
For primitive drawings, I am a big fan of XFig.  The UI is a little clunky, but it can save to dozens of graphics formats and creates figures that are downright trivial to include in a LaTeX document.  The biggest thing for me is that the file format is text-based, so it is completely possible to script more complicated drawings.
AnswerKenntnis:
I too use Mathematica for figures and found it wasn't a great leap from there to using it for drawings. You can draw 2D or 3D primitives pretty easily:

Rectangle[{xmin, ymin}, {xmax, ymax}]


and, like python/matplotlib, being able to parameterise everything allows you to redraw an image for multiple scenarios (or Animate or Manipulate it).

For me the most useful feature is that you can define things in terms of the maths. The MetaPost example mentioned by Marek, in which two curves can be defined and the intersection computed by the drawing package, is handled inherently by Mathematica.
AnswerKenntnis:
I thought a lot about this question since I graduated and began teaching. I think Adobe Illustrator is the best vector image software. It doesn't require any code to draw images; you only have to learn to use some "important" tools. I'm in no way a graphic designer or a professional in Illustrator and I drew this:



Moreover,you can always find tutorials about drawing anything with IllustratorYou can export images from Matlab or Autocad to Illustrator (.ai or .eps)
AnswerKenntnis:
Software for drawing geometry diagrams
Sometimes I use SAGE.
AnswerKenntnis:
To start with, for scientific drawing usually vector graphics is more suitable - scalable, convenient to modify and produce less bulky files.

For simple general-purpose graphics I use OpenOffice.org Draw (I prefer it to Incscape).

For abstract diagrams there is yEd - Graph Editor.

Both are free, for Win/Linux/MacOSX, easy to learn and can export to vector graphics and pdf.
AnswerKenntnis:
I use TKPAINT which still works very well.

http://www.netanya.ac.il/~samy/tkpaint.html

First, one has to download ActiveTcl for Windows or its Tcl counterparts for Linux or whatever you use. It can draw filled or empty disks, ellipses, squares, rectangles, splines, rotate them, quickly copy them, move them, texts with many fonts, colors, grid, and it may be exported as EPS - encapsulated postscript as well - which is a standard way to embed similar diagrams in TeX papers on the arXiv and beyond.

I've used it in many papers when I was writing them.

Cheers
LM
AnswerKenntnis:
Try ConceptDraw Pro for 2D diagrams. This is what I did recently for optics:

Ray Tracing for Convex Lens

Here you can find samples for science and education illustrations.
AnswerKenntnis:
For electric circuits, CircuitLab is a nice online editor and simulator. There are some restrictions to what you can do without an account or with a free account - I can't remember the details - but you can use print-screen to get nice pictures out of it. I like it because it is really at the level of simplicity I need: if I'm explaining a basic electric-circuits question, I do not want to spend more than two minutes drawing, say, five resistors in some parallel/series configuration. CircuitLab gets the job done.

For a tour, see their YouTube video Getting Started with CircuitLab.



Edit: If you want a png output to include in a post on this site, you can go over to Electrical Engineering, which has a built-in implementation in the post editor and simply bring back the image link. Thanks to Chris White for the tip!
AnswerKenntnis:
There is an add-in for Microsoft Word called Science Teacher's Helper.
http://www.helpscience.com

SmartDraw is also an excellent program for creating diagrams.
http://www.smartdraw.com
AnswerKenntnis:
You can also use PLotly, a collaborative, web-based graphing platform with APIs in Python, R, MATLAB, Julia, and Perl. You you can find the code to make these examples in their documentation.

.
AnswerKenntnis:
Look at SCaVis software. It has many examples illustrating how to draw various graphs. Here is the link to the examples
QuestionKenntnis:
If you view the Earth from far enough away can you observe its past?
qn_description:
From my understanding of light, you are always looking into the past based on how much time it takes the light to reach you from what you are observing.

For example when you see a star burn out, if the star was 5 light years away then the star actually burnt out 5 years ago.

So I am 27 years old, if I was 27 light years away from Earth and had a telescope strong enough to view Earth, could I theoretically view myself being born?
AnswersKenntnis
AnswerKenntnis:
Yes, you can.  And you do not even need to leave the Earth to do it.  

You are always viewing things in the past, just as you are always hearing things in the past.  If you see someone do something, who is 30 meters away, you are seeing what happened $(30\;\mathrm{m})/(3\times10^8\;\mathrm{m}/\mathrm{s}) = 0.1\;\mu\mathrm{s}$
in the past.  

If you had a mirror on the moon (about 238K miles away), you could see about 2.5 seconds into earth's past.  If that mirror was on Pluto, you could see about 13.4 hours into Earth's past.

If you are relying on hearing, you hear an event at 30 m away about 0.1 s after it occurs.  That is why runners often watch the starting pistol at an event, because they can see a more recent picture of the past than they can hear.

To more directly answer the intent of your question:  Yes, if you could magically be transported 27 lightyears away, or had a mirror strategically placed 13.5 lightyears away, you could see yourself being born.
AnswerKenntnis:
I'd have to say yes - at least theoretically. If there was an observer (not you) 27 light years away, he could see your birth. Awesome as it sounds, it isn't practically feasible. For one thing, the observer would need a large enough telescope to observe you. If the telescope isn't large enough, the resolution would be low, and the observer wouldn't be able to make out things far away. To get an idea, the size (diameter of mirror for instance) of the telescope that'll enable you to see an object of about 10m at that distance is about $10^{8}$km while the diameter of earth is about 12800 km. 

But again, you yourself wouldn't be able to make this observation, as the information regarding your birth would have gone at the speed of light, and you wouldn't be able to intercept it.
AnswerKenntnis:
No. Because you cannot reach the speed of light, even if you had started travelling away from Earth the day you were born, you could never catch the light carrying the image of your being born.
AnswerKenntnis:
I think the other posters are forgetting that nature allows for much more possibilities. True, you can't catch up to light, but that might not be necessary. As General Relativity tells us, space-time is a 4 dimensional manifold. Einstein's equations tie its local structure to the energy-momentum density. But that still leaves open a lot of possibilities for the global structure of the universe. 

Imagine that the universe would be something like a torus or a sphere, that is finite but unbounded. This concretely means that if you go long in enough in one direction, you will ultimately end up where you started again. Now, that is also true for light. If then the expansion of the universe did not happen so fast that light going around got behind the cosmological horizon, it is theoretically possible that we could see our own galaxy in an earlier stage of its evolution. 

The global shape of the universe is still an open problem. The most commonly accepted model though assumes an infinite flat space-time. Other models like Poincar├⌐ dodecahedral space and the Picard horn do seem to fit current cosmological data. Well, they did last time I looked, maybe they have been refuted by now.

But if the universe is well described by a Poincar├⌐ dodecahedral space FLRW model, then there is a chance that we can see our own past for a limited amount of time, i.e. until it disappears beyond the cosmological horizon. Don't hope to see yourself though.
AnswerKenntnis:
Technically, yes, though the idea of an absolute past kinda screws with the whole question. Better would be a light cone-based analysis. But yes, if you were in the past light cone of a past Earth, then you would see that past Earth.
AnswerKenntnis:
No, I don't think the concept of time works quite like that.  Yes, if a star is 5 light years away and it burned out - yes it burned out 5 years ago for that point in time.  Space time is relevant for that time period.  

In your example: 
You are 27 Years old (Present Day Earth Time)

You are teleported 27 Light Years Away to Planet X (Your Present Age is 27 on Planet X).

You look back at Earth - You are non-existent on Earth.

Because Earth's time does not bend back.  It took 27 years for Earth's image to reach you at the time you left. So unless you were at both planets at the same time you would be 27 on Planet X while on Earth you would be 54 Years old.  Earth's time does not bend back, it only moves forward.
QuestionKenntnis:
Very strange shadow phenomenon
qn_description:
I was laying on my bed, reading a book when the sun shone through the windows on my left. I happened to look at the wall on my right and noticed this very strange effect. The shadow of my elbow, when near the pages of the book, joined up with the shadow of the book even though I wasn't physically touching it. 

I have posted a video of the event here. (Note: the video seems to be wrong way up but you still get the idea of what is happening)

What is causing this? Some sort of optical illusion where the light get's bent? 
Coincidentally, I have been wondering about a similar effect recently where if you focus your eye on a nearby object, say, your finger, objects behind it in the distance seem to get curved/distorted around the edge of your finger. It seems awfully related...

EDIT: I could see the bulge with my bare eyes to the same extent as in the video! The room was well light and the wall was indeed quite bright.
AnswersKenntnis
AnswerKenntnis:
As said by John Rennie, it has to do with the shadows' fuzzyness. However, that alone doesn't quite explain it.

Let's do this with actual fuzzyness:



I've simulated shadow by blurring each shape and multiplying the brightness values1. Here's the GIMP file, so you can see how exactly and move the shapes around yourself.

I don't think you'd say there's any bending going on, at least to me the book's edge still looks perfectly straight.

So what's happening in your experiment, then?

Nonlinear response is the answer. In particular in your video, the directly-sunlit wall is overexposed, i.e. regardless of the "exact brightness", the pixel-value is pure white. For dark shades, the camera's noise surpression clips the values to black. We can simulate this for the above picture:



Now that looks a lot like your video, doesn't it?

With bare eyes, you'll normally not notice this, because our eyes are kind of trained to compensate for the effect, which is why nothing looks bent in the unprocessed picture. This only fails at rather extreme light conditions: probably, most of your room is dark, with a rather narrow beam of light making for a very large luminocity range. Then, the eyes also behave too non-linear, and the brain cannot reconstruct how the shapes would have looked without the fuzzyness anymore.

Actually of course, the brightness topography is always the same, as seen by quantising the colour palette:





1To simulate shadows properly, you need to use convolution of the whole aperture, with the sun's shape as a kernel. As Ilmari Karonen remarks, this does make a relevant difference: the convolution of a product of two sharp shadows $A$ and $B$ with blurring kernel $K$ is

$$\begin{aligned}
  C(\mathbf{x}) =& \int_{\mathbb{R}^2}\!\mathrm{d}{\mathbf{x'}}\:
                    \Bigl(
                      A(\mathbf{x} - \mathbf{x}') \cdot B(\mathbf{x} - \mathbf{x'})
                    \Bigr) \cdot K(\mathbf{x}')
      \\        =& \mathrm{IFT}\left(\backslash{\mathbf{k}} \to
                      \mathrm{FT}\Bigl(\backslash\mathbf{x}' \to 
                          A(\mathbf{x}') \cdot B(\mathbf{x}')
                        \Bigr)(\mathbf{k})
                      \cdot \tilde{K}(\mathbf{k})
                    \right)(\mathbf{x})
\end{aligned}
$$

whereas seperate blurring yields

$$\begin{aligned}
  D(\mathbf{x}) =& \left( \int_{\mathbb{R}^2}\!\mathrm{d}{\mathbf{x'}}\:
                      A(\mathbf{x} - \mathbf{x}')
                    \cdot K(\mathbf{x}')  \right)
      \cdot \int_{\mathbb{R}^2}\!\mathrm{d}{\mathbf{x'}}\:
                      B(\mathbf{x} - \mathbf{x'})
                    \cdot K(\mathbf{x}')
      \\        =& \mathrm{IFT}\left(\backslash{\mathbf{k}} \to
                      \tilde{A}(\mathbf{k}) \cdot \tilde{K}(\mathbf{k})
                    \right)(\mathbf{x})
                  \cdot \mathrm{IFT}\left(\backslash{\mathbf{k}} \to
                           \tilde{B}(\mathbf{k}) \cdot \tilde{K}(\mathbf{k})
                          \right)(\mathbf{x}).
\end{aligned}
$$

If we carry this out for a narrow slit of width $w$ between two shadows (almost a Dirac peak), the product's Fourier transform can be approximated by a constant proportional to $w$, while the $\mathrm{FT}$ of each shadow remains $\mathrm{sinc}$-shaped, so if we take the Taylor-series for the narrow overlap it shows the brightness will only decay as $\sqrt{w}$, i.e. stay brighter at close distances, which of course surpresses the bulging.

And indeed, if we properly blur both shadows together, even without any nonlinearity, we get much more of a "bridging-effect":



But that still looks nowhere as "bulgy" as what's seen in your video.
AnswerKenntnis:
It's because the Sun is not a point source, so the edges of the shadows are slightly fuzzy. This is my rather crude attempt to show why this happens:



There are far better diagrams in the Wikipedia article on the umbra that explains what is going on. The fuzzy bit at the edge of the shadow is called the penumbra.

The reason you see the bulge where the shadows approach is due to the penumbra and the fact that the human eye isn't that great at handling contrast. As the two shadows approach, but before they touch, their penumbras (penumbrae?) overlap. This means the region between the shadows is darker than the rest of the penumbra. Another rather crude diagram follows:



This isn't a great diagram because the density of the penumbra isn't constant, but rather shades from black to white across its width. However Google Draw doesn't do gradient fills so I'm stuck with a rather poor representation. Anyhow, it should hopefully be obvious that where the penumbras overlap they darken each other, so the region between the two shadows gets darker. Because the eye isn't good at handling a wide contrast range it looks as if the shadows have grown a bulge towards each other.
AnswerKenntnis:
I believe you'll find the cause is diffraction, as described in this article.
The photo shows a similar effect when holding two fingers close together. 

Black Drop Effect described in Sky and Telescope

As you bring your elbow and book together, you're creating a diffraction pattern as can be seen in the image in the article below, a brighter light in the middle with black bands on either side.  As you bring them even closer together, the black bands grow closer together.  This is why they appear to suddenly "jump" towards each other.   

Image of standard diffraction pattern
AnswerKenntnis:
I do not guarantee this is the correct answer, but you can verify it practically. 

The light rays causing the shadow of your hand and that of your book, are not parallel. The bulging of shadows as they are near each other, means that both the  bulged shadows are of the elbow. So, at the point when your elbow is near your book, two light sources (or bending of light such that unparallel rays of light act on the elbow causing two shadows) are acting on the elbow. 

I think you too have guessed this.You can verify it by doing two things:


Opening the window so that direct light can fall. This will tell if bending is caused by the glass window.
Change of height of the book and the elbow wrt. the  window. Try to see if the effect is observed at all heights.
AnswerKenntnis:
Coincidentally, I have been wondering about a similar effect recently where if you focus your eye on a nearby object, say, your finger, objects behind it in the distance seem to get curved/distorted around the edge of your finger. It seems awfully related...


You are right they are related. This is another diffraction effect. You are seeing single edge diffraction, also known as the knife edge effect. Here is a web page describing it. 

Light passing very near your finger is deflected. Since the direction has changed, it appears to come from a different place.
AnswerKenntnis:
Try this out in a room that has one of those ceiling fans with four light bulbs. The multiple sources of light produce multiple shadows that are clearly visible and you can see this sort of phenomenon take effect. As the shadows overlap it gives this appearance and produces the phenomena spoken of. In normal situations the multiple sources of light encountered can often times be light that is reflected off of walls and nearby objects, anything that is white or lighter colored. The multiple shadows produced are so faint they are not even noticeable until they overlap other shadows. Sources of light from different angles produce multiple shadows. When your elbow moves toward the book, for example, multiple shadows from both objects overlap where they appear to combine. What looks like the edge of the shadow of a single object is the edge of where the layers of shadows overlap each other the most.
QuestionKenntnis:
List of freely available physics books
qn_description:
I'm trying to amass a list of physics books with open-source licenses, like Creative Commons, GPL, etc. The books can be about a particular field in physics or about physics  in general.

What are some freely available great physics books on the Internet?
edit: I'm aware that there are tons of freely available lecture notes online. Still, it'd be nice to be able to know the best available free resources around.

As a starter:
http://www.phys.uu.nl/~thooft/theorist.html



jump to list sorted by medium / type



Table of contents sorted by field (in alphabetical order):


Chaos Theory
Earth System Physics
Mathematical Physics
Quantum Field Theory
AnswersKenntnis
AnswerKenntnis:
Books

Galileo and Einstein very interesting book, 200 pages, by Michael Fowler , Text for Physics 109, Fall 2009  (from Babylonians and Greeks to Einstein)

Physics Made Easy  Karura notes

Classical and quantum mechanics via Lie algebras  by Arnold Neumaier, Dennis Westra , 502 pages, (arxiv) 

by Hans de Vries: 'Physics Quest' Understanding Relativistic Quantum Field Theory
- I love this  'book in progress' to understand Special Relativity, and beyond. To see how a real Lorentz contraction do happen (ch. 4) and how magnetic field is induced  by electrostactic field and Non-simultaneity (it is like a Coriollis effect)  

by Benjamin Crowell: 'Light and Matter' - General Relativity
  explore other physics topics here http://www.lightandmatter.com/

by Bo Thid├⌐: Electromagnetic Field Theory  - advanced Electrodynamics textbook

Elecromagnetic Fields and Energy MIT Hypermedia Teaching Facility, by Herman A. Haus and James R. Melcher (with media)

HyperPhysics - everything, in short.  

the physics hypertextbook detailed online book, very interesting Work in Progress.  

Relativity - The Special and General Theory by Albert Einstein (1920)

Feynman Lectures (pdfs) (final index)  

Wikipedia Physics a portal to start digging. A colaborative gigantic work. 

WikiBooks -SR a textbook on Relativity. 

WikiSource - Relativity Portal find here "The Measure of Time" by Henri Poincar├⌐ and many other original sources.

Stanford Encyclopedia of Philosophy a plethora of info related to physics (for ex. singularities)

EXPLORING THE BIOFLUIDDYNAMICS OF SWIMMING AND FLIGHT David Lentink
The Physics of Waves by HOWARD GEORGI of Harvard
The Physics of Ocean Waves (for physicists and surfers), by Michael Twardos at UCI
Photonics - The Basics and Applications 92 pages , University of Pennsylvania
Photonic Crystals: Molding the Flow of Light 305 pages, Joannopoulos et al, Princeton Univ Press
Computational Genomics  Algorithms in Molecular Biology, Lecture notes by Ron Shamir  (pdfs)

Motion Mountain by Christoph Schiller



Journals open access and online collections

PSE-list-open-access-journals    

Directory of Open Access Journals free, full text, quality controlled scientific and scholarly journals (6286, been 2735 searchable at article level)  

MathPages  - lectures on various subjects in physics and mathematics.

livingreviews journal  articles by invitaton on relativity and beyond  

livingreviews blog       about the journal articles

Calphysics research on the electromagnetic quantum vacuum  (with care, controversial material)

MIT - OpenCourseWare  Several courses available
MIT OCW fundamentals-of-photonics-quantum-electronics  download pdfs 



Sources to use with precaution

Preprints

ARXIV    door to papers that I cannot afford (sometimes good ideas) -- Cornell univ controlled
I follow this archive thru this MIT's blog The Physics arXiv Blog  

VIXRA    free to post the ugly, the bad, the crazy, and sometimes good ideas
Independent researchers can publish here. The arXiv is usually closed to authors without academic affiliation.  



Portals

Archive.org Access to a world of original digitized books, and much more.
NASA ADS Absctract Data Service search
Scribd - a generic social publishing site where I find books (scientific/technical) with full or partial access.
scholar.google.com  from the giant that is changing the observable universe of Human beings
Cosmos Portal  from Digital Library
Encyclopedia of Earth  from Digital Library
NanoHub - A resource for nanoscience and technology    



Multimedia

youtube Berkeley Chanel  with courses

Richard Feynman - Science Videos - 4 original videos (recorded at Auckland)  arguably the greatest science lecturer ever.  

Videos for Shiraz's lectures on String Theory  

Leonard Susskind - Modern Theoretical Physics from his "physics for everyone" blog  

Fundamentals of Nanoelectronics NanoHub - Lectures (Purdue Univ ref. ECE 495N)
  including Lecture 10: Shr├╢dinger's Equation in 3-D (mp4)   



Math

Multivariable calculus and vector analysis A set of on-line readings (Interactive click and drag with LiveGraphics3D) ;  explore tab Topics

KhanAcademy (videos)  mission: to deliver a world-class education to anyone anywhere

Math and physics online tools:

Online Latex Equation Editor (right click the result and apply anywhere)
wolframalpha - computational knowledge engine, do you want to calculate or know about?
 sage online  - support a viable open source alternative to Magma, Maple, Mathematica, and MATLAB.
Euler Math Toolbox  free software for numerical and for symbolic computations
geogebra     - Free mathematics software for learning and teaching   



Modeling and simulation

OpenModelica Physical modeling and simulation environment
Elmer Open Source Finite Element Software for Multiphysical Problems  (examples)
Mason  Multiagent based simulation   (IA)
ECJ    Evolutionary Computation      (IA)
Breve  A 3d Simulation Environment for Multi-Agent Simulations and Artificial Life 
(IA)
NanoHub-Periodic Potential Lab solves the time independent Schroedinger Equation in a 1-D spatial potential variation

demonstrations.wolfram 7050 applets 



Astronomy and astrophysics

Books and reviews

Cosmology today-A brief review (arxiv 2011)
This is a brief review of the standard model of cosmology. We first introduce the FRW models and their flat solutions for energy fluids playing an important role in the dynamics at different epochs. We then introduce different cosmological lengths and some of their applications. The later part is dedicated to the physical processes and concepts necessary to understand the early and very early Universe and observations of it.

review of Big Bang Nucleosynthesis (BBN)  (arxiv 2008)

Portals

astro-canada Introduction to astronomy, light, instruments, etc.  

Data

simbad search data on celestial bodies with the proper tools.
NASA PDS: The Planetary Data System data related to Nasa missions  

Sky viewers

Skyview , Nasa  SkyView is a Virtual Observatory on the Net
WWT World Wide Telescope, Microsoft  

Simulation and presentations

Celestia  free space simulation that lets you explore our universe in three dimensions.
astrolab  presentations_astronomiques (FR)



Other resources

Kirk McDonald page at Princeton.edu a handful of resources on EM,QED,QM (+-5Gb ;-) 

Springerlink's LaTeXsearch you can search articles by using latex formulas input
AnswerKenntnis:
Quantum field theory

Fields, by W. Siegel 

Quantum Field Theory, by Mark Srednicki

Superspace, or One thousand and one lessons in supersymmetry
by S.J. Gates Jr, M.T. Grisaru, M. Rocek and W. Siegel
AnswerKenntnis:
Mathematical Tools for Physics, James Nearing

Also available in paperback from Dover.  Undergraduate-level math methods book.  Clear writing, many problems and exercises (usually without solution).  IMHO better than Boas.
AnswerKenntnis:
Structure and Interpretation of Classical Mechanics Sussman, Wisdom, Mayer

A No-Nonsense Introduction to General Relativity, Sean Carroll
AnswerKenntnis:
Applications of Classical Physics, Kip Thorne and Roger Blandford

Unpublished so far, this book covers special relativity, thermodynamics and statistical mechanics, continuum mechanics, and some general relativity at the graduate level.  I've read about a third of it.  It's well-written and surprisingly-polished for being a freely-available preprint.

Problem sets and solutions are available on the website as well.
AnswerKenntnis:
Sidney Coleman's QFT lecture notes
and videos
AnswerKenntnis:
The art of computational science, Piet Hut and Jun Makino

A computational lab for N-body experiments. Includes books and source code for doing simple N-body simulations.
AnswerKenntnis:
1951 Lectures on Advanced Quantum Mechanics Second Edition, Freeman J. Dyson
Introductory Lectures on Quantum Field Theory, Luis Alvarez-Gaume, Miguel A. Vazquez-Mozo
AnswerKenntnis:
Earth System Physics

Radiative Transfer in the Earth System, Charlie Zender (All of Zender's books are GNU FDL)

Natural Aerosols in the Climate System, C. Zender

Particle Size Distributions: Theory and Application to Aerosols, Clouds, and Soils, C. Zender

Introduction to Physical Oceanography, Robert Stewart open source, accepting contributions
AnswerKenntnis:
Fitzpatrick's The Physics of Plasmas is excellent. He also has notes on Classical Electromagnetism, Quantum Mechanics, and more, but I'm less familiar with these works.
AnswerKenntnis:
Street-Fighting Mathematics, Sanjoy Mahajan

Short book covering dimensional analysis, estimation, and visualizing mathematics.  The goal is to make mathematics easier and more useful for scientists and engineers, specifically physicists.  (Mahajan is a physicist/educator).  Uses mostly undergrad and high school level math.

A version is available free as a pdf under the creative commons license. 

The paper version is available for purchase from the MIT press.
AnswerKenntnis:
Handbook of Quantum Information "an encyclopaedia of everything quantum"
AnswerKenntnis:
Howard Georgi's The Physics of Waves

I wish someone would make the (now mostly out of print) Berkeley Physics Course volumes public domain.  Or at least Dover could start cranking out cheap versions.
AnswerKenntnis:
Making, probing and understanding ultracold Fermi gases

"A review on superfluidity and the BEC-BCS crossover in ultracold Fermi gases."

Wolfgang Ketterle (Nobel 2001), Martin W. Zwierlein
AnswerKenntnis:
The FreeScience website links to a large listing of free books on a wide range of physics, math, and other fields of science.
AnswerKenntnis:
Chaos: Classical and Quantum, Cvitanovic┬┤, Artuso, Mainieri, Vattay 

Mathematical Principles of Natural Philosophy, Newton, translated by Motte
AnswerKenntnis:
Photonic Crystals:
Molding the Flow of Light
second edition

http://ab-initio.mit.edu/book/
AnswerKenntnis:
A nice set of books have been made available through project Gutenberg, are made available through the Open Ebooks Library, including:

Handbook of Formula and Physical constants

Space, Time and Gravitation - An Outline of the General Relativity Theory, Sir Arthur Stanley Eddington (beware of this book)

Relativity : the Special and General Theory, Albert Einstein
AnswerKenntnis:
Quantum Mechanics for Engineers


http://www.eng.fsu.edu/~dommelen/quantum/


I stumbled across this book the other day when I was looking for a text on nuclear physics. It seems like a handy resource.

To quote the preface:

The book was primarily written for engineering graduate students who find themselves caught up in nano technology. It is a simple fact that the typical engineering education does not provide anywhere close to the amount of physics you will need to make sense out of the literature of your field. You can start from scratch as an undergraduate in the physics department, or you can read this book.
AnswerKenntnis:
Two freely available stellar physics books by George W. Collins II:


The Fundamentals of Stellar Astrophics
The Virial Theorem in Stellar Astrophysics
AnswerKenntnis:
Someone has mentioned the archive.org.

I have found Landau's Course of Theoretical Physics there, without Volumes 4 and 9.

You can also get Feynman's book on Quantum Electrodynamics there.
AnswerKenntnis:
The OpenStax College Physics text looks promising as a low-level introduction to the subject (algebra-based).

Meanwhile, looking at the other list(s) here, is it really true that Feynman's lectures (and QED) are legally available online for free today? I have serious doubts about that. Has anyone yet made an effort to go through this list and verify the legality (let alone "openness") of the resources linked?
AnswerKenntnis:
I am a big fan of David Tong's lecture notes on Relativity, Classical Mechanics, QFT, Stat Mech, Soitons, String Theory and Kinetic theory available free.

http://www.damtp.cam.ac.uk/user/tong/teaching.html

I had learned classical mechanics first time as an undergrad from his notes, and I absolutely fell in love with it. The notes on QFT are also excellent to prep you for things like Peskin & Schroeder.
AnswerKenntnis:
I would like to recommend my favourite site
http://www.freebookcentre.net/Physics/Physics-Books-Online.html
QuestionKenntnis:
Stupid yet tricky question: Why do we actually see the sun?
qn_description:
I haven't yet gotten a good answer to this: If you have two rays of light of the same wavelength and polarization (just to make it simple for now, but it easily generalizes to any range and all polarizations) meet at a point such that they're 180 degrees out of phase (due to path length difference, or whatever), we all know they interfere destructively, and a detector at exactly that point wouldn't read anything.

So my question is, since such an insanely huge number of photons are coming out of the sun constantly, why isn't any photon hitting a detector matched up with another photon that happens to be exactly out of phase with it? If you have an enormous number of randomly produced photons traveling random distances (with respect to their wavelength, anyway), that seems like it would happen, similar to the way that the sum of a huge number of randomly selected 1's and -1's would never stray far from 0. Mathematically, it would be:

$$\int_0 ^{2\pi} e^{i \phi} d\phi = 0$$

Of course, the same would happen for a given polarization, and any given wavelength.

I'm pretty sure I see the sun though, so I suspect something with my assumption that there are effectively an infinite number of photons hitting a given spot is flawed... are they locally in phase or something?
AnswersKenntnis
AnswerKenntnis:
First let's deal with a false assumption:


  similar to the way that the sum of a huge number of randomly selected 1's and -1's would never stray far from 0.


Suppose we have a set of $N$ random variables $X_i$, each independent and with equal probability of being either $+1$ or $-1$. Define
$$ S = \sum_{i=1}^N X_i. $$
Then, yes, the expectation of $S$ may be $0$,
$$ \langle S \rangle = \sum_{i=1}^N \langle X_i \rangle = \sum_{i=1}^N \left(\frac{1}{2}(+1) + \frac{1}{2}(-1)\right) = 0, $$
but the fluctuations can be significant. Since we can write
$$ S^2 = \sum_{i=1}^N X_i^2 + 2 \sum_{i=1}^N \sum_{j=i+1}^N X_i X_j, $$
then more manipulation of expectation values (remember, they always distribute over sums; also the expectation of a product is the product of the expectations if and only if the factors are independent, which is the case for us for $i \neq j$) yields
$$ \langle S^2 \rangle = \sum_{i=1}^N \langle X_i^2 \rangle + 2 \sum_{i=1}^N \sum_{j=i+1}^N \langle X_i X_j \rangle = \sum_{i=1}^N \left(\frac{1}{2}(+1)^2 + \frac{1}{2}(-1)^2\right) + 2 \sum_{i=1}^N \sum_{j=i+1}^N (0) (0) = N. $$
The standard deviation will be
$$ \sigma_S = \left(\langle S^2 \rangle - \langle S \rangle^2\right)^{1/2} = \sqrt{N}. $$
This can be arbitrarily large. Another way of looking at this is that the more coins you flip, the less likely you are to be within a fixed range of breaking even.



Now let's apply this to the slightly more advanced case of independent phases of photons. Suppose we have $N$ independent photons with phases $\phi_i$ uniformly distributed on $(0, 2\pi)$. For simplicity I will assume all the photons have the same amplitude, set to unity. Then the electric field will have strength
$$ E = \sum_{i=1}^N \mathrm{e}^{\mathrm{i}\phi_i}. $$
Sure enough, the average electric field will be $0$:
$$ \langle E \rangle = \sum_{i=1}^N \langle \mathrm{e}^{\mathrm{i}\phi_i} \rangle = \sum_{i=1}^N \frac{1}{2\pi} \int_0^{2\pi} \mathrm{e}^{\mathrm{i}\phi}\ \mathrm{d}\phi = \sum_{i=1}^N 0 = 0. $$
However, you see images not in electric field strength but in intensity, which is the square-magnitude of this:
$$ I = \lvert E \rvert^2 = \sum_{i=1}^N \mathrm{e}^{\mathrm{i}\phi_i} \mathrm{e}^{-\mathrm{i}\phi_i} + \sum_{i=1}^N \sum_{j=i+1}^N \left(\mathrm{e}^{\mathrm{i}\phi_i} \mathrm{e}^{-\mathrm{i}\phi_j} + \mathrm{e}^{-\mathrm{i}\phi_i} \mathrm{e}^{\mathrm{i}\phi_j}\right) = N + 2 \sum_{i=1}^N \sum_{j=i+1}^N \cos(\phi_i-\phi_j). $$
Paralleling the computation above, we have
$$ \langle I \rangle = \langle N \rangle + 2 \sum_{i=1}^N \sum_{j=i+1}^N \frac{1}{(2\pi)^2} \int_0^{2\pi}\!\!\int_0^{2\pi} \cos(\phi-\phi')\ \mathrm{d}\phi\ \mathrm{d}\phi' = N + 0 = N. $$
The more photons there are, the greater the intensity, even though there will be more cancellations.



So what does this mean physically? The Sun is an incoherent source, meaning the photons coming from its surface really are independent in phase, so the above calculations are appropriate. This is in contrast to a laser, where the phases have a very tight relation to one another (they are all the same).

Your eye (or rather each receptor in your eye) has an extended volume over which it is sensitive to light, and it integrates whatever fluctuations occur over an extended time (which you know to be longer than, say, $1/60$ of a second, given that most people don't notice faster refresh rates on monitors). In this volume over this time, there will be some average number of photons. Even if the volume is small enough such that all opposite-phase photons will cancel (obviously two spatially separated photons won't cancel no matter their phases), the intensity of the photon field is expected to be nonzero.

In fact, we can put some numbers to this. Take a typical cone in your eye to have a diameter of $2\ \mathrm{┬╡m}$, as per Wikipedia. About $10\%$ of the Sun's $1400\ \mathrm{W/m^2}$ flux is in the $500\text{ΓÇô}600\ \mathrm{nm}$ range, where the typical photon energy is $3.6\times10^{-19}\ \mathrm{J}$. Neglecting the effects of focusing among other things, the number of photons in play in a single receptor is something like
$$ N \approx \frac{\pi (1\ \mathrm{┬╡m})^2 (140\ \mathrm{W/m^2}) (0.02\ \mathrm{s})}{3.6\times10^{-19}\ \mathrm{J}} \approx 2\times10^7. $$ The fractional change in intensity from "frame to frame" or "pixel to pixel" in your vision would be something like $1/\sqrt{N} \approx 0.02\%$. Even give or take a few orders of magnitude, you can see that the Sun should shine steadily and uniformly.
AnswerKenntnis:
Chris White wonderfully addresses this with some Statistics, but there's a less math-y way of looking at it, too. Firstly, to dispel this notion:


  So my question is, since such an insanely huge number of photons are coming out of the sun constantly, why isn't any photon hitting a detector matched up with another photon that happens to be exactly out of phase with it?


There's an equal chance that a photon will be matched up with another photon of the same phase as it will be with an opposite phase. The phase of each photon entering is an independent variable. If we're talking about two photons, then there's an equal chance of constructive interference a there is of destructive interference. This holds even if you scale up. (See last section if you're not convinced of this)

There are basically three things that you must note here:


The average value of a distribution is not always the most likely value. Indeed, it may not even be a possible value.
Our eyes measure intensity, not amplitude. We do not distinguish between positive and negative amplitude. Rhodopsin works by absorbing energy, which does not distinguish between the sign of the phase
Interference is local, not global. If one of your retinal rod cells receives positive-phase light and the other receives negative-phase light, there will be no cancelling out.


Energy conservation argument

Here's a very simple way of looking at it. Due to energy conservation, if there is destructive interference there must be constructive interference elsewhere. Otherwise one could cleverly place detectors and create/destroy energy at will.

Since the light from the sun is incoherent, at any given point in time, approximately half of the spots on a sphere drawn around it will have constructive interference, and half will have destructive (not necessarily full destructive, just that the net energy is less) interference. These spots will change at random -- if a spot had constructive interference at one moment, it could have destructive interference the next.

With this in mind, there will always be some significant fraction of your rod/cone cells (which take up a small sliver of this imaginary sphere) receiving constructively-interfered light.  That's enough for you to be able to see.

Why it holds even when you scale up

I'm using + to mean positive phase, and - to mean negative phase. I'm neglecting the fact that phase is not just a binary value, as this involves calculations (see Chris White's answer). A number next to the sign is the new amplitude if it has changed.

The basic thing going on here is that the mean value is not always the most probable value. Take the case of three photons:

 1   2   3   Amplitude  Intensity
 +   +   +   +3         9
 +   +   -   +1         1
 +   -   +   +1         1
 +   -   -   -1         1 
 -   +   +   +1         1
 -   +   -   -1         1
 -   -   +   -1         1
 -   -   -   -3         9


(Avg intensity is 3)

Note the lack of a 0 in the output column. 0 is the mean output amplitude, but it never is observed as a value of the output phase. In the case of a continuous set of phases, a case of total destructive interference is possible, and it is the mean phase, however, there are man, many other final phase values that are more probable.

If you make this chart for any odd value, you'll always have no total destructive interference. If you make it for any even value, half of the time you get destructive interference, however the other half you get constructive interference, so total destructive interference does not occur. In all cases, the average intensity will always be equal to the number of incident photons. You can scale this up as much as you want, it won't change.
AnswerKenntnis:
Your integral is a great representation for the sum of a collection of oscillators that are coherent in time and have the same amplitudes.  But your critical mistake is in assuming that those oscillators have constant frequencies and amplitudes.  That's just not true, because the sources for each of those oscillators is changing violently in time.  (The "surface" of the sun is a violent place.)  And that means that your integral isn't a good model for the sun.

In particular, all those different oscillators have different amplitudes.  And your integral represents the limit of a sum of a really large number of oscillators, with all different amplitudes.  So it should really be more like
\begin{equation}
\int_0^{2\pi} A(\phi)\, e^{i\phi} d\phi~,
\end{equation}
where $A(\phi)$ is the total amplitude of all the oscillators with phases between $\phi$ and $\phi+d\phi$ (loosely speaking).  And this integral is not zero except for very special functions $A(\phi)$.  And in random processes, "very special" things happen "almost never".

So the question becomes: what is $A(\phi)$?  Well, it's time-dependent because it represents the state of the oscillators at that instant in time.  But just thinking of one instant of time, it's the sum resulting from a pretty random distribution of oscillators.  Now, you do have a really large total number of oscillators (because the sun is large), but it's still a finite number.  And the integrand narrows that finite number down to an infinitesimal.  So $A(\phi)$ won't actually be averaging over a large number of oscillators at all.  Even if the average for $A(\phi)$ were zero, you would never really get zero; it would generally be some random nonzero number.  It certainly won't be a constant function of $\phi$.  And there's no reason for it to be periodic in $\phi$.  Therefore, the integral will be nonzero in general.

In fact, the total value of the integral will essentially be a random number.  So you can ask, what's the probability a random (real) number being exactly zero?  And the answer is: zero.  You will never see a perfect total cancellation of photons from the sun.
AnswerKenntnis:
Even though you are talking of photons, you are not thinking of them as particles.

Particle means that  given a screen depicted with  an x and y axis , (or your retina), each individual photon will hit at a specific (x,y) point and be detected as a particle. The interference appears with a build up of many many individual  hits on the screen, if there is the necessary phase coherence. 

It is true that the classical wave framework of light blends smoothly with the photon particle framework but that does not mean that the individual photons are spread out all over the (x,y) plane. Each will hit one point. It might help if you contemplate the build up of the quantum mechanical probabilistic interference pattern one electron at a time in the two slit experiment which shows the interference pattern, a probability distribution. Photons are equally particles and quantum mechanical probability waves.

The zillions of photons from the sun are not coherent and their hits will appear randomly on the screen; or your retina creating an image of the sun, but be carefull to wear appropriate goggles so  as not to burn it.

Edit in answer to comment:

The concept of light as waves works because there is consistency between the particle/probability_wave nature of the photon and the classical electromagnetic wave that creates interference patterns visible to the naked eye . When one mixes the two concepts, photon and classical wave, paradoxical situations seem to appear. Chris (photons) and Mike (classical waves) are giving you the mathematics of it.  In your question you mix the two frameworks, classical wave and photons. When you say 1s and -1s will add up statistically close to zero you are using the particle concept, because the addition happens at a specific (x,y). When you are assigning the pluses and minuses you are using the classical concept , where the phase is kept over the whole x,y plane. This is not true for incoherent sources from the sun. It   is true for lasers where the two frameworks overlap consistently and phases are kept over the x,y plane. The sun is not a laser. If it were a laser, depending on the position of the screen interference patterns would appear, and there would be regions with zero energy, the energy having gone to the bright regions. Energy is conserved in all physics frameworks.
AnswerKenntnis:
Two photons of the same wavelength do not interfere destructively everywhere. Typically, you would obtain fringes. The total energy remains the same as two photons but distributed differently. For two other photons, you might obtain another pattern. If you add many many photons, all these patterns will merge so you won't see them (You can see interferences only when most of the photons are coherent). Overall, what you can see is a uniform irradiation.
AnswerKenntnis:
I would prefer to deal with scattering waves instead of photons (itΓÇÖs too hard for me to envision photons with frequency) but the answer is the same.

Naively, I first would say that light coming from the sun to the earth is an example of scattering in the forward direction and is in-phase. Why? Sunlight, coming from a far distance, scatters from the atmosphere and all the scattered wavelets add constructively (their light paths donΓÇÖt change very much) with each other in the forward direction. Therefore, the waves all arrive at earth pretty much in-phase. 

However, if we bring in some lateral scattering, then I think about it this way:  sunlight arriving at earthΓÇÖs atmosphere (made up of zillions of independent molecules randomly arrayed) will have secondary wavelets with phases that have no particular relationship to one another. That is, the wavelets arriving at some point P has a jumble of different phases and tend not to interfere in a sustained constructive or destructive fashion. So to answer your question: some photons do interference destructively but not in a sustain fashion.

This is best appreciated from a phasor viewpoint ΓÇô as the wavelets arrive at some point P the phasors have randomly large phase angles differences with respect to each other. When adding the tips-to-tails, they sum up to zero, exactly like your integral shows.
QuestionKenntnis:
How can you weigh your own head in an accurate way?
qn_description:
I read some methods but they're not accurate. They use the Archimedes principle and they assume uniform body density which of course is far from true. Others are silly like this one:

Take a knife then remove your head.
Place it on some scale
Take the reading
re-attach your head.

I'm looking for some ingenious way of doing this accurately without having to lie on your back  and put your head on a scale which isn't a good idea.
AnswersKenntnis
AnswerKenntnis:
Get someone to relax their neck as much as possible, stabilize their torso, then punch them in the head with a calibrated fist and measure the initial acceleration.  Apply $\vec F=m \vec a$.
AnswerKenntnis:
Petroleum engineers would all provide you with the same answer "use Compton scattering", as this is how the mass density of rock formations gets measured deep in oil wells. 

A more complete answer is: Compton scattering can provide you with a measurement of the bulk density of your head. Combine this with a volumetric measurement (dipping your head in a tank and measuring the rise in level), and you're done. You also need a (once off) calibration of the density measurement, as explained below. The big advantage of such a Compton measurement is that it can be repeated and automated at will. Based on accuracies obtained in boreholes, I would expect Compton measurement capable of yielding head weights with a sub-percentage accuracy.

The Compton scattering measurement is done by scanning your head with a gamma ray beam, and measuring the beam attenuation. The gamma ray source should create photons with high enough energies to interact by Compton scattering, but low enough to avoid pair production. 137Cs provides a good source. Although Compton scattering depends on electron density and not on mass density, the measurements can be calibrated to give the correct bulk density for human heads using the ratio of the electron density to the nucleon density. This is based on the observation that the electron density is equal to the bulk density multiplied by Z/A where Z is the average atomic number and A the average atomic weight of the atoms in a human head. In practice, this calibration is best done using heads decapitated from dead bodies.
AnswerKenntnis:
Thanks a lot for your votes for the "answer" below.  Unfortunately I
think now the
solution does not work.  It is great for two slices, but that is the
end.

There is another solution that should give 3 slices, which is still a
bit short. And I am afraid I do not see how to use the two available steps to start building a recursion :-)

Why ?  It is clearly possible to cut the rod in $n$ "homogenous"
slices, and then do the $n$ measurements, so as to get a set of $n$
equations with $n$ unknown, the weights of each slice. So we feel the problem conquered. The hitch is
that these equations are linearly dependent and have an infinity of
solutions, because balancing the rod depends only on the center of mass
and the total weight.

Equations are a tricky business, especially when we forget to solve them.

Too bad, Especially today:
I am in France right now, and the country has its national holidays :
july 14th, aka Bastille day.

So I was hoping that this year, cutting heads could be declared
unnecessary.

Let's try to do better for next year.
And do not lose hope ... mistakes are interesting too, in their own way, and very educational.



My first reaction was to measure volume with archimedes principle. But
that does not work since the body density is variable. I was realizing
that as @Manishearth made the comment.

An alternative, is to put yourself on a scale composed of a plank with
a central pivot so that only your head is on one side. And you balance
the difference between head and body with weights. This does not
work a priori because you do not know the mass repartition of your
body, and not all the mass has the same distance to the pivot.

Then what you can do is to do this measurement many times, while
moving progressively the body from one side of the plank to the other
side, measuring each time the difference in torque due to the weight.

I have not yet worked out the mathematics (and I am not sure I can
still do that, probably yes with some effort) but I think that should provide data to determine the
longitudinal weight repartition of the body.  This reminds me a bit of
the way medical scanners work, though this weight problem is probably
simpler.

To be convinced that the approach may work, one can first approximate the
body as a rod, such that the two halves are each homogenous but have
different weights. Two measurements should allow finding the
respective weights of the two halves.



You then get the following equations:

$m_1\ l/2\ =\ m_2\ l/2\ +\ m_3l$

$m_1\ 3l/4\ +\ m_2\ l/4\ =\ m_4l$

The weigths $m_3$ and $m_4$ are known, and the length $l$ is of course measured and known. Thus the equations can be solved to get $m_1$ and $m_2$.

This can be generalized to any number of slices.

In my above quip about being able to do the mathematics is not with respect to resolving a set of linear equations. Rather, I was wondering is there is some kind of transform to express a continuous description of this approach. As I said, I was somewhat inspired by the way medical scanners are working (or maybe the analogy hit me afterwards ... hard to know).

This can be generalized to many homogenous segments approximating the
body density repartition.
AnswerKenntnis:
Step 1: Take a trip to deep space (space suit recommended; means of transportation left as an exercise for the reader). It is important to compute the Hill sphere of your body to make sure it is large enough at this stage. Really it's a Hill-roughly-person-shaped-spheroid-blob, but feel free to assume a spherical you to simplify the calculation.

Step 2: Deploy a constellation of micro-satellites in orbit around yourself (see Fig. 1).

Step 3: Carefully track the motion of every satellite.

Step 4: Process telemetry data to measure the mass distribution of your body, head included.

Step 5 (optional): Return to Earth.

Step 6: Profit!


Figure 1: Micro-satellites deployed in gravimetric tracking mode. Careful monitoring of telemetry is required to determine the mass distribution of the central body.

(composited from source images here and here)
AnswerKenntnis:
Here are some methods I came up with:

Newton's Method:


Measure the whole body's mass, let's call it $M$
Now detach the head, and put it a distance $d$ apart from the body
Measure the gravitational attraction of the two parts of the body(let's call it F)
We have a system of equations to solve:
$$m_1+m_2=M \\ \frac{G m_1 m_2}{d^2}=F \\ \Rightarrow \frac{G}{d^2}m(M-m)=F$$
which is a quadratic equation and has two real roots. Head's mass is probably the smaller one!


Einstein's Method 1:


Shine a light ray, close to the person's head
Measure the deflection angle $\delta \phi$
The mass can be calculated from this equation:
$$M=\frac{c^2 b \delta \phi}{4 G}$$
where $b$ is the distance of closest approach, and $c$ is the speed of light.


Einstein's Method 2:


Measure the wavelength of an out-coming ray(an absorption line for instance) from your head.
Calculate the redshift, and the rest is easy peasy .


Heisenberg's Method:


Measure the whole body's weight, using a scale.
Continuously measure the rest of the body without looking at the head(Now this step is crucial, no one should look).
After a long time measure the head, if it's still connected to your body return to step (2), otherwise continue.
Now that the head is effectively detached, you can weigh the rest of the body and calculate the difference.


Fermi's Method:


By all means you can consider the head to be a sphere with radius $r=0.1$ meters.
The density of head is more than water($10^3 \ \frac{kg}{m^3}$) and less than stone($5.5 \times 10^3 \ \frac{kg}{m^3}$); we'll take the geometrical mean $\rho = 2.3 \times 10^3 \ \frac{kg}{m^3}$
Now the mass will be:$m=\frac{4}{3}\pi r^3 \rho = 10 kg$("$\approx$" is not necessary :)


Hooke's Method:

The neck can be modeled as a spring, let's assume its stiffness is $k$.


lie on the ground and measure the neck's rest length($l_0$).
Hang from a tree upside down, and measure the neck's length($l_h$); now if we know $k$, the head's weight will be:
$$W_h=(l_h-l_0)k \tag{1}$$
We still don't know $k$, and we have to find it. While you are still upside-down, hang some known weights to your head and measure the neck's length respectively. 
Plot the weights with respect to spring's length, the slope will be its stiffness constant. Now using formula $(1)$ we can calculate head's weight.


Mansfield's Method:

Take MRI pictures of the head, which, using my technique will give the 3D density plot of head within few seconds. Now the rest of the problem is just banana cake.

Zeno's Method:

While this method might not be practically efficient, it has incredible philosophical advantages!


Cut half of what has remained from the head, 
weigh it,
Go to step (1)


Pair Annihilation:

Find your dual(which presumably has opposite quantum numbers than yours), slowly bang your heads together(be careful with the rest of the body). We will measure the released energy(call it $E$), your head's mass will be:
$$m=\frac{E}{2c^2}$$

and finally
Qmechanics Method:

No need to weigh your head; this is a noisy question which will drown more serious ones, let's close it!



To be completed ...
AnswerKenntnis:
I would do it like this:



The muscles of the neck have to be as relaxed as possible so that it approximates a flexible linkage, such that at some point along this linkage (which we can identify as the division between the head and the body, and thus the mass of the head includes a portion of the neck), if we separate the body into two free-body diagrams, there is no net vertical force being exerted between the two.

An assistant can read the scale.

The real difficulty is introduced if you define "head" as that portion of the skeleton, and ajeacent tissue, which excludes all vertebrae of the spine and everything below.
AnswerKenntnis:
1 Person A lays down on a carousel with its neck over the edge. Person B knocks A out. Person C spins the carousel. B clocks the time. Person D takes pictures of A's head as it goes around.

2 Measure the angle of the head by looking at the pictures.

3 Do the vector math.
AnswerKenntnis:
Assume the body to be made of N sections $S_i$ of uniform  linear density $\lambda_i$ and length $l_i$ and center of mass $r_i$ (from one end). Increase N as much as you want to get a more accurate reading. The sections need not be of the same length. Try to slice your body into approximately homogenous sections (homogenous with respect to mass-per unit length, not volume density).

Now, set up a seesaw-like system with a plank on a log or other pivot. Find a friend or grad student.

Now, sleep on the plank with the boundary between section $S_j$ and section $S_{j+1}$ right above the plank. This will be at a distance $L_j=\sum\limits_{i=1}^{j} l_i$ from one end. Have a friend attempt to balance the system using weights. He will get that some weight $M_j$ will balance the plank when placed at $R_j$.

Repeat this measurement for $j= 1 \to N-1$. Now, write torque equations for each system, assuming that each section has a linear density $\lambda_i$. These equations will be of the form:

$$M_j R_j=\sum\limits_{i=1}^N \int\limits_{L_j}^{L_j+l_i} \lambda_i xdx$$

Solving the integral, we get:

$$M_j R_j=\sum\limits_{i=1}^N \frac{\lambda_i}{2}(l_i^2 +2l_iL_j) $$

You now have $N-1$ equations in the $N$ variables $\lambda_i$. Your $N$th equation will be $\sum l_i\lambda_i =M_{you}$

Now, simply calculate the head-mass for the idealized system, and that will be approximately your head mass. Accuracy increases as $N$ does, though you can also increase accuracy by smartly partitioning the system.



The simpler method is decapitation and measurement.

Reattaching the head is, of course, left as an exercise to the reader.



Alternate solution #2:

Break in to LIGO. Go wiggle your head near the interferometer. Memorize the readings on your way out.

(Note: In all seriousness, this probably won't work because LIGO isn't tuned to detect the gravitational waves of head wiggles)
AnswerKenntnis:
Method 1: The Chemist's method

Firstly, take pictures of your head from at least $400^3=64000000$ different equally spaced out angles. Samples: 



Then load them onto the computer, and use some 3D visualising software (maybe Scientfic Computing.SE can help you with that. ) to intersect the images at different angles, to get good, but of course, approximate enough (you don't want to crash your computer. !) description of your face. Load the data onto Wolfram Mathematica (or something, I don't know if Mathematica works actually,   )  and then get a good parameterisation of your head's surface.

$$x=0.1\cos\theta\sin\varphi$$

$$y=0.1\sin\theta\cos\varphi$$

$$z=r\cos\varphi$$

The above are the parameterisations you would get if you're head's surface was a perfect sphere.

Then, take some random samples of the flesh of your head using a knife and an injection-giving machine.  Now, your head will be perforated, but that's ok, (After all, what's that in comparison to the great gain for physics! It will be so useful in phenemonology, right?!)

  

Perforated head. Just try to ensure that your head doesn't crumble or that will be a a loss to physics.

Not very painful.  

Now, measure the densities of these samples, and plot them as a scalar field across an image containing it. .

Now, do a simple volume integral (assuming that your brain is still functioning, which it shoouldn't be ,  because then the experiment, would be unfair. ) . 

$$M=\iiint_V \rho\left(x,y,z\right) \mbox{ d}V$$

And that's the mass of your head. 

Method 2: The Murderer's method


Commit suicide.
Be reborn with an identical twin (PSEUDOSCIENCE ALERT!!!). 
Enssure that your identical twin goes through the exact same conditions as you.


Step 1 to 3 are strongly encouraged if you already have an identical twin (to ensure accuracy, due to 3.) but compulsory if you don't. 


After you're as old as you were when you were reading this question, kill him. 
Measure the mass of his head. That's the mass of your head. 


Method 3: The Teacher's method

Put your head on the table. 



Now, attach the head to a spring balance, and measure the force required to drag it at a constant velocity. . .

Then, just apply: 

$$F_k=\mu_kgm$$

Method 4: A Quantum Mechanic's method

. Pluck out your head. Give NASA/USSR/ISRO/some random space organisation ssome money to launch it into a place with very low potential, and then keep observing your head. 



You may also try throwing your head as hard as possible. It may work, with some small probability.

Apply Schrdoinger's equation. (Time-independent, potentialless, non-relativistic)

$$i\frac{\hbar}{2m}\nabla^2\Psi=\frac{\partial\Psi}{\partial t}$$

Now, determine $|\Psi|$ experimentally through your observations.  Apply the above equation. 

Method 5: Another Quantum Mechanic's method

. Similar to Method 4 but instead use path integrals. Measure the phase intensityr and apply : :

$$A=\sqrt{\frac{2i\pi\hbar }{mN}}$$

Method 6: A standard-modelist's method

Somehow Use  the Standard Model $\mathcal L$

$$\mathcal L =  - \frac{1}{4}{H^{\mu \nu \rho }}{H_{\mu \nu \rho }} + i\hbar {c_0}\bar \psi \not \nabla \psi  + {c_0}\bar \psi \phi \psi  + h.c. + {\left\| {\not \nabla \phi } \right\|^2} - U\left( \phi  \right){\text{ }}$$

$$$$ . 

To compute the mass of your head . ! . ! .

Method 6: A general relativist's method

Pluck out your head. Throw it into outer space. Calculate the metric tensor near your head . . Compare it with the Kerr metric tensor.

$$  c_0^{2} d\tau^{2} 
=  c_0^{2}\left( 1 - \frac{r_{s} r}{\rho^{2}} \right)  \mbox{d}t^{2} - \frac{\rho^{2}}{\Delta} \mbox{d}r^{2} - \rho^{2}  \mbox{d}\theta^{2}  
 - \left( r^{2} + \alpha^{2} + \frac{r_{s} r \alpha^{2}}{\rho^{2}} \sin^{2} \theta \right) \sin^{2} \theta  \mbox{d} \phi^{2} 
+ \frac{2r_{s} r\alpha \sin^{2} \theta }{\rho^{2}}  c_0   \mbox{d}t  \mbox{d}\phi  $$

Match the values and you have your head's mass. 

Method 7 (or was it 8? Or 6? ). : .  The Special Relativist's method.

Settle down completely to rest. Now, let someone come by, and somehow (if it's even possible)  convert your head's rest energy into the Kinetic Energy of a donkey with a known mass.  Now, using the velocity of the donkey, apply

$$m=\frac12m_\mathrm{donkey}\frac{v_\mathrm{donkey}^2}{c_0^2}$$

Method 8: The Weird Person's method*

Change youur neck to a spring balance and hang upside down.      


But I guess the best method is still @user2574624's /.
AnswerKenntnis:
A brilliant book by Tom Cutler: 211 Things a Bright Boy Can Do has a section which shows you how to weigh your head accurately.
This method is easy to understand, and quite practical. It doesn't even involve harming yourself as most of the other methods listed above do. You can actually do it.

Here's the excerpt:


  There canΓÇÖt be many problems more vexing than that of weighing your head. For a start, how do you decide where your head ends and your neck begins? And once youΓÇÖve decided this and marked the junction with an indelible pen, what next? ItΓÇÖs a riddle that beat even Archimedes. So here is a method that will produce a good estimate of the weight of your head the next time you find yourself at a loose end.  


Required:
┬╖ A plastic kiddie pool
┬╖ A plastic rainwater barrel
┬╖ A plastic bucket
┬╖ A plastic measuring jug
┬╖ A plastic chair
┬╖ Some bathroom scales
┬╖ Your head  


  Method
  
  
  On a nice day, inflate your kiddie pool in the yard. Divert any bathers from wading into it.  
  Put your barrel into the pool and fill it (the barrel) with warm water, right up to the top. Do not allow it to overflow.  
  Shave all your hair off. YouΓÇÖll get a more accurate measurement this way. Because you are bald no water can be removed by capillary action.  
  Stand on your chair, hold your breath, and lower your head slowly into the water until it is submerged up to your AdamΓÇÖs apple. The displaced water will flow down the sides and collect in the bottom of the pool. Slowly take your head out again.  
  Pour the collected water (from the pool) into your bucket. This is the most annoying part of the experiment because the pool is full of water and you need to move it without spilling any. You can, if you are ambitious, begin the experiment by digging a deep hole in the lawn, into which you now crawl with your bucket, so as to siphon the water from the pool.
  Measure, and jot down, the volume of water in your bucket by pouring it into your measuring jug. If the jugΓÇÖs too small, do it in steps.
  Empty the bucket and the jar for the next measurement.
  Put the barrel back and fill it to the brim again. Take all your clothes off and weigh yourself. Note down this weight.
  Get on the chair and climb carefully into the barrel, completely submerging yourself. After a moment climb out and pour the displaced water into the bucket and then into the measuring jug, keeping a note of the final amount.
  Multiply your body weight by the ratio between the two figures. This will give you the weight of your head.
  Put your clothes back on.
  




$P.S:$ I've added a bit to the text because some parts were unclear. :)
AnswerKenntnis:
The solution has to be static. Any motion through the neck (relative DOF from body) is going to require very detailed modeling of the bones and muscles to achieve the proper result.

I propose a straight up weight of the head, with careful attention in keeping the neck as loose as possible and as horizontal as possible. See below for an example. You want no muscle forces on the neck (apply anesthetic if needed). Now move the head slowly up and down until you get a linear relationship between elevation and head force, and so you need to average the force around where you think the natural position of the neck is.
AnswerKenntnis:
Here is a way to measure it using the person's moment of inertia.  Also, compared to many of the other answers to this question, this method is not only feasible, but actually reasonably safe for the subject.


Measure the mass $m$ of the person; the length $l_b$ and average cross-sectional width (from the front) $w_b$ of the person's body (excluding the head); and the length $l_h$ and average cross-sectional width $w_h$ of the head of the person whose head we wish to know the mass of.
Obtain a rigid stretcher that includes restraint straps, like this one:

Securely mount the person whose head is to be measured onto the stretcher, using the included restraint straps, so that the 'front' of the person is facing upward.
Measure the moment of inertia $I_{gross}$ of the person in the stretcher about an axis extending upward through their neckn.
Measure the moment of inertia $I_{tare}$ of the stretcher (without the person in it) about the same axis.


Now compute the net moment of inertia:
$$I = I_{net} = I_{gross} - I_{tare}$$

For simplicity, I will model the head and body of the subject as if they were thin rectangular plates rotated about the axis at one end.

Let $m_h$ be the mass of the head of the person, and $m_b$ be the mass of their body.  We know, then that $m_h + m_b = m$, or in other words, that the masses of their head and body together add up to the mass of the entire person.

$$I = \frac{m_bh_b^2}{3} + \frac{m_bw_b^2}{12} + \frac{m_hl_h^2}{3} + \frac{m_hw_h^2}{12}$$

Solving this equation yields:

$$m_h = \frac{4 l_h^2 m+w_h^2 m+12 I}{4 h_b^2+w_b^2+4 l_h^2+w_h^2}$$

Going further:

If you wished, you could use a more accurate model of the person, resulting in a more complicated equation for their moment of inertia.  Also, if one wished to obtain a more accurate measure, one could measure the person's moment of inertia about several different axes, in order to compensate for nonuniformities in the density of the body as a whole, rather than just differences in the density of the head and the rest of the body.
AnswerKenntnis:
A safe (albeit not entirely comfortable) method is the following:

Ask a partner to hang you upside down using a rope around your ankles. Tell him to swing you with a small amplitude. You relax your body and ask him to measure the swing period. Now bend your head, keep it in fixed position, and again ask your partner to measure your swing period.

Knowing the local gravitational acceleration $g$, and using the expression relating the swing period to the distance the center-of-mass is away from the pivot point:
$$t_{swing}^2 = \frac{4\pi^2}{g} \frac{\int\limits_{0}^{L} m(z) zdz}{\int\limits_{0}^{L} m(z) dz}$$ (with $z$ measuring the distance away from the pivot point), from the change in period you can work out the change in the position of your center of mass. 

Provided you weigh yourself afterwards (or better yet: yourself including the rope) and provided you have an accurate estimate of the change in height of the center-of-mass of your head (tip: record the scene with a digital video camera, and ensure you have access to reliable information on the center-of-mass of human heads), you get a good estimate of the mass of your head.
AnswerKenntnis:
You'll need something like this. The green things are pulleys(consider massless, frictionless)

Also, get some ropes, tie them around your feet and gently lower yourself until your head is under the liquid.



Measure the mass of the displaced water $m_d$ .

At this point, a buoyant force $F_b$ is acting vertically upwards. Weight $W$ is acting downwards.



Case 1(weight exceeds buoyant force)

If weight is more(try and feel with your neck muscles, in which direction the force is acting), make the apparatus as follows.



The the strings to your head(the ears or around under your chin is a good spot to tie) Experiment with different weights and obtain a mass $m_e$ such that you feel no force on your neck muscles.

Now 
$$W=m_e g+F_b$$
$$mg=m_e g+m_d g$$



Case 2(Buoyant force exceeds weight.)

Set the apparatus as follows.


Obtain a mass $m_e$ in the same way such that you feel no external force.

Now 
$$W+m_e g=F_b$$
$$W=F_b-m_e g$$
$$W=m_dg -m_eg$$



Now about calculating that force you feel in your neck muscles, you can attach a modified (extremely sensitive)spring balance like instrument which rests between your chin and shoulders, calibrated to indicate "zero" force in space.

One drawback is the slight compressibility of the body(around your neck) which might absorb some of the buoyant force.
AnswerKenntnis:
Let us take the matrix of tensometers as mattress (about 450x150 points 5mm matrix distance). Let us put examine man/woman on this matrix. Then the total weight is sum of partial forces being measured by tensometers multiplied by gravity. Weight of head is the sum of forces measured by tensometers under head multiplied by gravity. To eliminate muscles force of neck, you may apply anesthetic for examined object.

The sophisticated procedure to eliminate forces of neck┬┤s muscles will come soon.
AnswerKenntnis:
OK, here is my take:

In a large bath immerse the body in salt water so that it floats. Get a photo , the plate perpendicular to the horizontal plane along the body, and measure the height out of water.

Get the volume of body and head by measuring the weight of the body up to the neck and then fully immersed, knowing the density of water.

Suppose for ease of algebra it is two balls 

a large  ball with density $\rho_1$, volume $V_1$,  and a small with density $\rho_2$, volume $V_2$ and $\rho_1 < \rho_2$. The mass of $V_2$ + mass of $V_1$ is $M$

Let the ratio of the height out of water of $V_1$ to the height of $V_2$ be $a$ . 

We have :

$\rho_1V_1+\rho_2V_2=M$
and  $\frac{\rho_1}{\rho_2}=a$

solving 

$\rho_1=a\rho_2$

$a\rho_2V_1+\rho_2V_2=M$

Hence $\rho_2=\frac{M}{V_2+aV_1}$

Now with a computer numerical integration one can do an integral over heights in the photo  for a real body with irregular features and work with averages.
AnswerKenntnis:
A more accurate solution than the "relax your neck muscles" and similar ones: 

Go to the morgue. Cut off the head of a recently deceased person with a similar body type. Measure the ratio of the weights of the head and body. Measure your weight. 

Even more accurate: measure the density of the head you just cut off. Now measure the volume of your own head by immersing it into water. The volume of the head does not vary as much as of the body, where muscle, fat, etc. play a role. For better accuracy, cut off many heads for an average.
AnswerKenntnis:
What about MRI? I don't know the specifics, but a single google search reveals this link (http://link.springer.com/article/10.1007%2Fs002230050015#page-1) which suggests one can measure tissue density using it, and it's rather obvious one could also use MRI to reconstruct a 3D image of the head.

Alternatively, measure the relevant tissue densities of a cadaver then use MRI to evaluate the actual volume occupied by each tissue type which could give an estimate.
AnswerKenntnis:
Float free on water. Now lift your head clear of water. Measure displaced water. (Take account of water was displaced by head)
AnswerKenntnis:
If you can find a way to calculate the density of your head by looking at the amount of the head that is - for example - bone, brain or space, and take the density of each of them, then you could get an average density for the whole head. Then simply measure the volume of the head by possibly submerging it and work out the mass.

Of course this isn't accurate in the slightest, but at least it's an answer.
AnswerKenntnis:
put on a tight-fitting helmet of known mass.
attach a known spring between the helmet and a solid wall.
have someone give the helmet+head a knock, and measure the frequency of vibration.


That gives the mass of head + helmet. Then subtract mass of helmet.
AnswerKenntnis:
You should be able to do it by jumping off a table onto a spring scale.
Simply film the jump with a high speed camera and look for the shockwave that moves through your body as you are landing and note when it reaches your neck. At the same time note the distance the spring has moved (call it point x). Then note how much further the spring moved afterwards to measure your full weight. 
Now drop a mass off the table that equals moving the spring to point x. Deduct that weight from your weight and you have hour head weight.

ps. you should be able to get rid of the camera and calculate the shockwave/compressionn wave speed using 9.9m per s per s and the height to your neck and height of the table.
AnswerKenntnis:
Drop your head on hard surface (distance can be 1mm). Measure impact. Do this randomly, from various positions.
QuestionKenntnis:
Why does Stephen Hawking say black holes don't exist?
qn_description:
Recently, I read in the journal Nature that Stephen Hawking wrote a paper claiming that black holes do not exist.  How is this possible? Please explain it to me because I didn't understand what he said.

References:


Article in Nature News: Stephen Hawking: 'There are no black holes' (Zeeya Merali, January 24, 2014).
S. Hawking, Information Preservation and Weather Forecasting for Black Holes, arXiv:1401.5761.
AnswersKenntnis
AnswerKenntnis:
The paper by Dr. Stephen Hawking doesn't say that black holes don't exist. What he says is that black holes can exist without "event horizons". To understand what an event horizon is, we first have to understand what is meant by escape velocity. This last one is the speed you need to escape a body. Now, here is where the event horizon and the escape velocity comes in play: the event horizon is the boundary between where the speed needed to escape a black hole is less than that of light, and where the speed needed to escape a black hole is greater than the speed of light.
 

So Hawking says that  instead of event horizon, there may be "apparent horizons" that would hold light and information only temporarily before releasing them back into space in a "garbled form".
AnswerKenntnis:
This is really a footnote to Adobe's answer.

Light cannot escape from an event horizon. But how can you check that light can never escape? You can watch the surface for some time $T$, but all you have proved is that light can't escape in the time $T$. This is what we mean by an apparent horizon, i.e. it is a surface from which light can't escape within a time $T$.

To prove the surface really was an event horizon you would have to watch it for an infinite time. The problem is that Hawking radiation means that no event horizon can exist for an infinite time. The conclusion is that only apparent horizons can exist, though the time $T$ associated with them can be exceedingly long, e.g. many times longer than the current age of the universe.

A point worth mentioning because it's easy to overlook: when you start learning about black holes you'll start with a solution to Einstein's equations called the Schwarzschild metric, and this has a true horizon. However the Schwarzschild metric is time independent so it would only describe a real black hole if that black hole had existed for an infinite time and would continue to exist for an infinite time. Both of these are not possible in the real universe. So the Schwarzschild metric is only an approximate description of a real black hole, though we expect it to be a very good approximation. 
AnswerKenntnis:
Craig Feinstein asked:
Does Stephen Hawking believe that General Relativity is wrong?

Here is my answer (I will shift my answer there if some one reopen that question):

Stephen Hawking did NOT say that black holes do not exist. Hawking used to think balckholes are oblivious. Now he admits (like some other people do) balckholes have perfect memory , just like any other quantum systems. So what Hawking said is that balckholes are not forgetful.

In order for balckholes to have perfect memory (ie satisfies unitary quantum evolution), the classical GR must be wrong. The blackhole horizon in the classical GR must be modified. I believe that, in well defined quantum gravity (yet to be developed), the horizon carries degenerate states that give rise to blackhole entropy. There is no such thing as inside horizon. In other words, blackhole horizon behaves like a hard-wall (with  nearly degenerate states). This picture contradicts with classical GR.

Hawking's picture for the horizon, I believe, is similar.
This actually is a very old idea. See:

http://relativity.livingreviews.org/open?pubNo=lrr-2011-8&page=articlesu33.html

http://arxiv.org/abs/gr-qc/0303006

http://iopscience.iop.org/1742-6596/410/1/012137

So I believe that "Stephen Hawking believe that the classical General Relativity is wrong near the blackhole horizon". I think most people (not including me) believe that classical General Relativity is correct near the blackhole horizon. 

I believe that the classical General Relativity is wrong, since I believe that a well defined quantum theory (including quantum gravity) has a finite UV cutoff. A finite UV cutoff is incompatible with  General Relativity principle.
A finite UV cutoff is even incompatible Lorentz symmetry. A finite UV cutoff may modify our picture about the blackhole horizon, but I do not know how.
QuestionKenntnis:
Why does a helium filled ballon move forward in a car when the car is accelerating?
qn_description:
I noticed that when I had a helium filled, latex ballon inside of my car, it moved forward in the cabin as I accelerated forward. The faster I accelerated forward, the faster the ballon went from the back of the car to the front of the car. The balloon didn't have a string.  This became a game with my 4 year old as we drove home. We figured out where the balloon would go based on how fast I accelerated, turned corners etc.  I expected that it would act a lot like the water in a cup does, but it was the total opposite it seemed.  What forces caused this behavior?  I assumed it has something to do with the fluid dynamics in the closed cabin, but I can't figure it out.  Help?
AnswersKenntnis
AnswerKenntnis:
It travels forwards instead of backwards in an accelerating car for the same reason that a helium balloon travels upwards instead of downwards under the influence of gravity. Why is that?

In an accelerating car, for all intents and purposes the acceleration can be considered a change in the amount and direction of gravity, from pointing straight down to pointing downwards and backwards. The balloon doesn't know and doesn't care if the acceleration is from  gravity or from the acceleration of the car; it just tries to move in the direction it naturally moves, namely, against the direction of the acceleration. Thus, it moves forwards when you accelerate. Hopefully you find this explanation intuitively satisfying.

Another more rigorous way to view the problem is through Lagrangian minimization. The balloon can be considered a low-density object embedded in a higher-density fluid constrained within the confines of the car. Under the influence of gravity pointing sideways, the total system potential energy decreases the farther forward the balloon is situated. Since the force is the gradient of the potential, the balloon will try to move forward.
AnswerKenntnis:
When your car accelerates forward, the air inside moves back relative to the car. This creates a slightly high pressure in the rear of the vehicle and a low pressure up front.

Since helium is lighter than air, it moves away from the region of high pressure. A similar balloon filled with $CO_2$ would move back, since it is heavier than the surrounding air
AnswerKenntnis:
Fun question. Here's my "me-too" answer.

Suppose the car has just emerged from a river, so there's a lot of water in it, and the balloon is tied to the floor.

Then you drive away.



The air in the car is just like a bunch of water :)
AnswerKenntnis:
It acts precisely like water in a cup. Or, more specifically, like the air in the cup. Since the helium is a much lower density than the nitrogen and other gasses in your car, it can be visualized like an air bubble in a bottle. The container for the helium(the balloon) has negligible mass. 

When you accelerate forward, the water in a bottle will move backwards, just like you'd expect. The air bubble will appear to move forward to fill that void left by the water, just like the helium balloon. 

Additional forces may come into play as well. There is friction from the rubber on the ceiling. Also, when you accelerate, the center of gravity of your car moves backwards on the wheelbase. The nose of your car lifts, and the tail of your car sinks. When you stop, the opposite happens, and when you turn left, your car leans right, as you can feel. This may tilt the ceiling allowing the balloon to move in response to the tilted ceiling. This could have an effect as well.
AnswerKenntnis:
I am not good at explaining things but here it goes 

try to understand it this way 

assume the rectangle to be you car, now the dotted area is filled with normal air and their is a helium balloon in between

So when you accelerate everything in the car tries move backward with respect to the car even the helium balloon, so then why the balloon goes forward, 

As you may know helium is less denser than the air and so it is lighter then the air, so when you accelerate the air starts moving backwards but as the helium is lighter it is pushed forward by the air collecting at its back the same phenomenon when you fill a bucket and a ball kept at its bottom starts rising up because it is less denser than water.
AnswerKenntnis:
When you're in an accelerating frame of reference, the acceleration is perceived as gravity. For instance in an accelerating train, you stand on an angle, in the direction of motion. It feels like you're on a slope, and de facto, you are.

The helium-filled ball is simply moving against the direction of what looks like gravity: the same thing that it does when you're not in an accelerating frame. It does that for the same reason: buoyancy due to being lighter than air.

When you let go of the balloon in an accelerating train, it will not move directly forward, but on an angle: forward and up.  That is the same angle at which you stand, if you balance without support. If while standing in this position you hold a balloon on a string, the string will point in the same direction, parallel to your body, and if you let go of the balloon, it will move in that direction.  That is the direction which corresponds to "up", slanted due to the vector combination of the force of gravity (which acts on you downward perpendicularly to the floor) and the fictitious force which appears to be pulling you backwards due to acceleration.

So, in summary:


A balloon rises "up" due to buoyancy.
What determines which way is up "up" is a vector combination of gravitational pull, and the fictitious force caused by acceleration, which mimics gravity.
AnswerKenntnis:
I wondered this myself...

Then it hit me, the analogy in my head of a ball rolling on a surface was wrong... the balloon isn't something floating in nothing, it is a body that is less massive than the air it is displacing. 

So when you break, everything tries to move forward, but the air wins because it has more inertia, and the balloon is forced backwards.

it is essentially the same thing as what normally happens with a ballon, only with the vectors turned on their side... normally all air and balloons and helium are pulled to the earth, but since the air is more dense it is pulled more... and displaces the balloon with the helium.
AnswerKenntnis:
The simplest way to think about the helium balloon in the accelerating car is to invoke Einstein's Principle of Equivalence: a constant acceleration is the same, in all respects, as a gravitational force. To make matters as simple as possible, we'll ignore the real force of gravity (i.e. what's pulling the car toward the Earth) and just think about the forward acceleration of the car.
According to the Principle of Equivalence, the effect of this acceleration is exactly the same as a gravitational force pointing to the rear of the car. But everyone knows what a helium balloon, immersed in air, does in the presence of gravity: it rises, i.e. it moves in the direction opposite to the gravitational force.

So when the car accelerates, the balloon moves forward. Incidentally, the Principle also explains why, when the car accelerates, a passenger is jerked backwards: because, unlike the balloon, the passenger falls downward, in the same direction as the gravitational force.

In answering this question, I'm assuming you accept that a helium balloon rises in the presence of Earth's gravity. Because helium has a smaller density than air, the buoyant force of air will push it toward the sky against gravity
QuestionKenntnis:
Don't heavier objects actually fall faster because they exert their own gravity?
qn_description:
The common understanding is that, setting air resistance aside, all objects dropped to Earth fall at the same rate. This is often demonstrated through the thought experiment of cutting a large object in half, the halves of which clearly can't then fall more slowly just by being sliced in two.

However, I believe the answer is that when two objects fall together, attached or not, they do "fall" faster than an object of less mass alone does. This is because not only does the Earth accelerate the objects toward itself but the objects also accelerate the Earth toward themselves. Considering the formula:

$F_g = G m_1 m_2/d^2$

We can see that the force of gravity is dependent on both the masses, not just that of the more massive object.

Of course in everyday situations, we can for all practical purposes treat objects as falling at the same speed. But I'm hoping not for a discussion of practicality or what's measurable or observable, but what we think is actually happening.

Am I right or wrong?

What really clinched this for me was considering dropping a small Moon-massed object close to the Earth and a small Earth-massed object close to the Earth. This made me realize that falling isn't one object moving toward some fixed frame of reference, but that the Earth is just another object, and falling consists of multiple objects mutually attracting in space.
AnswersKenntnis
AnswerKenntnis:
Using your definition of "falling," heavier objects do fall faster, and here's one way to justify it: consider the situation in the frame of reference of the center of mass of the two-body system (CM of the Earth and whatever you're dropping on it, for example). Each object exerts a force on the other of

$$F = \frac{G m_1 m_2}{r^2}$$

where $r = x_2 - x_1$ (assuming $x_2 > x_1$) is the separation distance. So for object 1, you have

$$\frac{G m_1 m_2}{r^2} = m_1\ddot{x}_1$$

and for object 2,

$$\frac{G m_1 m_2}{r^2} = -m_2\ddot{x}_2$$

Since object 2 is to the right, it gets pulled to the left, in the negative direction. Canceling common factors and adding these up, you get

$$\frac{G(m_1 + m_2)}{r^2} = -\ddot{r}$$

So it's clear that when the total mass is larger, the magnitude of the acceleration is larger, meaning that it will take less time for the objects to come together. If you want to see this mathematically, multiply both sides of the equation by $\dot{r}\mathrm{d}t$ to get

$$\frac{G(m_1 + m_2)}{r^2}\mathrm{d}r = -\dot{r}\mathrm{d}\dot{r}$$

and integrate,

$$G(m_1 + m_2)\left(\frac{1}{r} - \frac{1}{r_i}\right) = \frac{\dot{r}^2 - \dot{r}_i^2}{2}$$

Assuming $\dot{r}_i = 0$ (the objects start from relative rest), you can rearrange this to

$$\sqrt{2G(m_1 + m_2)}\ \mathrm{d}t = -\sqrt{\frac{r_i r}{r_i - r}}\mathrm{d}r$$

where I've chosen the negative square root because $\dot{r} < 0$, and integrate it again to find

$$t = \frac{1}{\sqrt{2G(m_1 + m_2)}}\biggl(\sqrt{r_i r_f(r_i - r_f)} + r_i^{3/2}\cos^{-1}\sqrt{\frac{r_f}{r_i}}\biggr)$$

where $r_f$ is the final center-to-center separation distance. Notice that $t$ is inversely proportional to the total mass, so larger mass translates into a lower collision time.

In the case of something like the Earth and a bowling ball, one of the masses is much larger, $m_1 \gg m_2$. So you can approximate the mass dependence of $t$ using a Taylor series,

$$\frac{1}{\sqrt{2G(m_1 + m_2)}} = \frac{1}{\sqrt{2Gm_1}}\biggl(1 - \frac{1}{2}\frac{m_2}{m_1} + \cdots\biggr)$$

The leading term is completely independent of $m_2$ (mass of the bowling ball or whatever), and this is why we can say, to a leading order approximation, that all objects fall at the same rate on the Earth's surface. For typical objects that might be dropped, the first correction term has a magnitude of a few kilograms divided by the mass of the Earth, which works out to $10^{-24}$. So the inaccuracy introduced by ignoring the motion of the Earth is roughly one part in a trillion trillion, far beyond the sensitivity of any measuring device that exists (or can even be imagined) today.
AnswerKenntnis:
The paradox appears because the "rest frame" of the Earth is not an inertial reference frame, it is accelerating. Keep yourself in the CM reference frame and, at least for two bodies, there is no paradox. Given an Earth of mass M, a body of mass $m_i$ will fall towards the center of mass $x_{CM}=(M x_M  + m_i x_i)/(M+m_i)$ with an acceleration $GM/(x_i-x_M)^2$. Note that $\ddot x_{CM}=0$

Really we have only hidden the paradox, because of course $x_{CM}$ is different for each $m_i$. But this is a first step to formulate the problem in a decent inertial frame.



The paradox resurfaces again if you want to get rid of $(x_i-x_M)$. In most applications, now that you are in a non accelerating reference system, you want to consider distances related to it, ie $x_i-X_{CM}$. The solution is to redefine the mass. As 
$x_i-x_{CM}= M (x_i - x_M) /(M+m_i)$, we can say that the object $i$ falls into the Mass Center with an acceleration $G{M^3 \over (M+m_ i)^2}{1 \over (x_i-x_{CM})^2}$ You could say that the actual mass of the "earth at center of mass" is this correction.



Once you are into the trick of changing the value of the mass, you can still stick to the reference frame of the earth. In this reference frame the quotient between force and acceleration is $Mm_i/M+m_i$ You can claim that this is the actual mass of the body during the calculation. This is called the reduced mass $m_r$ of the system, and you can see that for small $m_i$, it is almost equal to $m_i$ itself. You can ever write some of the previous formulae using the reduced mass $m_r$ in combination with the original masses, for instance the above ${M^3 \over (M+m_ i)^2}= M {m_r^2\over m^2}$, but I am not sure of how useful it is.  In any case, you see that you were right about the "heavier implies faster" but that it is perfectly managed.



For three objects, m_1 and m_2 falling into M, the question is how to compare the case to m_1+m_2 falling into M. You separate the forces between internal, between 1 and 2, and external, against M. Look at the point $x_0= {m_1 x_1 + m_2 x_2 \over m_1+m_2}$ . This point it is not accelerated by the internal forces. And the external forces move them as
$$\ddot x_0={1 \over m_1+m_2} (m_1 {G M \over (x_1-x_M)^2} + m_2 {G M \over (x_2-x_M)^2})={F_1+F_2 \over m_1 + m_2}$$ 



This is becoming long... ┬íI can not put all the Principia in a single answer!. So you can forget all the previous stuff, consider it is just to a mean to fix notation and get some practice, an read the answer:

If the two bodies are at the same distance $x$ of the "external" earth, they suffer the same external acceleration $g=GM/(x-x_M)^2$, and the same happens with $x_0$. If both bodies are in an approximation where $g$ can be considered constant, which was the case originally considered by Galileo (and the modern $g=9.8m/s^2$), then they have the same acceleration -and also the combined position $x_0$-. If they are not at the same distance nor in an approximation of constant -equal everywhere- field, then you can still save the movement of $x_0$ to work as if it were a gravitational force for some single mass $m_T$, but then the manipulation of the equations will produce in the relative positions of $x_1$ and $x_2$ some accelerations of the order of $1/(x_0-x_M)^3$. Such forces are the "tidal forces".
AnswerKenntnis:
The new version of the question is much clearer than before, and I removed my down vote accordingly. 

The answer is yes: in principle there is such an effect. When the mass of the dropped object is small compared to the mass of the planet, the effect is very small, of course, but in principle it's there.
AnswerKenntnis:
I agree. My understanding is the same as well.

Assuming that the earth, mars and moon are of the same size - If the earth and mars where to be suspended in space (mars falling on earth), they would come into contact faster than - if earth and moon were to be suspended in space (moon falling on earth) owing to the fact that mars would cause the earth to accelerate towards it more than moon. This is provided that the distance between the two objects are the same initially. The earth would attract both at the same rate for any given distance.

I have also posted here regarding what would fall first in case of three objects being involved, asking if my understanding is correct. 
It is the classic apple-feather experiment revisited. I hope it clarifies @KeithThompson 's  question above.
AnswerKenntnis:
Yes, a heavy object dropped from the same height will fall faster then a lighter one. This is true in the rest frame of either object. You can see this from $F=GmM/r^2=m*a=m*d^2r/dt^2$.

The fastest "falling" (since we are reredefining falling) object however is a photon, which has no mass.
AnswerKenntnis:
Drop a 5 lb iron barbell and a 25 lb iron barbell simultaneously.  The'll hit the ground simultaneously.  The only possible caveat is the recoil effect Ted Bunn brings up above.
AnswerKenntnis:
A simple explanation is that it takes more FORCE to ACCELERATE an object of greater MASS.
A=F/M....or with a constant FORCE, the ACCELERATION is inversely proportional to the MASS. The is a result of inertia. 

A larger mass dropped (on a massive objects) requires a GREATER force to accelerate; additionally a GREATER mass exerts a GREATER gravitaional force. 

Setting newtons second law to the gravitional force law cancles mass out and therefore renders MASS not related to the ACCELERATION of two gravitationally related objects.
AnswerKenntnis:
The free fall time of two point masses is $ t = \frac{\pi}{2} \sqrt{ \frac{r^3}{2 G(m1+m2)}}  $.

The free fall time is dependent on the sum of the two masses. For a given total mass, the free fall time is independent of the ratio of the two masses. The free fall time is the same whether m1 = m2, or m1 >> m2.

When a body is picked up to a certain height and then dropped, the time to fall to the Earth does not depend on the mass of the object. If you lift a ping-pong ball and then drop it, it will take the same time to fall to the Earth as a bowling ball. Splitting the Earth into two masses does not change the sum of those masses, or the free fall time.

However, when an external body is brought to a certain height above the Earth and then dropped, the free fall time does depend on the mass of the external body. Because the sum of the Earth and the external body obviously does depend on the mass of the external body.


  "Most bodies fall at the same rate on earth, relative to the earth, because the earth's mass M is extremely large compared with the mass m of most falling bodies. The body and the earth each fall toward their common center of mass, which for most cases is approximately the same as relative to the earth. In principle, the results of a free fall experiment depend on whether falling masses originate on earth, are extraterrestrial, are sequential or concurrent, or are simultaneous for coincident or separated bodies, etc. When falling bodies originate from the earth, all bodies fall at the same rate relative to the earth because the sum m + M remains constant.
   -- ArXiv:Deterrents to a Theory of Quantum Gravity


Assumptions:
  The Earth is isolated (there is no moon, sun, etc).
  The Earth is non-rotating.
  The Earth has no atmosphere. (A hot air balloon would fall upwards because it is less dense than the atmosphere it displaces.)
QuestionKenntnis:
Superluminal neutrinos
qn_description:
I was quite surprised to read this all over the news today:


  Elusive, nearly massive subatomic particles called neutrinos appear to travel just faster than light, a team of physicists in Europe reports. If so, the observation would wreck Einstein's theory of special relativity, which demands that nothing can travel faster than light.


—source

Apparently a CERN/Gran Sasso team measured a faster-than-light speed for neutrinos.


Is this even remotely possible?
If so, would it be a real violation of Lorentz invariance or an "almost, but not quite" effect?


The paper is on arXiv; a webcast is/was planned here.

News conference video here
AnswersKenntnis
AnswerKenntnis:
You have a few longer answers which were already updated, but here is a concise statement of the situation in mid-2014:


An independent measurement by the ICARUS collaboration, also using neutrinos traveling from CERN to Gran Sasso but using independent detector and timing hardware, found detection times "compatible with the simultaneous arrival of all events with equal speed, the one of light."
In an edited press release (and probably in the peer-reviewed literature as well), all four of the neutrino experiments at Gran Sasso report results consistent with relativity.
The mumblings that begin a few months after the initial report, that a loose cable caused a timing chain error, have been accepted by the experimenters.  Fr├⌐d├⌐ric Grosshans links to a nice discussion by Matt Strassler
which includes this image:

You can clearly see that the timing offset was introduced in mid-2008 and not corrected until the end of 2011.


It's important to remember the scale of the problem here.  In vacuum, the speed of light is one foot per nanosecond.  In copper/poly coaxial cable it's slower, about six inches per nanosecond, and in optical  fiber it's comparable.  A bad cable connector can take a beautiful digital logic signal and reflect part of it back to the emitter, in a time-dependent way, turning the received signal into an analog mess with a complicated shape.  And a cable can go bad if somebody hits it the wrong way with their butt while they are working in the electronics room.

(I actually had something similar happen to me on an experiment: I had an analog signal splitter "upstairs" that sent a signal echo back to my detectors "downstairs", and a runty little echoed pulse came back upstairs after about a microsecond and got processed like another event.  I wound up spending several thousand dollars on signal terminators to swallow the echo downstairs.  It was an unusual configuration and needed unusual termination hardware and I must have answered the question "but couldn't you just" a hundred times.)

Gran Sasso is an underground facility for low-background experiments ΓÇö the detectors can't see GPS satellites directly, because there's a mountain in the way, and their access to the surface is via a tunnel whose main purpose is to carry traffic for a major Italian motorway.  I'm quite impressed that they had ~100┬áns timing resolution between the two laboratories; the "discovery" came about because they were trying to do ten times better than that.

As an experimentalist I don't begrudge the OPERA guys their error at all.  I'm sure they spent an entire year shitting pineapples because they couldn't identify the problem.  When they finally did release their result, they had the courage to report it at face value.  The community was properly incredulous and the wide interest prompted a large number of other checks they could make.  Independent measurements were performed.  An explanation was found.  Science at its best.
AnswerKenntnis:
Last (?) Edit: The "problem" is solved: it was mainly a problem in the timing chain, due to a badly screwed optical fibre. A high level description of the problem is given here and a more detailed explanation of the investigation is here.

List of possible systematic biases

I thought it might be a good idea to list the possible systematic biases which could lead xkcd's character to win his bet. As many physicists (including, I guess, many people from the OPERA collaboration), I think it will end like the Pioneer anomaly. Of course, the current list only contains biases which are unlikely, but less unlikely than a causality violation.

Location errors and clocks drifts

The arXiv paper studied them, and seem to exclude it. The distance seems to be known within 20 cm and the synchronisation seems to be within 15 ns (6.9 statistical and 7.4 systematic). If this would however end up to be the explanation, it would be quite boring.

Update: Rumors seems to tell that the boring explanation is the good one. 

Not the same neutrinos detected

The neutrinos are emitted on a 10.5 ┬╡s window, 175 times longer than the observed effect. It  might be possible that the neutrino emitted early are not exactly the same as the one emitted late. Neutrino oscillation might, for example, then make early neutrino more detectable by the distant detector.

However, the detectors were built to measure the oscillation, so I guess that the OPERA collaboration thought about it, and rejected it for whatever reason. I suppose an explanation along these lines would mean interesting new particle physics.

Update: This possibility excluded by a new experiment with 3 ns pulses.

Errors in the statistical timing analysis

The timing itself is based on a quite elaborate statistical analysis. Furthermore, the pulses are quite long (10 μs), so an error in this analysis could easily be of the good order of magnitude.

Update: This possibility excluded by a new experiment with 3 ns pulses.
AnswerKenntnis:
Is this even remotely possible?
  


Well... "possible," yes, but kind of like how tunneling through a brick wall is "possible": while you can't definitively prove it impossible, you'd feel pretty safe saying "this will never happen." Relativity is really well-tested, and it's really hard to conceive of a way that neutrinos could travel faster than light without it having other consequences that we would have discovered by now. That being said, I don't know the field inside-out and I'm sure some theorist has come up with some wacky idea that allows it. I asked another question that might come up with something.


  
  If so, would it be a real violation of Lorentz invariance or an "almost, but not quite" effect?
  


If the results from OPERA are accurate, this effect would be a full-blown real Lorentz violation, not just an apparent effect like Cerenkov radiation or astronomical superluminal motion. That's why everyone is so excited about it. (Unless the neutrinos are tachyons; in that case, I guess Lorentz invariance is technically still intact, but the observation of a tachyon would be equally big news.)

The setup of CERN and OPERA is conceptually very simple, basically just two observers located a known distance apart with synchronized clocks. It's a direct measurement of average velocity. There's no complicated theoretical analysis that needs to be done to determine whether the speed of light was exceeded. Either they are wrong about either the distance (mismeasurement, or there is a spacetime "rift" within the Earth :-P) or the time (clock synchronization error or drift), or they have actually discovered superluminal neutrinos.
AnswerKenntnis:
UPDATE 2011-10-15

This phenomena may have been explained. The crux of the problem had to do with differing reference frames - the distance traveled according to the satellites which measured the time was different from the distance traveled according to us on earth. If you're going to measure speed (distance / time), you have to get the distance and time both from the same reference frame. We were getting distance from our reference frame and time from the (very fast) satellite's reference time.

This article explains it in a very accessible way:


  To understand how relativity altered the neutrino experiment, it helps to pretend that we're hanging out on one of those GPS satellites, watching the Earth go by underneath you. Remember, from the reference frame of someone on the satellite, we're not moving, but the Earth is. As the neutrino experiment goes by, we start timing one of the neutrinos as it exits the source in Switzerland. Meanwhile, the detector in Italy is moving just as fast as the rest of the Earth, and from our perspective it's moving towards the source. This means that the neutrino will have a slightly shorter distance to travel than it would if the experiment were stationary. We stop timing the neutrino when it arrives in Italy, and calculate that it moves at a speed that's comfortably below the speed of light.
  
  "That makes sense," we say, and send the start time and the stop time down to our colleagues on Earth, who take one look at our numbers and freak out. "That doesn't make sense," they say. "There's no way that a neutrino could have covered the distance we're measuring down here in the time you measured up there without going faster than light!"
  
  And they're totally, 100% correct, because the distance that the neutrinos had to travel in their reference frame is longer than the distance that the neutrinos had to travel in our reference frame, because in our reference frame, the detector was moving towards the source. In other words, the GPS clock is bang on the nose, but since the clock is in a different reference frame, you have to compensate for relativity if you're going to use it to make highly accurate measurements.


The original paper publishing these findings is here: Times of Flight between a Source and a Detector observed from a GPS satelite.



Original Post

Sources: [1] (Associated Press), [2] (Guardian.co.uk), [3] (Original Publication - Cornell University)


  Scientists around the world reacted with cautious shock on Friday to results from an Italian laboratory that seemed to show that certain subatomic particles can travel faster than light.
  
  The journey would take a beam of light around 2.4 milliseconds to complete, but after running the Opera experiment for three years and timing the arrival of 15,000 neutrinos, the scientists have calculated that the particles arrived at Gran Sasso 60 billionths of a second earlier, with an error margin of plus or minus 10 billionths of a second. The speed of light in a vacuum is 299,792,458 metres per second, so the neutrinos were apparently travelling at 299,798,454 metres per second.


Ignoring the boilerplate media hype about the possibilities of time travel and alternate dimensions - I'm looking for academic sources that might suggest how this could be true, or alternatively, how this discrepancy could be accounted for.



I read the published article, Measurement of the neutrino velocity with the OPERA detector in the CNGS beam, with their findings. It looks like they took an insane amount of care with their measurement of distance and time.

One of the most common skepticism of people who no nothing about the experiment is stuff like:


  You might worry about[...] have they correctly accounted for the time delay of actually reading out the signals? Whatever you are using as a timing signal, that has to travel down the cables to your computer and when you are talking about nanoseconds, you have to know exactly how quickly the current travels, and it is not instantaneous. [2]


This experiment doesn't use that sort of 'stopwatch' timing mechanism though. There is no 'T=0', and no single firing of neutrinos. What is detected is watermark patterns in the steady stream of particles. The streams at the input and output are time stamped using the same satellites and any position along each stream has a precise time associated with it. By identifying identical patterns at input and output streams, they can identify how long it took particles to travel between the points. [1]



As for distance, they use GPS readings to get the east, north, and altitude position along the path travelled to great precision. So much so that they even detect slow earth crust migration and millimetres of changes in distance between source and destination when something like an earthquake occurs. When your particles are travelling on the scale (730534.61 ┬▒ 0.20) metres, this is more than enough precision:



It's going to take a lot more than grassroots skepticism to think of what could have caused this discrepancy. I've seen suggestions such as the gravity of the Earth being different along the path of the neutrinos, which warps space/time unevenly. The neutrino might not actually be travelling as far as they think if space/time is contracted at one or more points along the path where gravity varies.

Anyway, I'll be interested in seeing how it pans out. Like most scientists, my guess is an unaccounted for systematic error (because they definitely have statistical significance and precision on their side) that has yet to be pointed out, but it probably won't take too long with all the theoretical physicists that will be pouring through this experiment.
AnswerKenntnis:
According to Dr. Phil Plait, there's a rumour that it's been a faulty connection. In summary: nothing is wrong with the calculation, the theoretical assumptions, rotation of the Earth, etc... A hardware problem caused the 60 ns time gap.

It's still gossip, so take this with abundance of caution, but here's what he had to say:


  
    According to sources familiar with the experiment, the 60 nanoseconds discrepancy appears to come from a bad connection between a fiber optic cable that connects to the GPS receiver used to correct the timing of the neutrinosΓÇÖ flight and an electronic card in a computer. After tightening the connection and then measuring the time it takes data to travel the length of the fiber, researchers found that the data arrive 60 nanoseconds earlier than assumed. Since this time is subtracted from the overall time of flight, it appears to explain the early arrival of the neutrinos. New data, however, will be needed to confirm this hypothesis.
  


—source
AnswerKenntnis:
Suppose this is real, that the neutrinos arrive very slightly faster than light would through the vacuum. Wouldn't that point to there being a slightly higher c which actually limits speeds, and some slight slow-down for light from this maximum due to interactions of the electromagnetic field with other particles, including virtual particles?

After all, you can move an electron faster than a photon in glass, and we don't call it the end of relativity, we call it Cherenkov radiation.

So the definition of refractive index might need adjusting, but effectively the vacuum has a non-zero refractive index, or rather the vacuum is not entirely empty. Which we know. 

It makes sense that a neutrino is not subject to the same interactions, given its famed reluctance to interact with anything. Perhaps it is just an indication that the particles in a vacuum are more likely to be electromagnetic-interacting than weak-interacting.

Or am I labouring under a false premise? Is the speed of light in a vacuum already adjusted for virtual particle interactions?
AnswerKenntnis:
There are strong reasons for disbelieving this result.

[This paragraph is disproved  by the Nov. 17 result.] It uses an experimental design that was never intended for this purpose, and that is inherently poorly suited to it; the beam pulses were 10,000 ns wide, and the shift they claim to have measured is only 60 ns. This means that the shift can only be detected statistically, and it makes the result extremely vulnerable to unanticipated systematic errors, e.g., correlations between the time of emission of the neutrinos and their energy (which strongly affects the efficiency of detection) or the direction of emission. They did another run at the end of October, with beam pulses 1-2 ns wide. That's the correct design if you want to measure the speed of the neutrinos reliably. They should have simply waited until after they had those data before announcing their results. (In fact, five senior members of the collaboration did not put their names on the paper.) I have a bet running with a colleage, for a six-pack of Fat Tire, that the new run will show that the original result was bogus. The result may be announced as soon as November or December. [The result was announced Nov. 17, and I lost my six-pack.]

Another reason to disbelieve it is that there are strong and fairly model-independent reasons to believe that it cannot be correct. A superluminal neutrino beam would have lost a lot of its energy via radiation, but a measurement by another detector shows that this was not the case: http://arxiv.org/abs/1110.3763 Superluminal motion for neutrinos would also cause superluminal motion for electrons, which is contrary to observation  http://arxiv.org/abs/1109.5682 , and it would also have caused a suppression of pion decay, so that the beam could never have been produced in the first place http://arxiv.org/abs/1109.6630 . All of this holds regardless of the details of the model. E.g., it holds both for tachyonic neutrinos without a preferred frame and for models in which neutrinos are not tachyonic and there is a preferred frame.

Yet another reason for disbelief is that the velocity of propagation of neutrinos has been measured to much higher precision by other techniques, so if you want to believe the OPERA result, you have to posit a very strange energy-dependence of the velocity.
AnswerKenntnis:
ΓÇóIs this even remotely possible?

Well yes, of course it's possible in the same way that it's possible that invisible neutrino fairies are messing around with the neutrinos underground and hence causing havoc with the mental health of physicists around the world. It's just... unlikely, very unlikely, just as the 4-sigma evidence for new CP violation in like-sign dimuons was possible, only to fall flat on its face when ATLAS and CMS failed to see the same thing. But, it's still possible! Even so, let's focus on what's more likely: There are no neutrino fairies, and the conflict between data and special relativity lies with >> 6-sigma likelyhood of it being an error with the experiment.

ΓÇóIf so, would it be a real violation of Lorentz invariance or an "almost, but not quite" effect?

It would be one hell of a kick-up-Einsten's-backside violation of Lorentz invariance. All particles show the same speed limit as light, yet neutrinos with a rest mass greater than light possess a larger speed limit?
AnswerKenntnis:
May be the case that this problem has to do with the ┬½one-way┬╗ light speed and the referential that is used. 
Afaik the only known measures of the c are done in a ┬½two-way┬╗ version (mean value in a closed path). 
When a photon is released in space it starts its journey at c speed independently of the source and of the receiver. The CMB referential clearly is the only referential to ┬½observe┬╗ the light as isotropic. As the Earth moves we observe a dipole, and in different directions we measure different wavelengths for the same physical object (photon).


This paper (Cosmological Principle and Relativity - Part I) analyses the anisotropy of light speed for a moving observer.   

Fig.3 and eq. 18, pag 14 

The one-way light speed is : $c_{A}^{r}=\frac{c_{0}}{1+V/c_{0}\cdot\cos\phi_{A}}$  

Using $c_0=299792.458$ Km/s is two-way light speed, $V\;$  is the speed of the lab in relation to the CMB: $V=V_{SS}+V_E$=369$\pm$30 km/s  (data from here)
  gives the max value of $\frac{\left|c_{V\pm\delta V}-c_{V}\right|}{c_{V}}\cdot10^{5}$=10.2.

All experimental measures of |v-c|/c are within this limit.  

Anyway Einstein is correct.  

I suspect that the syncronization used in the GPS is in the same as in the above paper and not as Einstein did.
In the pic  Sat A must be synchronized with C at the same time thru the shortest red path and thru the longest blue path. At the same time B is in sync with C thru other paths with different lengths. IMO this is only possible if they are synchronised as in the above paper (instant observer) and not in the Einstein way that only considers one path  between the observer and any other point (Synchronisation around the circumference of a rotating disk gives a non vanishing time difference that depends on the direction used). 

Anyway Einstein is correct, and the neutrinos are not superluminal.
A careless reading of the paper might make you think that it is contrary to Einstein, but it is not.
AnswerKenntnis:
I will bet all my beans into the idea that they didn't estimate the spacetime curvature inside the earth well and over the beam trajectory, and what they actually discovered is a great way to measure space-time inside the Earth.

EDIT it seems this effect is settled to be a missing correction due to sattelite-speed terms: http://arxiv.org/abs/1110.2685. Until i hear or read any counter-claims to that paper, i'll consider this to be a settled matter
AnswerKenntnis:
Did anybody catch this?


  Speedy neutrino result may be due to instrument glitch


http://www.newscientist.com/blogs/shortsharpscience/2012/02/speedy-neutrino-result-may-be.html


  Loose Cable Explains Faulty 'Faster-than-light' Neutrino Result 


http://www.space.com/14654-error-faster-light-neutrinos.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+spaceheadlines+%28SPACE.com+Headline+Feed%29
AnswerKenntnis:
There was a very reliable report of finding a monopole in 1980s by Caberera(?). There was no other explanation of the glitch in the arrangement of the SQUID, but a capture of one monopole. That never repeated. Never confirmed. Never rejected as being a fake effect. I believe this question needs a couple of years more investigation. A question of physics should be repeatedly be confirmed before a postulate or an inference can be derived. Even after that derivation a sensitive experiment should be perceived to break it through further.
AnswerKenntnis:
This is not a true answer... none is knowing the explanation, so far.
However, I will post this "consideration" anyway...
Now, November 21, 2011, with 3ns pulses, the new value for the "missing time" is 62.1ns +/-3.7 (only 20 events).
My answer is only a "would-be" consideration that, if read by the experimenters, could give them some "debug" clues. 

First of all my assumptions:

it is unlikely that the neutrinos go superluminal or SR is not holding true anymore


it is unlikely that the distance is measured incorreclty
it is unlikely that the GPS setup/usage is incorrect
it is also unlikely that the light speed has been measured incorreclty so far. 


Then two hypotesis:


the "missing time" is 62.5ns (compatible with 62.1 +/-3.7ns)
the electronics involved in the time measurement has some clock domain running at 16MHz.


Of course the conclusion would be to investigate if there is one circuit running on one clock pulse less than expected by design / testing.
AnswerKenntnis:
I do not agree with the superluminal neutrinos news for very simple reasons. The difference they found with respect to the speed of light is very small, so some errors in the calulations must have been made. Neutrino is not faster than light.
The Special Theory of Relativity (STR) of Einstein, through the principle of the speed limit, makes the magnetic force come from the electric one and the magnetic force is an electric force, as physicists know; an easy demonstration of that can be found in chapter 3 of my file at the following link (also English inside):

http://www.fisicamente.net/FISICA_2/UNIFICAZIONE_GRAVITA_ELETTROMAGNETISMO.pdf

If you get rid of the speed limit principle, the magnetic field cannot exist anymore.

Moreover, as c=1/square root of(epsilon x ┬╡), if you change c with a c'>c, then you have to accept a ┬╡'<┬╡, so you have to accept different intensities of magnetic fields from a given electric current, so you have to get rid of the electromagnetism, but it's describing so well the currents, the fields, the real world etc. Therefore, there's a mistake in the computation of the speed of neutrinos, in the calculations on the run lenght, in the interaction time calculations, during the generation and also the detection of those evanescent particles!

(another interesting file, also related to this subject):

http://www.mednat.org/new_scienza/strani_legami_numerici_universo.pdf
QuestionKenntnis:
What are the justifying foundations of statistical mechanics without appealing to the ergodic hypothesis?
qn_description:
This question was listed as one of the questions in the proposal (see here), and I didn't know the answer. I don't know the ethics on blatantly stealing such a question, so if it should be deleted or be changed to CW then I'll let the mods change it.

Most foundations of statistical mechanics appeal to the ergodic hypothesis. However, this is a fairly strong assumption from a mathematical perspective. There are a number of results frequently used in statistical mechanics that are based on Ergodic theory. In every statistical mechanics class I've taken and nearly every book I've read, the assumption was made based solely on the justification that without it calculations become virtually impossible.

Hence, I was surprised to see that it is claimed (in the first link) that the ergodic hypothesis is "absolutely unnecessary". The question is fairly self-explanatory, but for a full answer I'd be looking for a reference containing development of statistical mechanics without appealing to the ergodic hypothesis, and in particular some discussion about what assuming the ergodic hypothesis does give you over other foundational schemes.
AnswersKenntnis
AnswerKenntnis:
The ergodic hypothesis is not part of the foundations of statistical mechanics. In fact, it only becomes relevant when you want to use statistical mechanics to make statements about time averages. Without the ergodic hypothesis statistical mechanics makes statements about ensembles, not about one particular system. 

To understand this answer you have to understand what a physicist means by an ensemble. It is the same thing as what a mathematician calls a probability space. The ΓÇ£Statistical ensembleΓÇ¥ wikipedia article explains the concept quite well. It even has a paragraph explaining the role of the ergodic hypothesis.

The reason why some authors make it look as if the ergodic hypothesis was central to statistical mechanics is that they want to give you a justification for why they are so interested in the microcanonical ensemble. And the reason they give is that the ergodic hypothesis holds for that ensemble when you have a system for which the time it spends in a particular region of the accessible phase space is proportional to the volume of that region. But that is not central to statistical mechanics. Statistical mechanics can be done with other ensembles and furthermore there are other ways to justify the canonical ensemble, for example it is the ensemble that maximises entropy.

A physical theory is only useful if it can be compared to experiments. Statistical mechanics without the ergodic hypothesis, which makes statements only about ensembles, is only useful if you can make measurements on the ensemble. This means that it must be possible to repeat an experiment again and again and the frequency of getting particular members of the ensemble should be determined by the probability distribution of the ensemble that you used as the starting point of your statistical mechanics calculations. 

Sometimes however you can only experiment on one single sample from the ensemble. In that case statistical mechanics without an ergodic hypothesis is not very useful because, while it can tell you what a typical sample from the ensemble would look like, you do not know whether your particular sample is typical. This is where the ergodic hypothesis helps. It states that the time average taken in any particular sample is equal to the ensemble average. Statistical mechanics allows you to calculate the ensemble average. If you can make measurements on your one sample over a sufficiently long time you can take the average and compare it to the predicted ensemble average and hence test the theory.

So in many practial applications of statistical mechanics, the ergodic hypothesis is very important, but it is not fundamental to statistical mechanics, only to its application to certain sorts of experiments.

In this answer I took the ergodic hypothesis to be the statement that ensemble averages are equal to time averages. To add to the confusion, some people say that the ergodic hypothesis is the statement that the time a system spends in a region of phase space is proportional to the volume of that region. These two are the same when the ensemble chosen is the microcanonical ensemble. 

So, to summarise: the ergodic hypothesis is used in two places:


To justify the use of the microcanonical ensemble.
To make predictions about the time average of observables.


Neither is central to statistical mechanics, as 1) statistical mechanics can and is done for other ensembles (for example those determined by stochastic processes) and 2) often one does experiments with many samples from the ensemble rather than with time averages of a single sample.
AnswerKenntnis:
As for references to other approaches to the foundations of Statistical Physics, you can have a look at the classical paper by Jaynes; see also, e.g., this paper (in particular section 2.3) where he discusses the irrelevance of ergodic-type hypotheses as a foundation of equilibrium statistical mechanics. Of course, Jaynes' approach also suffers from a number of deficiencies, and I think that one can safely says that the foundational problem in equilibrium statistical mechanics is still widely open.

You may also find it interesting to look at this paper by Uffink, where most of the modern (and ancient) approaches to this problem are described, together with their respective shortcomings. This will provide you with many more recent references.

Finally, if you want a mathematically more thorough discussion of the role of ergodicity (properly interpreted) in the foundations of statistical mechanics, you should have a look at Gallavotti's Statistical Mechanics - short treatise, Springer-Verlag (1999), in particular Chapters I, II and IX.

EDIT (June 22 2012): I just remembered about this paper by Bricmont that I read long ago. It's quite interesting and a pleasant read (like most of what he writes): Bayes, Boltzmann and Bohm: Probabilities in Physics.
AnswerKenntnis:
I searched for "mixing" and didn't find it in other answers. But this is the key. Ergodicity is largely irrelevant, but mixing is the property that makes equilibrium statistical physics tick for many-particle systems. See, e.g., Sklar's Physics and Chance or Jaynes' papers on statistical physics.

The chaotic hypothesis of Gallavotti and Cohen basically suggests that the same holds true for NESSs.
AnswerKenntnis:
You may be interested in these lectures:

Entanglement and the Foundations of Statistical Mechanics

The smallest possible thermal machines and the foundations of thermodynamics

held by Sandu Popescu at the Perimeter Institute, as well as in this paper

Entanglement and the foundations of statistical mechanics.

There is argued that:


"the main postulate of statistical mechanics, the equal a priori probability postulate, should be abandoned as misleading and unnecessary" (the ergodic hypothesis is one way to ensure the equal a priori probability postulate)
instead, it is proposed a quantum basis for statistical mechanics, based on entanglement. In the Hilbert space, it is argued, almost all states are close to the canonical distribution.


You may find in the paper some other interesting references on this subject.
AnswerKenntnis:
I do not agree with Marek's statement that ''in many practial applications of statistical mechanics, the ergodic hypothesis is very important, but it is not fundamental to statistical mechanics, only to its application to certain sorts of experiments.''

The ergodic hypothesis is nowhere needed. See Part II of my book

Classical and Quantum Mechanics via Lie algebras
for a treatment of statistical mechanics independent of assumptions of ergodicity or mixing, but still recovering the usual formulas of equilibrium thermodynamics.
AnswerKenntnis:
I have recently published an important paper, Some special cases of Khintchine's conjectures in statistical mechanics: approximate ergodicity of the auto-correlation function of an assembly of linearly coupled oscillators. REVISTA INVESTIGACI├ôN OPERACIONAL VOL. 33, NO. 3, 99-113, 2012
http://rev-inv-ope.univ-paris1.fr/files/33212/33212-01.pdf
which advances the state of knowledge as to the answer to this question.

In a nutshell: one needs to justify the conclusion of the ergodic hypothesis, without assuming the ergodic hypothesis itself.  The desirability of doing this has been realised for a long time, but rogorous progress has been slow.  Terminology: the erdodic hypothesis is that every path wanders through (or at least near) every point.  This hypothesis is almost never true.  The conclusion of the ergodic hypothesis: almost always, infinite time averages of an observable over a trajectory are (at least approximately) equal to the average of that observable over the ensemble. (Even if the ergodic hypothesis holds good, the conclusion does not follow. Sorry, but this terminology has become standard, traditional, orthodox, and it's too late to change it.) The ergodic theorem: unless there are non-trivial distinct invariant subspaces, then the conclusions of the ergodic hypothesis hold.

Darwin (http://www-gap.dcs.st-and.ac.uk/history/Obits2/Darwin_C_G_RAS_Obituary.html) and Fowler (http://www-history.mcs.st-andrews.ac.uk/Biographies/Fowler.html), important mathematical physicists (Fowler was Darwin's student and Dirac was Fowler's), found the correct foundational justification for Stat Mech in the 1920s, and showed that it agreed with experiment in every case usually examined up to that time, and also for stellar reactions.  Khintchine, the great Soviet mathematician, re-worked the details of their proofs (The Introduction to his slim book on the subject has been posted on the web at http://www-history.mcs.st-andrews.ac.uk/Extras/Khinchin_introduction.html), made them accessible to a wider audience, and has been much studied by mathematicians and philosophers of science interested in the foundations of statistical mechanics or, indeed, any scientific inference (see, for one example, http://igitur-archive.library.uu.nl/dissertations/1957294/c7.pdf and, for another example, Jan von Plato Ergodic theory and the foundations of probability, in B. Skyrms and W.L. Harper, eds, Causation, Chance and Credence. Proceedings of the Irvine Conference on Probability and Causation, vol. 1, pp. 257-277, Kluwer, Dordrecht 1988).  Khintchine's work went further, and in some conjectures, he hoped that any dynamical system with a sufficiently large number of degrees of freedom would have the property that the physically interesting observables would approximately satisfy the conclusions of the ergodic theorem even though the dynamical system did not even approximately satisfy the hypotheses of the ergodic theorem.  His arrest, he died in prison, interrupted the possible formation of a school to carry out his research program, but Ruelle and Lanford III made some progress.

In my paper I was able to prove Khintchine's conjectures
for basically all linear classical dynamical systems.  For quantum mechanics the situation is much more controversial, of course.  Nevertheless Fowler actually based his theorems about Classical Statistical Mechanics on Quantum Theory, although Khintchine did the reverse: first proving the classical case and then attempting, unsuccessfully, to deal with the modifications needed for QM.  In my opinion, the quantum case does not introduce anything new.



Why measurement is modelled by an infinite time-average in Statistical Mechanics

This is the point d'appui for the ergodic theorem or its substitutes.

Masani, P., and N. Wiener, "Non-linear
Prediction," in  Probability and Statistics, The Harald Cramer Volume,
ed. U. Grenander, Stockholm, 1959, p. 197: ┬½As indicated by von Neumann ...
in measuring a macroscopic quantity $x$ associated with a physical or biological
mechanism... each reading of $x$ is actually the average over a time-interval
$T$ [which] may appear short from a macroscopoic viewpoint, but it is large
microscopically speaking.  That the limit $\overline x$, as $T \rightarrow
\infty$, of such an average exists, and in ergodic cases is independent of the
microscopic state, is the content of the continuous-parameter $L_2$-Ergodic
Theorem.  The error involved in practice in not taking the limit is naturally to
be construed as a statistical dispersion centered about $\overline x$.┬╗
Cf. also Khintchine, A., op. cit., p. 44f., ┬½an observation which gives the
measurement of a physical quantity is performed not instantaneously, but requires
a certain interval of time which, no matter how small it appears to us, would, as
a rule, be very large from the point of view of an observer who watches the
evolution of our physical system. [...] Thus we will have to compare experimental
data  ... with time averages taken over very large intervals of time.┬╗  And
not the instantaneous value or instantaneous state. Wiener, as quoted in Heims,
op. cit., p. 138f.,
 ┬½every observation ... takes some finite time, thereby introducing
uncertainty.┬╗

Benatti, F.  Deterministic Chaos in Infinite Quantum
Systems, Berlin, 1993, Trieste Notes in Physics, p. 3, ┬½Since
characteristic times of measuring processes on macrosystems are greatly longer
than those governing the underlying micro-phenomena, it is reasonable to think
of the results of a measuring procedure as of time-averages evaluated along
phase-trajectories corresponding to given initial conditions.┬╗ And Pauli, W., Pauli Lectures on Physics, volume 4, Statistical
Mechanics, Cambridge, Mass., 1973, p. 28f., ┬½What is observed macroscopically
are time averages... ┬╗

Wiener, "Logique, Probabilite
et Methode des Sciences Physiques," ┬½Toutes les lois de probabilite connues
sont de caractere asymptotique... les considerations asymptotiques n'ont
d'autre but dans la Science que de permettre de connaitre les proprietes des
ensembles tres nombreux en evitant de voir ces proprietes s'evanouir dans la
confusion resultant de las specificite de leur infinitude.  L'infini permet
ainsi de considere des nombres tres grands sans avoir a tenir compte du fait
que ce sont des entites distinctes.┬╗



Why we need to replace ensemble averages by phase averages, which can be accomplished in different ways, the traditional way is to use the ergodic hypothesis.

These quotations express the orthodox approach to Classical Stat Mech.  The classical mechanics system is in a particular state, and a measurement of some property of that state is modelled by a long-time average over the trajectory of the system.  We approximate this by taking the infinite time average.  Our theory, however, cannot calculate this, anyway we don't even know the initial conditions of the system so we do not know which trajectory... what our theory calculates is the phase average or ensemble average.  If we cannot justify some sort of approximate equality of the ensemble average with the time average, we cannot explain why the quantities our theory calculates agree with the quantities we measure.

Some people, of course, do not care.  That is to be anti-foundational.
QuestionKenntnis:
Is it necessary to consume energy to perform computation?
qn_description:
As far as I know, today most of the computers are made from semiconductor devices, so the energy consumed all turns into the heat emitted into space.

But I wonder, is it necessary to consume energy to perform computation?


If so, is there a theoretical numerical lower bound of the energy usage ? (I even have no idea about how to measure the amount of "computation")
If not, is there a physically-practical Turing-complete model that needs no energy?




edit:
Thank @Nathaniel for rapidly answering the question and pointing out it's actually Landauer's principle. Also thank @horchler for referring to the Nature News and the related article. There are lots of useful information in the comments; thank every one! This whole stuff is really interesting!
AnswersKenntnis
AnswerKenntnis:
What you're looking for is Landauer's principle. You should be able to find plenty of information about it now that you know its name, but briefly, there is a thermodynamic limit that says you have to use $k_BT \ln 2$ joules of energy (where $k_B$ is Boltzmann's constant and $T$ is the ambient temperature) every time you erase one bit of computer memory. With a bit of trickery, all the other operations that a computer does can be performed without using any energy at all.

This set of tricks is called reversible computing. It turns out that you can make any computation reversible, thus avoiding the need to erase bits and therefore use energy, but you end up having to store all sorts of junk data in memory because you're not allowed to erase it. However, there are tricks for dealing with that as well. It's quite a well developed area of mathematical theory, partly because the theory of quantum computing builds upon it.

The energy consumed by erasing a bit is given off as heat. When you erase a bit of memory you reduce the information entropy of your computer by one bit, and to do this you have to increase the thermodynamic entropy of its environment by one bit, which is equal to $k_B \ln 2$ joules per kelvin. The easiest way to do this is to add heat to the environment, which gives the $k_BT \ln 2$ figure above. (In principle there's nothing special about heat, and the entropy of the environment could also be increased by changing its volume or driving a chemical reaction, but people pretty much universally think of Landauer's limit in terms of heat and energy rather than those other things.)

Of course, all of this is in theory only. Any practical computer that we've constructed so far uses many orders of magnitude more energy than landauer's limit.
QuestionKenntnis:
What is the actual significance of the amplituhedron?
qn_description:
A news has recently became viral that physicists have discovered a geometrical object that simplifies a lot our models quantum physics. For an outsider like me, it is difficult to actually understand the significance of that finding. 

Is it actually a new model that literaly makes every quantum physics book obsolete - or is it just a tool for a very specific calculation or effect that barely change everything else on the field?
AnswersKenntnis
AnswerKenntnis:
There was a presentation of the idea at SUSY2013 by Nima Arkani-Hamed which is available on video at http://susy2013.ictp.it/video/05_Friday/2013_08_30_Arkani-Hamed_4-3.html

The amplituhedron is a buzzword for a description of a way to solve maximally supersymmetric (i.e. N=4) Yang-Mills theory in 4 dimensions. Ordinary Yang-Mills theory is a generalization of quantum gauge field theories which include electrodynamics and quantum chromodynamics. The supersymmetric extensions have not been found in nature so far.

The usual way to calculate scattering amplitudes in quantum field theory is by adding together the effects of many Feynman diagrams, but the number and complexity of diagrams increases rapidly as the number of loops increases and if the coupling is strong the sum converges slowly making it difficult to do accurate calculations. 

The new solution for Super Yang-Mills uses the observation that the theory has a superconformal invariance in space-time and another dual superconformal invariance in momentum space. This constrains the form that the scattering amplitudes can take since they must be a representation of these symmetries. There are further constraints imposed by requirements of locality and unitarity and all these constraints together are sufficient to construct the scattering amplitudes in the planar limit without doing the sum over Feynman diagrams. The mathematical tools needed are twistors and grassmanians. The answer for each scattering amplitude takes the form of a volume of a high dimensional polytope defined by the positivity of grassmanians, hence the name amplituhedron.

The first thing to say about this is that so far it is only applicable to the planar limit of one specific quantum field theory and it is not one encountered in nature. It is therefore very premature to say that this makes conventional quantum field theory obsolete. Some parts of the theory can be generalised to more physical models such as QCD but only for the tree diagrams and the planar limit. There is some hope that the ideas can be broadened beyond the planar limit but that may be a long way off.

On its own the theory is very interesting but of limited use. The real excitement is in the idea that it extends in some way to theories which could be physical. Some progress has been made towards applying it in maximal supergravity theories, i.e. N=8 sugra in four dimensions. This is possible because of the observation that this theory is in some sense the square of the N=4 super Yang Mills theory. At one time (about 1980) N=8 SUGRA was considered a candidate theory of everything until it was noticed that its gauge group is too small and it does not have chiral fermions or room for symmetry breaking. Now it is just considered to be another toy model, albeit a very sophisticated one with gravity, gauge fields and matter in 4 dimensions. If it can be solved in terms of something like an amplituhedron it would be an even bigger breakthrough but it would still be unphysical.

The bigger hope then is that superstring theory also has enough supersymmetry for a similar idea to work. This would presumably require superstring theory to have the same dual superconformal symmetry as super Yang Mills, or some other even more elaborate infinite dimensional symmetry. Nothing like that is currently known.

Part of the story of the amplituhedron is the idea that space, time, locality and unitarity are emergent. This is exciting because people have always speculated that some of these things may be emergent in theories of quantum gravity. In my opinion it is too strong to call this emergence. Emergence of space-time implies that space and time are approximate and there are places such as a black hole singularity where they cease to be a smooth manifold. The amplituhedron does not give you this. I think it is more accurate to say that with the amplituhedron space-time is derived rather than emergent. It is possible that true emergence may be a feature in a wider generalisation of the theory especially if it can be applied to quantum gravity where emergence is expected to be a feature. Having space-time and unitarity as a derived concept may be a step towards emergence but it is not the same thing.

For what my opinion is worth I do think that this new way of looking at quantum field theories will turn out to something that generalises to something that is really part of nature. I have advocated the idea that string theory has very large symmetries in the form of necklace algebras so these ideas seem on the right track to me. However I do think that many more advances will be required to work up from super yang mills to sugra and then string theory. They will have to find a way to go beyond the planar limit, generalise to higher dimensions, include gravity and identify the relevant symmetries for string theory. Then there is just the little issue of relating the result to reality. It could be a long road.
AnswerKenntnis:
The publication of the paper by Arkani-Hamed and Trnka is imminent. It should offer a completely new way to calculate the probability amplitudes in quantum field theories ΓÇô relatively to the usual Feynman diagrams ΓÇô but the observable results are finally the same. See


  http://motls.blogspot.com/2013/09/amplituhedron-wonderful-pr-on-new.html?m=1


for some basic info, links, and the big picture. The resulting observable quantities are the same as calculated by other methods so you don't have to throw away the usual textbooks on quantum mechanics and quantum field theory; they haven't been invalidated. 

However, the calculation using the shape in an infinite-dimensional space, the amplituhedron, should provide us with completely new perspectives how to look at the dynamics ΓÇô perspective that is timeless, obscures the location of objects and events in the space and time, and obscures the unitarity (the requirement that the total quantum-calculated probability of all possibilities remains 100%), but it unmasks some other key structures that dictate what the probabilities should be, structures we were largely ignorant about.

If the new picture becomes sufficiently generalized, you could perhaps throw away the old books because you will get an entirely new framework to compute these things and to think about all these things. But once again, you don't have to throw them away because the physical results are the same.
AnswerKenntnis:
Most of the success in this whole area is in N=4  super yang mills seems great. At least at tree level the scattering amplitude for tree level is the same as the scattering amplitude for non-supersymmetric theories or QCD for gluon scattering. However, it seems to me that all the progress has been made in the context of integrand. Now integrands are not trivial things but an amplitude is much more difficult. More importantly, one needs to regularize. Is it always clear that when one simplifies these integrands, we do not lose the information when one simplify these terms out? For example for pure Yang Mills there are all plus(or minus) helicity amplitude, at tree level they all vanish. However, at one loop level they have a finite contribution. In supersymmetric theory these all plus helicity one loop amplitude tend to vanish and they do not contribute. However, is it plausible that we lose certain information when we simplify these integrands in terms of these very interesting expression?
AnswerKenntnis:
Certainly texts that address the probabilities of different particle interactions will be incomplete without mentioning this overall connection to geometry.  Perhaps the biggest text book change would be the loss of "unitarity", the notion that the sum off the probabilities of each possible interaction should add up to one.

Beyond that, this new tool is expected to make further investigations into particle physics easier.
AnswerKenntnis:
"Locality is the notion that particles can interact only from
  adjoining positions in space and time."


True, except for the time part. Time is derived from expanding space. And, there's also that little thing of 'low pass filtered expanding space' and 'particle locality'. Particle is only as much local as its leading and trailing edges (in terms of 2D visualization of information propagation in a low pass filtered system/space)... interacting with edges of another particle (which is why electron size differs depending on which other particle is used to measure it).

Is space works as it seems to be working, every particle in existence has an information  'tail' that extends outwards by as much as 99.9% of the particle's true size. There's a whole sea of invisible information in the space out there, visibly interacting only when combined edges of interacting particles reach the threshold value needed for visible interaction. The rest of the time, that information appears as 'virtual particles'. In other words, who needs 'virtual photons' when electrons themselves can act on much, much larger scales than their apparent size?
QuestionKenntnis:
Why do tuning forks have two prongs?
qn_description:
I believe the purpose of a tuning fork is to produce a single pure frequency of vibration.  How do two coupled vibrating prongs isolate a single frequency?  Is it possible to produce the same effect using only 1 prong?  Can a single prong not generate a pure frequency?  Does the addition of more prongs produce a "more pure" frequency?

The two prong system only supports a single standing wave mode, why is that?
AnswersKenntnis
AnswerKenntnis:
I am by no means an expert in tuning fork design, but here are some physical considerations:


Different designs may have different "purities," but don't take this too far. It is certainly possible to tune to something not a pure tone; after all, orchestras usually tune to instruments, not tuning forks.
Whatever mode(s) you want to excite, you don't want to damp with your hand. Imagine a single bar. If you struck it in free space, a good deal of the power would go into the lowest frequency mode, which would involve motion at both ends. However, clamping a resonator at an antinode is the best way to damp it - all the energy would go into your hand. A fork, on the other hand, has a natural bending mode that will not couple very well to a clamp in the middle.
AnswerKenntnis:
If there were only one prong (imagine holding a metal rod in your hand), then the oscillation energy of the prong would quickly be dissipated by its contact with your hand.  On the other hand, a fork with two prongs oscillates in such a way that the point of contact with your hand does not move much due to the oscillation of the fork.  This causes the oscillations to be safe from damping due to contact with your hand, so they continue for a longer period of time.
AnswerKenntnis:
Q. How do two coupled vibrating prongs isolate a single frequency? 

howstuffworks.com has an article on How Tuning Forks Work


  The way a tuning fork's vibrations interact with the surrounding air is what causes sound to form. When a tuning fork's tines are moving away from one another, it pushes surrounding air molecules together, forming small, high-pressure areas known as compressions. When the tines snap back toward each other, they suck surrounding air molecules apart, forming small, low-pressure areas known as rarefactions. The result is a steady collection of rarefactions and compressions that, together, form a sound wave.
  
  The faster a tuning fork's frequency, the higher the pitch of the note it plays. For instance, for a tuning fork to mimic the top key on a piano, it needs to vibrate at 4,000 Hz. To mimic the lowest key, on the other hand, it would only need to vibrate at 28 Hz.


Two prongs on a tuning fork oscillate such that they both move together, then they both move apart. These compressions and rarefactions of air between and behind the prongs is what creates the stronger compression waves in the air and hence louder sound of this primary mode of vibration.

In contrast, when you pluck a string, the fundamental frequency is produced by the vibration of the whole string, but the string is also vibrating in halves, thirds, fourths, fifths, etc. This causes overtones making the frequency not as pure, but rather harmonic.

via wikipedia:



Same thing in woodwind and brass instruments when you blow air through a tube, or vibrate a reed playing air through a tube, or strike a bell, whose shape is set up to accentuate different harmonics. The relative loudness of the different harmonic overtones gives each instrument its own timbre.

A tuning fork is designed such that the harmonic overtones are quiet compared to its fundamental pitch. I found this great YouTube video showing a tuning fork model which shows the different modes the fork vibrates in, and models the strength of each mode of vibration.

The video also shows the constraints of holding the tuning fork on the end, which eliminates the rigid body modes (which were already quiet to begin with) but also dampens some of the other harmonic modes, creating and even more pure tone with very low amplitude harmonics. Daniel A. Russell at The Pennsylvania State University has a page showing animations of these vibrational modes.

Holding the tuning fork at the end does little to dampen the mode of vibration which creates the primary frequency. If you also hold the end of the fork against a hard surface, the small up and down movement will cause resonance in the surface, amplifying the primary frequency even more.



Q. Is it possible to produce the same effect using only 1 prong? Can a single prong not generate a pure frequency? Does the addition of more prongs produce a "more pure" frequency?

One prong wouldn't have the additional compression effect of two prongs moving closer together, creating a louder primary frequency. But more importantly, the second loudest mode of a tuning fork (the "clang" mode, the high-pitched sound you hear when it is first struck) is dampened because you strike the fork at a modal point about 1/4 the length of the prongs from its vibrating end.

Additional prongs don't create more dampening effects, yet they also create more vibrating modes, so the sound is less "pure".

Related question: Why don't tuning forks have three prongs?

Edit: Reference paper, with formulae and data on vibration mode frequencies, etc.
AnswerKenntnis:
Two-prong system surely supports more than one mode, consider:


squeezing the prongs together (mode that you want)
twisting the both prongs relative to stem
twisting each prong relative to its base
wobble/barrel of both prongs
sound wave travelling in the metal from one edge to another
etc...


If you are a designer of the tuning fork you want one mode to dominate, that is you want all other modes to dissipate quickly.

In fact holding the tuning fork in your hand already helps to dampen some of the modes.

Furthermore, the stem is sometimes put on a table or similar amplify the sound.

My guess is, tuning fork is made of particular metal to achieve stability, the cross-section of the prong is considered to get rid of some secondary modes, the losses in are optimised to ensure narrow enough frequency as well as being able to hear the tuning fork, relative ease of manufacturing and perhaps a dozen more considerations that only musicians could think of.
AnswerKenntnis:
The reason is that to work properly the tuning fork has to have a balanced motion. It is normally used held in the hand. If you just had one prong, the energy of the oscillation would very quickly be transferred from the handle to the skin of the hand, and would be lost. The result would be that the oscillation would die away very quickly.  If you have a tuning fork with two prongs of equal size, they can oscillate with motion equal and opposite to each other - balanced in other words. Because the motion of one prong balances out the motion of the other, there is no motion of the handle. Because there is no mechanical energy going into the handle, no energy can be lost into the hand, so the oscillation lasts a long time.
AnswerKenntnis:
Resonance amplifies and sustains the tone much longer than with just one prong.

Think about this: if you've ever sung in the shower or in the car, have you noticed that some pitches sound abnormally louder than others? That's because the dimensions of the shower are just right such that those notes are amplified via resonance. For example the shower width might be an integer multiple of a certain pitch's wavelength, so the wave bounces back and fourth, riding itself and getting bigger. Like when you push a kid on the swing at just the right times so she gets higher and higher.

The 2 prongs on the fork resonates the sound, just like your shower walls. Each prong forces the other prong to vibrate at the same rate, thus sustaining the sound longer. If there were only 1 prong, the sound would be much quieter and it would die off much quicker. Try it with a butter knife.
QuestionKenntnis:
Why am I not burned by a strong wind?
qn_description:
So I was thinking... If heat I feel is just lots of particles going wild and transferring their energy to other bodies, why am I not burned by the wind?

When I thought about it more I figured out that wind usually carries some humidity, and since particles of liquid are moving same speed as the wind, they are basically static relative to each other, so no energy is transferred between them (wind and water particles). And if that water sticks to my skin and wind blows, it'll evaporate thus taking energy from my skin and make me feel cold.

Thing is, I don't think that's really the case but even if it is, if I somehow dry out the wind, will it burn me if it's strong enough? And winds can reach some pretty high velocities (though I must admit I'm not sure if they are comparable to movement of atoms in warm bodies etc...).

So. Bottom line. Can I be burned by wind in some perfect scenario?
AnswersKenntnis
AnswerKenntnis:
Air molecules $(\require{mhchem}\ce{N2_}$ and $\ce{O_2})$ have an average velocity of around $500\text{ m/s}$, varying some depending on the temperature. This means that a nice $5\text{ m/s}$ wind is a hundred times slower, and the energy represented by wind is 10,000 times smaller than the thermal energy. Therefore, wind does not have considerably more energy than calm air and will not burn you.

Very high-speed winds, such as those in tornadoes, hurricanes, or the wind you would experience while sky-diving, are still only around $50\text{ m/s}$, so the energy density in the wind is still just 1% of the thermal energy density. Likewise, the ram pressure the air exerts on you would be small compared to the homogenous atmospheric pressure, so no large effects should be observed. Thus, one would not expect even high winds to burn you.

The transfer of heat between you and the air is fairly complicated, and does not depend solely on the energy density of the air. Wind usually makes you feel colder, in fact. Heat travels across gradients of temperature. The air right next to your skin will be at the same temperature as your skin, but the air a small distance away will be at the ambient temperature. This creates a gradient of temperature, and heat travels across the gradient. When there is wind, the difference in temperature between your skin and the ambient air is the same, but the temperature falls down to the ambient temperature a shorter distance from your skin. This increases the temperature gradient, so that you cool down faster with a wind.

Humidity also plays a role; heat transfer is not very simple. However, I think this suffices to explain why we should not expect wind to burn you. You will burn up if you travel through the air at extremely-high velocity. This happens to meteors and other astronomical objects moving at orbital velocities ($\sim10^4\text{ m/s}$) when they enter Earth's atmosphere. It is also relevant for fast-moving aircraft, which do experience winds as fast as the thermal velocities of the molecules in the air. I've heard it said that the SR-71 Blackbird, the fastest airplane ever built, heated up so much due to aerodynamic heating that it had to be built to be loose at low speed so that the parts would fit together at top speed. See "Aerodynamic heating" for more.
AnswerKenntnis:
The other answers address your question quite well.  Just as a reminder of the ability to be burned by a strong enough wind, the image below shows the Chelyabinsk meteor during entry into Earth's atmosphere last year over Russia. :)
AnswerKenntnis:
If heat I feel is just lots of particles going wild and transferring their energy to other bodies, why am I not burned by the wind?


I think the most direct answer to your question is that heat is the random movement of molecules, with speeds on the order of $v_{rms} = \sqrt{\frac{3RT}{m}}$ which is in the hundreds of meters/second range, whereas the layer of air surrounding you moves in a not-quite random way (there is a boundary layer, for example), and not usually as fast.

If we think of friction, on the other hand, I think the problem is that while friction /drag will heat you up and the air surrounding you, the stream of air is also very efficient at removing heat from you, so depending on the temperature of the wind, the two effects will partially cancel each other out.

Now, when you start getting to transonic speeds, the (adiabatic?) compression of the air in front of you will heat it up significantly (I believe this is why the SR-71 was made out of titanium). At hypersonic speeds, this can give you a nasty case of plasmification.
AnswerKenntnis:
"Can I be burned by wind in some perfect scenario?"

Drag Effects

Drag Effects are the main source of the effect you are after in your primary question.

There is always an effect of heat from the wind drag, but in most circumstances the heat transfer is a net loss to your skin due to moisture evaporation and the air temperature being below your body temperature. 

See also:

http://en.wikipedia.org/wiki/Drag_%28physics%29

http://en.wikipedia.org/wiki/Parasitic_drag#Skin_friction

Moisture Effects

You mention the effect of moisture in the air, keep in mind that it condensing on you would heat you as it loses energy converting from gas to liquid state, and this would rarely happen, unless your skin is very dry and the air very moist (hot and humid days are no fun, are they?), or unless you're exposed to steam (hence the more terrible effects of steam burns versus boiling water burns.) It's better to consider your wind where the air-moisture to skin-moisture balance is such that no energy is gained or lost. Since stronger wind would increase the evaporation rate, that balance would vary based on the wind speed, so consider that at higher speeds your skin would need to be very dry or the air to be very moist to maintain that balance.

See also: 

http://en.wikipedia.org/wiki/Enthalpy_of_vaporization

The Bounds of Reason

Since terminal velocity is 120 mph, or about 54 m/s, it is unlikely you would ever be exposed to relative wind very much faster than that, since it would pick you up and carry you along with it. Also, your body needs moisture for you to live, so for it to become very dry, you would have to be dead. Altogether, I would say for you to ever feel a net positive heating from wind would be very rare.

However, in conclusion, yes, you can be burned by the wind.
AnswerKenntnis:
it is possible to be  burnt by a combination of very warm air (~ 46┬░c or maybe lower ) and a moderate fan. People in ill health, drunk or unconcious are at most risk. Eg. http://www.sciencedirect.com/science/article/pii/S0003497500013229

Edit: more thoughts. I use these "forced air warming" devices every day at work. The other situation that is assocated with burns is if the hosing delivering the warm air is in contact with an unconscious patient this may lead to a burn - so this is a conduction burn, not a convection burn - but the heat is delivered via conduction. To tie this back to the OP - if a hot air wind is heating a highly conductive surface then contact with this surface may lead to a burn more readily - effectively this conductive surface concentrates the heat. Also tissue that is compressed is more more vulnerable to burns - so if the hot surface is pressed against the skin this speeds up the burning because the body part is unable to carry the heat away with increased blood flow.
QuestionKenntnis:
How can a black hole produce sound?
qn_description:
I was reading this article from NASA -- it's NASA -- and literally found myself perplexed. The article describes the discovery that black holes emit a "note" that has physical ramifications on the detritus around it.


  Sept. 9, 2003: Astronomers using NASAΓÇÖs Chandra X-ray Observatory have found, for the first time, sound waves from a supermassive black hole. The ΓÇ£noteΓÇ¥ is the deepest ever detected from any object in our Universe. The tremendous amounts of energy carried by these sound waves may solve a longstanding problem in astrophysics. 
  
  The black hole resides in the Perseus cluster of galaxies located 250 million light years from Earth. In 2002, astronomers obtained a deep Chandra observation that shows ripples in the gas filling the cluster. These ripples are evidence for sound waves that have traveled hundreds of thousands of light years away from the clusterΓÇÖs central black hole. 
  
  ΓÇ£The Perseus sound waves are much more than just an interesting form of black hole acoustics,ΓÇ¥ says Steve Allen, of the Institute of Astronomy and a co-investigator in the research. ΓÇ£These sound waves may be the key in figuring out how galaxy clusters, the largest structures in the Universe, grow.ΓÇ¥


Except: 


Black holes are so massive that light, which is faster than sound, can't escape.
Sound can't travel in space (space has too much, well, space)
It's a b-flat?


So: How can a black hole produce sound if light can't escape it?
AnswersKenntnis
AnswerKenntnis:
I'm not going to address the production mechanism,1 just the nature of the "sound" in this case.



What you think of as the hard vacuum of outer space could just as well be seen as a very, very, very diffuse, somewhat ionized gas. That gas can support sound waves as long as the wavelength is considerably longer than the mean free path of the atoms on the gas.

As for the tone, there is a simple relationship between the tone of the same name in different octaves, so once they know the dominant frequency they can figure its place on the scale.



1 Though it won't be happening inside the event horizon -- which is where "not even light can escape" holds -- but in the region around the hole proper where it accumulates gas and dust and the magnetic fields from the hole play merry havoc with the ionized components of the accumulated stuff.
AnswerKenntnis:
As the other guys have already covered most of the topic, I'd like to quote some things. Light can't escape only from the inside of event horizon because it has already fallen into it. But after reading the article now, we could indicate some points.


The article specifically says a "supermassive blackhole". They're a way too bulk in size when compared to other blackholes (Schwarzchild for example). So, they're not strong enough to produce an astonishing tidal force upon the falling matter at the event horizon. And that's because the central singularity is located very deep inside the BH (quite a large distance from the event horizon).
As you say, outer space is just hard kinda vacuum and not completely vacuum. It consists about a few hydrogen atoms per cubic meter. In your case, the gases in the Perseus cluster is good enough to serve as a medium for these very low frequency sound waves to travel through.


The sound waves are just longitudinal pressure waves and hence the fate "they require medium". As the BH pulls material inwards (into it), the gravitational pressure causes it to expel matter and energy out of it which we perceive as gas jets and we named them as "relativistic jets".



Astronomer Steve Allen told, (I don't know him, but looks like he's a big guy)


  The ripples were caused by the rhythmic squeezing and heating by the intense gravitational pressure of the jumble of galaxies packed together in the cluster. As the black hole pulls material in, it also creates jets of material shooting out above and below it, and it is these powerful jets that create the pressure that creates the sound waves.


But I'm happy that we haven't really heard the black hole sound. The actual observation was due to X-Rays. I can't even imagine, "How sound waves reach us traveling through the bare leftover hydrogen atoms distributed throughout the galaxy?" Indeed, they may be able to travel through the jets but can't reach us that fast..! Actually, these pressure waves have traveled along with the jets traversing throughout the medium by their stretch/squeeze physical fantasy. I think the long-term curiosity got vanished by this kind of observation.   

Why these X-Ray clouds are always hot and don't cool down at all?

The same topic spoken around NASA indicates something satisfiable...


  The sound waves were indirectly detected using the Chandra telescope because the cluster gas is very hot and thus emits an especially energetic form of light called X rays, as well as less energetic visible light. And the gas is so hot because of the effects of the black hole. More than an acoustic curiosity, these sound waves transport energy that keeps gas throughout the cluster warmer than it would otherwise be. These warmer temperatures, in turn, regulate the rate of new star formation, and hence the evolution of galaxies and galaxy clusters.


And your article too agrees the fact:


  The sound waves, seen spreading out from the cavities in the recent Chandra observation, could provide this heating mechanism.
AnswerKenntnis:
I am going to speculate on a production mechanism to complement @dmckee's answer.

It is true that light cannot escape from a black hole, except it can lose energy through Hawking radiation.

Suppose the black hole oscillates,vibrates,  i.e. within it compression waves exist this would of course be another way of losing energy. This because the radius of the horizon would be changing in time, i.e. the gravitational field in space around it.

The detection of these sound waves around the hole would also be a measurement of  gravitational waves .

I googled "vibrating black hole" . the first hit:


  A black hole can vibrate, and its vibrations produce gravitational waves (ripples in the fabric of spacetime). These waves carry away the "hair" of any newborn black hole, leaving it hairless when quiescent, and they also carry encoded in themselves the values of the hole's mass and spin [Bill Press, Richard Price, and Saul Teukolsky].


A second way of gravitational waves from the same source:


  A small black hole orbiting a massive hole or any other massive body produces gravitational waves, and those waves carry, encoded in themselves, full maps of the body's warped spacetime. If we can detect the waves and extract their maps, we can use the maps to determine the nature of the massive body, and if it is a black hole, we can use the maps to test the no-hair prediction [Kip Thorne and Fintan Ryan].


So it is a scientific speculation and not a science fiction one.
AnswerKenntnis:
First, for additional references, there is the original press release. Also, a similar report from a different black hole is here. It seems the Chandra people like this sort of thing. It is also worth noting that as far as I can tell, there are only press releases and no published scientific articles on this phenomenon.



Now to address the questions.


  Black holes are so massive that light, which is faster than sound, can't escape.


Well, light from inside the black hole cannot escape. But active black holes create violent neighborhoods around them. In general, there will be an accretion disk - a relatively flat disk of material slowly spiraling into the black hole. Friction due to differential rotation in this disk can make it glowing hot, especially close to the black hole. Furthermore, angular momentum and magnetic fields conspire to make jets shooting out of the "poles" of the black hole (pretty much all black holes in astronomy are expected to be rotating significantly). Again, the matter never actually got into the event horizon, since by definition we would not see it again.

So what is this sound? As best I can tell from the press releases, these particular black holes cycle through periods of low accretion and high accretion. There may be a lot of material falling in for a long time, powering jets that pump energy out beyond even the galaxy in which the black hole resides.1 Then there will be a few million years of almost nothing falling into the black hole, during which the jets are little more than trickles. This cycle repeats semi-regularly, causing periodic bursts of energy to be sent out.


  Sound can't travel in space (space has too much, well, space).


Well, yes and no. It is true there is some material even in the space between galaxies. On the other hand, it is extremely diffuse. In fact, it has been said (I cannot remember the source) that the densest clouds of material in interstellar space are more diffuse than the best vacuums we can make in laboratories. So you might imagine there can be some propagation, but not in the traditional sense.

Really, when the Chandra people say there is "sound," what they mean is this. Thus pulses of energy sent out form shock waves2 that propagate through the intergalactic medium. If you look far enough, you will see these periodic dense regions in a pattern not unlike the ripples in a pond. Since "sound" in the normal sense consists of overdensities traveling through the air (albeit without shocks in most cases), and since these are periodic overdensities in the diffuse gas between galaxies, we may as well make a connection between them in terminology.


  It's a b-flat?


Take this with a large grain of salt. I doubt the duty cycle of the black hole is so regular as to produce a monochromatic "pitch." This is more of a whimsical calculation. Nathaniel in a comment did the reverse calculation - going from "note" to frequency. To see how the scientists changed a frequency $f$ (basically the reciprocal of the time between active periods of the black hole) to a named note, see this Wikipedia entry. In short, the number of the pitch is
$$ p = 69 + 12 \log_2\left(\frac{f}{440~\mathrm{Hz}}\right). $$
They plugged in an $f$ and must have gotten a $p$ around $-614$ or $-626$ (the wording is ambiguous). The B-flat above middle C has $p = 70$, the one below middle C has $p = 58$, the next one down is $p = 46$, etc.



Addendum: Why is this interesting? Beyond the whimsy, there is scientific value to this. Believe it or not, most of the mass (or at least, most of the mass of normal, non-dark matter) of a cluster of galaxies is found outside the galaxies themselves, in this intergalactic (aka intracluster) medium. It makes up a significant portion of the universe despite its low density, due entirely to the large volume it occupies. More interestingly, this gas is very hot - millions of degrees, despite the "background temperature" of the universe arguably being a few degrees above absolute zero. It is so hot that it glows in X-rays, which is why Chandra - a space-based X-ray telescope - studies it. These sound waves might explain a significant source of heating and injection of turbulence into this matter, which can have broad implications for extragalactic cosmology and galaxy formation. This all falls under the key term "AGN feedback," which is quite a hot topic in astrophysics these days.



1 Our own Milky Way's supermassive black hole probably had an active phase at some point. Astronomers recently detected the so-called Fermi bubbles that indicate two jets were shooting out of the center of our galaxy millions of years ago.

2 "Shock waves" have a technical meaning in fluid dynamics. Basically, there is a discontinuity in the density - ahead of the shock, everything is normal and diffuse, but as it passes there is almost instantaneous compression and heating. Think of the blast wave from an explosion.
AnswerKenntnis:
Actually, its not the Black Hole (singularity or anything below event horizon) which is emitting sound waves.

A Black Hole sucks matter from its neighborhood.  These being sucked matters produce sound waves when they are on its way before entering Event Horizon of black hole (light can't escape from inside event horizon). The produced sound waves are nothing but cavity-based ripples in Perseus cluster gas. The ripples are formed by "jet of materials" (traveling at near speed of light) pushing back cluster gas. Due to its high energy, it can be detected at millions of light years from source.

But, what expels matter at such high speeds near the ultimate sinkhole?
The answer: Its the enormous electromagnetic forces which shot it away before it got beyond the black hole's event horizon. These powerful forces are generated as magnetized, hot gas swirls toward the black hole creating extreme voltages which accelerate particles away from the disk in opposite directions.
AnswerKenntnis:
So: How can a black hole produce sound if light can't escape it?


Light can't escape a black hole only if it's inside the event horizon. Light can escape (the gravitational pull of) a black hole if it's outside of the horizon. Likewise, sound can escape the gravitational pull if it's sufficiently far away from the horizon. I would guess that the horizon is different for light and for sound, but that's just a hunch.
AnswerKenntnis:
I'd be more concerned about how they are detecting this sound than how it escapes... I'm not aware of any medians in space for sound to travel through but then again I've never studied space... or physics for that matter..

I do however know that you can thing of sounds as compression and decompression of particles in an array... When anything gets pulled away it leaves an empty space that will get filled by particles around this space (vacuum effect... pressure pushes these particles in) when those particles leave they now have created empty spaces, so now particles around them (probably even including themselves) go back toward that space.... and so on and so one, you create a ripple effect (keep in mind this is a 3D ripple effect... picture wobbling spheres inside one another, not a stone hitting a body of water haha) This ripple (3D ripple) can be considered a wave, it has a frequency, and therefor when this motion of compression reaches an eardrum it will be detected...

Again, I still don't know how this sound travels anywhere since space is in itself a vacuum from what I know...
QuestionKenntnis:
What was the major discovery on gravitational waves made March 17th, 2014, in the BICEP2 experiment?
qn_description:
The Harvard-Smithsonian Centre for Astrophysics held a press conference today to announce a major discovery relating to gravitational waves. What was their announcement, and what are the implications?

Would this discovery be confirmation of gravitational waves as predicted by general relativity (even though Sean Carroll links to the Nobel website implying that G-waves was detected decades ago, while my book on GR (B. Schutz) says they're still looking...I'm confused) 

Also, regarding inflation theory, would this discovery confirm inflation or refute it? Or something else, a la string theory?
AnswersKenntnis
AnswerKenntnis:
The actual paper (pdf) is very heavy in error quantification - and rightly so.  They presented an experiment result that is statistically extremely difficult to obtain.  But for the rest of us, conclusion is the most important part.  The abstract says:


  The observed B-mode power spectrum is well fit by a lensed-$\lambda$CDM + tensor theoretical model with tensor/scalar ratio r = 0.20


As far as I can tell, this is their conclusion.  To start to understand, we have to get a grasp on the component theories.


Lambda-CDM_model is a cosmology model, basically
The Cosmic Microwave Background (CMB) is the light from the last scatter from when the early universe was still opaque
The scalar density perturbations (fluctuations) of the CMB are from quantum fluctuations in the energy universe, and these were the "seeds" that allowed galaxies to form, giving rise to the structure we live among.
The tensor/scalar ratio (variable r) in the paper is a measure of the magnitude of tensor fluctuations of the CMB relative to the already-measured scalar fluctuations.  I satisfy myself by saying the tensor fluctuations are "vector" fluctuations.
B-mode is the type of polarization signal.  Apparently another type of this signal was discovered earlier, but this is beyond my understanding.


The paper is also very clear that the r value is not 0.  Their experiment proves this fact to $5.9 \sigma$.  By my standards, that makes the proposition true.

The physicist who predicted this was visited by someone who worked on it, and there's a video of it online.  First words said was that "it's 5 sigma at .2".  The 5 sigma just means it's right.  The .2 is referring to the r value above.  That was the shock that the media is referencing.  The fact r=.2 is new information to science here.

The blog post by another user's blog is also very informative.  It's also much more heavy on the implications of the discovery.  For instance, the discovery gives us much better information on the energy at which the inflation epoch took place.  However, the impacts of the discovery are extraordinarily far reaching.  So, I would say that the specific discovery at hand here is that r does not equal zero, and is close to 0.2.



Here are some quotes from the first part of the paper which hint at the motivations for the experiment in the first place.  Emphasis mine:


  Inflation predicts that the quantization of the gravitational field coupled to exponential expansion produces a primordial background of stochastic gravitational waves with a characteristic spectral shape (Grishchuk 1975; Starobinsky 1979; Rubakov et al. 1982; Fabbri & Pollock 1983; Abbott & Wise 1984; also see Krauss
  & Wilczek 2013). 


The wording here is crucial, note "quantization of the gravitational field".  This is quantum gravity, and a theory-based prediction that led to a measured result.  To me, this fact is even more incredible than getting direct evidence for gravitational waves.  In fact, from my reading, this seems to be from treating the graviton's properties within the context of quantum fields.

For more detail:


  Though unlikely to be directly detectable in modern instruments, these gravitational waves would have imprinted a unique signature upon the CMB. Gravitational waves induce local quadrupole anisotropies in the radiation field within the last-scattering surface, inducing polarization in the scattered light (Polnarev 1985). This polarization pattern will include a ΓÇ£curlΓÇ¥ or B-mode component at degree angular scales that cannot be generated primordially by density perturbations.


This is going over how to get from gravitational waves to the polarization.  I'm still a little iffy on exactly what property of gravitational waves leads to this.  However, their visuals page gives a helpful hint for me.  See their depiction of polarization.  The "density wave" is what I had typically associated with a gravity wave.  However, I recognize that a more complicated alternative is also possible.  This is trivially true because general relativity uses tensors.  It's the difference between pushing a slinky forward-and-back versus side-to-side.  If we're talking about those side-to-side modes, then I would expect that to polarize things passing through... as opposed to just redshifting them and blueshifting them back.

For more on that:


  Gravitational lensing of the CMBΓÇÖs light by large scale structure at relatively late times produces small deflections of the primordial pattern, converting a small portion of E-mode power into B-modes.


This looks like it covers some of the more fine detail, but also explains why this work is set apart from previous experiments that are said to have results regarding both the E-mode and B-modes.  The polarization effect, as long as it's sufficiently ancient, would seem to necessarily have come from quantum gravity effects.



I have 3 even more detailed points that I have found from various writeups of this event.  These relate to what was measured, what makes BICEP2 different from other experiments, and why the experiment is so important.  These specific details are:


The pattern sought was a 45$^{\circ}$ polarization relative to the temperature (?) gradient
Setting a lower bound on the value of r was the novel contribution of BICEP2
A relationship called the Lyth bound calculates the time/energy of inflation from this r value


The first bullet comes from a youtube video by minute physics.  They state that the density, motion, and temperature of matter at the genesis of the CMB impacts its polarization.  Making no reference to gravity waves, we expect polarization at 0$^{\circ}$ and 90$^{\circ}$ relative to the temperature gradient (note: there is some confusion in the video whether this is density of temperature graident, they say one thing, but write another).  They go on to say that the BICEP2 result is that about 15% of the polarization comes from the 45$^{\circ}$ "jiggles", which are tale-tale signs of gravity waves.  It's much more difficult to explain why this should be true.

Next bullet - let's clarify why this matters when the same thing has been measured by previous experiments.  Other experiments have estimated the r value, but those estimates are inherently clustered around r=0.  This still constrains the value, but it is not effective to determine if it is non-zero, which has value for theorists.  Without a doubt, this is related to my first bullet - that the critical measurement involves measuring polarization angle relative to the density gradient of a scalar field.



Third bullet, something called the Lyth bound/relationship is oft-quoted in discussions of this subject.  For more reading, there is discussion on Quora.  The equation is:

$$ \Delta \phi = m_p N_e \sqrt{ \frac{ r}{8} }  $$

The variable $N_e$ has been measured by previous experiments.  It is being cited in various places, including follow-up academic articles, that the BICEP2 result narrows down the above equation to $\Delta \phi \approx 9.6 M_{pl}$.  The remaining variable is just the Planck mass.  I believe that this number is interpreted to be the energy that inflation "traversed".  In more practical terms, this gives us the energy/time at which inflation happened.  This is where people are coming from when they mention how this result allows us to look further into the past.
AnswerKenntnis:
They announced that through observation of the Cosmic Microwave Background, via the BICEP2 experiment in Antarctica, particularly the polarization on a 2-4 degree angular scale, gravitational waves from inflation during the early universe are being indirectly observed.

Link to FAQs about the release:

http://bicepkeck.org/faq.html

Link to pre-print:

http://bicepkeck.org/b2_respap_arxiv_v1.pdf

Link to press release images and captions:

http://bicepkeck.org/visuals.html

Based upon measurements of the B-mode of polarization of CMB photons, BICEP2 reports, regarding the lamba-CMB + tensor model, that the tensor to scalar ratio (r) is 

$r = 0.20_{-0.05}^{+0.07}$ at a 68% confidence level, without considering the effects of dust.

$r = 0.16_{-0.05}^{+0.06}$ is reported, when corrected for a model than considers dust.

Other results from the South Pole Telescope and Planck Satellite, base upon large scale tempeature measurements of the CMB have constained the tensor/scalar ratio to $r<0.11$ at the 95% confidence level.
AnswerKenntnis:
Dr. Matt Strassler has some great info on his site, see here:

http://profmattstrassler.com/2014/03/17/a-primer-on-todays-events/
http://profmattstrassler.com/2014/03/17/bicep2-new-evidence-of-cosmic-inflation/
http://profmattstrassler.com/2014/03/18/if-its-holds-up-what-might-bicep2s-discovery-mean/

Here's a summary of some key points in my own words (any mistakes are my own):

It's believed that in the first tiny fraction of a second after the Big Bang, the universe underwent an extremely fast expansion known as inflation.  We can see leftover radiation, not from that far back, but from 380,000 years after the Big Bang, which was the point at which the universe was finally cool enough for electrons to bind to protons.  This radiation has been studied and been found to be very uniform.  It has slight temperature variations, which have also been studied.  These observations support the idea of inflation.  The temperature variations are understood as being due to quantum fluctuations in the early universe, and these fluctuations got blown up (expanded) by inflation along with everything else.

In addition to the temperature variations, there are also small variations in the polarization of the radiation.  These come in two types, E-mode and B-mode.  E-mode polarization has a similar cause to the temperature variations.  (Prof. Strassler has a nice picture illustrating the difference.)  The E-mode polarization was studied a while ago, and nothing surprising was found there.  The B-mode polarization on small scales (that is, observed across small patches of sky) can arise from the E-mode polarization, or from gravitational lensing that happened as the radiation was on its way to us.  But large-scale B-mode polarization is evidence of something else: gravitational waves produced by inflation.  This is what's been claimed by the new result: large scale B-mode polarization.

Some important things to remember:

(1) While the experimenters detected B-mode polarization at the more than 5 sigma level, you can't say it's a 5 sigma detection of inflationary gravitational waves.  To quote Prof. Strassler: "For that, they need enough data to show their observed data agrees in detail with the predictions of inflation.  The 5.2 sigmas refers to the level of the detection of B-mode polarization that is not merely due to lensing."

(2) This is not a direct observation of gravitational waves.  Rather, it's believed that they're observing how gravitational waves produced in the first fraction-of-a-second of the universe affected radiation produced 380,000 years later.

(3) This is not the first indirect observation of gravitational waves.  That was the  HulseΓÇôTaylor binary pulsar, which resulted in a Nobel prize.  Basically two neutron stars were found to be orbiting each other and losing energy in a way which is consistent with what you'd expect if they're radiating gravitational waves.

(4) It is the first indirect observation of gravitational waves produced by inflation.

(5) It's a very important result (if correct) because it's evidence for inflation, and can help to determine which theory of inflation is correct.

(6) It's also a very important result (if correct) because it tells us the energy density during inflation.  This is suggestive of new beyond-the-Standard-Model physics at this energy scale.  And as it turns out, the energy scale, $10^{16}$ GeV, is right around where the coupling constants for the electromagnetic, weak, and strong forces unify.

(7) Like all new experimental results, it should be taken with a grain of salt until another experiment reproduces it.

Again, that's just my summary, I highly recommend reading through all the links I posted above.
AnswerKenntnis:
Astrophysicists of the BICEP2 collaboration announced the detection of inflationary gravitational waves in the B-mode power spectrum, providing strong evidence for Guth's theory of inflation and the Big Bang.
AnswerKenntnis:
We have only looked back to the CMB time thus far - i.e. about 300,000 years after BB. Now having viewed this gravitational imprint on the CMB, we have pushed back our earliest observation to about 10^-35 seconds. That's a huge leap in the scope of our comprehension (a factor of about 10^47), and it's that which is probably the most impressive aspect of this result to the layman.

Also, we now have the very first indication that gravity is quantised.

And it seems that spacetime truly can expand faster than light. Perhaps one day our engineers will be able to exploit this amazing property of our universe.
QuestionKenntnis:
Superfields and the Inconsistency of regularization by dimensional reduction
qn_description:
Question:

How can you show the inconsistency of regularization by dimensional reduction in the $\mathcal{N}=1$ superfield approach (without reducing to components)?



Background and some references:

Regularization by dimensional reduction (DRed) was introduced by Siegel in 1979 and was shortly after seen to be inconsistent Siegel in 1980.  Despite this, it is commonly used in supersymmetric calculations since it has most of the advantages of (normal) dimensional regularization (DReg) and (naively) preserves supersymmetry.

The demonstration of the inconsistency of DRed is based on the combination of 4-dimensional identities, such as the product of epsilon-tensors
$$ \varepsilon^{\mu_1\mu_2\mu_3\mu_4} \varepsilon^{\nu_1\nu_2\nu_3\nu_4}
   \propto \det\big((g^{\mu_i\nu_j})\big) 
$$
and the d-dimensional projections of 4-dimensional objects.
Details can be found in the references above and below, although the argument is especially clear in  Avdeev and Vladimirov 1983.

Various proposals have been made on how to consistently use DRed and most involve restrictions on the use of 4-dimensional identities using epsilon-tensors and $\gamma_5$ matrices. (Note that the treatment of $\gamma_5$ in DReg is also a little tricky...). This means we also have to forgo the use of Fierz identities in the gamma-matrix algebra (which is also a strictly 4-dimensional thing - or whatever integer dimension you're working in). This means we lose most of the advantages that made DRed attractive in the first place - maintaining only the fact that it's better than DReg in SUSY theories.
The latest such attempt is Stockinger 2005, but it's also worth looking at the earlier discussions of Delbourgo and Jarvis 1980, Bonneau 1980 and (especially) Avdeev and Vladimirov 1983 & Avdeev and Kamenshchik 1983. The pragmatic discussion in Jack and Jones 1997 is also worth reading - it also contains a fairly complete set of references.

Anyway, all of the "fixes" are hard to do when using superfields, since the $D$-algebra has all of the "bad" 4-dimensional algebra built in.

My question is: What is the easiest way of showing the inconsistency of DRed in the superfield approach? (I want an answer that does not rely on reducing to components!).
I'm guessing that it should somehow follow from the $D$-algebra acting on dimensionally reduced superfields.
AnswersKenntnis
QuestionKenntnis:
How Does Mass Leave the Body When you Lose Weight?
qn_description:
When your body burns calories and you lose weight, obviously mass is leaving your body.  In what form does it leave?  In other words, what is the physical process by which the body loses weight when it burns its fuel?  Someone asked this question on the whiteboard at work.  Someone said it leaves the body in the form of heat but I knew this is wrong since heat is simply the internal kinetic energy of a lump of matter and doesn't have anything do with mass.  Obviously the chemical reactions going on in the body cause it to produce heat but this alone won't reduce its mass.
AnswersKenntnis
AnswerKenntnis:
There's a lot of detail you could go into with regard to this question, as is done in the other answers and comments, but I think the answer itself is pretty simple. Imagine a surface that just barely surrounds your body, as if you shrink-wrapped a body in plastic. By the law of conservation of mass (valid in non-relativistic physics), the only way your body can lose any amount of mass is for that amount of mass to pass out through the surface. So you just have to consider what bodily functions cause that to happen. I think they've all been identified in the comments:


Excretion
Exhaling
Sweating


Actually, any dead skin cells, strands of hair, etc. that fall off you would also count, although my guess is that those represent a minor contribution.

As a bonus, the "shrink-wrap view" also makes it easy to identify the ways in which you gain mass, by looking for all processes that cause matter to be drawn in through the invisible surface:


Eating & drinking - solids and liquids through the esophagus and gastrointestinal tract
Inhaling - gas through the trachea and lungs


The thing is, when most people talk about losing weight, they're referring to a long-term average loss of mass, which means that the processes in the first list have to remove more mass over some extended period of time than the ones in the second list bring in. This clearly requires some of the preexisting mass in your body to be converted into the waste forms that you can dispose of through excretion, exhaling, and sweating. This preexisting mass generally tends to be body fat. The other answers do a pretty good job filling in the details of how the fat gets converted to waste products.
AnswerKenntnis:
Essentially, losing of weight occurs by means of burning fuels precisely like your car does when it burns petrol and emits exhaust gases.

The only difference is that for humans that fuel is to be found in the form of sugars. The fat is what you want to get ultimately rid off, of course, but sugars are more easily processed and so this is what you are removing first.

The basic aerobic cycle is the Krebs cycle.



But to reach it first, glucose needs to be broken down first to pyruvate (by an anaerobic process of glycolysis) and then to acetyl-CoA. One can gain some energy from this reduction but not much. The real energy is hiding in the actual Krebs cycle but for it one needs (besides lots of other stuff) the mentioned acetyl-CoA and oxygen (this explains why you don't get enough energy when not breathing properly) producing the carbon dioxide and some energy that is stored in the $ATP$ (adenosine-triphosphate) and transported to wherever it is needed inside the cell (you are mostly interested in muscle contractions performed by muscle cells). So, you'll burn whatever amount of sugar you have ready in the body. You'll also lose carbon (initially stored in the glucose) by exhaling in the form of $CO_2$. There is also additional hydrogen produced and carried away in the form of $NADH$ and $FADH_2$. It's hard to estimate where it will end up though, as it is (similarly to $ATP$) used all over the organism.

Now, the body is not storing sugars in the form of glucose. Instead, they are stored as a glycogen (mainly in liver and muscles) which is a polysaccharide similar to starch. This is then quickly broken down to glucose as needed. But the body can keep only a small amount of glycogen (corresponding roughly to an hour of running, depending on one's fitness).

There is another form of storage of sugar. Body can convert it to fat. This is done when there is already enough glycogen in the organism. The body fat can then be reduced to acetyl-CoA (by lipolyses and then by beta oxidation), but this requires a lot more oxygen and so is not used when glycogen is at the disposal. But with regular exercise body can be trained to also burn greater proportion of fats than glycogen (this is of course necessary for long-range runners and cyclists because there is no way they would get enough energy just from glycogen).

To get a rough idea about the amount of mass you'll burn, read the calories content of some food. Sugar has something like 4kCal for 1g and fat 9kCal for 1g. One hour of running corresponds to something like 700kCal so if you are burning 50% sugars and 50% fat you'll be 100g lighter. All of these numbers are just very rough estimates depending on what kind of exercise you do and the general state of your body.



Note that you'll also lose lots of water and minerals during the exercise. But I am not counting this to the mass balance as you need to replenish those in order to be healthy. Also, gradually some muscles will form, so this will actually add weight.
AnswerKenntnis:
When you exercise, you "burn" more glucose, the simplified reaction for which (from Wikipedia) is:

${\rm C_6H_{12}O_6 + 6~O_2 ΓåÆ 6~CO_2 + 6~H_2O}$

So when you exhale, the carbon in the carbon dioxide, and the hydrogen and the oxygen in the water vapor, came from the glucose being burned, thereby removing that mass from the body.
AnswerKenntnis:
As others have pointed out, in metabolism you breathe in oxygen and exhale carbon dioxide with the net reaction being

$$CH_2O + O_2 \to CO_2 + H_2O$$

The carbon dioxide gets exhaled, and the water is lost through some combination of other things.  Let's think about whether this is a reasonable way to explain weight loss.

A breath is maybe one liter of air.  I breathe about ten times a minute, so that's 15,000 liters of air a day.  Air is about 20% oxygen, so 3,000 liters of oxygen.  Air weighs about 1kg/m^3, so about 3kg of oxygen.  The air you exhale still has some oxygen.  I don't know how good we are at extraction, but let's say we get a third of it, so 1kg of oxygen used by our bodies each day.

Carbon is lighter than oxygen and there's only one carbon exhaled per two oxygens, so that's about 300g of carbon lost per day.  Fat is basically a long carbon chain, so that says that if I don't eat, I'll lose about 300 g of body mass a day (assuming I do drink water).

It's not too bad a rough estimate to say that you lose weight through your nose.

I remember in middle school science class we learned about an experiment in which some guy a long time ago grew an entire tree in a pot, carefully covering the pot to keep stuff from getting into the soil.  The entire pot+tree system gained roughly the weight of the tree by the end.  So basically the tree got all its mass from the air.  You are pretty much an inverse tree.
AnswerKenntnis:
When you lose weight, it is mostly fat that disappears from under your skin (or someone else's skin). How does the fat get out? Most of it doesn't directly evaporate. Instead, it is burned much like fuel in your car. 

Fat contains mainly carbon, hydrogen, and oxygen, much like typical organic compounds. When it's "ideally" burned, only water and carbon dioxide - $H_2 O$ and $CO_2$ - are left. The gas $CO_2$ gets out by breathing and farting (not sure about the relative significance) - in which you only inhale $O_2$, the oxygen - while $H_2 O$ is released from the body by urination and sweating. Well, maybe, some other material may also be added to the excrements.

Of course, this is too idealized because by burning fats, one also produces some "less clean" compounds that are added both to urine as well as excrements (you asked, I am just answering). Farts also contain other gases that carry a certain small fraction of the mass of the burned fat - including methane. Biology is complicated but the difference between animals and cars is not so dramatic.

Gaining weight is the nearly reverse process except that the carbon is not taken from the air or $CO_2$ - we would have to be plants able to do photosynthesis do achieve this goal. Instead, it is taken from the food.
AnswerKenntnis:
It is like an automobile engine.  Think if you stopped putting gas in your car and just left it running.  It would use up the existing fuel and the output would be:

1) Energy to propel the car released from breaking the bonds in the gas 

2) Heat as a by product of the combustion and friction on the internal systems 

3) Leftovers from combustion that aren't used by the car (H2O, CO, CO2, etc)

Same principles at play in your body just a bit more biologically oriented.
AnswerKenntnis:
Additionally, some fats (cholesterols, etc) are converted to bile and excreted by the liver into the digestive track.  On top of this many of the byproducts of cell death are excreted in this fashion.  That brown color to your poop is the product of red blood cells being broken down, and expelled by the liver.

Some minerals and other substances that are water soluble may be excreted by the kidneys.
AnswerKenntnis:
Here is a credible article that explains how MUCH of weight loss occurs through exhaling CO2
http://en.wikipedia.org/wiki/User:Znode/Misc/The_Physics_of_Losing_Weight

Plus, we loose mass through our intestines as the liver filters the blood in the form of bile.
http://www.liverdoctor.com/liver-problems/weight-loss/

And we obviously lose water through kidney function.
AnswerKenntnis:
Here's a good way to think of it:

Consider your body like a leather wine bag... with a hole in the bottom.


If you add less that what is leaking out, the bag will get smaller.
If you add more than is leaking out, the bag will get larger.


As your body extracts the nutrients it requires out of stored cells (some carried in the blood stream and some stored as fat), those cells are disposed of (think poop).

With exercise, your body requires more fuel so it extracts nutrients out of more cells. If you are not putting enough in to replenish the supply, your body weight goes down.
AnswerKenntnis:
When you burn up a log in a campfire, it weighs less than when you started, right?

Where did the mass go?

CO, CO2, H2O, and other combustion products.

It went up in smoke.

A candle? Same thing.

When you oxidize fat, sugar, whatever, what do you get? Same idea.
AnswerKenntnis:
It is carbon oxidation as mentioned before. When you breathe in $O_2$ and exhale $CO_2$ carbon is leaving your body and that is how you are losing weight.

What happens when you work out? You breathe more so more carbon mass can leave your body. Don't try to cheat and breathe hard while not exercizing or you will hyperventilate.

By the way, carbon oxidation is what also powers cars and campfires. Photosynthesis (using the sun to reduce $CO_2$ to $C$ and $O_2$) is the reverse of oxidation. Most organisms on earth are part of this sun cycle (you can trace all the food you and your car use back to plant), exceptions are some bacteria that can get energy from Lava and other means.
AnswerKenntnis:
I recently lost a good deal of weight and would say it seems the water mostly leaves through urination, not exhalation.  Basically, I could tell when my weight was going to be decreasing because I'd have a lot of pee in the middle of the night, without having much to drink before bed.

As the other answers say respiration byproducts are co2 Whig is exhaled and water, which, I think, makes it's way to the bladder at some point.
AnswerKenntnis:
Weight loss occurs when  a large amount of calories or fat is turned into energy through excercise or activity.  That waste is then released from the body in one's urine, stool, & sweat.  When your body converts fat into accessible energy, the process generates heat that is used to regulate body temperature, according to MayoClinic.com. Once stored fat is converted to energy, your body uses it to fuel activity and metabolic functions in much the same way it uses immediate energy from food. MayoClinic.com notes that waste material produced during the conversion of body fat into energy, specifically water and carbon dioxide, leaves your body through urine, sweat and exhaling.
AnswerKenntnis:
Years ago I worked for a weight loss supplement company. We had to reach out to the customers to help them along. Many of the men said while on the program it hurt to urinate. I was told that it is why it was so important to drink water In abundance esp during weight loss... BECAUSE when the fat breaks down it comes out through your urine. In a mans urethra, the opening at the tip of the penis is so small that if the fat loss is not diluted enough in the body it is too heavy to pass through that pin hole size opening in a man and becomes backed up and painful. When I lose weight I pee buckets very easily. And the more I drink water the more I lose the pounds on the scale even beyond the initial  5 lbs of water weight . But I've also been told that during rapid weight loss, your fecal excretion should appear greater than the volume of the food you've eaten. Particularly on the hCG diet I found this to be mostly true since the food volume is tiny compared to the " waste" going out. But I've also noticed that my mouth takes on a funky taste too, so maybe some of these posts have a point with the waste coming out in exhaling. Lastly , when losing weight there are times I feel so energetic that I can't even sleep earlier than 1:00 am. I just don't feel tired . But that could also be contributed to feeling lighter with less burden from the body weight... In summary - I think all explanations can be true based on my own observations during weight loss.
AnswerKenntnis:
Scientifically, the answer can be expressed in a very deep level of detail.  But ultimately, the answer is simple: Heat.
AnswerKenntnis:
This is simple.  The answer is it leaves your body in the form of energy.

Your body burns sugars while producing things like heat (you get hot when exercising) and motion (you exercise).  So, we physically convert mass into energy which gets transfered mechanically into whatever we are doing.  That could be a distance traveled (walking/biking etc) or an increase in another objects potential energy (lifting weights etc)
QuestionKenntnis:
Why is a laserpointer able to "erase" a glow-in-the-dark sticker?
qn_description:
a while ago I tried to charge a glow-in-the-dark sticker using a simple red laser pointer. It was a large sticker, of the type used to mark emergency exits and fire extinguishers here in Germany. I thought I could draw a picture this way, with glowing strokes.

To my surprise, the exact opposite happened - the sticker was already glowing a little bit and the areas I illuminated with my laser turned dark instantly, as if the charge of the sticker was removed by the laserlight.

The effect was reversible, the darkened areas would charge and glow as usual if exposed to normal light.

So I was able to draw a picture after all - however, it was a negative.

Can anyone explain why the sticker was discharged instead of charged by the laserlight?
AnswersKenntnis
AnswerKenntnis:
How interesting. 

Presumably the "glow-in-the-dark" effect comes from the decay of a meta-stable excited state. It gets charged by sufficiently energetic photons, and decays slowly because some selection rule prevents a direct transition without an external influence.

If this is the case, we can guess that the laser is exciting the meta-stable state to a still higher energy state which is not subject to the selection rule so that it promptly falls to the ground state1, from which the low energy photons in the laser don't have the energy to push back into the meta-stable state. 

I think I'll try to make a demo out of that. Thanks.



1 Presumably you could see the light of this decay if you filtered out the intense red from the laser. Try a green or blue filter. Or a spectrograph.
AnswerKenntnis:
"dmckee" is probably right about the mechanism. In our lab, we often work with high-power 1064 nm NIR laser beams, and an obvious issue when dealing with them is that the light is invisible to the eye. For visualization purposes, many companies like ThorLabs and Newlight Photonics sell IR visualization cards that consist of a pink material bonded to a laminated card. The cards need to be "charged" by holding them to a light source, but after charging they do not emit any light. However, when they are struck by NIR light, they glow red brightly, and the spot that has been struck by NIR light will slowly be "depleted" until it no longer responds to NIR exposure. I strongly suspect that the material is excited to a metastable triplet state by visible light, and that NIR light promotes it to a state which has large decay channels.

In any case, if your glow-in-the-dark sticker is one of the usual glowing green types, you probably already know that they can't be excited using light which is of a longer wavelength than they emit. That's why red light will not charge glow stickers, but blue and UV will.
QuestionKenntnis:
Why is there a scarcity of lithium?
qn_description:
One of the major impediments to the widespread adoption of electric cars is a shortage of lithium for the batteries.  I read an article a while back that says that there is simply not enough lithium available on the entire planet to make enough batteries to replace every gasoline-powered car with one electric car.  And that confuses the heck out of me.

The Big Bang theory says that in the beginning, there was a whole bunch of hydrogen, and then lots of hydrogen started to clump together and form stars, and those stars produced lots of helium through fusion, and then after helium, all the rest of the elements.  That's why hydrogen is the most common element in the universe by far, and helium is the second most common.

Well, lithium is #3 on the periodic table.  By extrapolation, there ought to be several times more lithium around than, say, iron or aluminum, which there is definitely enough of for us to build plenty of cars with.  So why do we have a scarcity of lithium?
AnswersKenntnis
AnswerKenntnis:
Actually, what you've read about the production of nuclei is not quite correct. There are several different processes by which atomic nuclei are produced: 


Big Bang nucleosynthesis is the fusion of hydrogen nuclei to form heavier elements in the early stages of the universe, as it cooled from the big bang. There are rather specific thermal requirements for this process to occur, so there was only a short time window in which heavier elements could form, meaning that the only fusion to actually happen in significant amounts was the conversion of hydrogen (and deuterium) to helium, and an extremely tiny amount of lithium.
Stellar nucleosynthesis is the fusion of hydrogen and other nuclei in the cores of stars. This is something separate from big bang cosmology, since stars didn't form until millions of years into the universe's lifetime.

Now, contrary to what you might have read, not all elements are formed in stellar nucleosynthesis. There are specific "chains" of nuclear reactions that occur, and only the elements that are produced by those reactions will exist in a star in appreciable quantities. Most stars produce their energy using either the proton-proton chain (in lighter stars) or the CNO cycle (in heavier stars), both of which consume hydrogen and form helium. Once most of the hydrogen has been consumed, the star's temperature will increase and it will start to fuse helium into carbon. When the helium runs out, it will fuse carbon into oxygen, then oxygen into silicon, then silicon into iron. (Of course the actual process is more complicated - see the Wikipedia articles for details.) Several other elements are produced or involved along the way, including neon, magnesium, phosphorous, and others, but lithium is not among them. In fact, stars have a tendency to consume lithium, rather than producing it, so stars actually tend to have only small amounts of lithium.
Supernova nucleosynthesis is the fusion of atomic nuclei due to the high-pressure, high-energy conditions that arise when a large star explodes in a type II supernova. There are certain similarities between this and big bang nucleosynthesis, namely the high temperatures and pressures, but the main difference is that an exploding star will have "reserves" of heavy elements built up from a lifetime of nuclear fusion. So instead of just forming a lot of helium as occurred just after the big bang, a supernova will form a whole spectrum of heavy elements. In fact supernovae are the only natural source of elements heavier than iron, since it actually requires an input of energy to produce those elements as fusion products. I believe some amount of lithium would be formed in a supernova along with all the other elements, but since a large star would have used up its hydrogen and helium in the central region where most of the action takes place, lithium is probably not a particularly common reaction product.
AnswerKenntnis:
The key word in what you've heard is "available" because there is quite a lot of lithium in the earth that is not so easy to obtain.  The notion of "available Lithium" probably means known land reserves, which according to this page amount to 14 million tons.

The amount dissolved in seawater is estimated at 230 billion tons (which is enough for lots of batteries).  Seawater extraction does not seem to be economically viable yet, but people are studying it.

The estimated concentration of lithium in the Earth's crust ranges from 1 to 31 ppm, so if we excavate the whole crust, we'll get between 20 and 600 trillion tons.  In other words, if our civilization ever came to a point where we really needed lots of lithium, we wouldn't have to go too far to find it.
AnswerKenntnis:
This is a small complement to David's and Scott's answers

As usual Wikipedia's page on Lithium contains useful information :


  Both natural isotopes have anomalously low nuclear binding energy per nucleon compared to the next lighter and heavier elements, helium and beryllium, which means that alone among stable light elements, lithium can produce net energy through nuclear fission. The two lithium nuclei have lower binding energies per nucleon than any other stable compound nuclides other than deuterium, and helium-3. As a result of this, though very light in atomic weight, lithium is less common in the solar system than 25 of the first 32 chemical elements. 
  
  [...]
  
  Γü╖Li is one of the primordial elements (or, more properly, primordial nuclides) produced in Big Bang nucleosynthesis. A small amount of both Γü╢Li and Γü╖Li are produced in stars, but are thought to be burned as fast as produced. Additional small amounts of lithium of both Γü╢Li and Γü╖Li may be generated from solar wind, cosmic rays hitting heavier atoms, and from early solar system Γü╖Be and ┬╣Γü░Be radioactive decay. 


So, basically, Lithium is (barely) produced as David Zaslavsky told you in his answer, and the reason the production is low because Lithium is barely stable.

But as @Scott Carnahan tells in his answer, the notion of lithium scarcity is linked with its repartition on earth. And the reason it is difficult to obtain is ultimately its  high chemical reactivity, which means that it is basically diluted everywhere, and is rarely concentrated in easy to mine deposits.  On the same wikipedia page as above, they say :


  Although lithium is widely distributed on Earth, it does not naturally occur in elemental form due to its high reactivity.
  
  [...] 
  
  According to the Handbook of Lithium and Natural Calcium, "Lithium is a comparatively rare element, although it is found in many rocks and some brines, but always in very low concentrations. There are a fairly large number of both lithium mineral and brine deposits but only comparatively a few of them are of actual or potential commercial value. Many are very small, others are too low in grade."
AnswerKenntnis:
The common mistake behind this statement is that lithium is seen as a fuel that's consumed and discarded. After all, that's how oil works. There's not enough cheap lithium for that. But like steel, lithium will be recycled..
AnswerKenntnis:
According to this NPR story, there is no shortage of lithium for batteries:


  "I don't know of any serious person in
  the automotive industry or in the
  lithium industry who believed that
  there is a serious, long-term supply
  problem," he says. "In fact, for the
  next 10 years there will probably be
  an oversupply of lithium because so
  many companies have now moved into the
  market."
  
  And unlike the impact of mining other
  natural resources, concentrating
  lithium is an "environmentally benign"
  process, Fletcher says. "It's about as
  low-impact as mining can get. They're
  really just pumping water up ... and
  there are really no toxic chemicals in
  a lithium-ion battery."
AnswerKenntnis:
Lithium is more abundant in the Earth's crust than lead. However, it is more reactive than such metals and less abundant than other reactive metals such as sodium. Because of this it does not tend to accumulate in rich geological deposits in a form that makes it easy to extract. Its lightness may be another factor.

Reactive metals such as lithium can form salts which dissolve in water. These are then left in deposits when enclosed areas of water dry up. Lithium is 1000 times less abundant in the Earth's crust than other reactive metals such as Sodium, Calcium and Potassium so it is still only found in relatively small quantities in such deposits.

However, some compounds of lithium are sufficiently soluble that it is present in some dried up sea deposits. About half the accessible Lithium on Earth is said to be beneath the Bolivian dessert and if extracting it in the future becomes as important as extracting oil is now, then there is not likely to be such a big shortage.
QuestionKenntnis:
Why are radiators always placed under windows?
qn_description:
I don't know if anyone else has noticed this but in most buildings and most rooms, radiators are predominantly placed under a window. 

Now, in my eyes, that is the worst place to put them; hot air rises, reaches the window (which no matter how well insulated it's still letting out heat, in loose terms) and the thermal energy of the air disperses around the window area, thus not doing much to warm up the room. 

Am I wrong to think this? I mean, I can hold my hand close to my window and feel that it is colder there than at the other end of my room, but then again, my room does warm up when the radiator is on.
AnswersKenntnis
AnswerKenntnis:
The reason is because the heat loss occurs mostly in the windows and the fenestration. The idea is that you would like the incoming air to be heated up. Also, it creates an air curtain that prevents more heat from being lost through this exposed areas. The final reason is to make the temperature of the room more or less uniform. If the heaters were placed at the center of the room, you would create a large temperature gradient, resulting in drafts and discomfort for the occupant.
AnswerKenntnis:
Partly practical, the wall under the windows isn't useful for anything else.
We had a house where the heaters were placed in the middle of the only empty walls, so nowhere you could put furniture, bookcases, etc.

Before double glazing there would be a draft from the windows so the idea was to heat this incoming air by having a radiator immediately below the window
AnswerKenntnis:
Since this is a physics forum I assume the OP is interested in a quantitative answer in terms of the efficiency of the system and how it differs based on the relative positioning of heat sources and heat sinks.  The math required to analyzed such a system is too much for me to manage right now, but I believe the following principles apply and are objectively correct:


The dissipation of heat through the glass will increase in proportion to the difference of the indoor and outdoor temperatures; the larger the gap, the faster the loss of energy to the room.
The dissipation of heat within the room follows the inverse square law (subject to perturbations such as drafts, etc.) in proportion to distance from the heat source.


To optimize the room for minimum heat loss, move the heat sources away from the windows.

To optimize the room for maximum heat uniformity, move the heat sources near the windows.
AnswerKenntnis:
As Programmer mentions, by putting the radiator in front of the area most prone to heat loss and ingress of cold air, you are effectively screening off the room from cold air.  However, there is also the fact that radiators are often quite a bit hotter than other heat sources such as forced air.  Therefore it makes sense to put it in the coldest part of the room, not only due to efficiency reasons, but due to the fact that if it gets too hot you can easily open the window and let out some of the excess heat.
AnswerKenntnis:
As described in the other answers, putting the radiators (or hot air vents in a forced-air system) under the windows offsets the greater heat loss of the windows, but there is another reason. As room air flows over the surface of a window, it will lose heat to the window (and the outside). This can cause moisture in the air to condense out onto the window. In cold enough conditions, the window will accumulate frost or even layers of ice on the inside surface. The hot air rising off a radiator will have less tendency to deposit moisture onto the window (the glass surface will be somewhat warmer).
AnswerKenntnis:
As the hot air goes up and the cold air goes down, the radiator is located where there is a better circulation, i.e. even though the window is double glazed, there will always be cold air entering the division by the material itself. So the cold will push the hot air inside the room.

Another explanation can be the fact that external walls can have thermal bridges and cold air can enter the room and cause heat loss. By placing the radiator on the exterior wall, the thermal bridge will still exist, but the effect of cold air entering the room, will be compensated by the heater (in environmental terms, it isn't good at all to have thermal bridges in buildings, but it is cheaper to build). Today, we can see radiators and heaters either underneath the windows or close to the maximum number of doors, as the temperature in the rooms vary and helps air circulation.
AnswerKenntnis:
My guess would be it's there to prevent the cooling air from accumulating below the window and flowing into the rest of the room.

Anything goes to avoid cold feet ;)
AnswerKenntnis:
Reason is quite simple. If you have a window without radiator and outside is really cold, the window glass would be cold as well. This will lower temperature of the air around the window and this air will flow immediately down (physics).

So if someone want well heated room with very cold floor .. do it, put radiator to opposite side. 

That's why it is wise to put radiator under the window. ... to heat up the cold air, which is flowing from the window area.
AnswerKenntnis:
Because under window is the place where delta-T (the Change in
Temperature) is largest in the whole room.
The larger is delta-T, more efficient the system is.
Window even when closed is still a coldest place, because it is a
thinnest wall.
Cold air flows downwards, thus under window.


What is an efficiency of a radiator depends on whether your goal is to heat the room or cool some device. It is another question anyway.
AnswerKenntnis:
A number of reason:
 - the wall under the window is basically useless
 - placing the heater under the window allows you have a more even temperature throughout the room: the window is the coldest place in the room, if you were to place a heater on the other side of the room and wanted to reach a certain temperature by the window, the other part of the room would have to be much hotter
AnswerKenntnis:
As the hot air goes up and the cold air goes down, the radiator is located where there is a better circulation, ie, even though the window is double glazed, there will always be cold air entering the division by the material itself. So the cold will push the hot air inside the room. Another explanation can be the fact that external walls can have thermal bridges and cold air can enter the room and cause heat loss. By placing the radiator on the exterior wall, the thermal bridge will still exist, but the effect of cold air entering the room, will be compensated by the heater (in environmental terms, it isn't good at all to have thermal bridges in buildings, but it is cheaper to build). Today, we can see radiators and heaters either underneath the windows or close to the maximum number of doors, as the temperature in the rooms vary and helps air circulation.
AnswerKenntnis:
Its because the hot air that is created by the radiator heats the colder window making the glass warmer... therefore the glass is allot less likely to condensate and at the same time keeps the room allot warmer because the heat will radiate out wards away from the window along with any drafts or such ... also if the radiator is placed on a wall without a window the room gets allot hotter allot more quickly than liked... and a sudden rise in temperature unsettles the human body.
QuestionKenntnis:
Gauge symmetry is not a symmetry?
qn_description:
I have read before in one of Seiberg's articles something like, that gauge symmetry is not a symmetry but a redundancy in our description, by introducing fake degrees of freedom to facilitate calculations.

Regarding this I have a few questions:


Why is it called a symmetry if it is not a symmetry? what about Noether theorem in this case? and the gauge groups U(1)...etc?
Does that mean, in principle, that one can gauge any theory (just by introducing the proper fake degrees of freedom)?
Are there analogs or other examples to this idea, of introducing fake degrees of freedom to facilitate the calculations or to build interactions, in classical physics? Is it like introducing the fictitious force if one insists on using Newton's 2nd law in a noninertial frame of reference?
AnswersKenntnis
AnswerKenntnis:
In order:


Because the term "gauge symmetry" pre-dates QFT. It was coined by Weyl, in an attempt to extend general relativity. In setting up GR, one could start with the idea that one cannot compare tangent vectors at different spacetime points without specifying a parallel transport/connection; Weyl tried to extend this to include size, thus the name "gauge". In modern parlance, he created a classical field theory of a $\mathbb{R}$-gauge theory. Because $\mathbb{R}$ is locally the same as $U(1)$ this gave the correct classical equations of motion for electrodynamics (i.e. Maxwell's equations). As we will go into below, at the classical level, there is no difference between gauge symmetry and "real" symmetries.
Yes. In fact, a frequently used trick is to introduce such a symmetry to deal with constraints. Especially in subjects like condensed matter theory, where nothing is so special as to be believed to be fundamental, one often introduces more degrees of freedom and then "glue" them together with gauge fields. In particular, in the strong-coupling/Hubbard model theory of high-$T_c$ superconductors, one way to deal with the constraint that there be no more than one electron per site (no matter the spin) is to introduce spinons (fermions) and holons (bosons) and a non-Abelian gauge field, such that really the low energy dynamics is confined --- thus reproducing the physical electron; but one can then go and look for deconfined phases and ask whether those are helpful. This is a whole other review paper in and of itself. (Google terms: "patrick lee gauge theory high tc".)
You need to distinguish between forces and fields/degrees of freedom. Forces are, at best, an illusion anyway. Degrees of freedom really matter however. In quantum mechanics, one can be very precise about the difference. Two states $\left|a\right\rangle$ and $\left|b\right\rangle$ are "symmetric" if there is a unitary operator $U$ s.t. $$U\left|a\right\rangle = \left|b\right\rangle$$ and $$\left\langle a|A|a\right\rangle =\left\langle b|A|b\right\rangle $$ where $A$ is any physical observable. "Gauge" symmetries are those where we decide to label the same state $\left|\psi\right\rangle$ as both $a$ and $b$. In classical mechanics, both are represented the same way as symmetries (discrete or otherwise) of a symplectic manifold. Thus in classical mechanics these are not separate, because both real and gauge symmetries lead to the same equations of motion; put another way, in a path-integral formalism you only notice the difference with "large" transformations, and locally the action is the same. A good example of this is the Gibbs paradox of working out the entropy of mixing identical particles -- one has to introduce by hand a factor of $N!$ to avoid overcounting --- this is because at the quantum level, swapping two particles is a gauge symmetry. This symmetry makes no difference to the local structure (in differential geometry speak) so one cannot observe it classically.


A general thing -- when people say "gauge theory" they often mean a much more restricted version of what this whole discussion has been about. For the most part, they mean a theory where the configuration variable includes a connection on some manifold. These are a vastly restricted version, but covers the kind that people tend to work with, and that's where terms like "local symmetry" tend to come from. Speaking as a condensed matter physicist, I tend to think of those as theories of closed loops (because the holonomy around a loop is "gauge invariant") or if fermions are involved, open loops. Various phases are then condensations of these loops, etc. (For references, look at "string-net condensation" on Google.)

Finally, the discussion would be amiss without some words about "breaking" gauge symmetry. As with real symmetry breaking, this is a polite but useful fiction, and really refers to the fact that the ground state is not the naive vacuum. The key is commuting of limits --- if (correctly) takes the large system limit last (both IR and UV) then no breaking of any symmetry can occur. However, it is useful to put in by hand the fact that different real symmetric ground states are separately into different superselection sectors and so work with a reduced Hilbert space of only one of them; for gauge symmetries one can again do the same (carefully) commuting superselection with gauge fixing.
AnswerKenntnis:
The (big) difference between a gauge theory and a theory with only rigid symmetry is precisely expressed by the Noether first and second theorems:

While in the case of a rigid symmetry, the currents corresponding to the group generators are conserved only as a consequence of the equations of motion. This is called that they are conserved "on-shell", in the case of a continuous gauge symmetry, the conservation laws 
become valid "off-shell", that is independently of the equations of motion. 
This implies for example the conservation of the electric charge irrespective of the equation of motion.

Now, the conservation law equations can be used in principle to reduce the number of fields.

The procedure is as follows: 


Work on the subspace of the field configurations satisfying the conservation laws. However, there will still be residual gauge symmetries on this subspace. In order to get rid of those:
Select a gauge fixing condition for each conservation law.


This will reduce the "number of field components" by two for every gauge symmetry.
The implementation of this procedure however is very difficult, because it actually requires to solve the conservation laws, and moreover, the reduced space of field configurations is very complicated.
This is the reason why this procedure is rarely implemented and other techniques like BRST are used.
AnswerKenntnis:
1) Why is it called a symmetry if it is not a symmetry? what about Noether theorem in this case? and the gauge groups U(1)...etc?

Gauge symmetry is a local symmetry in CLASSICAL field theory. This may be why
people call gauge symmetry a local symmetry. But we know that our world is quantum.
In quantum systems, gauge symmetry is not a symmetry, in the sense that the gauge transformation does not change any quantum state and is a do-nothing transformation.
 Noether's theorem is a notion of classical theory. Quantum gauge theory (when described by the physical Hilbert space and Hamiltonian) has no 
Noether's theorem. Since the gauge symmetry is not a symmetry, the gauge group
does not mean too much, in the sense that two different gauge groups can sometimes
describe the same gauge theory. For example, the $Z_2$ gauge theory
is equivalent to the following $U(1)\times U(1)$ Chern-Simons gauge theory:

$$\frac{K_{IJ}}{4\pi}a_{I,\mu} \partial_\nu a_{J,\lambda} \epsilon^{\mu\nu\lambda}$$ with $$K= \left(\begin{array}[cc]\\ 0& 2\\ 2& 0\\ \end{array}\right)$$ in (2+1)D. 

Since the gauge transformation is a do-nothing transformation and the gauge group is unphysical, it is better to describe gauge theory without using
gauge group and the related gauge transformation. This has been achieved by string-net theory. Although the string-net theory is developed to describe topological order, it can also be viewed as a description of gauge theory without using gauge group.

The study of topological order (or long-range entanglements) shows that
if a bosonic model has a long-range entangled ground state, then the low energy effective theory must be some kind of gauge theory. So the low energy effective
gauge theory is actually a reflection of the long-range entanglements in the ground state. 

So in condensed matter physics, gauge theory is not related to geometry or curvature. The gauge theory is directly related to and is a consequence of the long-range entanglements in the ground state. So maybe the gauge theory in our vacuum is also a direct reflection of the  long-range entanglements in the vacuum.

2) Does that mean, in principle, that one can gauge any theory (just by introducing the proper fake degrees of freedom)? 

Yes, one can rewrite any theory as a gauge theory of any gauge group.
However, such a gauge theory is usually in the confined phase and the effective theory at low energy is not a gauge theory.

Also see a related discussion:
Impossibility of breaking gauge-symmetry in lattice gauge theories
AnswerKenntnis:
When talking about symmetry, one should always indicate: symmetry of what?

If I measure the length of a stick in inches and then in centimeters, i.e. in different gauges, then I get two different answers, although the stick is the same in both cases. Similarly, when I measure the phase of a sine wave with two clocks that have different phases, then I get two different phases, and phase shifts form the group U(1). In the first example the stick is invariant under the change of gauge from centimeters to inches, but this has nothing to do with any physical symmetry of the stick. Noether's theorem has to do with symmetries of the Lagrangian. E.g. if the Lagrangian has spherical symmetry, then total angular momentum is conserved. The Noether theorem obviously also applies to quantum systems. A change of gauge is not a physical transformation, that is all. In quantum field theory one starts with a simple Lagrangian (e.g. Dirac Lagrangian), and then changes it so that it becomes invariant under local gauge changes, i.e. one then changes the derivative in the Dirac equation into a D which has a "gauge field" in it: to make this sound cryptic, one then says that "local gauge invariance has generated a gauge field", although this is not true. Imposing local gauge invariance simply puts a constraint on what sort of Lagrangians can be written. It is similar to demanding that a function F(z) be analytic in the complex plane, this also has serious consequences.
QuestionKenntnis:
Why does kinetic energy increase quadratically, not linearly, with speed?
qn_description:
As Wikipedia says:


  [...] the kinetic energy of a non-rotating object of mass $m$ traveling at a speed $v$ is $mv^2/2$.


Why does this not increase linearly with speed? Why does it take so much more energy to go from 1 m/s to 2 m/s than it does to go from 0 m/s to 1 m/s?

My intuition is wrong here, please help it out!
AnswersKenntnis
AnswerKenntnis:
The previous answers all restate the problem as "Work is force dot/times distance". But this is not really satisfying, because you could then ask "Why is work force dot distance?" and the mystery is the same.

The only way to answer questions like this is to rely on symmetry principles, since these are more fundamental than the laws of motion. Using Galilean invariance, the symmetry that says that the laws of physics look the same to you on a moving train, you can explain why energy must be proportional to the mass times the velocity squared.

First, you need to define kinetic energy. I will define it as follows: the kinetic energy E(m,v) of a ball of clay of mass m moving with velocity v is the amount of calories of heat that it makes when it smacks into a wall. This definition does not make reference to any mechanical quantity, and it can be determined using thermometers. I will show that, assuming Galilean invariance, E(v) must be the square of the velocity.

E(m,v), if it is invariant, must be proportional to the mass, because you can smack two clay balls side by side and get twice the heating, so

$$ E(m,v) = m E(v)$$

Further, if you smack two identical clay balls of mass m moving with velocity v head-on into each other, both balls stop, by symmetry. The result is that each acts as a wall for the other, and you must get an amount of heating equal to 2m E(v).

But now look at this in a train which is moving along with one of the balls before the collision. In this frame of reference, the first ball starts out stopped, the second ball hits it at 2v, and the two-ball stuck system ends up moving with velocity v.

The kinetic energy of the first ball is mE(2v) at the start, and after the collision, you have 2mE(v) kinetic energy. But the heating is the same, so

$$ mE(2v) = 2mE(v) + 2mE(v)$$

$$ E(2v) = 4 E(v)$$

which implies that E is quadratic.

Noncircular force-times-distance

Here is the noncircular version of the force-times-distance argument that everyone seems to love so much, but is never done correctly. In order to argue that energy is quadratic in velocity, it is enough to establish two things:


Potential energy on the Earth's surface is linear in height
Objects falling on the Earth's surface have constant acceleration


The result then follows.

That the energy in a constant gravitational field is proportional to the height is established by statics. If you believe the law of the lever, an object will be in equilibrium with another object on a lever when the distances are inversely proportional to the masses (there are simple geometric demonstrations of this that require nothing more than the fact that equal mass objects balance at equal center-of-mass distances). Then if you tilt the lever a little bit, the mass-times-height gained by 1 is equal to the mass-times-height gained by the other. This allows you to lift objects and lower them with very little effort, so long as the mass-times-height added over all the objects is constant before and after.This is Archimedes' principle.

Another way of saying the same thing uses an elevator, consisting of two platforms connected by a chain through a pulley, so that when one goes up, the other goes down. You can lift an object up, if you lower an equal amount of mass down the same amount. You can lift two objects a certain distance in two steps, if you drop an object twice as far.

This establishes that for all reversible motions of the elevator, the ones that do not require you to do any work (in both the colloquial sense and the physics sense--- the two notions coincide here), the mass-times-height summed over all the objects is conserved. The "energy" can now be defined as that quantity of motion which is conserved when these objects are allowed to move with a non-infinitesimal velocity. This is Feynman's version of Archimedes.

So the mass-times-height is a measure of the effort required to lift something, and it is a conserved quantity in statics. This quantity should be conserved even if there is dynamics in intermediate stages. By this I mean that if you let two weights drop while suspended on a string, let them do an elastic collision, and catch the two objects when they stop moving again, you did no work. The objects should then go up to the same total mass-times-height.

This is the original demonstration of the laws of elastic collisions by Christian Huygens, who argued that if you drop two masses on pendulums, and let them collide, their center of mass has to go up to the same height, if you catch the balls at their maximum point. From this, Huygens generalized the law of conservation of potential energy implicit in Archimedes to derive the law of conservation of square-velocity in elastic collisions. His principle that the center of mass cannot be raised by dynamic collisions is the first statement of conservation of energy.

For completeness, the fact that an object accelerates in a constant gravitational field with uniform acceleration is a consequence of galilean invariance, and the assumption that a gravitational field is frame invariant to uniform motions up and down with a steady velocity. Once you know that motion in constant gravity is constant acceleration, you know that

$$ mv^2/2 + mgh = C $$

so that Huygens dynamical quantity which is additively conserved along with Archimedes mass times height is the velocity squared.
AnswerKenntnis:
The question is especially relevant from a didactical point of view because one has to learn to distingish between energy (work) and momentum (quantity of motion). 

The kinematic property that is proportional to $v$ is nowadays called momentum, it is the "quantity of motion" residing in a moving object, it's definition is $p:= mv$.

The change of momentum is proportional to the impulse: impulse is the product of a force $F$ and the timespan $\Delta t$ it is applied. This relation is also known as the second law of Newton: $F \Delta t = \Delta p$ or $F dt = dp$. When one substitutes $mv$ for $p$ one gets it's more common form: $F= m \frac{\Delta v}{\Delta t} = ma$.

Now for an intuitive explanation that an object with double velocity has four times as much kinetic energy.
Say A has velocity $v$ and B is an identical object with velocity $2v$.
B has a double quantity of motion (momentum) - that's were your intuition is correct!
Now we apply a constant force $F$ to slow both objects down to standstill. From  $F \Delta t = \Delta p$ it follows that the time $\Delta t$ needed for B to slow down is twice as much (we apply the same force to A and B). Therefore the braking distance of B will be a factor of 4 bigger then the braking distance of A (its starting velocity, and therefore also its mean velocity, being twice as much, and its time $\Delta t$ being twice as much, so the distance, $s = \bar{v}\Delta t$, increases 2x2=4 times).
The work $W$ needed to slow down A and B is calculated as the product of the force and the braking distance $W=Fs$, so this is also four times as much. The kinetic energy is defined as this amount of work, so there we are.
AnswerKenntnis:
Let me just throw in an intuitive explanation.
You could re-phrase your question as:


  Why does velocity only increase as the square root of kinetic energy, not linearly?


Well, drop a ball from a height of 1 meter, and it has velocity v when it hits the ground.

Now, drop it from a height of 2 meters. Will it have a velocity of 2v when it hits the ground?

No, because it travels the second meter in a lot less time (because it's already moving), so it has less time to gain speed.
AnswerKenntnis:
The only real physical reason (which is not really a fully satisfying answer) is that $E \sim v^2$ is what experiments tell us. For example, gravitational potential energy on the Earth's surface is proportional to height, and if you drop an object, you can measure that the height it falls is proportional to the square of its speed. Thus, if energy is to be conserved, the kinetic energy has to be proportional to $v^2$.

Of course, you could question why gravitational potential energy is proportional to height, and once that was resolved, question why some other kind of energy is proportional to something else, and so on. At some point it becomes a philosophical question. The bottom line is, defining kinetic energy to be proportional to the square of the speed has turned out to make a useful theory. That's why we do it.

On the other hand, you could always say that if it were linear in velocity, it would be called momentum ;-)

P.S. It may be worth mentioning that kinetic energy is not exactly proportional to $v^2$. Special relativity gives us the following formula:

$K = mc^2\left(1/\sqrt{1 - v^2/c^2} - 1\right)$

For low speeds, this is essentially equal to $mv^2/2$.
AnswerKenntnis:
As Piotr suggested, accepting the definition of work $W=\mathbf{F}\cdot d\mathbf{x}$, it follows that the kinetic energy increases quadratically. Why? Because the force and the infinitesimal interval depend linearly on the velocity. Therefore, it is natural to think that if you multiply both quantities, you need to end up with something like $K v^{2}$, where $K$ is an 'arbitrary' constant.

A much more interesting question is why the Lagrangian depends on the velocity squared. Given the homogeneity of space, it can not contain explicitly $\mathbf{r}$ and given the homogeneity of time, it can not depend on the time. Also, since space is isotropic, the Lagrangian can not contain the velocity $\mathbf{v}$. Therefore, the next simplest choice should be that the Lagrangian must contain the velocity squared. I do think that the Lagrangian is more fundamental in nature than the other quantities, however, its derivation involves the definition of work or equivalently, energy. So probably you won't buy the idea that this last explanation is the true cause of having the kinetic energy increasing quadratically, although, I think it is much more satisfactory than the first explanation.
AnswerKenntnis:
In comes down to definitions.

Momentum is defined as $p = mv$. Momentum grows linearly with velocity making momentum a quantity that is intuitive to understand (the more momentum the harder an object is to stop). Kinetic energy is a less intuitive quantity associated with an object in motion. KE is assigned such that the instantaneous change in the KE yields the momentum of that object at any given time:

$\frac{dKE}{dv} = p$

A separate question one might ask is why do we care about this quantity? The answer is that in  a system with no friction, the sum of the kinetic and potential energies of an object is conserved:

$\frac{d(KE + PE)}{dt} = 0 $
AnswerKenntnis:
One way to look at this question of yours is as follows:

$$ E(v) = \frac{m v^2}{2} \; . $$

So, if we multiply the velocity by a certain quantity, i.e., if we scale the velocity, we get the following,

$$ E(\lambda v) = \frac{m (\lambda v)^2}{2} = \lambda^2  \frac{m v^2}{2} = \lambda^2 E(v)\; . $$

That is, if you scale your velocity by a factor of $\lambda$, your Energy is scaled by a factor of $\lambda^2$ ΓÇö this should answer your question (just plug in the numbers).
AnswerKenntnis:
For every relatively equal (in percents) increase of the speed, the applied force must be present over increasingly (quadratically) long travel distance. F=m*a. At the same time force*distance=work, where work=energy.
AnswerKenntnis:
I think it follows from the first law of Thermodynamics.   It turns your definition of work into a conserved property called energy.   If you define work in the $Fdx$ style (as James Joule did) then the  quadratic expression for kinetic energy will follow with the symmetry arguments.

In his excellent answer, Ron Maimon cleverly suggests using heat to avoid a reference to work. To determine the number of calories he uses a thermometer.  A perfect thermometer will measure $\partial{E}/\partial{S}$ so when he's done defining entropy, he still needs a non-mechanical definition of work.  (In fact, I believe it is Joule's contribution to show that the calorie is a superfluous measure of energy.)  The weakness in Ron's answer is that he also needs the second law of thermodynamics to answer the question.

To see this explicitly, write the first law in terms of the Gibbs equation:
$$
dE  = TdS + vdp + Fdx
$$
This equation defines $v = \partial{E}/\partial{p}$.  For a conservative system set $dE=0$ and to follow Huygens, set $dS=0$  to get $vdp = - Fdx$ and to follow Maimon we set $dx=0$ to get $vdp = -TdS$.  These are two ways of measuring kinetic energy.

Now to integrate.  Huygens assumes $p$ is only a function of $v$.  For small changes in $v$ we make the linear approximation $p = mv$, where $m \equiv dp/dv$. Plug that in, integrate, and you get the quadratic dependence.   In fact, it's not too hard to see that if you use gravity for the force that $F = mg$ which leads to 
$$
\frac{1}{2} m v^2 + mgh = C .
$$
Raimon also has to assume the independence of $p$ on $S$.   To integrate he will have to evaluate $T$ as a function of $S$ (and possibly $p$) or use the heat capacity.

Now notice that we required the changes in  $v$ to be small.  In fact, kinetic energy is not always proportional to $v^2$.  If you go close to the speed of light the whole thing breaks down and for light itself there is no mass, but photons do have kinetic energy equal to $c p$ where $c$ is the speed of light. Therefore, it's better to think of kinetic energy  as
$$
E_{kin} = \int v dp 
$$
and just carry out the integration to find the true dependence on $v$.

So, in summary,  I suggest the "why" of the question is the same as the "why" of the first law.
AnswerKenntnis:
Imagine a cannon that fires a bed spring at 10 m/s (22 mph); the momentum of the fired bed spring carries it 25 meters. A second cannon fires the bed spring at 20 m/s (44 mph); the momentum of the spring carries it 50 meters. Doubling the speed, doubles the momentum.

In a second set of tests the first cannon fires the spring into an immovable brick wall from a distance of one meter. Upon impact the spring which has a length of 20 cm compresses to a length of 16 cm (20% compressed). When the second cannon fires the spring into the wall, traveling at twice the speed, the 24 cm spring compresses to a length of 4 cm (80% compressed); this of course requires 4 times more energy than the first collision with the wall. Double the speed, quadruple the kinetic energy!

Please correct me if I'm off base. thanks
AnswerKenntnis:
Just to post another, more mathematical, version of this that is not dependent on thermodynamics, but rather just vector calculus and Newton's laws, let's consider Newton's second law:

$$\sum {\vec F} = m{\vec a}$$

Now, apply the definition of work, $W = \int d{\vec s} \cdot{\vec F}$

We have, assuming that $s$ is the actual path traveled by the particle, and using some clever changes of variables:

$$\begin{align}
\sum W &= m\int d{\vec s(t)}\cdot {\vec a}\\
&=m\int dt\frac{d{\vec s}}{dt}\cdot {\vec a}\\
&= m\int dt \,{\vec v} \cdot {\vec a}\\
&= m\int dt\,{\vec v}\cdot \frac{d{\vec v}}{dt}\\
&= m\int {\vec v} \cdot d{\vec v}\\
&= \frac{1}{2}m\left(v_{f}^{2} - v_{i}^{2}\right)\\
&= \Delta {\rm KE}
\end{align}$$

So, we see that the definition of work is synonymous with quadratic dependence on the velocity.  Who cares?  Well, now, we fix some requirements on the force.  Namely, we assume our forces are conservative.  What does this mean?  Well, it means that our force is curl free  $\rightarrow {\vec \nabla} \times {\vec F}=0$.  This is mathematically equivalent to many things, but the most important two are that $\int d{\vec s}\cdot {\vec F}$ doesn't depend on the path you integrate over, but only the endpoints of the curve, and second, that ${\vec F} = -{\vec \nabla}\phi$ for some function $\phi(x,y,z,t)$.  Once you know this, it's relatively easy to show that $\int {\vec ds}\cdot {\vec F} = \phi_{0} - \phi_{f}$

Then, you have:

$$0 = \Delta {\rm KE} + \sum \Delta {\rm PE}_{i}$$

where the sum is over the potentials for the various forces (and I deviously substituted PE for $\phi$, since we are obviously talking about potential energy now.)  We have now proved that the total energy does not change.  Therefore, the standard definition of work gives us a conserved quantity, which we can call the energy (so long as we assume the absence of nonconservative forces, but in the presence of these, energy is not conserved, and we start having to worry about losses to heat and radiation).
AnswerKenntnis:
The general form of the kinetic energy includes higher order corrections due to relativity. The quadratic term is only a Newtonian approximation valid when velocities are low in comparison to the speed of light c.

There is another fundamental reason for which kinetic energy cannot depend linearly with the velocity. Kinetic energy is a scalar, velocity is a vector. Moreover, if the dependency was linear this would mean that the kinetic energy would vary by substituting $\mathbf{v}$ by $-\mathbf{v}$. I.e. the kinetic energy would depend of the orientation, which again makes no sense. The Newtonian quadratic dependence and the relativistic corrections $v^4$, $v^6$... satisfy both requirements: kinetic energy is a scalar and invariant to substituting $\mathbf{v}$ by $-\mathbf{v}$.
AnswerKenntnis:
Basically, momentum is related to force times time, and KE is related to force times distance.  It is all a metter of frame of reference, either time or distance.  The relationship between time and distance for a starting velocity of zero is $d = \frac{at^2}{2}= \frac{tV}{2}$.  Plug this into the the equations you get the KE$ = \frac{pV}{2} = \frac{p^2}{2m}$

Woolah - magic!
AnswerKenntnis:
I have a quantitative answer which is a thought experiment avoiding all but the simplest equations.

An object going from velocity v=0 to v=1 needs to be pushed or pulled in some way. In my explanation I will use the same method to push the object from v=0 to v=1 then from v=1 to v=2, then v=2 to v=3, etc. I will show how the energy of movement embodied in the object goes up from 0 to 1 to 4 to 9, etc.

Start with two identical balls, m1 and m2. Between the two balls is a spring, s1, which is held in compression. Assume the mass of the spring is very small. The potential energy in the spring is PE=2 and all 3 actors have velocity v=0.

A. v=0. All objects have 0 velocity so kinetic energy KE=0.

B. v=1. Release the spring and m1 shoots off to the left with velocity v=1. m2 goes in the opposite direction with v=-1. The kinetic energy of both balls is the same and is KE=1 because all of the potential energy of the spring has been transferred symmetrically to the balls.

C. v=2. Now place another identical ball, m3, just to the right of m1 and also travelling at v=1 and with a compressed spring, s2, between them. Nothing has changed about m1, it is still happily travelling at v=1. So what's the total energy of the m1, s2 and m3 system? It's 1+2+1=4 being m1's KE, s2's PE and m3's KE.

Now release the spring and m1 shoots off to the left with v=2 and m3's velocity goes from v=1 to v=0 making its KE=0. Because we've said that the mass of the spring is very small so its KE is almost zero then all of the energy which was in the system before the spring was released is now in m1. So the KE of m1 is KE=4. Phew, KE is proportional to v squared!

D. v=3. Simply repeat the process to make m1 go from v=2 to v=3 by pushing off another identical ball, m4. First, work out the total energy of the two ball and spring system before the spring is released. It's 4+2+4=10. After the spring is released m4 has v=1 which we've established is equivalent to KE=1. So m1 has the remaining energy of the system which is KE=9.

E. v=4. Repeat the process. Energy of system before the spring is released, 9+2+9=20. KE of m1 after spring is released, KE=20-4=16.

I'm not happy with assuming away the mass of the spring so a neater explanation has a spring attached to each ball and the balls interact via their springs which are in contact.
AnswerKenntnis:
Kinetic energy is defined as $\frac{1}{2}mv^2$ (in classical mechanics at least).

When the motion of an object is subjected to a physical law that is constant through time (for instance $\ddot{r}=-\frac{GM}{r^2}$ where GM is a constant), then when you integrate both sides with respect to distance and multiply by the mass $m$ of the object you get:

$$\frac{1}{2}mv_2^2 - \frac{GMm}{r_2} = \frac{1}{2}mv_1^2 - \frac{GMm}{r_1}$$

Assuming that the law is constant through time, then between the initial and final states the object's quantity $\frac{1}{2}mv^2 - \frac{GMm}{r}$ is conserved through time as well.

If instead of $-\frac{GM}{r^2}$ the physical law is some other function $f(r)$ constant through time, then the object's quantity $\frac{1}{2}mv^2 - F(r)$ where F is a primitive of f is conserved through time as well. 

That quantity is called energy. Then we give a name to the two terms: the term that depends on velocity ($\frac{1}{2}mv^2$) is referred to as kinetic energy, and the term that depends on distance ($-F(r)$) is referred to as potential energy.

It is useful to define these quantities, because if we assume that the acceleration of an object is a function of distance constant through time (as is the case with the law of gravitation, Coulomb's law, Hooke's law, ...), and if we know the value of $F(r)$ and the value of the velocity at a given distance $r_1$ (which are both derived from measurements), then we can deduce directly the velocity of the object at any other distance without having to calculate the integral of $f(r)$ every time.

Since kinetic energy is a defined quantity it is meaningless to ask why it increases quadratically with velocity, it does because it is defined that way. The above argument gives a reason as to why it is defined that way.


  Why does it take so much more energy to go from 1 m/s to 2 m/s than it
  does to go from 0 m/s to 1 m/s?


It isn't harder to accelerate something from 1 m/s to 2 m/s than from 0 m/s to 1 m/s, at a constant acceleration it takes the same time, however it takes 3 times more distance (so it takes 4 times more distance to accelerate from 0 m/s to 2 m/s than from 0 m/s to 1 m/s).

Let's say you accelerate your object at some constant rate so that it takes a time $\tau$ to go from 0 m/s to 1 m/s. Then it will take the same time $\tau$ to go from 1 m/s to 2 m/s. 

Its velocity as a function of time will be $v(t) = \frac{1}{\tau}t$. In particular, $v(\tau) = 1$ and $v(2\tau) = 2$. Its distance traveled as a function of time will be $d(t) = \frac{1}{2\tau}t^2$

It takes a distance $d(\tau) = \frac{\tau}{2}$ to accelerate it from 0 m/s to 1 m/s, while it takes a distance $d(2\tau) = 2\tau$ to accelerate it from 0 m/s to 2 m/s.

As you can see, $d(2\tau) = 4d(\tau)$. At no point do you need to invoke kinetic energy to explain this observation, it takes 4 times more distance because the object is moving faster between $\tau$ and $2\tau$ than between $0$ and $\tau$. Similarly, at a constant deceleration rate it takes 4 times more distance to brake to a stop at velocity $2v$ than at velocity $v$, not because kinetic energy makes it somehow harder to brake when we are going faster, but simply because it takes twice longer to brake (the time to go from $2v$ to $v$ is the same as the time to go from $v$ to $0$), and because we are moving faster than $v$ (hence covering more distance) during half the braking time.
AnswerKenntnis:
Your intuition is NOT wrong and I will show you why.  Say you have a machine that can accelerate a ball to 5 meters per second consistently.  Put that machine on a car that is traveling at 5 meters per second and have it accelerate a ball.  How fast is that ball traveling?  The answer is 10 meters per second (5+5=10) as any physicist will tell you.  If the machine is not moving, it changes the ball's velocity from rest to 5 m/s.  When it is moving, it changes the ball's velocity from 5 to 10 m/s.  The machine does not magically know that it is moving and so it always uses the same amount of energy, say electrical, whenever it accelerates the ball.  If the kinetic energy formula were actually valid, that machine would not be able to cause the ball to accelerate to 10 m/s in this situation. 

Those that tell you differently are only parroting the usual physics lessons. They can "prove" the kinetic energy formula is valid but you have to assume that work (the product of force and distance) is a proven fact; it is not.  They might show an experiment that equates potential energy with kinetic that seems valid; this also depends on the assumption that work is a valid scientific concept.  The example I gave in the first paragraph can be "explained" away by a clever physicist, maybe, but it is a direct test proving your intution is correct.
QuestionKenntnis:
Why do we not have spin greater than 2?
qn_description:
It is commonly asserted that no consistent, interacting quantum field theory can be constructed with fields that have spin greater than 2 (possibly with some allusion to renormalization). I've also seen (see Bailin and Love, Supersymmetry) that we cannot have helicity greater than 1, absenting gravity. I am yet to see an explanation as to why this is the case; so can anyone help?
Thanks.
AnswersKenntnis
AnswerKenntnis:
Higher spin particles have to be coupled to conserved currents, and there are no conserved currents of high spin in quantum field theories. The only conserved currents are vector currents associated with internal symmetries, the stress-energy tensor current, the angular momentum tensor current, and the spin-3/2 supercurrent, for a supersymmetric theory.

This restriction on the currents constrains the spins to 0,1/2 (which do not need to be coupled to currents), spin 1 (which must be coupled to the vector currents), spin 3/2 (which must be coupled to a supercurrent) and spin 2 (which must be coupled to the stress-energy tensor). The argument is heuristic, and I do not think it rises to the level of a mathematical proof, but it is plausible enough to be a good guide.

Preliminaries: All possible symmetries of the S-matrix

You should accept the following result of O'Raferteigh, Coleman and Mandula--- the continuous symmetries of the particle S-matrix, assuming a mass-gap and Lorentz invariance, are a Lie Group of internal symmetries, plus the Lorentz group. This theorem is true, given its assumptions, but these assumptions leave out a lot of interesting physics:


Coleman-Mandula assume that the symmetry is a symmetry of the S-matrix, meaning that it acts nontrivially on some particle state. This seems innocuous, until you realize that you can have a symmetry which doesn't touch particle states, but only acts nontrivially on objects like strings and membranes. Such symmetries would only be relevant for the scattering of infinitely extended infinite energy objects, so it doesn't show up in the S-matrix. The transformations would become trivial whenever these sheets close in on themselves to make a localized particle. If you look at Coleman and Mandula's argument (a simple version is presented in Argyres' supersymmetry notes, which gives the flavor. There is an excellent complete presentation in Weinberg's quantum field theory book, and the original article is accessible and clear), it almost begs for the objects which are charged under the higher symmetry to be spatially extended. When you have extended fundamental objects, it is not clear that you are doing field theory anymore. If the extended objects are solitons in a renormalizable field theory, you can zoom in on ultra-short distance scattering, and consider the ultra-violet fixed point theory as the field theory you are studying, and this is sufficient to understand most examples. But the extended-object exception is the most important one, and must always be kept in the back of the mind.
Coleman and Mandula assume a mass gap. The standard extension of this theorem to the massless case just extends the maximal symmetry from the Poincare group to the conformal group, to allow the space-time part to be bigger. But Coleman and Madula use analyticity properties which I am not sure can be used in a conformal theory with all the branch-cuts which are not controlled by mass-gaps. The result is extremely plausible, but I am not sure if it is still rigorously true. This is an exercise in Weinberg, which unfortunately I haven't done.
Coleman and Mandula ignore supersymmetries. This is fixed by HaagΓÇôLopuszanskiΓÇôSohnius, who use the Coleman mandula theorem to argue that the maximal symmetry structure of a quantum field theory is a superconformal group plus internal symmetries, and that the supersymmetry must close on the stress-energy tensor.


What the Coleman Mandula theorem means in practice is that whenever you have a conserved current in a quantum field theory, and this current acts nontrivially on particles, then it must not carry any space-time indices other than the vector index, with the only exceptions being the geometric currents: a spinor supersymmetry current, $J^{\alpha\mu}$, the (Belinfante symmetric) stress-energy tensor $T^{\mu\nu}$, the (Belinfante) angular momentum tensor $S^{\mu\nu\lambda} = x^{\mu} T^{\nu\lambda} - x^\nu T^{\mu\lambda}$, and sometimes the dilation current $D^\mu = x^\mu T^\alpha_\alpha$ and conformal and superconformal currents too.

The spin of the conserved currents is found by representation theory--- antisymmetric indices are spin 1, whether there are 1 or 2, so the spin of the internal symmetry currents is 1, and of the stress energy tensor is 2. The other geometric tensors derived from the stress energy tensor are also restricted to spin less then 2, with the supercurrent having spin 3/2.

What is a QFT?

Here this is a practical question--- for this discussion, a quantum field theory is a finite collection of local fields, each corresponding to a representation of the Poincare group, with a local interaction Lagrangian which couples them together. Further, it is assumed that there is an ultra-violet regime where all the masses are irrelevant, and where all the couplings are still relatively small, so that perturbative particle exchange is ok. I say pseudo-limit, because this isn't a real ultra-violet fixed point, which might not exist, and it does not require renormalizability, only unitarity in the regime where the theory is still perturbative.

Every particle must interact with something to be part of the theory. If you have a noninteracting sector, you throw it away as unobservable. The theory does not have to be renormalizable, but it must be unitary, so that the amplitudes must unitarize perturbatively. The couplings are assumed to be weak at some short distance scale, so that you don't make a big mess at short distances, but you can still analyze particle emission order by order

The Froissart bound for a mass-gap theory states that the scattering amplitude cannot grow faster than the logarithm of the energy. This means that any faster than constant growth in the scattering amplitude must be cancelled by something.

Propagators for any spin

The propagators for massive/massless particles of any spin follow from group theory considerations. These propagators have the schematic form

$$ s^J\over s-m^2$$

And the all-important s scaling, with its J-dependence can be extracted from the physically obvious angular dependence of the scattering amplitude. If you exchange a spin-J particle with a short propagation distance (so that the mass is unimportant) between two long plane waves (so that their angular momentum is zero), you expect the scattering amplitude to go like $\cos(\theta)^J$, just because rotations act on the helicity of the exchanged particle with this factor.

For example, when you exchange an electron between an electron and a positron, forming two photons, and the internal electron has an average momentum k and a helicity +, then if you rotate the contribution to the scattering amplitude from this exchange around the k-axis by an angle $\theta$ counterclockwise, you should get a phase of $\theta/2$ in the outgoing photon phases.

In terms of Mandelstam variables, the angular amplitude goes like $(1-t)^J$, since t is the cosine of the scattering variable, up to some scaling in s. For large t, this grows as t^J, but "t" is the "s" of a crossed channel (up to a little bit of shifting), and so crossing t and s, you expect the growth to go with the power of the angular dependence. The denominator is fixed at $J=0$, and this law is determined by Regge theory.

So that for $J=0,1/2$, the propagators shrink at large momentum, for $J=1$, the scattering amplitudes are constant in some directions, and for $J>1$ they grow. This schematic structure is of course complicated by the actual helicity states you attach on the ends of the propagator, but the schematic form is what you use in Weinberg's argument. 

Spin 0, 1/2 are OK

That spin 0 and 1/2 are ok with no special treatment, and this argument shows you why: the propagator for spin 0 is

$$ 1\over k^2 + m^2$$

Which falls off in k-space at large k. This means that when you scatter by exchanging scalars, your tree diagrams are shrinking, so that they don't require new states to make the theory unitary.

Spinors have a propagator

$$ 1\over \gamma\cdot k + m $$

This also falls off at large k, but only linearly. The exchange of spinors does not make things worse, because spinor loops tend to cancel the linear divergence by symmetry in k-space, leaving log divergences which are symptomatic of a renormalizable theory.

So spinors and scalars can interact without revealing substructure, because their propagators do not require new things for unitarization. This is reflected in the fact that they can make renormalizable theories all by themselves.

Spin 1

Introducing spin 1, you get a propagator that doesn't fall off. The massive propagator for spin 1 is

$$ { g_{\mu\nu} - {k_\mu k_\nu\over m^2} \over k^2 + m^2 }$$

The numerator projects the helicity to be perpendicular to k, and the second term is problematic. There are directions in k-space where the propagator does not fall off at all! This means that when you scatter by spin-1 exchange, these directions can lead to a blow-up in the scattering amplitude at high energies which has to be cancelled somehow.

If you cancel the divergence with higher spin, you get a divergence there, and you need to cancel that, and then higher spin, and so on, and you get infinitely many particle types. So the assumption is that you must get rid of this divergence intrinsically. The way to do this is to assume that the $k_\mu k_\nu$ term is always hitting a conserved current. Then it's contribution vanishes.

This is what happens in massive electrodynamics. In this situation, the massive propagator is still ok for renormalizability, as noted by Schwinger and Feynman, and explained by Stueckelberg. The $k_\mu k_\nu$ is always hitting a $J^\mu$, and in x-space, it is proportional to the divergence of the current, which is zero because the current is conserved even with a massive photon (because the photon isn't charged).

The same argument works to kill the k-k part of the propagator in Yang-Mills fields, but it is much more complicated, because the Yang-Mills field itself is charged, so the local conservation law is usually expressed in a different way, etc,etc. The heuristic lesson is that spin-1 is only ok if you have a conservation law which cancels the non-shrinking part of the numerator. This requires Yang-Mills theory, and the result is also compatible with renormalizability.

If you have a spin-1 particle which is not a Yang-Mills field, you will need to reveal new structure to unitarize its longitudinal component, whose propagator is not properly shrinking at high energies.

Spin 3/2

In this case, you have a Rarita Schwinger field, and the propagator is going to grow like $\sqrt{s}$ at large energies, just from the Mandelstam argument presented before.

The propagator growth leads to unphysical growth in scattering exchanging this particle, unless the spin-3/2 field is coupled to a conserved current. The conserved current is the Supersymmetry current, by the HaagΓÇôLopuszanskiΓÇôSohnius theorem, because it is a spinor of conserved currents.

This means that the spin-3/2 particle should interact with a spin 3/2 conserved supercurrent in order to be consistent, and the number of gravitinos is (less then or equal to) the number of supercharges.

The gravitinos are always introduced in a supermultiplet with the graviton, but I don't know if it is definitely impossible to introduce them with a spin-1 partner, and couple them to the supercurrent anyway. These spin-3/2/spin-1 multiplets will probably not be renormalizable barring some supersymmetry miracle. I haven't worked it out, but it might be possible.

Spin 2

In this case, you have a perturbative graviton-like field $h_{\mu\nu}$, and the propagator contains terms growing linearly with s.

In order to cancel the growth in the numerator, you need the tensor particle to be coupled to a conserved current to kill the parts with too-rapid growth, and produce a theory which does not require new particles for unitarity. The conserved quantity must be a tensor $T_{\mu\nu}$. Now one can appeal to the Coleman Mandula theorem and conclude that the conserved tensor current must be the stress energy tensor, and this gives general relativity, since the stress-tensor includes the stress of the h field too.

There is a second tensor conserved quantity, the angular momentum tensor $S_{\mu\nu\sigma}$, which is also spin-2 (it might look like its spin 3, but its antisymmetric on two of its indices). You can try to couple a spin-2 field to the angular momentum tensor. To see if this works requires a detailed analysis, which I haven't done, but I would guess that the result will just be a non-dynamical torsion coupled to the local spin, as required by the Einstein-Cartan theory.

Witten mentions yet another possiblity for spin 2 in chapter 1 of Green Schwarz and Witten, but I don't remember what it is, and I don't know whether it is viable.

Summary

I believe that these arguments are due to Weinberg, but I personally only read the sketchy summary of them in the first chapters of Green Schwarz and Witten. They do not seem to me to have the status of a theorem, because the argument is particle by particle, it requires independent exchange in a given regime, and it discounts the possiblity that unitary can be restored by some family of particles.

Of course, in string theory, there are fields of arbitrarily high spin, and unitarity is restored by propagating all of them together. For field theories with bound states which lie on Regge trajectories, you can have arbitrarily high spins too, so long as you consider all the trajectory contributions together, to restore unitarity (this was one of the original motivations for Regge theory--- unitarizing higher spin theories).

For example, in QCD, we have nuclei of high ground-state spin. So there are stable S-matrix states of high spin, but they come in families with other excited states of the same nuclei.

The conclusion here is that if you have higher spin particles, you can be pretty sure that you will have new particles of even higher spin at higher energies, and this chain of particles will not stop until you reveal new structure at some point. So the tensor mesons observed in the strong interaction mean that you should expect an infinite family of strongly interacting particles, petering out only when the quantum field substructure is revealed.

Some comments

James said:


It seems higher spin fields must be massless so that they have a gauge symmetry and thus a current to couple to
A massless spin-2 particle can only be a graviton.


These statements are as true as the arguments above are convincing. From the cancellation required for the propagator to become sensible, higher spin fields are fundamentally massless at short distances. The spin-1 fields become massive by the Higgs mechanism, the spin 3/2 gravitinos become massive through spontaneous SUSY breaking, and this gets rid of Goldstone bosons/Goldstinos.

But all this stuff is, at best, only at the "mildly plausible" level of argument--- the argument is over propagator unitarization with each propagator separately having no cancellations. It's actually remarkable that it works as a guideline, and that there aren't a slew of supersymmetric exceptions of higher spin theories with supersymmetry enforcing propagator cancellations and unitarization. Maybe there are, and they just haven't been discovered yet. Maybe there's a better way to state the argument which shows that unitarity can't be restored by using positive spectral-weight particles.

Big Rift in 1960s

James askes 


Why wasn't this pointed out earlier in the history of string theory?


The history of physics cannot be well understood without appreciating the unbelievable antagonism between the Chew/Mandelstam/Gribov S-matrix camp, and the Weinberg/Glashow/Polyakov Field theory camp. The two sides hated each other, did not hire each other, and did not read each other, at least not in the west. The only people that straddled both camps were older folks and Russians--- Gell-Mann more than Landau (who believed the Landau pole implied S-matrix), Gribov and Migdal more than anyone else in the west other than Gell-Mann and Wilson. Wilson did his PhD in S-matrix theory, for example, as did David Gross (under Chew).

In the 1970s, S-matrix theory just plain died. All practitioners jumped ship rapidly in 1974, with the triple-whammy of Wilsonian field theory, the discovery of the Charm quark, and asymptotically freedom. These results killed S-matrix theory for thirty years. Those that jumped ship include all the original string theorists who stayed employed: notably Veneziano, who was convinced that gauge theory was right when t'Hooft showed that large-N gauge fields give the string topological expansion, and Susskind, who didn't mention Regge theory after the early 1970s. Everybody stopped studying string theory except Scherk and Schwarz, and Schwarz was protected by Gell-Mann, or else he would never have been tenured and funded.

This sorry history means that not a single S-matrix theory course is taught in the curriculum today, nobody studies it except a few theorists of advanced age hidden away in particle accelerators, and the main S-matrix theory, string-theory, is not properly explained and remains completely enigmatic even to most physicists. There were some good reasons for this--- some S-matrix people said silly things about the consistency of quantum field theory--- but to be fair, quantum field theory people said equally silly things about S-matrix theory.

Weinberg came up with these heuristic arguments in the 1960s, which convinced him that S-matrix theory was a dead end, or rather, to show that it was a tautological synonym for quantum field theory. Weinberg was motivated by models of pion-nucleon interactions, which was a hot S-matrix topic in the early 1960s. The solution to the problem is the chiral symmetry breaking models of the pion condensate, and these are effective field theories.

Building on this result, Weinberg became convinced that the only real solution to the S-matrix was a field theory of some particles with spin. He still says this every once in a while, but it is dead wrong. The most charitable interpretation is that every S-matrix has a field theory limit, where all but a finite number of particles decouple, but this is not true either (consider little string theory). String theory exists, and there are non-field theoretic S-matrices, namely all the ones in string theory, including little string theory in (5+1)d, which is non-gravitational.

Lorentz indices

James comments: 


regarding spin, I tried doing the group theoretic approach to an antisymmetric tensor but got a little lost - doesn't an antisymmetric 2-form (for example) contain two spin-1 fields?


The group theory for an antisymmetric tensor is simple: it consists of an "E" and "B" field which can be turned into the pure chiral representations E+iB, E-iB. This was also called a "six-vector" sometimes, meaning E,B making an antisymmetric four-tensor.

You can do this using dotted and undotted indices more easily, if you realize that the representation theory of SU(2) is best done in indices--- see the "warm up" problem in this answer: Mathematically, what is color charge?
QuestionKenntnis:
What does it mean for two objects to "touch"?
qn_description:
If you've ever been annoyingly poked by a geek, you might be familiar with the semi-nerdy obnoxious response of 


  "I'm not actually touching you! The electrons in the atoms of my
  skin are just getting really close to yours!"


Expanding on this a little bit, it seems the obnoxious geek is right. After all, consider Zeno's paradox. Every time you try to touch two objects together, you have to get them halfway there, then quarter-way, etc. In other words, there's always a infinitesimal distance in between the two objects. 

Atoms don't "touch" each other; even the protons and neutrons in the nucleus of an atom aren't "touching" each other.

So what does it mean for two objects to touch each other?


Are atoms that join to form a molecule "touching"? I suppose the atoms are touching, because their is some overlap, but the subatomic particles are just whizzing around avoiding each other. If this is the case, should "touching" just be defined relative to some context? I.e, if I touch your hand, our hands are touching, but unless you pick up some of my DNA, the molecules in our hands aren't touching? And since the molecules aren't changing, the atoms aren't touching either? 
Is there really no such thing as "touching"?
AnswersKenntnis
AnswerKenntnis:
Wow, this one has been over-answered already, I know... but it is such a fun question! So, here's an answer that hasn't been, um, "touched" on yet... :)

You sir, whatever your age may be (anyone with kids will know what I mean), have asked for an answer to one of the deepest questions of quantum mechanics. In the quantum physics dialect of High Nerdese, your question boils down to this: Why do half-integer spin particles exhibit Pauli exclusion - that is, why do they refuse to the be in the same state, including the same location in space, at the same time?

You are quite correct that matter as a whole is mostly space. However, the specific example of bound atoms is arguably not so much an example of touching as it is of bonding. It would be the equivalent of a 10 year old son not just poking his 12 year old sister, but of poking her with superglue on his hand, which is a considerably more drastic offense that I don't think anyone would be much amused by.

Touching, in contrast, means that you have to push - that is, exert some real energy - into making the two objects contact each other. And characteristically, after that push, the two object remain separate (in most cases) and even bound back a bit after the contact is made.

So, I think one can argue that the real question behind "what is touching?" is "why do solid objects not want to be compressed when you try to push them together?" If that were not the case, the whole concept of touching sort of falls apart. We would all become at best ghostly entities who cannot make contact with each other, a bit like Chihiro as she tries to push Haku away during their second meeting in Spirited Away.

Now with that as the sharpened version of the query, why do objects such a people not just zip right through each other when they meet, especially since they are (as noted) almost entirely made of empty space?

Now the reflex answer - and it's not a bad one - is likely to be electrical charge. That's because we all know that atoms are positive nuclei surrounded by negatively charged electrons, and that negative charges repel. So, stated that way, it's perhaps not too surprising that when the outer "edges" of these rather fuzzy atoms get too close that their respective sets of electrons would get close enough to repel each other. So by this answer, "touching" would simply be a matter of atoms getting so close to each other that their negatively charged clouds of electrons start bumping into each other. This repulsion requires force to over come, so the the two objects "touch" - reversibly compress each other without merging - through the electric fields that surround the electrons of their atoms.

This sounds awfully right, and it even is right... to a limited degree.

Here's one way to think of the issue: If charge was the only issue involved, then why do some atoms have exactly the opposite reaction when their electron clouds are pushed close to each other? For example, if you push sodium atoms close to chlorine atoms, what you get is the two atoms leaping to embrace each other more closely, with a resulting release of energy that at larger scales is often described by words such as "BOOM!" So clearly something more than just charge repulsion is going on here, since at least some combinations of electrons around atoms like to nuzzle up much closer to each other instead of farther away.

What then guarantees that two molecules will come up to each other and instead say "Howdy, nice day... but, er, could you please back off a bit, it's getting stuffy?"

That general resistance to getting too close turns out to result not so much from electrical charge (which does still plays a role), but rather from the Pauli exclusion effect I mentioned earlier. Pauli exclusion is often skipped over in starting texts on chemistry, which may be why issues such a what touching means are also often left dangling a bit. Without Pauli exclusion, touching - the ability of two large objects to make contact without merging or joining - will always remain a bit mysterious.

So what is Pauli exclusion? It's just this: Very small, very simple particles that spin (rotate) in a very peculiar way always, always insist on being different in some way, sort of like kids in large families where everyone wants their unique role or ability or distinction. But particles, unlike people, are very simple things, so they only have a very limited set of options to choose from. When they run out of those simple options, they have only one option left: they need their own bit of space, apart from any other particle. They will then defend that bit of space very fiercely indeed. It is that defense of their own space that leads large collections of electrons to insist on taking up more an more overall space, as each tiny electron carves out its own unique and fiercely defended bit of turf.

Particles that have this peculiar type of spin are called fermions, and ordinary matter is made of three main types of fermions: Protons, neutrons, and electrons. For the electrons there is only one identifying feature that distinguishes them from each other, and that is how they spin: counterclockwise (called "up") or clockwise (called "down"). You'd think they'd have other options, but that too is a deep mystery of physics: Very small objects are so limited in the information they carry that they can't even have more than directions to choose from when spinning around.

However, that one options is very important for understanding that issue of bonding that must be dealt with before atoms can engage in touching. Two electrons with opposite spins, or with spins that can be made opposite of each other by turning atoms around the right way, do not repel each other: They attract. In fact, they attract so much that they are an important part of that "BOOM!" I mentioned earlier for sodium and chlorine, both of which have lonely electrons without spin partners, waiting. There are other factors on how energetic the boom is, but the point is that until electrons have formed such nice, neat pairs, they don't have as much need to occupy space.

Once the bonding has happened, however - once the atoms are in arrangements that don't leave unhappy electrons sitting around wanting to engage in close bonds - then the territorial aspect of electrons comes to the forefront: They begin defending their turf fiercely.

This defense of turf first shows itself in the ways electrons orbit around atoms, since even there the electrons insist on carving out their own unique and physically separate orbits, after that first pairing of two electrons is resolved. As you can imagine, trying to orbit around an atom while at the same time trying very hard to stay away from other electron pairs can lead to some pretty complicated geometries. And that too is a very good thing, be cause those complicated geometries lead to something called chemistry, where different numbers of electrons can exhibit very different properties due to new electrons being squeezed out into all sorts of curious and often highly exposed outside orbits.

In metals it gets so bad that the outermost electrons essentially become community children that zip around the entire metal crystal instead of sticking to single atoms. That's why metal carry heat and electricity so well. In fact, when you look at a shiny metallic mirror, you are looking directly at the fastest-moving of these community-wide electrons. It's also why in outer space you have to be very careful about touching two pieces of clean metal to each other, because with all those electrons zipping around, the two pieces may very decide to bond into a single new piece of metal instead of just touching. This effect is called vacuum welding, and it's an example of why you need to be careful about assuming that solids that make contact will always remain separate.

But many materials, such a you and your skin, don't have many of these community electrons, and are instead full of pairs of electrons that are very happy with the situations they already have, thank you. And when these kinds of materials and these kinds of electrons approach, the Pauli exclusion effect takes hold, and the electrons become very defensive of their turf.

The result at out large-scale level is what we call touching: the ability to make contact without easily pushing through or merging, a large-scale sum of all of those individual highly content electrons defending their small bits of turf.

So to end, why do electrons and other fermions want so desperately to their own bits of unique state and space all to themselves? And why, in every experiment ever done, is this resistance to merger always associated with that peculiar kind of spin I mentioned, a form of spin that is so minimal and so odd that it can't quite be described within ordinary three-dimensional space?

We have fantastically effective mathematical models of this effect. It has to do with antisymmetric wave functions. These amazing models are instrumental to things such as the semiconductor industry behind all of our modern electronic devices, as well as chemistry in general and of course research into fundamental physics.

But if you ask the "why" question, that becomes a lot harder. The most honest answer is, I think, "because that is what we see: half-spin particles have antisymmetric wave functions, and that means they defend their spaces."

But linking the two together tightly - something called the spin-statistics problem - has never really been answered in a way that Richard Feynman would have called satisfactory. In fact, he flatly declared more than once that this (and several other items in quantum physics) were still basically mysteries for which we lacked really deep insights into why the universe we know works that way.

And that, sir, is why your question of "what is touching?" touches more deeply on profound mysteries of physics than you may have realized. It's a good question.



2012-07-01 Addendum

Here is a related answer I did for S.E. Chemistry. It touches on many of the same issues, but with more emphasis on why "spin pairing" of electrons allows atoms to share and steal electrons from each other -- that is, it lets them form bonds. It is not a classic textbook explanation of bonding, and I use a lot of informal English words that are not mathematically accurate. But the physics concepts are accurate. My hope is that it can provide a better intuitive feel for the rather remarkable mystery of how an uncharged atom (e.g. chlorine) can overcome the tremendous electrostatic attraction of a neutral atom (e.g. sodium) to steal one or more of its electrons.
AnswerKenntnis:
Common sense of touching can be expressed in "scientific means" as an event when exchange-repulsion interaction between 2 objects (you and the geek) extends some arbitrary value, say 1meV. I leave finding an agreeable threshold which is easy to measure to later discussion. :)
AnswerKenntnis:
As a useable heuristic I would go with something along the lines of 


  the intermolecular forces between the surface molecules of the bodies are comparable to the scale of one-to-one intermolecular forces between nearby{*} molecules due to other components of the same body


You could make it a little more strict by replacing "comparable to" with "non-negligible in comparison with" if you wanted.

Certainly any situation which generates non-negligible deformation of either body through intermolecular forces must count.



{*} In a solid---and I'm only discussing solids for the moment---each molecule is maintained in a roughly constant relationship with it's neighbors by a variety of electromagnetic forces. Of course at equilibrium the net is zero (at least averaged over time scales longer than the thermal motion timescale) and does not provide much of a scale. But that net is a combination of pushed and pulls from multiple neighbors. Take the average of the magnitudes of those one-to-one forces as the proper scale for comparison.

The situation in fluids is not a simple as the bits are not fixed in relationship to one another, but we can probably just use the same local average of magnitudes.



Under these definitions the annoying little eleven year boy in question is touching you and deserves to be smacked upside the head gently chided in this highly-developed, post-violence society.
AnswerKenntnis:
This is very legitimate question for something we usually take for granted.

I think it would be possible to define macroscopically touching as the situation, in which the total force between two electrically neutral rigid bodies is larger than pure gravitational (for some measurable value).  The difference is of course the normal component of the surface force plus friction.

The related question is "how to measure or define the normal component of the surface force"?  Normal component is obviously defined indirectly, as the opposite to the sum of normal components of all other forces!
AnswerKenntnis:
At least as long as you don't give a definition of "touching" yourself (like spatial DNA transfer), this is more a philosophical question than a physical one. 

You mention Zeno's paradox, but one could approach/interpret this in many different ways involving questions about 'intention', 'consciousness', the 'I' and so on. I mean there are a huge number of life forms on your skin, which you count as "you". Also, if you wear gloves/condom, is it not touching? The concept of touching is as least as difficult as the concept of points.

For my taste, you also "take physics too literal". Talking in physical terms is talking in terms included in a (man made) physical theory. Moreover, you don't even need a quantum mechanical wave function to observe that contact is an abstract thing. When long ranging forces can really be neglected seems to be a question, which should be eighter answered by 'never' or (and this is the answer I think suites best) by practical operative means.
QuestionKenntnis:
Does juggling balls reduce the total weight of the juggler and balls?
qn_description:
A friend offered me a brain teaser to which the solution involves a $195$ pound man juggling two $3$-pound balls to traverse a bridge having a maximum capacity of only $200$ pounds. He explained that since the man only ever holds one $3$-pound object at a time, the maximum combined weight at any given moment is only $195 + 3=198$ pounds, and the bridge would hold.

I corrected him by explaining that the acts of throwing up and catching the ball temporarily make you 'heavier' (an additional force is exerted by the ball to me and by me onto the bridge due to the change in momentum when throwing up or catching the ball), but admitted that gentle tosses/catches (less acceleration) might offer a situation in which the force on the bridge never reaches the combined weight of the man and both balls.

Can the bridge withstand the man and his balls?
AnswersKenntnis
AnswerKenntnis:
Suppose you throw the ball upwards at some speed $v$. Then the time it spends in the air is simply:

$$ t_{\text{air}} = 2 \frac{v}{g} $$

where $g$ is the acceleration due to gravity. When you catch the ball you have it in your hand for a time $t_{\text{hand}}$ and during this time you have to apply enough acceleration to it to slow the ball from it's descent velocity of $v$ downwards and throw it back up with a velocity $v$ upwards:

$$ t_{\text{hand}} = 2 \frac{v}{a - g} $$

Note that I've written the acceleration as $a - g$ because you have to apply at least an acceleration of $g$ to stop the ball accelerating downwards. The acceleration $a$ you have to apply is $g$ plus the extra acceleration to accelerate the ball upwards.

You want the time in the hand to be as long as possible so you can use as little acceleration as possible. However $t_{\text{hand}}$ can't be greater than $t_{\text{air}}$ otherwise there would be some time during which you were holding both balls. If you want to make sure you are only ever holding one ball at a time the best you can do is make $t_{\text{hand}}$ = $t_{\text{air}}$. If we substitute the expressions for $t_{\text{hand}}$ and  $t_{\text{air}}$ from above and set them equal we get:

$$ 2 \frac{v}{g} = 2 \frac{v}{a - g} $$

which simplifies to:

$$ a = 2g $$

So while you are holding one 3kg ball you are applying an acceleration of $2g$ to it, and therefore the force you're applying to the ball is $2 \times 3 = 6$ kg.

In other words the force on the bridge when you're juggling the two balls (with the minimum possible force) is exactly the same as if you just walked across the bridge holding the two balls, and you're likely to get wet!
AnswerKenntnis:
I love this class of problem as a fantastic physical example of the mean value theorem.  Allow me to describe a specific case that fits the following conditions:


The man plus the balls has a total weight of $m$
The entire system (man+balls) starts at rest and ends at rest


From these relatively simple assumptions, I will claim that the average normal force (the force the ground exerts upward) is equal to the weight of the the system.  In other words, for a given period of time of length $T$ we have this:

$$ m g = \frac{1}{T} \int_0^T \vec{F}(t) \cdot \vec{n} dt $$

This is a spectacular claim actually.  To simplify the notation, consider that $\vec{F}(t) \cdot \vec{n}$ is just equal to the weight a scale would read (this isn't a bad assumption, depending on the scale).  Imagine the man is juggling, standing on a scale, and the scale reads a value that depends on time, $w(t)$.  The average value the scale reads will be equal to gravity times his mass, including everything he's holding or wearing.

In the story of the man walking across the bridge juggling balls, the total weight is $201 lb$.  For every second he weighs $200 lb$, he spends one second weighing $202 lb$ or something similar.  The point is that the average value is the same.
AnswerKenntnis:
Put one ball down.  Walk the other across.  Go back, get the second ball.

Or, roll the two balls across, then run after them.

Or, the juggler takes off his shoes and walks across barefoot.

This is solved as a "nonlinear thinking" problem, not with "juggling is anti-gravity".  The ball-man system must be accelerated downwards with an average of 1 lb of force or the bridge will break.  Otherwise you could build a perpetual motion machine from two jugglers on a see-saw who take turns juggling.

(Also, running is like juggling in that the weight is up in the air much of the time--if this could work, you could also just hold the balls and run.)
AnswerKenntnis:
It depends on how long his arms are!!
(and how long the bridge is)
If he starts in first position, arms held high, and imparts -0.17G to his balls while crossing, he will make it.
Oops. I did the math wrong in my comment.

Besides, he can do a juggler's trick, and !gradually lower his center of gravity! as he walks across the bridge. The juggling is optional, a distraction from what they are really doing. He only has to accelerate at G*(1/201) to have the bridge bear, not 201 lbs (195+6), but 200 lbs. If he can crouch down to 2 ft, I get 5 seconds to cross the bridge.

1/2 ( 0.16 ft / s^2 ) t^2 = 2 ft

t = sqrt[ 4ft/(0.16ft) sec^2 ]
AnswerKenntnis:
Imagine for simplicity that the juggler at some instant repeats himself, i.e. that the juggler and balls (with masses $M$ and $2m$, respectively)  are in the precise same kinematic state at times $t_1$ and $t_2$. 

Consider man + 2 balls as the system, and bridge, etc., as the environment. 

Let $p(t)$ be (the vertical component of) the total momentum of the system.  

Newton's second law applied to the system yields: 

$$\tag{1} \dot{p}(t) ~=~ F_n(t) - F_g, $$

where 

$$\tag{2} F_g~=~(M+2m)g, $$

and where $F_n(t)$ is the normal force from the bridge, which may vary in time $t$ as the juggler does his routine.$^1$

Because of our simplifying assumption of repeating states, we have

$$\tag{3} 0~=~p(t_2)-p(t_1)~=~ \int_{t_1}^{t_2} F_n(t)dt  -  (t_2-t_1)F_g, $$

or 

$$\tag{4} F_g ~=~ \frac{1}{t_2-t_1} \int_{t_1}^{t_2} F_n(t)dt ~=~\langle F_n \rangle. $$

But if the average $\langle F_n \rangle$ is $F_g$, then clearly at at least one instance $t_3\in [t_1,t_2]$, one must have$^2$ 

$$\tag{5} F_n(t_3)\geq F_g.$$

In other words, the bridge collapses.



$^1$ The juggler is allowed to do whatever motion he thinks would benefit his case. Whether he wants to jump with both feet leaving the bridge, or lower his center-of-mass, or fall down, is up to him. It seems physically reasonable to assume that the normal force $F_n(t)$ is a piecewise continuous function of time $t\in [t_1,t_2]$, with only finitely many discontinuity points. In that case the integral $\int_{t_1}^{t_2} F_n(t)dt$ can be defined using the Riemann integral without involving the technically more complicated Lebesgue integral.
(Also note that the mean value theorem does not apply for discontinuous functions, and from a mathematical purist point of view, the mean value theorem is not needed, i.e., the crucial ineq.(5) may be established with considerations that are even more elementary.)

$^2$ Indirect proof of eq.(5): Assume 

$$\tag{6} \forall t\in [t_1,t_2]:~ F_n(t)~<~ F_g.$$ 

Then  

$$\tag{7} \int_{t_1}^{t_2} F_n(t)dt ~<~  (t_2-t_1)F_g,$$ 

if we assume piecewise continuity $t\mapsto F_n(t)$. But eq.(7) is inconsistent with eq.(3). QED.
AnswerKenntnis:
I think it might be possible if the man first throws one of the balls into the air before he steps on the bridge. In that case the man could apply 4 pounds of force upwards on one ball initially, then step on the bridge. At that point the bridge would be holding 198 pounds. The man can then accelerate the other ball upwards with 4 pounds of force before the other ball lands. This would mean that the bridge would be holding 199 pounds at that point. When both balls are in the air the bridge would be holding 195 pounds. Then the first ball would land in the man's hand, and the man would need to apply 4 pounds of force to decelerate it to rest. During deceleration the bridge would be holding 199 pounds. After deceleration the bridge would be holding 198 pounds. Then the man can repeat accelerating the ball upwards with 4 pounds of force as he crosses the bridge.

It may also be possible to do this if the balls had a large volume and you counted air resistance, in which case the air would help decelerate the balls as they fell down, but the man would still have to throw one of the balls into the air before he stepped on the bridge.
AnswerKenntnis:
It thinks that its reasonable to assume: "The entire system (man+balls) starts at rest and ends at rest". Then we can completely avoid integrals and dealing with time. For the moment, let's just consider the speeds of the balls and pretend his arms have unlimited length. We can only provide 5 pounds of force per second => an acceleration of 5/3 g, although this can be divided between two balls. The balls experience a downward acceleration of g each or 2g overall. Therefore, the total acceleration downwards (possibly divided between the two balls) is g/3 and we can't end up with them both at rest. The only way we could end up with them both at rest, is if we we were allowed 6 pounds of weight instead of 5 pounds (ie. same as carrying)
QuestionKenntnis:
How can I stand on the ground? EM or/and Pauli?
qn_description:
There is this famous example about the order difference between gravitational force and EM force. All the gravitational force of Earth is just countered by the electromagnetic force between the electrons on the surface of my feet and the ground.

But also wave-function of electrons(fermions) do not overlap each other due to Pauli exclusion principle.

So which one is the true reason of me not flopping inside the earth forever? Is it only one of them(my guess is Pauli exclusion) or is it both?
AnswersKenntnis
AnswerKenntnis:
OK, I'll make a complete revision of my original reply since it was quite sloppy.

First, I originally confused two issues that are nevertheless related, I confused stability of matter and the impenetrability of matter.

But, it must be clear that the two questions are related. If I have two chunks of matter of the same type on top of one another, one can't imagine that the explanation for the fact that these chunks don't "fall through" each other would be unrelated to the explanation of why we do not fall through the ground. So, in ultimate analysis, the question is tied to the question of stability of matter. 

Now, there are several steps in the problem. To explain the stability of matter, one has to explain why atoms are stable (and before that why nuclei are stable), then one has to explain why agregates of atoms like solids or liquids can be stable, i.e. why bulk matter is stable. The stability of bulk matter will then serve as the basis to explain why "we can stand on the ground".

Starting with the last step and assuming we already know about the stability of bulk matter, we can imagine that when we exert a pressure on stable bulk matter, we can expect by what it means to be in a stable equilibrium, that the piece of matter would exert an opposing pressure, trying to restore itself to its most stable configuration, provided stresses are not too great. So solving the problem of stability of bulk matter will help us understand what the nature of the restoring force will be.

Now, as is well known, electromagnetic forces can not be the sole explanation. There exist no stable equilibria when there are only electric charges interacting electromagnetically. I won't go through the proof here, but it is accessible to undergraduates, it can be found in the Feynman Lectures, book 2, chapter 5. It's an application of Gauss' law in the static case. The dynamic case only complicates matters in the wrong direction. As we know, accelerated charges radiate energy away, thus an electron orbiting a nucleus would soon fall inward if nothing prevented it, to take the classic example.

Enters Elliott Lieb and his paper 'The stability of matter' that can be easily found online. So, I'll quote a lot from there. It reviews a lot of the results in the field of mathematical physics of the problem of stability of matter. 

So what does Lieb essentially say about stability of atoms: that it is a consequence of a principle introduced by Sobolev. Sobolev's inequality states in a mathematically precise form that if one tries to compress the wavefunction anywhere, the kinetic energy will increase. It's a kind of stronger version of the HUP. (Note that at this point, Lieb does not use Pauli's exclusion principle. This is to be expected, take a hydrogen atom, it is stable, since there's only one electron, Pauli's exclusion principle can't be invoked here to explain its stability.)

Then, Lieb goes on to explain the stability of bulk matter by using Sobolev's inequality again. But this time around, he extends the inequality, and takes into account the fact that matter is made up of fermions. So, the Pauli exclusion principle is indeed used. So, again a lower bound for the kinetic energy is found, the interesting thing is that this lower bound is proportional to $N^{5/3}$ where $N$ is the number of fermions. If the particles were not fermions, the proportionality would have been $N$, which we can see by using the previous bound for 1 atom and multiplying by the number of atoms. So it is really the Pauli exclusion principle that contributes the factor $N^{2/3}$.

Lieb goes then to show that this factor is crucial. He uses Thomas-Fermi theory as a relevant approximation of the behaviour of bulk matter to demonstrate this. This is were the analysis becomes very intricate. I have no time to summarize it in more detail. So I'll just say that some theorems about the nature of TF theory are derived, these are then combined in the end to show that the minimum energy or ground state energy of the system is bounded from below. A numerical value for this bound is derived which is $-23Ry/$particle, $(1Ry \approx 13.6 eV)$.

The important take-away message is though that Fermi-statistics or the Pauli exclusion principle is indeed essential to explain the stability of bulk matter.

In Lieb's paper, there is an extra chapter which tackles the question of why matter doesn't explode, instead of implode. The interesting thing is that pure EM is sufficient to answer this question.
AnswerKenntnis:
Holy moly, there are a lot of confused parts of answers out there.   Here is a way to begin to sort out the different related principles, physically. 


The question currently asks "EM or/and Pauli?".    Short answer: Neither, although it is true that electromagnetism is the force involved (rather than the strong or weak nuclear forces, the only other candidates), and it is true that the size of atoms is determined by the uncertainty and exclusion principles, as it were.   
Neglect or remove gravity from the situation.   Then the question really becomes, "Why are solids, at the earth's crust say (but it doesn't matter), solid, i.e. why do they resist strain, to the quantitative degree that they do?"
We can idealize the problem.   Why do single crystals resist strain?   We replace rocks and dirt by interlaced microcrystals, or mixtures bound by friction.   In other words, why are crystals rigid?   Why can't we walk on water until it freezes?   
What are the energy scales of the problem?   Well the typical force to be considered will be the force required to disrupt the crystal lattice.  This is about the thermal energy at melting, = k*T(melting).  So for water (since I like to think in electron volts) about 0.025 ev/molecule, for rocks about 10 times as much.  This is because the water molecules in an ice crystal are bound by hydrogen bonds with strength about 0.1 e.v. and atoms in quartz are bound by covalent bonds of about 1 ev.   So yes, electromagnetism is the involved force.
To be specific, the force on the bottom of a shoe, normally about (1kg*9.8 m/sec^2)/cm^2 = 2 x 10 -15 kg m s-2/(the area corresponding to a molecule of water on the surface of the crystal)  times the distance such a force must move the molecule (maybe 10-10 m?) = 2 x 10^-25 J or (using 6.25 x 10^18 ev/joule) 1.2 x 10-6 ev gets compared to kT as above. So the force of the shoe walking on ice supplies only about 4.8 x 10-5 of the required energy to melt or deform a crystal of ice.  So we don't sink through solid ice. 
Here we learn an important lesson.   In physics, to talk about "a cause" is to secretly talk about the about the calculations and equations underlying computing the magnitude of the phenomenon.   So, Weisskopf, (see below) points out that the same equations with different numbers, this time using the pressure developed at the bottom of a mountain actually does reach the energy scale required to deform or melt crystals of rock (say quartz), and therefore computes the maximal height of mountains on earth (or any planet), using only a few fundamental constants.   This then carries over.   To compute the maximal height of mountains on white dwarf stars (fanciful, I think), or importantly on the surface of neutron stars (real) use the same principles, but now invoke the exclusion principal, or other forces, since these are now relevant to the energy scales involved.
On a more mundane scale and as a check, if we concentrate one's weight on a much smaller area like the blade of an ice skate, to yield a higher force, we indeed can deform ice crystals, which is said to be the reason for the low friction we experience when ice skating.
Back to earth, shoes.   At these very small energies, compared to any atomic energy scale, to say nothing of the scales involved in the weak or strong interactions, we can should just consider atoms as single units, not as separate electrons or nuclei.   Quantum mechanics is required to allow us to neglect all the higher energy degrees of freedom which have been "frozen out" at the low (300K) average energy of the molecules involved.  Weisskopf has a good exposition of this in his popular book "Knowledge and Wonder", see esp chap. 7 on "The quantum ladder"  
We are left with a more well defined problem:  why do liquids freeze and become solid, rigid crystalline shapes, and why are crystals rigid in the first place?   Rephrased: why does lowering the temperature a little bit, lead to "freezing out" of the translational degrees of freedom of individual molecules or atoms, with the simultaneous aquisition of a single rigid global translational phase (lattice) for the positions of all the molecules/atoms in a crystal.   In other words, why, after the phase transition, this change in the microscopic symmetry of the material, is there rigidity? 
This also suggests the answer to the original question.  In order for strain to become large, molecules need to break from the global phase and leave their position in the crystal lattice, and the lattice resists this change cooperatively, i.e. many molecules must get disrupted, so the energy required is large, considerably larger than the energy available from the tiny pressures involved from the pressure from your weight on the sole of your shoe.   There is more, but I'll stop here for now.


P.S.   All of the Weisskopf popular physics books are terrific
AnswerKenntnis:
Ok, I'll bite the bullet and get the downvotes, but my answer is EM.

Why? Well, you can't stand on water and you can't stand on air. The Pauli principle applies to those cases but doesn't make matter solid. It's the (fundamentally) cristalline structure that makes an object solid enought to stand on. This is yes, related to QM (what is not) but certainly EM in nature.



Edit:
Marek asked me to specify a bit more and add some meat to my answer. Fair enough.

The question itself is not a well-posed one. Electro-magnetism is quantum mechanical in nature, and it's basically impossible to speak about atoms without speaking about Pauli's principle and EM. Actually one could say that Pauli's principle is at the basis of chemistry, together with EM. So in this sense, what is left to answer? There can't be an atom without either of the two. In this sense the answer is BOTH.

I choose therefore to interpret the question differently: is the exclusion principle acting between the soles of my shoes and the ground, that which is holding me up?

Now, the reason why we can stand on ground is undoubtedly chemical in nature - there are chemical bonds between atoms and molecules in the ground that make it solid (as opposed to, say, liquid). Frozen water is governed by EM, by the fact that the molecule is electrically charged, and not by chemical covalent bonds.

In this sense, for me, EM is probably more important than Pauli's into making something solid (and not liquid or gaseous).
AnswerKenntnis:
I've even heard different explanations from my physics profs with Phd-s on this question. My EM teacher said it's EM and my statistical mechanics prof said it was PEP. I think myself it's PEP and EM together but the reason behind EM must be clarifed. What keeps an object solid is molecular and crystal bonds, which involve electrons being stuck in a potential well. In order to break the structure we'd have to break these strong bonds which are oriented because of the shape of the orbitals involved. So this accounts for solidity of normal matter like crystals. That being said it's not what keeps us afloat however, it's is an essential requirement of floating nonetheless. Electrons respond to EM fields, so if materials have EM forces on them there must be an EM field present. But materials are generally electrically neutral and so they neither generate nor respond to EM fields. Some of you may say but when you get down to a very small scale there maybe nonzero fields between electrons. Yes but those fields are compensated by the positive charges, if this wasn't so charges would move, especially in conductors to compensate any field. So what's left really is the PEP to account for us not falling through the floor I think.
AnswerKenntnis:
I would just like to point out that even if the earth's crust were a liquid, although we couldn't walk on it's surface, the liquid would still be subject to pressure. The only reason one can "fall through" a liquid is because the molecules are moving out of the way; thus, one is not penetrating/passing through the matter, one is simply pushing the matter aside, and it is he fluid nature of the liquid medium that allows this to occur. When you think about the crust in terms of a solid, for in reality it IS a solid, you must hear in mind that atoms are mostly empty space. So, if there is so much empty space, why can't the molecules of your body and the molecules of the earth's surface just sort of slip past one another, allowing you to effectively pass through the matter? Or better still, why can't the Particles of your body actually and physically move through other matter/particles? This is where the PEP comes into play. The electrons as well as protons and neutrons that make up ordinary matter cannot get close enough together to allow this to happen, especially under ordinary circumstances. So yes, the PEP is at work here. Without it, two bits of matter could conceivably occupy the exact same space simultaneously! 

As for the "states of matter portion" that is attributed to EM, because of the electromagnetic bonds involved in the chemical makeup of ordinary matter.
AnswerKenntnis:
You do not fall through the floor because of both Coulomb repulsion and the Pauli exclusion principle.

Electrons are attracted to atomic nuclei and repelled from each other. Pauli is what requires that the electrons sit in different energy states, and not all in the low-lying, centre-biased s shells overlapping the nucleus (see picture). It is largely these higher states, and the delocalised states involved in compounds, which are responsible for the various types of bonding which hold matter together. The mutual repulsion of electrons between atoms holds them apart, and prevents one object passing through another.

Quantum Electro-Dynamics (containing both Pauli and Coulomb effects) is a sufficient theory to explain the interactions underlying all of Chemistry, Materials Science, and Why Things Don't Fall Through Each Other Studies. That includes all crystal behaviour (metals, semiconductors, etc), all organic behaviour (hydrocarbons, polymers), liquids, gasses, plasma.

Pauli doesn't prevent overlapping

Pauli is responsible for a very interesting effect, called energy level splitting. As two hydrogen atoms are brought together, the electron each atom has becomes 'aware' of the other, and the previously equal energy levels split into pairs; now the electrons are either in a symmetric or an anti-symmetric configuration, and these have slightly different energies. This splitting happens all over the place, and is responsible for the band gap that makes semiconductors possible, spin polarisation, etc. It is this symmetry/antisymmetry that Pauli is all about.

Pauli does not preclude two electron's wavefunctions from overlapping; far from it. Electrons overlap all the time. It only precludes two electrons from being in the same state. So if I have a hydrogen atom, I can put two electrons into the s orbital, so their wavefunctions overlap completely, but only if they are in different spin states. 

Matter is not mostly empty

Questions about things falling through things are a natural response to silly offhand remarks about matter being largely empty, with comparisons of an atom to a pea at the length of a football field etc. In reality, it is more reasonable to consider the electrons to each be in one of a number of cloud shapes centred on the nucleus; the lowest energy states actually involve the electron having some probability of being inside the nucleus, and are more likely to be close to the nucleus than further away. So actually the nucleus is surrounded by a dense cloud of electrons (probability-wise), and as two atoms are pushed closer together, these clouds repel each other.
AnswerKenntnis:
I wonder if we are not all short-changing the Heisenberg Uncertainty Principle in this discussion. It is certainly the HUP, not electrostatics or Pauli, that stops the hydrogen atom from collapsing. And it is obviously false to say that in the absence of Pauli, all the electrons in bigger atoms would crowd into exactly the same wave Hydrogen ground state wave function...because the Helium atom certainly doesn't do that, and Pauli places no restrictions on the wave function there. In the absence of Pauli, multiple electron atoms would be spherically symmetric but the electron distribution within would still show some structure, as does Helium. (Although it is arguable that the structure is simply an artifact of the electron-basis-state representation, and that in the density matrix representation the charge distribution is spherically symmetric. But that's another story.)

What prevents two hydrogen atoms from passing right through each other? They could if the electron from A could neutralize the proton from B. And it is not Pauli that stops them from doing so, nor electrostatics which positively wants to neutralize the charges, but simply Heisenberg that stops them from doing so.
AnswerKenntnis:
The interaction between the nucleus (the core of the atoms) and the electrons is electromagnetic, but the Pauli exclusion principle keeps them from falling into the core. This is why matter has volume, and why different objects cannot occupy the same point in space. 

You can find more details about the Pauli exclusion principle here.

Then there is the interaction between atoms that make up the different structures in nature, the rocks and the whole Earth (solids) beneath you, for instance. In these structures the atoms are bound together, mostly by electromagnetic interactions, and to pass some other atoms between them you have to invest some energy against that bound. For certain objects this energy is a lot.

Think of crystals. Our puny human fingers are way too weak to "pass between" the crystal atoms. If we build some strong machines, those can pass between the atoms in a crystal - that's called cutting. Like when the silicon crystal is sliced in a computer chip factory. Needs a lot of energy...

Then there other objects that are much less bound - you can e.g. pass between the molecules of water, it's called swimming (could be swimming in other liquids too). Those molecules are not interacting with each other much. You still cannot make your atoms occupy the same space as the H or O atoms in the water, and that's because of Pauli. And of course, you are walking around on the surface of the Earth - passing between the atoms of the air, because atoms and molecules in gasses are not bound to each other at all, you just have to move them out of your way (Pauli) but no bounding (EM) to work agains.

So in this case you mentioned, the bounding force between the atoms that make up the material beneath you is stronger than the gravitational force pushing you and the ground together, thus not enough to break those bounds and let you "slide down".
AnswerKenntnis:
Coulomb repulsion including the inverse square dependence has been experimentally verified down to nuclear lengths (by Rutherford's gold foil experiment) and even the fm (femtometer) scale (Breton V et al 1991 Phys. Rev. Lett. 66 572ΓÇô5).  

To get 1000 N of force from a mol of electrons (seems like a reasonable order of magnitude for a contact area) we need to get them $ r = \sqrt{10^{23}k e^2 / 10^3} \approx 10^{-4} $m apart.   This shows that the Coulomb repulsion is more than sufficient to hold us up before we get even close to the quantum mechanical regime. 

More importantly, the Pauli Exclusion Principle is not a force.  It simply says two fermions, in this case electrons, cannot occupy the same quantum state.  It doesn't say that fermions necessarily push apart if you were to get them very close to the same quantum state (whatever that means), nor does it say that fermionic wavefunctions cannot occupy the same space (as in, x,y,z).  I think if you look at the charge distribution in orbitals of any molecule or atom, you will find plenty of non-negligible spatial overlap.
AnswerKenntnis:
My first answer contained a very sloppy statement that EM interaction is irrelevant because of neutrality of matter which had to be corrected. I consider interaction of two neutral atoms and leave out the question of why some matter is solid and other is not.


Higher-order EM interaction does exist between atoms. What is maybe counterintuitive, it is an attractive interaction called van der Waals force. 
Electrons repel each other but manage to stay together even if there are 50 of them in an atom. We could imagine moving two hydrogen atoms together until they (almost) form a helium atom. Electric repulsion is not going to become infinite, the total energy would be in fact lower. Electric repulsion does not prevent one atom going through another.
However, two electrons in the same state just can not exist close to each other (Pauli exclusion) and the atomic orbitals would have to distort to avoid overlap. There is no special force that does that so eventually it comes back to an electromagnetic interaction but the fact is that without Pauli exclusion this interaction would not happen.


I should also mention the name of Lennard-Jones whose potential describes an interaction between two atoms. Note the $r^{-12}$ term for the short-range Pauli repulsion.
QuestionKenntnis:
Why do chimneys have these spiral "wings"?
qn_description:
While walking around I noticed something very peculiar. Many chimneys had spiral "wings", while others didn't. I came up with two possibilities:


The wind circles around the chimney upwards which pushes whatever gases being released even higher into the sky.
The wind circles around the chimney downwards which prevent the chimney from going left or right and rather "push" it downwards to make it more steady.


I feel that both of those possibilities are silly. So, why do some chimneys have those spiral "wings"? And why other taller chimneys don't?
AnswersKenntnis
AnswerKenntnis:
The spirals are used to prevent the formation of K├írm├ín vortex sheets downwind of the chimney.  They work by diverting the wind upwards on one side of the chimney and downwards on the other, creating a three-dimensional airflow pattern that disrupts the vortex sheet.  Without them, the vortex shedding could cause vortex-induced vibration in the chimney, which in strong winds might be enough to damage the (relatively thin-walled and flexible) chimney.

Here's a very nice animation of K├írm├ín vortex shedding from Wikipedia, courtesy of Cesareo de La Rosa Siqueira:



The circle on the left represents the (smooth) chimney, viewed from above, with wind coming from the left; the cyan and purple dots are tracer particles showing how air passing the chimney on either side joins the vortex train generated behind the chimney.  The K├írm├ín vorticity is an essentially two-dimensional phenomenon; in three dimensions, each vortex would basically be a tall rotating column of air, with one end on the ground and the other end joining together with the adjacent vortices in a complex turbulent region at the altitude of the top of the chimney.

As the vortices are shed on alternate sides of the chimney, each one imparts a counter-force on the chimney itself.  Under suitable conditions, these oscillating forces could drive the chimney itself to vibrate from side to side.  The helical projections on the chimney prevent this by disrupting the vortices as they form, or at least causing them to form out of phase at different altitudes.
AnswerKenntnis:
Apart from material construction which has a large effect on the natural frequency of the tower, height and regional wind characteristics have a large play in which ones have them and which do not.
QuestionKenntnis:
How does hot water clean better than cold water?
qn_description:
I had a left over coffee cup this morning, and tried to wash it out.  I realized I always instinctively use hot water to clean things as it seems to work better.  

A google search showed other people with similar results, but this yahoo answer is a bit confusing in terms of hot water "exciting" dirt.    

What is the physical interaction between hot water and oil or a material burnt onto another vs the cold water interaction?
AnswersKenntnis
AnswerKenntnis:
The other answers are correct, but I think that you might benefit from a more "microscopic" view of what is happening here.

Whenever one substance (a solute) dissolves in another (a solvent), what happens on the molecular scale is that the solute molecules are surrounded by the solvent molecules.

What causes that to happen? As @Chris described, there are two principles at work - thermodynamics, and kinetics.

In plain terms, you could think of thermodynamics as an answer to the question "how much will dissolve if I wait for an infinite amount of time," whereas kinetics answers the question "how long do I have to wait before X amount dissolves." Both questions are not usually easy to answer on the macroscopic scale (our world), but they are both governed by two very easy to understand principles on the microscopic scale (the world of molecules): potential and kinetic energy.

Potential Energy

On the macroscopic scale, we typically only think about gravitational potential energy - the field responsible for the force of gravity. We are used to thinking about objects that are high above the earth's surface falling towards the earth when given the opportunity. If I show you a picture of a rock sitting on the surface of the earth:



And then ask "Where is the rock going to go?" you have a pretty good idea: it's going to go to the lowest point (we are including friction here).

On the microscopic scale, gravitational fields are extremely weak, but in their place we have electrostatic potential energy fields. These are similar in the sense that things try to move to get from high potential energy to lower potential energies, but with one key difference: you can have negative and positive charges, and when charges have the opposite sign they attract each other, and when they have the same sign, they repel each other.

Now, the details of how each individual molecule gets to have a particular charge are fairly complicated, but we can get away with understanding just one thing:

All molecules have some attractive potential energy between them, but the magnitude of that potential energy varies by a lot. For example, the force between the hydrogen atom on one water molecule ($H_2O$) and the oxygen atom on another water molecule is roughly 100 times stronger than the force between two oxygen molecules ($O_2$). This is because the charge difference on water molecules is much greater (about 100 times) than the charge difference on oxygen molecules.

What this means is we can always think of the potential energy between two atoms as looking something like this:



The "ghost" particle represents a stationary atom, and the line represents the potential energy "surface" that another atom would see. From this graph, hopefully you can see that the moving atom would tend to fall towards the stationary atom until it just touches it, at which point it would stop. Since all atoms have some attractive force between them, and only the magnitude varies, we can keep this picture in our minds and just change the depth of the potential energy "well" to make the forces stronger or weaker.

Kinetic Energy

Let's modify the first potential energy surface just a little bit:



Now if I ask "where is the rock going to go?," It's a little bit tougher to answer. The reason is that you can tell the rock is "trapped" in the first little valley. Intuitively, you probably can see that if it had some velocity, or some kinetic energy, it could escape the first valley and would wind up in the second. Thinking about it this way, you can also see that even in the first picture, it would need a little bit of kinetic energy to get moving. You can also see that if either rock has a lot of kinetic energy, it will actually go past the deeper valley and wind up somewhere past the right side of the image.

What we can take away from this is that potential energy surfaces tell use where things want (I use the term very loosely) to go, while kinetic energy tells us whether they are able to get there.

Let's look at another microscopic picture:



Now the atoms from before are at their lowest potential energy. In order for them to come apart, you will need to give them some kinetic energy.

How do we give atoms kinetic energy? By increasing the temperature. Temperature is directly related to kinetic energy - as the temperature goes up, so does the average kinetic energy of every atom and molecule in a system.

By now you might be able to guess how increasing the temperature of water helps it to clean more effectively, but let's look at some details to be sure.

Solubility

We can take the microscopic picture of potential and kinetic energies and extract two important guidelines from it:


All atoms are "sticky," although some are stickier than others
Higher temperatures mean that atoms have larger kinetic energies


Going back to the coffee cup question, all we need to do now is think about how these will play out with the particular molecules you are looking at.

Coffee is a mixture of lots of different stuff - oils, water-soluble compounds, burnt hydrocarbons (for an old coffee cup), etc. Each of these things has a different "stickiness." Oils are not very sticky at all - the attractive forces between them are fairly weak. Water-soluble compounds are very "sticky" - they attract each other strongly because they have large charges. Since water molecules also have large charges, this is what makes water-soluble compounds water-soluble - they stick to water easily. Burnt hydrocarbons are not very sticky, sort of like oils.

Since molecules with large charges tend to stick to water molecules, we call them hydrophilic - meaning that they "love" water. Molecules that don't have large charges are called hydrophobic - they "fear" water. Although the name suggests they are repelled by water, it's important to know that there aren't actually any repelling forces between water and hydrophobic compounds - it's just that water likes itself so much, the hydrophobic compounds are excluded and wind up sticking to each other.

Going back to the dirty coffee cup, when we add water and start scrubbing, a bunch of stuff happens:

Hydrophilic Compounds

Hydrophilic compounds dissolve quickly in water because they stick to water pretty well compared to how well they stick to each other and to the cup. In the case where they stick to each other or the cup better than water, the difference isn't huge, so it doesn't take much kinetic energy to get them into the water. So, warm water makes them dissolve more easily.

Hydrophobic Compounds

Hydrophobic compounds (oils, burnt stuff, most stains) don't stick to the water. They stick to each other a little bit (remember that the forces are much weaker compared to water since the charges are very small), but water sticks to itself so well that the oils don't have a chance to get between the water molecules. We can scrub them, which will provide enough energy to knock them loose and allow the water to carry them away, but if we were to increase the kinetic energy as well by increasing the water temperature, we could overcome both the weaker forces holding the hydrophobic compounds together, while simultaneously giving the water molecules more mobility so they can move apart and let the hydrophobic compounds in. And so, warmer water makes it easier to wash away hydrophobic compounds as well.

Macroscopic View

We can tie this back to the original thermodynamics vs. kinetics discussion. If you increase the temperature of the water, the answer to the question "How much will dissolve" is "more." (That was the thermodynamics part). The answer to "How long will it take" is "not as long" (kinetics).

And as @anna said, there are other things you can do to make it even easier. Soap for example, is made of long chain molecules with one charged end and one uncharged end. This means one end is hydrophilic, while the other end is hydrophobic. When you add soap to the picture, the hydrophilic end goes into the water while the hydrophobic end tries to surround the oils and burnt stuff. The net result is little "bubbles" (called micelles) made up of soap molecules surrounding hydrophobic molecules that are in turn surrounded by water.
AnswerKenntnis:
First, I would be very skeptical of anecdotal claims here - there are all sorts of psychological biases that can influence the perceived outcome.

That said, water is used to clean surfaces by way of carrying away large particles and dissolving small ones. In the case of carrying pieces of dirt away, any temperature will do.

When it comes to dissolving, there are two relevant (and often conflated) concepts: thermodynamics and kinetics.

By thermodynamics I mean how much material can be dissolved before the water becomes saturated. This matters if you leave something soaking in a small amount of water for a long time. If the water can't hold all the material you want it to dissolve away, then indeed it will leave some residue. Warmer water tends to dissolve slightly larger amounts of solids, but note this only applies if you keep the water warm throughout the soaking process.

In practice, it's usually rather difficult to saturate water. The limiting process is more often kinetics - how fast materials dissolve in the water. If you picture water as a collection of particles, warmer water has faster moving particles, and so it should come as no surprise that these collide more frequently with the material needing to be dissolved, doing the job faster. If you've ever tried to dissolve solid sugar into iced tea, you've experienced how kinetics can make the process nearly interminably slow, even though thermodynamically the ice water can hold the sugar in solution.
AnswerKenntnis:
Just to throw a counter-example in here: there may be some cases where the opposite is true.

Hot water can cause proteins to denature and coagulate (as in the case of egg white in boiling water), so you might do better to clean certain proteinaceous substances with colder water to avoid them coagulating and sticking to the side of the container.
AnswerKenntnis:
I'll give this as a partial answer, let's just call it the gross overview of the reasons. Hopefully somebody will come along and give a physics justification. If I get some extra time soon, I will try to expand it out.

Part of it is that hot water melts oil/grease/etc while cold water would solidify it. So oily/greasy things will come off easier when hot than cold. It depends on the nature of the fat whether it is liquid or solid at room temperature or at the cold-water temperature, but the general trend is there.

Also, hot water is a better solvent than cold water (which is why sugar is easier to mix into hot tea than cold). Hot water has the ability to store more dissolved solids than cold before it becomes saturated. So particulate matter will be carried away better by hot water than cold.
AnswerKenntnis:
Answering this question best requires defining the verb "clean", and the word "better" well. 

Some of the other answers are spectacular in getting into how hydrogen atoms have great attraction when bonded to oxygen in a water molecule and even how much gravitational forces compare on a micro or macro level, but none define the terms well enough to give a truly succinct answer, in my opinion. So I'll start with defining the important terms first, then the answer will be shorter, and probably less spectacular but maybe more useful. 

In this context, I'll say that "clean" means "to aid in the removal of a substance as thoroughly as possible" and I'll define "better" as "accomplishing this removal in a manner that is faster, easier, or apparently more thorough or complete." Now that we have that sorted out...

1) Think of every molecule of water as a tiny janitor, with a tiny broom and a tiny waste basket. Now imagine that when his waste basket is full he is done for the day...he can accomplish no more work, period.

2) When The tiny janitor is at his coldest his broom and his wastebasket work differently than when he's at his hottest. 

3) When he's at his coldest, his broom has very big bristles and smaller particles are missed by it...they don't get caught in the broom. Also, when he is at his coldest, his wastebasket won't stretch to fit very much waste either, so overall Mr. janitor is at his worst when he is cold. 

4) When he is at his hottest however, Mr Janitors broom becomes capable of moving smaller and smaller particles with every push of his broom. More like a fine brush than a stiff broom, but his broom acts more quickly too, as if making many more passes to collect smaller particles in the same amount of time. Also, when he is at his hottest, his waste basket can stretch to fit much more waste inside itself. Together these 2 improvements in performance add up to a lot better cleaning overall. 

There are only a couple cases where this might not be true, or quite as true when cold water actually prevents the waste material from breaking into smaller and smaller pieces. 

The first is soap. Hot water definitely cuts grease better, but cold water cuts soap better. You can test this when you wash your hands and see for yourself that rinsing the soap off your hands happens much faster with cold water, because it doesn't allow the soap to thin out as much. 

The second might be (I'm not sure) with existing dirty water. Rinsing dirty water with clean cold water is more effective that using hot water for pretty much the same reason. It does not allow for an increase of busy little janitors thinning out the dirty water. 

An extreme example of this would be to ask yourself which is easier, to clean up a frozen chunk of dirty ice off your living-room floor before it melts, or after. 

I know there is no science in my answers but I think these analogies have value all the same...thanks
AnswerKenntnis:
The molecules of hot water have high kinetic energy than the cold water. So they diffuse with dust particles easily. Thus carrying away impurities with it, hot water is better than cleaning than cold water.

I am a class 9 student. Our teacher explained us that the rate of diffusion increases with temperature.
AnswerKenntnis:
Hot water, by definition, has faster moving molecules than cold water. (The molecular speed translates into heat.) All other things being equal, faster moving molecules do a better job of cleaning than slower moving molecules (in "displacing" the dirt and transporting it away using water.
QuestionKenntnis:
Why do people categorically dismiss some simple quantum models?
qn_description:
Deterministic models. Clarification of the question:

The problem with these blogs is that people are inclined to start yelling at each other. (I admit, I got infected and it's difficult not to raise one's electronic voice.) I want to ask my question without an entourage of polemics.

My recent papers were greeted with scepticism. I've no problem with that. What disturbes me is the general reaction that they are "wrong". My question is summarised as follows: 

Did any of these people actually read the work and can anyone tell me where a mistake was made?

Now the details. I can't help being disgusted by the "many world" interpretation, or the Bohm-de Broglie "pilot waves", and even the idea that the quantum world must be non-local is difficult to buy. I want to know what is really going on, and in order to try to get some ideas, I construct some models with various degrees of sophistication. These models are of course "wrong" in the sense that they do not describe the real world, they do not generate the Standard Model, but one can imagine starting from such simple models and adding more and more complicated details to make them look more realistic, in various stages.

Of course I know what the difficulties are when one tries to underpin QM with determinism. Simple probabilistic theories fail in an essential way. One or several of the usual assumptions made in such a deterministic theory will probably have to be abandoned; I am fully aware of that. On the other hand, our world seems to be extremely logical and natural. 

Therefore, I decided to start my investigation at the other end. Make assumptions that later surely will have to be amended; make some simple models, compare these with what we know about the real world, and then modify the assumptions any way we like.

The no-go theorems tell us that a simple cellular automaton model is not likely to work. One way I tried to "amend" them, was to introduce information loss. At first sight this would carry me even further away from QM, but if you look a little more closely, you find that one still can introduce a Hilbert space, but it becomes much smaller and it may become holographic, which is something we may actually want. If you then realize that information loss makes any mapping from the deterministic model to QM states fundamentally non-localΓÇöwhile the physics itself stays localΓÇöthen maybe the idea becomes more attractive.

Now the problem with this is that again one makes too big assumptions, and the math is quite complicated and unattractive.
So I went back to a reversible, local, deterministic automaton and asked: To what extent does this resemble QM, and where does it go wrong? With the idea in mind that we will alter the assumptions, maybe add information loss, put in an expanding universe, but all that comes later; first I want to know what goes wrong.

And here is the surprise: In a sense, nothing goes wrong. All you have to assume is that we use quantum states, even if the evolution laws themselves are deterministic. So the probability distributions are given by quantum amplitudes. The point is that, when describing the mapping between the deterministic system and the quantum system, there is a lot of freedom. If you look at any one periodic mode of the deterministic system, you can define a common contribution to the energy for all states in this mode, and this introduces a large number of arbitrary constants, so we are given much freedom.

Using this freedom I end up with quite a few models that I happen to find interesting. Starting with deterministic systems I end up with quantum systems. I mean real quantum systems, not any of those ugly concoctions. On the other hand, they are still a long way off from the Standard Model, or even anything else that shows decent, interacting particles.

Except string theory. Is the model I constructed a counterexample, showing that what everyone tells me about fundamental QM being incompatible with determinism, is wrong? No, I don't believe that. The idea was that, somewhere, I will have to modify my assumptions, but maybe the usual assumptions made in the no-go theorems will have to be looked at as well. 

I personally think people are too quick in rejecting "superdeterminism". I do reject "conspiracy", but that might not be the same thing. Superdeterminism simply states that you can't "change your mind" (about which component of a spin to measure), by "free will", without also having a modification of the deterministic modes of your world in the distant past. It's obviously true in a deterministic world, and maybe this is an essential fact that has to be taken into account. It does not imply "conspiracy".

Does someone have a good, or better, idea about this approach, without name-calling? Why are some of you so strongly opinionated that it is "wrong"? Am I stepping on someone's religeous feelings? I hope not.

References: 

"Relating the quantum mechanics of discrete systems to standard canonical quantum mechanics",  arXiv:1204.4926 [quant-ph];

"Duality between a deterministic cellular automaton and a bosonic quantum field theory in $1+1$ dimensions", arXiv:1205.4107 [quant-ph];

"Discreteness and Determinism in Superstrings", arXiv:1207.3612 [hep-th].



Further reactions on the answers given. (Writing this as "comment" failed, then writing this as "answer" generated objections. I'll try to erase the "answer" that I should not have put there...)

First: thank you for the elaborate answers.

I realise that my question raises philosophical issues; these are interesting and important, but not my main concern. I want to know why I find no technical problem while constructing my model. I am flattered by the impression that my theories were so "easy" to construct. Indeed, I made my presentation as transparent as possible, but it wasn't easy. There are many dead alleys, and not all models work equally well. For instance, the harmonic oscillator can be mapped onto a simple periodic automaton, but then one does hit upon technicalities: The hamiltonian of a periodic system seems to be unbounded above and below, while the harmonic oscillator has a ground state. The time-reversible cellular automaton (CA) that consists of two steps $A$ and $B$, where both $A$ and $B$ can be written as the exponent of physically reasonable Hamiltonians, itself is much more difficult to express as a Hamiltonian theory, because the BCH series does not converge. Also, explicit $3+1$ dimensional QFT models resisted my attempts to rewrite them as cellular automata. This is why I was surprised that the superstring works so nicely, it seems, but even here, to achieve this, quite a few tricks had to be invented.

@RonMaimon. I here repeat what I said in a comment, just because there the 600 character limit distorted my text too much. You gave a good exposition of the problem in earlier contributions: in a CA the "ontic" wave function of the universe can only be in specific modes of the CA. This means that the universe can only be in states $\psi_1,\ \psi_2,\ ...$ that have the property $\langle\psi_i\,|\,\psi_j\rangle=\delta_{ij}$, whereas the quantum world that we would like to describe, allows for many more states that are not at all orthonormal to each other. How could these states ever arise? I summarise, with apologies for the repetition:


We usually think that Hilbert space is separable, that is, inside every infinitesimal volume element of this world there is a Hilbert space, and the entire Hilbert space is the product of all these.
Normally, we assume that any of the states in this joint Hilbert space may represent an "ontic" state of the Universe.
I think this might not be true. The ontic states of the universe may form a much smaller class of states $\psi_i$; in terms of CA states, they must form an orthonormal set. In terms of "Standard Model" (SM) states, this orthonormal set is not separable, and this is why, locally, we think we have not only the basis elements but also all superpositions. 
The orthonormal set is then easy to map back onto the CA states. 


I don't think we have to talk about a non-denumerable number of states, but the number of CA states is extremely large. In short: the mathematical system allows us to choose: take all CA states, then the orthonormal set is large enough to describe all possible universes, or choose the much smaller set of SM states, then you also need many superimposed states to describe the universe. The transition from one description to the other is natural and smooth in the mathematical sense. 

I suspect that, this way, one can see how a description that is not quantum mechanical at the CA level (admitting only "classical" probabilities), can "gradually" force us into accepting quantum amplitudes when turning to larger distance scales, and limiting ourselves to much lower energy levels only. You see, in words, all of this might sound crooky and vague, but in my models I think I am forced to think this way, simply by looking at the expressions: In terms of the SM states, I could easily decide to accept all quantum amplitudes, but when turning to the CA basis, I discover that superpositions are superfluous; they can be replaced by classical probabilities without changing any of the physics, because in the CA, the phase factors in the superpositions will never become observable.

@Ron I understand that what you are trying to do is something else. It is not clear to me whether you want to interpret $\delta\rho$ as a wave function. (I am not worried about the absence of $\mathrm{i}$, as long as the minus sign is allowed.) My theory is much more direct; I use the original "quantum" description with only conventional wave functions and conventional probabilities.



(New since Sunday Aug. 20, 2012)

There is a problem with my argument. (I correct some statements I had put here earlier). I have to work with two kinds of states: 1: the template states, used whever you do quantum mechanics, these allow for any kinds of superposition; and 2: the ontic states, the set of states that form the basis of the CA. The ontic states $|n\rangle$ are all orthonormal: $\langle n|m\rangle=\delta_{nm}$, so no superpositions are allowed for them (unless you want to construct a template state of course). One can then ask the question: How can it be that we (think we) see superimposed states in experiments? Aren't experiments only seeing ontic states?

My answer has always been: Who cares about that problem? Just use the rules of QM. Use the templates to do any calculation you like, compute your state $|\psi\rangle$, and then note that the CA probabilities, $\rho_n=|\langle n|\psi\rangle|^2$, evolve exactly as probabilities are supposed to do.

That works, but it leaves the question unanswered, and for some reason, my friends on this discussion page get upset by that.

So I started thinking about it. I concluded that the template states can be used to describe the ontic states, but this means that, somewhere along the line, they have to be reduced to an orthonormal set. How does this happen? In particular, how can it be that experiments strongly suggest that superpositions play extremely important roles, while according to my theory, somehow, these are plutoed by saying that they aren't ontic? 

Looking at the math expressions, I now tend to think that orthonormality is restored by "superdeterminism", combined with vacuum fluctuations. The thing we call vacuum state, $|\emptyset\rangle$, is not an ontological state, but a superposition of many, perhaps all, CA states. The phases can be chosen to be anything, but it makes sense to choose them to be $+1$ for the vacuum. This is actually a nice way to define phases: all other phases you might introduce for non-vacuum states now have a definite meaning.

The states we normally consider in an experiment are usually orthogonal to the vacuum. If we say that we can do experiments with two states, $A$ and $B$, that are not orthonormal to each other, this means that these are template states; it is easy to construct such states and to calculate how they evolve. However, it is safe to assume that, actually, the ontological states $|n\rangle$ with non-vanishing inner product with $A$, must be different from the states $|m\rangle$ that occur in $B$, so that, in spite of the template, $\langle A|B\rangle=0$. This is because the universe never repeats itself exactly. My physical interpretation of this is "superdeterminism": If, in an EPR or Bell experiment, Alice (or Bob) changes her (his) mind about what to measure, she (he) works with states $m$ which all differ from all states $n$ used previously. In the template states, all one has to do is assume at least one change in one of the physical states somewhere else in the universe. The contradiction then disappears.  

The role of vacuum fluctuations is also unavoidable when considering the decay of an unstable particle.

I think there's no problem with the above arguments, but some people find it difficult to accept that the working of their minds may have any effect at all on vacuum fluctuations, or the converse, that vacuum fluctuations might affect their minds. The "free will" of an observer is at risk; people won't like that.

But most disturbingly, this argument would imply that what my friends have been teaching at Harvard and other places, for many decades as we are told, is actually incorrect. I want to stay modest; I find this disturbing.

A revised version of my latest paper was now sent to the arXiv (will probably be available from Monday or Tuesday). Thanks to you all. My conclusion did not change, but I now have more precise arguments concerning Bell's inequalities and what vacuum fluctuations can do to them.
AnswersKenntnis
AnswerKenntnis:
I can tell you why I don't believe in it. I think my reasons are different from most physicists' reasons, however.

Regular quantum mechanics implies the existence of quantum computation. If you believe in the difficulty of factoring (and a number of other classical problems), then a deterministic underpinning for quantum mechanics would seem to imply one of the following. 


There is a classical polynomial-time algorithm for factoring and other problems which can be solved on a quantum computer.
The deterministic underpinnings of quantum mechanics require $2^n$ resources for a system of size $O(n)$.
Quantum computation doesn't actually work in practice.


None of these seem at all likely to me. For the first, it is quite conceivable that there is a polynomial-time algorithm for factoring, but quantum computation can solve lots of similar periodicity problems, and you can argue that there can't be a single algorithm that solves all of them on a classical computer, so you would have to have different classical algorithms for each classical problem that a quantum computer can solve by period finding.

For the second, deterministic underpinnings of quantum mechanics that require $2^n$ resources for a system of size $O(n)$ are really unsatisfactory (but maybe quite possible ... after all, the theory that the universe is a simulation on a classical computer falls in this class of theories, and while truly unsatisfactory, can't be ruled out by this argument). 

For the third, I haven't seen any reasonable way to how you could make quantum computation impossible while still maintaining consistency with current experimental results.
AnswerKenntnis:
This could have been a comment, but as it actually anwers the question asked in the title, I'll post it as such:

As far as I can tell there's no rational reason to dismiss these models out of hand - it's just that quantum mechanics (QM) has set the bar awfully high: So far, there's no experimental evidence that QM is wrong, and no one has come up with a viable alternative.

Ultimately, your theory needs to reproduce all experimentally verified predictions of QM (or rather may only deviate within the experimental precision). However, there's of course no need to reproduce arbitrary predictions - in fact, if you did, you'd end up with a re-formulation - ie a new interpretation - of ordinary QM. If your model tells us large-scale quantum computation is impossible, then it's up to the experimentalists to prove you wrong.

Any objections beyond that are just psychology at work: It takes quite some effort for most people to convince themselves that QM is a valid description of the world we live in, and once such a believe is ingrained, it easily becomes dogma.
AnswerKenntnis:
[ text I had put here is moved to the original question, but I prefer not to erase the comments that were posted here. ]
AnswerKenntnis:
Foundational discussions are indeed somewhat like discussions about religious convictions, as one cannot prove or disprove assumptions and approaches at the foundational level. 

Moreover, it is in the nature of discussions on the internet that one is likely to get responses mainly from those who either disagree strongly (the case here) or who can add something constructive (difficult to do in very recent research). I think this fully explains the responses that you get.

I myself read superficially through one of your articles on this and found it not promising enough to spend more time on the technical issues.

However, I agree that both many-worlds and pilot waves are unacceptable physical explanations of quantum physics, and I am working on an alternative interpretation. 

In my view, particle nonlocality is explained by negating particles any ontological existence. Existent are quantum fields, and on the quantum field level, everything is local. Nonlocal features appear only when one is imposing on the fields a particle interpretation, which, while valid under the usual asssumptions of geometric optics, fails drastically art higher resolution. Thus nothing needs to be explained in the region of failure. Just as the local Maxwell equations for a classsical electromagnetic field explain single photon nonlocality (double slit experiments), and the stochastic Maxwell equations explain everything about single photons (see http://www.mat.univie.ac.at/~neum/ms/optslides.pdf), so local QFT explains general particle nonlocality.

My thermal interpretation of quantum mechanics (see the section http://www.mat.univie.ac.at/~neum/physfaq/topics/found0.html from my theoretical physics FAQ at http://www.mat.univie.ac.at/~neum/physfaq/physics-faq.html, and Chapter 10 of my book http://lanl.arxiv.org/abs/0810.1019) gives a view of physics consistent with actual experimental practice and without any of the strangeness introduced by the usual interpretations. I believe this interpretation to be satisfactory in all respects, though it requires more time and effort to analyse the standard conundrums along these lines, with a clear statistical mechanics derivation to support my so far mainly qualitative arguments.

In presenting my foundational views in online discussions, I had similar difficulties as you; see, e.g., the PhysicsForums thread
   ''What does the probabilistic interpretation of QM claim?''
   http://www.physicsforums.com/showthread.php?t=480072
AnswerKenntnis:
There are two questions here: Why criticize your models? And are there better ideas? I will try to answer the second question in a separate answer. Here I only give some comments of a general nature to adress the first question.

I personally agree with you, and I think most people who care about this stuff do too, that it is disconcerting to have a theory in which the information produced by observations is not contained in the theory itself, but is produced out of thin-air by an act of measurement. The natural idea is that when we see a bit of information produced through an act of observation, then the value of this bit was somehow contained in the complete description of nature independent of the act of observation. This was Einstein's reality principle, and I agree that it is preferred for a theory to obey it.

When a theory doesn't obey the reality principle, one has to note that macroscopic reality does obey it, and find the bits in the macroscopic world by a philosophically contorted roundabout exercize in mysticism. But since physics is empirical, and positivism is fruitful, I take the point of view that any framework that explains the results of observations must ultimately be philosophically ok, even if it requires contortions, and even if it is not correct! So Newton's mechanics, even though it is wrong, is not necessarily empirically disproved given only observations of human beings and so on, so it must not be philosophically incompatible with free will. Similarly, quantum mechanics might be wrong, but we have no empirical data that shows it is wrong, so it should be philosopically consistent to say QM is all there is. This means QM should describe observers too, and if there is no mathematical contradiction with this view, there should not be a philosophical contradiction either, even if there is a contradiction with experiment. This is the many-worlds philosophy, and it's the self-consistent answer if quantum mechanics is correct. It might be annoying, but I don't think it is too annoying--- one should just learn to live with many-worlds as a fine philosophical position.

But it is wrong to just say "many-worlds" at this point, because the quantum description has not been tested in the realm where the many-worlds have a real logical-positivist manifestation--- most obviously when doing factoring of enormously large numbers using a quantum computer. Until we do this, it is definitely conceivable that nature is only very closely approximately quantum for small systems of a few particles, in the cases we tested the theory already, and is just not quantum for highly entangled many particle systems.

Even if the world turns out to be really quantum, and a quantum computer factors numbers all the time, finding a deterministic substructure is useful for giving a computationally tractable small truncation of quantum mechanics in cases which are not a quantum computer, and it is possible that this truncation can be useful for quantum simulations. This is so needed that I think finding a substructure to quantum mechanics is a central important problem, personally, regardless of whether it turns out to be right. For this reason, I devoted much time to understanding your approach.

The problem with your construction is that it works too well, it is too easy to transform a quantum system into a beable basis, so that the global wavefunction is evolving in a way that is deterministic using the global Hamiltonian. Since you introduce the Hilbert space early and use it to do the transformation of basis to the internal states of the automaton, there is no obvious barrier to transforming a quantum computer into a beable basis, nor is there any barrier to violating Bell's inequality locally. These do not suggest that the no-go theorems are flawed, rather they suggest that the transformation to a beable basis with a permutation Hamiltonian does not produce a true classical system.

The precise way in which I believe that this system fails to be classical is in state-preparation on the interior. The process of state-preparation involves a measurement, entangling some interior subsystem with a macroscopic subsystem, and then a reduction of the macroscopic system according to Born's rule, leaving a pure quantum state of the interior subsystem. In your paper on Born's rule, you suggested how reduction should happen in a CA system, but your precise models don't really respect this intuition, in that the measurement of intermediate states always produces one of the eigenstates of the observable in the interior, no matter how complicated the observable and how entangled its eigenstates. This is what allows you to reproduce quantum mechanics on the interior subsystems, I am somewhat certain that this does not keep the state unsuperposed in the beable basis. Because these internal reductions don't respect the probability structure, you are really doing quantum mechanics not CA, and this is the only reason that you have such an easy time sidestepping the no-gos.

The fact that you sidestep the no-gos with no difficulty suggests strongly that your construction is leaving the space of allowed classical probability distributions on the CA somehow. The only place where this can happen is during internal state-preparation, during measurements of internal operators. This is how you prepare Bell states or quantum computers, after all. These interior operations must be producing states (after projection) which cannot be interpreted as classical probability states of the automaton, although the Hamiltonian evolution never does this. This is not a proof, but I would bet lots of money (if I had any). I asked for a proof here: In t'Hooft beable models, do measurements keep states classical?

This is part I of the answer, I post it separately, so that people who agree with this part don't have to upvote the second part, which is devoted to a different approach to getting quantum mechanics out of automata, to answer the second question.
AnswerKenntnis:
This question tries to reproduce quantum mechanics from classical automata with a probabilistically unknown state.

Probability distributions on Automata states

Start with a classical CA and a probability distribution on the CA. To keep things general, I allow the CA to have some non-determinstic evolution, but only stochastic probability, no quantum evolution, and it's not necessary, you can always put the probability in the initial conditions, with no stochasticity at intermediate times, it's just an option.

The first point about these stochastic systems is detailed here: Consequences of the new theorem in QM? (in the section on duck feet). If the flow of probability is always between states where the probability is only infinitesimally different from te stationary distribution, then the classical flow conserves entropy, and is reversable, even if it is probabilistic and diffusive. This is the central motivation for the construction, and one should review how the particle diffusing the in the heat exchanger diffuser bounces back and forth reversibly from room to room, in a linear way described by an operator with a mostly complex eigenvalue, even though it is at all times only diffusing between different allowed regions.

Consider a classical probability distribution on a CA, $\rho(B)$ where B is the state of all the bits comprising the automaton, then

$$ I = - \sum_B \rho(B) \log(\rho(B)) $$

is the information contained in fully knowing the automaton state, above that provided by the distribution. If you make a perturbation to first order, changing $\rho$ to $\rho+\delta\rho$, you find

$$ I = - \sum_B (\rho(B)+\delta\rho(B)) \log(\rho(B) + \delta\rho(B)) $$

When $\rho$ is uniform, the first order correction vanishes since the sum of $\delta\rho$ is zero, and the second order correction gives a quadratic metric structure on $\delta\rho$.

$$ I = - \sum_B \delta\rho(B)^2$$

This is what I identify as a pre-quantum structure on the space of perturbations to the uniform distribution. The reason it is so symmetrical (like a sphere, not like a simplex) is because the perturbation is small. The reversibility is required by conservation of entropy, and conservation of entropy requires that all the transformations on $\delta\rho$ are orthogonal.

The picture to zeroeth order is that nearly every state is equally likely, but some states are slightly more likely than others, and the information revealed by experiments is only producing a slight bias for some states rather than others. These slight biases then are more symmetric than the underlying space of probabilities on automata states, because these distributions never deviate enough from uniformity to see the corners of the probability space simplex. The corners are the states where the automata bits are known for certain, and if you are always far from these, you can find a symmetric and reversible probabilistic dynamics.

Here is the central problem with this approach--- it is impossible for an information containing perturbation $\delta \rho$ to be everywhere small. The reason is that an everywhere small $\delta \rho$ necessarily produces a state which is almost indistinguishable from the uniform state, and which therefore makes a perturbation which corresponds to you having learned much less than even 1 bit of information. For example, if you have N bit automaton, and you make a distribution where the probability of every bit value is between ${1\over 2} - \epsilon$ and ${1\over 2} + \epsilon$, you get an information content bounded above by a small multiple of $\epsilon$ bits.

The reason is that learning even one bit of information about an automaton state roughly cuts down the number of states you can occupy by a factor of 2. This means that the true probability distribution must be significantly small on at least half the configurations, and it cannot be a small perturbation. This means that the information expansion breaks down, and this is where I was stuck for a long time

Locally small perturbations

The reason the notion of "small perturbation" is failing is because a small perturbation, as in the duck-feet example, is not globally small, it only has the property that the ratio of the probabilities between two nearby states is small. If the states are made by independently varying lots of bits, there are many states with the same ratio of probability.

The fix might as well be the following easy trick: just raise everything to the M-th power. If you have a system with states indexed by i an integer in the range 1,2,...,N, and a perturbation

$$(\rho_i + \delta\rho_i) $$

You can take the M-th tensor power of $\rho$, to produce a product distribution on the tensor space with M indices $i_1,i_2,...,i_M$. This product distribution is defined by the condition that changing every value i from one value to another produces the same ratio change in probability.

Now it is allowed for $\delta\rho$ to be small even when the information in $\delta\rho$ is not, because the M-th power isn't small at all. In fact, in this system, because it's a tensor product, if you know that the information content of $\rho+\delta\rho$ is I bits overall, then you learn that

$$ M \sum_B \delta\rho^2  = 1 $$ 

In other words, the finite information perturbations to the stationary distribution on a system with M-copies forms a (real, not complex) Hilbert space, ever more perfectly as M goes to infinity. If the dynamics is duck-feet, meaning that the entropy is conserved with the small perturbation, then the time evolution of $\delta\rho$ is necessarily an orthogonal transformation, no matter what the underlying stochastic or deterministic evolution law is.

The basic idea is that you can make a quantum mechanics emerge from stochastic evolution of systems with many identical copies, under the condition that the copies interact symmetrically with each other, so that you don't know which copy is which.

To see how the inner product comes out, you consider the mutual information, which tells you how independent two different distributions are. To lowest order, this is found by taking the information in $\delta\rho_1$ and $\delta\rho_2$ and subtracting the information in $\delta\rho_1$ and $\delta\rho_2$ separately. Since these are the norms, you find

$$ I_{12} = ||\delta\rho_1 + \delta\rho_2 ||^2 - || \delta\rho_1 ||^2 - ||\delta\rho_2||^2 = \langle\rho_1 , \rho_2 \rangle $$

So that if you have two distributions, they share states to the degree that their inner product is nonzero.
AnswerKenntnis:
There's no doubt it's possible to reproduce quantum integrable models rather efficiently and simply using classical systems. And of all integrable systems, harmonic oscillators are one of the simplest. The real challenge is to reproduce quantum nonintegrable systems. Can you reproduce quantum chaos? Can you reproduce quantum nonintegrable spin models over a 1d spatial lattice? Trying perturbation theory from an integrable models runs into the problem that the number of feynman diagrams grows exponentially with the number of loops.
QuestionKenntnis:
What makes running so much less energy-efficient than bicycling?
qn_description:
Most people can ride 10 km on their bike. However, running 10 km is a lot harder to do. Why?

According to the law of conservation of energy, bicycling should be more intensive because you have to move a higher mass, requiring more kinetic energy to reach a certain speed. But the opposite is true. 

So, to fulfill this law, running must generate more heat. Why does it?

Some things I can think of as (partial) answers:


You use more muscles to run.
While running, you have more friction with the ground; continuously pouncing it dissipates energy to it.
While you move your body at a slow speed, you need to move your arms and legs alternately at higher and lower speeds.
AnswersKenntnis
AnswerKenntnis:
One word: inertia. When you're riding a bike on a level gradient you just need to give it a push to get going, then you can coast for quite a while before friction and air resistance slow you down. The human body doesn't have wheels that can store kinetic energy, so while running you have to give a good kick to get going, and then another kick to keep going on the next step, and so on. When hills are involved the difference is even more pronounced, since we run downhill the same way we do on the level, by continually pushing ourselves forward; whereas on a bicycle you can take advantage of the slope and just coast down it.

I suspect that raising and lowering your centre of mass isn't as inefficient as the other answers have suggested. This is because your legs are springy, so at least to some extent you're just converting energy back and forth between gravitational potential and the spring force in your legs. Humans are possibly the most efficient long-distance runners in the animal kingdom. There is a school of thought that says the reason we are bipeds is that we evolved as endurance hunters, chasing our prey until it collapsed from exhaustion rather than trying to outrun it over short distances. Whether that's true or not, we probably wouldn't do all that bouncing up and down if there wasn't a good reason for it. 

You might ask why, if using wheels is so much more efficient, didn't we evolve that instead? I don't know, but it seems no animal has been able to evolve wheeled locomotion.
AnswerKenntnis:
Many of  us have ridden  bicycles at
  some time in our lives. and in fact
   this mode of transportation has become
  markedly more popular recently as a result of the energy
  shortage.  Each morning at  my own
  university, Duke, people can be
  seen riding machines with masses
   of $10$ to $20$  kilograms and struggling
  to  reach one  of the  campus entrances
  at the top of a long,  steep hill.
  As in  many other aspects of animal
   locomotion, there is a paradox here.
  Why should people  encumber
  themselves with such  heavy apparatus,
  particularly while  going uphill?
  Ask  a rider this  question, and
  the response is usually: "It's easier
  than walking" or "It's faster than
  walking." But  why should it be?
  
  A number of incorrect  explanations
  are offered: "A bicycle  has gears."
  Shifting  gears allows the rider to
  vary the  speed at which the feet
  move; but even if  the foot speeds  of
  a cyclist and a pedestrian are
   matched, the cyclist still goes farther and in  less time on a  given
  amount of  energy than the pedestrian.
  "Your  weight is supported by
  the seat." But if you pedal  standing
  up, biking still is faster and less
   costly of energy than going  on foot.
  "Your center of gravity doesn't go
  up and down." But it  does if you
  pedal standing up. Why, then, is
  bicycling easier than walking  or
  running?
  
  [ΓÇª]*
  
  We can now appreciate why bicycle
  riders are willing to propel  the
  extra weight of a bicycle, even
  when going uphill. The cost of
  transport on a bicycle is low because active muscles are  not
  stretched while  pedaling, and mean
  muscle  efficiency is about $.25$, nearly
  its maximum value. The wheels
  stabilize the rider's  center of mass.
  Even if the rider  accelerates the
  center of mass vertically by  pedaling while standing up, active muscles
  need not be stretched.  When
  the center of mass falls, the cranks,
  sprockets, chain, and rear wheel
  constitute a system of  levers that
  transposes the vertical motion to a
  horizontal  one by supplying a perpendicular
  force.  Thus, humans can
  use external machinery to move
   along a level surface with the same
  muscular efficiencies that swimming and flying animals achieve
  naturally.


The Energetic Cost of Moving About: Walking and running are extremely inefficient forms of locomotion. Much greater efficiency is achieved by birds, fishΓÇöand bicyclists. V. A. Tucker,
American Scientist,
Vol. 63, No. 4 (July-August 1975), pp. 413-419



*Of course, most of the article is where I put "[...]". It's quite a good, fun read. There's even some kind of Galilean experiment with dropping pigeons and rats from heights.
AnswerKenntnis:
Bicycles make better use of inertia/momentum. As Nathaniel said, one push and you can coast for quite a while. That's just not possible while running.
Running wastes energy moving up and down. In addition to moving forward, running requires a substantial upward push to get your body airborne, giving you time to bring your other foot forward. You then cushion and spring forward and upward again. While bicycling does have an up-and-down component to the pedaling, because the bike doesn't leave the ground, the energy you use in pedaling is converted much more efficiently to forward motion.
Bicycling can translate weight to propulsion. While most serious bicyclists will tell you that pedaling is about spinning, not stepping, any 10-year-old can tell you that going butt-in-the-air and transferring your weight from left to right gets you going pretty quickly.
Pedaling with toe clips makes use of the entire leg motion. When your feet are locked to the pedals, it isn't just the pushing-down portion of the pedal stroke that is used; lifting your foot, pulling it forward, pushing down and pushing backward all keep tension on that chain and so add power to the stroke. When running, fully half of your foot's cycle is wasted energy from a forward-motion perspective.
Bicycling gives you mechanical advantage. Even with a single-gear bike, the motion of your foot is magnified when translated to the wheel. On a multi-speed bike, the ratio of the top gear is pretty high indeed. This allows two things; first, your effort is magnified, and second, your tempo slows, which reduces the amount of energy wasted moving the weight of your legs around.
AnswerKenntnis:
Running requires intense muscle contractions with a low duty cycle, whereas cycling uses long, smooth contractions. If you work at running, you can easily get to the point where it is no longer an aerobic challenge, yet still tough: you hardly have to breathe, yet the legs struggle with lactic acid buildup.
Running wastes a lot more energy on compensating motions: the runner has to move her torso and arms to compensate the kicking motion of the legs.  The leg motion is not symmetric in running: the forward kick of the recovering leg is faster than the backward movement of the drive leg, and so the arm on the same side as the recovering leg has to swing backward to compensate its motion.  Compensation on a bike is mostly limited to a side-to-side rocking during a hard effort.   
The runner basically flies through the air, but periodically comes down and touches the ground with one foot, and then exerts a force in order to get back up into the air. However, this is not done by an efficient, elastic bounce. The landing energy is dissipated, rather than stored and reused. In fact, the runner must exert energy in order to absorb the landing, and then exert more energy to get back into the air. The runner thus wastes considerable energy to stay in the air.
Depending on the nature of the footstrike, the runner may counter-productively be exerting energy which retards (brake) his forward motion, and then has to exert more energy to recover momentum.
AnswerKenntnis:
A lot of the answers here go into movement of your center of gravity etc. I think it's a lot simpler than that. 

When you are cycling your vertical movement on the pedals is translated into horizontal movement of the wheels, coupled with inertia a small amount of energy can go a long way. 

Whereas whilst running you are putting energy into moving horizontally, to get places, as well as vertically to reduce friction with the floor. However all the vertical movement is fighting gravity, and wasted as your vertical movement gets you no closer to your destination

This answer assumes that the distance is over a flat surface. As soon as you throw a three dimensional aspect on it then the system turns on its head. 

For horizontal movement the bike is best as its wheels are designed to reduce friction, unlike feet. 

However as soon as you come to an incline the lack of friction will cause the bike to roll backwards unless constant energy is put in to the system, whereas the increased friction one has from running allows anyone to stop and stay put. 

As soon as you get to an incline from a standing start, running is much more efficient than cycling.
AnswerKenntnis:
To expand on Nicks answer, when you run, you sort of jump a little bit so that you raise your center of mass, which costs you energy equal to

$$\Delta E = m g \Delta h$$

Now when you lower your center of mass, the energy is dissipated as the vertical acceleration gained by going down is not increasing your horizontal speed.

This is definitely one of the causes.

Also, one can think about dissipated power, which might give us more insight. For example we have the following identity:

$$ P = \cfrac{\operatorname{d} W}{\operatorname{d} t} = \vec{F} \cdot \cfrac{\operatorname{d} \vec{x}}{\operatorname{d} t} = \vec{F} \cdot \vec{v}$$

Also, let's assume, that power losses in both cases are similar and that the input power is the same in both cases (we use similar muscles in both cases after all).

Now we have the velocity very different in both cases, and we could probably also agree, that there is much more force produced if we run as we can speed up to our maximum speed very quickly. So far everything agrees with the formulae.

Now we can easily convince ourselves, that even if the efficiency is similar in both cases, the loss of energy in running would be greater because we do it for longer for the same distance travelled.
AnswerKenntnis:
It's down to the biological inefficiency of maintaining the mechanical constraint of one foot stationary wrt the floor, with the rest of the body moving. 

Let's suppose a skater on ice and another on rollers use approximately the same amount of energy to get to say 5 km/s. Both possess a different, but efficient compared to walking, mechanism that maintains that velocity, while satisfying any mechanical constraints. For the one on rollers, the point of contact between the ground and a roller must be stationary. For walking, the foot in contact with the ground must be stationary, and the biological mechanism for this introduces far greater biological losses.

However, there are far more efficient modes of biological transport such as hopping as in the case of Kangaroos, which uses half the energy of a marathon runner. Kinetic energy is converted into potential energy in the tendons while the mechanical constraint is maintained, with most converted back into kinetic energy upon leaving the gound.
AnswerKenntnis:
A quick guess would be that for running you are lifting your centre of mass with every step while with cycling your centre of mass has a constant height.  Therefore you are only doing work against air resistance/friction in the bike mechanics.  While running you are also working against gravity.
AnswerKenntnis:
We use muscles instead of wheels.

Using muscles involves friction inside your muscles, muscles act as both dampers and spring, but also as an actionner (like an engine). It uses many cells to do it.

The "isn't there any species which can rotate their limbs infinitely" question has already been asked.

From an evolutionary standpoint, you have to keep in mind that in biology, "parts" are always connected, so that the flow of blood and nerve can never be interrupted. In electricity, it's also a problem to make signals or a current flow between two rotating parts.

Biology is very flexible in its "design" (or whatever you call it), but keep in mind that live being are made of cells, even the bones or the horns and nails !

Maybe a viable way to evolve wheels, would be to dip them in some fluid that keep them alive inside the body, and when the animal wants to roll, he get it out and pushes himself to use inertia.

But don't ask me about rotational locomotion, that would be really hard to achieve/evolve for a live being.

Don't forget evolution works very much differently compared to man's inventions.
AnswerKenntnis:
The difference is in the underlying mechanism which transforms the chemical energy into kinetic energy of the vehicle or body. The answer is that the bicycle mechanism (due to having wheels etc..) is able to transform this energy better.

A classic analog is the lever or a pulley. One can use a lever or a pulley to lift a weight which would be very difficult (or even impossible) to lift with bare hands.

So to maintain same average speed with a bicycle one needs to use less chemical energy (than running) and as a result produce less heat.


  "Give me a place to stand and I can move the earth"   


                                            -- Archimedes on the principe of the lever (allegedly)
QuestionKenntnis:
Reading the Feynman lectures in 2012
qn_description:
The Feynman lectures are universally admired, it seems, but also a half-century old.
Taking them as a source for self-study, what compensation for their age, if any, should today's reader undertake?  I'm interested both in pointers to particular topics where the physics itself is out-of-date, or topics where the pedagogical approach now admits attestable improvements.
AnswersKenntnis
AnswerKenntnis:
The Feynman Lectures only need a little amending, but it's a relatively small amount compared to any other textbook. The great advantage of the Feynman Lectures is that everything is worked out from scratch Feynman's way, so that it is taught with the maximum insight, something that you can only do after you sit down and redo the old calculations from scratch. This makes them very interesting, because you learn from Feynman how the discovering gets done, the type of reasoning, the physical intuition, and so on.

The original presentation also makes it that Feynman says all sorts of things in a slightly different way than other books. This it is good to test your understanding, because if you only know something in a half-assed way, Feynman sounds wrong. I remember that when I first read it a million years ago, a large fraction of the things he said sounded completely wrong. This original presentation is a very important component, it teaches you what originality sound like, and knowing how to be original is the most important thing.

I think vol I is pretty much ok as an intro, although it should be supplemented at least with this stuff:


Computational integration: Feynman does something marvellous at the start of volume I (something unheard of in 1964), he describes how to Euler time-step a differential equation forward in time. Nowadays, it is a simple thing to numerically integrate any mechanical problem, and experience with numerical integration is essential for students. The integration removes the student's paralysis: when you are staring at an equation and don't know what to do. If you have a computer, you know exactly what to do! Integrating reveals many interesting qualitative things, and show you just how soon the analytical knowledge paistakingly acquired over 4 centuries craps out. For example, even if you didn't know it, you can see the KAM stability appear spontaneously in self-gravitating clusters at a surprisingly large number of particles. You might expect chaotic motion until you reach 2 particles, which then orbit in an ellipse. But clusters with random masses and velocities of some hundreds of particles eject out particles like crazy, until they get to one or two dozen particles, and then they settle down into a mess of orbits, but this mess must be integrable, because nothing else is ejected out anymore! You discover many things like this from piddling around with particle simulations, and this is something which is missing from volume I, since computers were not available. It's not completely missing, however, and it's much worse elsewhere.
The Kepler problem: Feynman has an interesting point of view regarding this which is published in the "Lost Lecture" book and audio-book. But I think the standard methods are better here, because the 17th century things Feynman redoes are too specific to this one problem. This can be supplemented in any book on analytical mechanics.
Thermodynamics: The section on thermodynamics does everything through statistical mechanics and intuition. This begins with the density of the atmosphere, which motivates the Boltzmann distribution, which is then used to derive all sorts of things, culminating in the Clausius Clayperon equation. This is a great boon when thinking about atoms, but it doesn't teach you the classical thermodynamics, which is really simple starting from modern stat-mech. The position is that the Boltzmann distribution is all you need to know, and that's a little backwards from my perspective. The maximum entropy arguments are better--- they motivate the Boltzmann distribution. The heat-engine he uses is based on rubber-bands too, and yet there is no discussion of why rubber bands are entropic, nor of free-energies in the rubber band or the dependence of stiffness on temperature.
Monte-Carlo simulation: this is essential, but it obviously requires computers. With monte-carlo you can make snapshots of classical statistical systems quickly on a computer and build up intuition. You can make simulations of liquids, and see how the atoms knock around classically. You can simulate rubber-band polymers, and see the stiffness depend on temperature. All these things are clearly there in Feynman's head, but without a computer, it's hard to transmit it into any of the student's heads.


For volume II, the most serious problem is that the foundations are off. Feynman said he wanted to redo the classical textbook point of view on E&M, but he wasn't sure how to do it. The Feynman lectures were written at a time just before modern gauge theory took off, and while they emphasize the vector potential a lot compared to other treatments of the time, they don't make the vector potential the main object. Feynman wanted to redo volume II to make it completely vector potential, but he didn't get to do it. Somebody else did a vector-potential based discussion of E&M based on this recommendation, but the results were not so great.

The major things I don't like in vol II:


The derivation of the index of refraction is done by a complicated rescattering calculation which is based on a plum-pudding style electron oscillators. This is essentially just the forward-phase index-of-refraction argument Feynman gives to motivate unitarity in the 1963 ghost paper in Acta Physica Polonika. It is not so interesting or useful in my opinion in vol II, but it is the most involved calculation in the series.
No special functionology: While the subject is covered with a layer of 19th-century mildew, It is useful to know some special functions, especially Bessel functions and spherical harmonics. Feynman always chooses ultra special forms which give elementary functions, and he knows all the cases which are elementary, so he gets a lot of mileage out of this, but it's not general enough.
The fluid section is a little thin--- you will learn how the basic equations work, but no major results. The treatment of fluid flow could have been supplemented with He4 flows, where the potential flow description is correct (it is clear that this is Feynman's motivation for the strange treatment of the subject), but this isn't explicit.
Numerical methods in field simulation: here if one wants to write an introductory textbook, one needs to be completely original, because the numerical methods people use today are not so good for field equations of any sort.


Vol III is extremely good, because it is so brief. The introduction to quantum mechanics there gets you to a good intuitive understanding quickly, and this is the goal. It probably could use the following:


A discussion of diffusion, and the relation between Schrodinger operators and diffusion operators: this is obvious from the path integral, but it was also clear to Schrodinger. It also allows you to quickly motivate the exact solutions to Schrodinger's equation, like the 1/r potential, something which Feynman just gives you without motivation. A proper motivation can be given by using SUSY QM (without calling it that, just a continued stochastic equation) and trying out different ground state ansatzes.
Galilean invariance of the Schrodinger equation: this part is not done in any book, I think only because Dirac omitted it from his. It is essential to know how to boost wavefunctions. Since Feynman derives the Schrodinger equation from a tight-binding model (a lattice approximation), the Galilean invariance is not obvious at all.


Since the lectures are introductory, everything in there just becomes second nature, so it doesn't matter that they are old. The old books should just be easier, because the old stuff is already floating in the air. If you find something in the Feynman Lectures which isn't completely obvious, you should study it until it is obvious--- there's no barrier, the things are self-contained.
AnswerKenntnis:
I'm not sure what you mean by saying: "the physics is out-of-date," because in some sense Newtonian mechanics is out-of-date. But we know that it is an effective theory (the low-speed limit of relativity) and is important to study and understand since it describes everyday-life mechanics accurately. 

Feynman lectures are the classic 101/102 physics resource. So, just read and learn. And Feynman is a master in his pedagogical approach (remember the challenger case?) 

The only thing is that you're not going to learn the techniques of Quantum field theory or other advanced and more recent research-level topics from Feynman lectures.
QuestionKenntnis:
Does the 4/3 problem of classical electromagnetism remain in quantum mechanics?
qn_description:
In Volume II Chapter 28 of the Feymann Lectures on Physics, Feynman discusses the infamous 4/3 problem of classical electromagnetism.  Suppose you have a charged particle of radius $a$ and charge $q$ (uniformly distributed on the surface).  If you integrate the energy density of the electromagnetic field over all space outside the particle, you'll get the total electromagnetic energy, which is an expression proportional to $c^2$.  The energy divided by $c^2$ is what we usually call the mass, so if we calculate the "electromagnetic mass" in this manner we'll get $m = \frac{1}{2}\frac{1}{4\pi\epsilon_0}\frac{q^2}{ac^2}$.  If, on the other hand, you took the momentum density of the electromagnetic field and integrated it over all space outside the particle, you'd get the total electromagnetic momentum, which turns out (for $v<<c$) to be proportional to the velocity of the particle.  The constant of proportionality of momentum and velocity is what we call mass, so if we calculated the electromagnetic mass in this way we would get $m = \frac{2}{3}\frac{1}{4\pi\epsilon_0}\frac{q^2}{ac^2}$, which is $\frac{4}{3}$ times the value we got before!  That is the 4/3 problem.

Feynman claims that this fundamental issue remains when we move to quantum electrodynamics.  Was he right, and if so has the situation changed since the 1960's when he was writing?  I've seen claims on the Internet (I don't have the links) that the 4/3 problem is still there in QED, but instead of $\frac{4}{3}$ the coefficient is something closer to 1.  Is that true, and if so what's the coefficient?  All of this is of course related to issues of self-energy and renormalization.

Any help would be greatly appreciated.
AnswersKenntnis
AnswerKenntnis:
This can be solved by adding the non-electromagnetic energy $E_{p}$ of the Poincar├⌐ stresses to $E_{em}$, the electron's total energy $E_{tot}$ now becomes:

$$\frac{E_{tot}}{c^{2}}=\frac{E_{em}+E_{p}}{c^{2}}=\frac{E_{em}+\frac{E_{em}}{3}}{c^{2}}=\frac{4}{3}\times\frac{E_{em}}{c^{2}}=\frac{4}{3}m_{es}=m_{em}$$

Thus the missing 4/3 factor is restored when the mass is related to its electromagnetic energy, and it disappears when the total energy is considered. They are just equal.
QuestionKenntnis:
Why can we see the dust particles in a narrow beam of light (and not in an all lighted area)?
qn_description:
Let us say that I am sitting in a room with all the drapes open. Bright sunlight is coming through the window. The whole room is brilliantly lighted. I will not be able to see the dust particles suspended in air.

Now, if I draw the drapes close, keeping a small slit open, allowing only a beam of sunlight to come in, I will readily see the suspended dust particles in that beam. The same thing will happen in a dark night with the beam of light from a handheld battery torch.

What will be the scientific explanation for this? I can not see the dust particles
when I have more light. But when I actually reduce the light and there is only one narrow beam present, I can see those minuscule particles. 

How does a narrow beam of light enable me to see those fine elements?
AnswersKenntnis
AnswerKenntnis:
Your inability to see the dust until you narrow the slit has nothing to do with the narrowness of the beam but instead the dynamic range of light that your eye can see at one time.

A bit of searching turns up reports of a contrast ratio for you eye at one time as between 100:1 and 1000:1.  This means if you're in a room with a range of brightness greater than about 100 to 1 the brightest things will all be washed out as white and the darkest things will all be essentially black.  This is obvious in photos that are "backlit" like this one:

These horses aren't black but because the ratio of the bright light to the dark horses exceeds the dynamic range of the camera the sky is washed out white and the horses are in silhouette.

Your eye can adjust over time to a huge range but it can't utilize the whole range all at once.

In the case of dust reflecting light, if you allow a lot of light into the room the relative brightness between the small amount of light the dust is reflecting and the rest of the illuminated room prevent you from seeing the dust.

This is fundamental to signal processing.  Why can't you hear a whisper in a noisy room?  The noise of the crowd obscures the whisper.  The difference between the signal you're trying to pick up and the background noise is called the signal-to-noise ratio.  In the case of dust, the light let into the room is scattered and reflected in the room and causes the room to be illuminated.  This is the noise that obscures the signal from light reflected off of the dust.
AnswerKenntnis:
Our eyes become more sensitive to light in dark rooms, especially if we give them a few minutes to adjust. Also the illuminated dust in a beam of light shows up because of the contrast between it and the dark background when the room is dark.

The dust isn't really microscopic.
AnswerKenntnis:
I would partly disagree with other answers here, and say that it is not related to the dynamic range of our eyes, or our eyes adjusting to the light.  This is a factor, true, but the main factor is the background contrast.

If you sit in your dark room, with a white piece of white paper in the sunbeam, you won't see the white fluff in front of the bright white paper.  It's not a matter of your eyes adjusting - it's more a matter that the background is bright enough to hide the specks, so they don't stand out.

Conversely, if you open up your whole room to the sunlight, so that the specks are hard to see, then hold a piece of black cardboard up, you'll see the specs of dust floating in front of the cardboard, even though the rest of the room is bright, and your eyes are adjusted to the bright room.
QuestionKenntnis:
Why would spacetime curvature cause gravity?
qn_description:
It is fine to say that for an object flying past a massive object, the spacetime is curved by the massive object, and so the object flying past follows the curved path of the geodesic, so it "appears" to be experiencing gravitational acceleration. Do we also say along with it, that the object flying past in reality exeriences NO attraction force towards the massive object? Is it just following the spacetime geodesic curve while experiencing NO attractive force?

Now come to the other issue: Supposing two objects are at rest relative to each other, ie they are not following any spacetime geodesic. Then why will they experience gravitational attraction towards each other? E.g. why will an apple fall to earth? Why won't it sit there in its original position high above the earth? How does the curvature of spacetime cause it to experience an attraction force towards the earth, and why would we need to exert a force in reverse direction to prevent it from falling? How does the curvature of spacetime cause this?

When the apple was detatched from the branch of the tree, it was stationary, so it did not have to follow any geodesic curve. So we cannot just say that it fell to earth because its geodesic curve passed through the earth. Why did the spacetime curvature cause it to start moving in the first place?
AnswersKenntnis
AnswerKenntnis:
To really understand this you should study the differential geometry of geodesics in curved spacetimes. I'll try to provide a simplified explanation.

Even objects "at rest" (in a given reference frame) are actually moving through spacetime, because spacetime is not just space, but also time: apple is "getting older" - moving through time. The "velocity" through spacetime is called a four-velocity and it is always equal to the speed of light. Spacetime in gravitation field is curved, so the time axis (in simple terms) is no longer orthogonal to the space axes. The apple moving first only in the time direction (i.e. at rest in space) starts accelerating in space thanks to the curvature (the "mixing" of the space and time axes) - the velocity in time becomes velocity in space. The acceleration happens because the time flows slower when the gravitational potential is decreasing. Apple is moving deeper into the graviational field, thus its velocity in the "time direction" is changing (as time gets slower and slower). The four-velocity is conserved (always equal to the speed of light), so the object must accelerate in space. This acceleration has the direction of decreasing gravitational gradient.

Edit - based on the comments I decided to clarify what the four-velocity is:

4-velocity is a four-vector, i.e. a vector with 4 components. The first component is the "speed through time" (how much of the coordinate time elapses per 1 unit of proper time). The remaining 3 components are the classical velocity vector (speed in the 3 spatial directions).

$$ U=\left(c\frac{dt}{d\tau},\frac{dx}{d\tau},\frac{dy}{d\tau},\frac{dz}{d\tau}\right) $$

When you observe the apple in its rest frame (the apple is at rest - zero spatial velocity), the whole 4-velocity is in the "speed through time". It is because in the rest frame the coordinate time equals the proper time, so $\frac{dt}{d\tau} = 1$.

When you observe the apple from some other reference frame, where the apple is moving at some speed, the coordinate time is no longer equal to the proper time. The time dilation causes that there is less proper time measured by the apple than the elapsed coordinate time (the time of the apple is slower than the time in the reference frame from which we are observing the apple). So in this frame, the "speed through time" of the apple is more than the speed of light ($\frac{dt}{d\tau} > 1$), but the speed through space is also increasing.

The magnitude of the 4-velocity always equals c, because it is an invariant (it does not depend on the choice of the reference frame). It is defined as:

$$ \left\|U\right\| =\sqrt[2]{c^2\left(\frac{dt}{d\tau}\right)^2-\left(\frac{dx}{d\tau}\right)^2-\left(\frac{dy}{d\tau}\right)^2-\left(\frac{dz}{d\tau}\right)^2} $$

Notice the minus signs in the expression - these come from the Minkowski metric. The components of the 4-velocity can change when you switch from one reference frame to another, but the magnitude stays unchanged (all the changes in components "cancel out" in the magnitude).
AnswerKenntnis:
When the apple was detatched from the branch of the tree, it was stationary, so it did not have to follow any geodesic curve.


Even when at rest in space, the apple still advances in space-time. Here is a visualization of the falling apple in distorted space-time:

http://www.youtube.com/watch?v=DdC0QN6f3G4
AnswerKenntnis:
As to the first paragraph, gravity shows up as geodesic deviation; initially parallel geodesics do not remain parallel.

Since, for a freely falling particle, the proper acceleration (the reading of an accelerometer attached to the particle) is zero, it is correct to say that a particle whose worldline is a geodesic has no proper acceleration.

But it is not correct to say that a freely falling particle has no coordinate acceleration.

Regarding the second paragraph, if a particle's wordline is not a geodesic, the particle will have a proper acceleration, the particle's accelerometer will not read zero.  Two particles that are preventing from falling towards one another will have weight.

Regarding the third paragraph, I think you need to sharpen your conception of worldines and geodesics.  If a particle exists, it has a worldline and the worldline of a particle that is free to fall is a geodesic even if the particle is momentarily stationary.
AnswerKenntnis:
Not everything needs to follow geodesic Spacetime curvature available to it. With external force, you can prevent a particle from following Spacetime curvature. Only "freely" falling particles follow Spacetime curvature available to them. So, when you see a stationary object not following Spacetime curvature, it's because an external force is preventing it from going to it's inertial trajectory... Means, it's not in "Free Fall".

Come to Apple: In terms of Spacetime, nothing is in rest. An Apple, when attached with tree, is also in motion. But, the motion exist fully in time with zero space component. This motion is NOT according to Spacetime curvature available to it because external forces holding root of Apple oppose it at microscopic level. When these external forces stop working, Apple starts to follow Spacetime curvature which converts time component of motion to space component. That's why Apple's acceleration is merely inertial motion. You can see removal of time component of motion in Gravitational Time Dilation.
QuestionKenntnis:
Is Angular Momentum truly fundamental?
qn_description:
This may seem like a slightly trite question, but it is one that has long intrigued me.

Since I formally learned classical (Newtonian) mechanics, it has often struck me that angular momentum (and generally rotational dynamics) can be fully derived from normal (linear) momentum and dynamics. Simply by considering circular motion of a point mass and introducing new quantities, it seems one can describe and explain angular momentum fully without any new postulates. In this sense, I am lead to believe only ordinary momentum and dynamics are fundamental to mechanics, with rotational stuff effectively being a corollary.

Then at a later point I learned quantum mechanics. Alright, so orbital angular momentum does not really disturb my picture of the origin/fundamentality, but when we consider the concept of spin, this introduces a problem in this proposed (philosophical) understanding. Spin is apparently intrinsic angular momentum; that is, it applies to a point particle. Something can possess angular momentum that is not actually moving/rotating - a concept that does not exist in classical mechanics! Does this imply that angular momentum is in fact a fundamental quantity, intrinsic to the universe in some sense?

It somewhat bothers me that that fundamental particles such as electrons and quarks can possess their own angular momentum (spin), when otherwise angular momentum/rotational dynamics would fall out quite naturally from normal (linear) mechanics. There are of course some fringe theories that propose that even these so-called fundamental particles are composite, but at the moment physicists widely accept the concept of intrinsic angular momentum. In any case, can this dilemma be resolved, or do we simply have to extend our framework of fundamental quantities?
AnswersKenntnis
AnswerKenntnis:
Note As David pointed out, it's better to distinguish between generic angular momentum and orbital angular momentum. The first concept is more general and includes spin while the second one is (as the name suggests) just about orbiting. There is also the concept of total angular momentum which is the quantity that is really conserved in systems with rotational symmetry. But in the absence of spin it coincides with orbital angular momentum. This is the situation I analyze in the first paragraph.



Angular momentum is fundamental. Why? Noether's theorem tells us that the symmetry of the system (in this case space-time) leads to the conservation of some quantity (momentum for translation, orbital angular momentum for rotation). Now, as it happens, Euclidean space is both translation and rotation invariant in compatible manner, so these concepts are related and it can appear that you can derive one from the other. But there might exist space-time that is translation but not rotation invariant and vice versa. In such a space-time you wouldn't get a relation between orbital angular momentum and momentum.

Now, to address the spin. Again, it is a result of some symmetry. But in this case the symmetry arises because of Wigner's correspondence between particles and irreducible representations of the Poincar├⌐ group which is the symmetry group of the Minkowski space-time. This correspondence tells us that massive particles are classified by their mass and spin. But spin is not orbital angular momentum! The spin corresponds to group $Spin(3) \cong SU(2)$ which is a double cover of $SO(3)$ (rotational symmetry of three-dimensional Euclidean space). So this is a completely different concept that is only superficially similar and can't really be directly compared with orbital angular momentum. One way to see this is that spin can be a half-integer, but orbital angular momentum must always be an integer.

So to summarize:


orbital angular momentum is a classical concept that arises in any space-time with rotational symmetry.
spin is a concept that comes from quantum field theory built on the Minkowski space-time. The same concept also works for classical field theory, but there we don't have a clear correspondence with particles, so I omitted this case.




Addition for the curious

As Eric has pointed out, there is more than just a superficial similarity between orbital angular momentum and spin. To illustrate the connection, it's useful to consider the question of how particle's properties transform under the change of coordinates (recall that conservation of total angular momentum arises because of the invariance to the change of coordinates that corresponds to rotation). Let us proceed in a little bit more generality and consider any transformation $\Lambda$ from the Lorentz group. Let us have a field $V^a(x^{\mu})$ that transforms in matrix representation ${S^a}_b (\Lambda)$ of the Lorentz group. Thanks to Wigner we know this corresponds to some particle; e.g. it could be scalar (like Higgs), bispinor (like electron) or vector (like Z boson). Its transformation properties under the element ${\Lambda^{\mu}}_{\nu}$ are then determined by (using Einstein summation convention)

$$ V'^a ({\Lambda^{\mu}}_{\nu} x^{\nu}) = {S^a}_b(\Lambda) V^b (x^{\mu}) $$

From this one can at least intuitively see the relation between the properties of the space-time ($\Lambda$) and the particle ($S$). To return to the original question: $\Lambda$ contains information about the orbital angular momentum and $S$ contains information about the spin. So the two are connected but not in a trivial way. In particular, I don't think it's very useful to imagine spin as the actual spinning of the particle (contrary to the terminology). But of course anyone is free to imagine whatever they feel helps them grasp the theory better.
AnswerKenntnis:
In the field of classical mechanics angular momentum is almost always derived form linear momemtum. This actually might be the problem because it is also possible to do it the other way around: linear momentum is a limiting case of angular momentum where the radius of rotation becomes infinite. In this view the split between rotational and linear vanishes - the new concept that is introduced is: infinity.

This is not a new idea of mine, it has been established since the 19th century. By using projective geometry one can integrate linear and angular kinematics and dynamics in one framework (i.e. a translation is a rotation around an infinite axis; a pure moment is a force along an infinite line of action). Keywords: Felix Klein, linear complexes.

Another issue is intrinsic angular momentum. I could say: study the fundamentals, the principles, and the maths, and eventually you'll get a holistic picture, but that's not what I believe. I think we are in need of some kind of geometrical electron model that permits us to depict intrinsical angular momentum.
AnswerKenntnis:
whether you call a similar concept "fundamental" is a matter of taste - and the proposition is just a meaningless emotional slogan. The angular momentum is surely an important quantity that is, in a very well-defined sense, as important as the normal momentum. Incidentally, both of them are conserved if the physical laws are symmetric with respect to translations and rotations, respectively.

So the real question is why the spin in quantum mechanics can't be reduced to the orbital motion - i.e. to the "linear motion" and ordinary "momentum". It's because the objects in quantum mechanics are described not just by their shape in space but by wave functions, and wave functions may be said to transform nontrivially (into something else) under rotations.

In particular, if the wave function (or a field) is a vector or a tensor or, most typically, a spinor, then it means that in a different coordinate system, the values of the components of the wave function will be different. This is possible even in the case when the wave function (or field) is fully localized at one point, i.e. nothing is rotating "orbitally".

The angular momentum is defined by the change of the phase of the wave function under rotations, which may come from the dependence of the wave function on space, but also from the transformations of the components of the wave function among each other, which is possible even if everything is localized at a point. So even point-like objects may carry an angular momentum in quantum mechanics, the spin.

Note that the spin is a multiple of $\hbar/2$ and $\hbar$ is sent to zero in the classical limit, so in the classical limit, the spin as the internal angular momentum becomes zero and disappears, anyway. 

Another new feature of the spin is that unlike the angular momentum, it may be half-integer, not just a multiple of $\hbar$: also $\hbar/2$ is possible. That's because the wave functions (and fields) may transform as spinors that change the sign if they're rotated by 360 degrees. Only a rotation by 720 degrees is topologically indistinguishable from "no rotation", so wave functions are obliged to return to their original values under a 720 degree rotation. But fermions change their signs under rotations by 360 degrees which corresponds to their half-integral spin.

If the word "fundamental" means that it can't be reduced to some other things such as a classical intuition about motion and rotation, then be sure that the spin is damn fundamental, much like the rest of quantum mechanics.

Best wishes
Lubos
AnswerKenntnis:
In classical mechanics, the fundamental entities change according to the framework you opt for. If you do classical Newtonian mechanics, I'd say the fundamental entities are postions and velocities. All the others can be derived from them and the dynamics of particles are described in terms of functions of these (forces are functions of time, positions and velocities).

But if you go to Hamiltonian mechanics, then positions and momenta become fundamental. And the Hamiltonian can be expressed as a function of these and possibly time. 

Clearly, in classical mechanics, angular momentum is always a derived quantity, because it is always an orbital angular momentum, never an intrinsic angular momentum. Even when you have an object rotating on an own axis, this can be understood as the particles constituting the object executing an orbital motion. Of course, you can write Hamiltonians which depend on the angular momentum of the top, but these are higher level descriptions, the angular momentum of the top could still be decomposed in principle into the orbital angular momenta of its constituents. This would not be a very practical approach to problem-solving of course.

Therefore, as you say, a fundamental intrinsic angular momentum is a novelty in quantum mechanics. The way it enters the equations is usually through the multiple-valuedness of the wave function. Say a spin 1/2 particle has to be described by two independent component wave functions (there could be more components, but these would not be independent). I don't know of any way around this. This is a basic fact of how nature works and it is related to the representations of the symmetry group of space-time.

Since the symmetry group of space-time is basically the same in quantum and in classical physics, I don't see however why it should not be possible to describe particles with intrinsic momenta in classical mechanics. I think it is certainly possible in principle. The question is, is it useful? Since all our elementary particles have to be described at the quantum level, what use is a classical theory of particles with intrinsic momenta? Except in the sense of tackling problems like the top by simplification or something?

EDIT: As a matter of fact, classical field theories do have spin. Think of the Maxwell equations for instance.
AnswerKenntnis:
Lubosh wrote: "The angular momentum is defined by the change of the phase of the wave function under rotations, which may come from the dependence of the wave function on space, but also from the transformations of the components of the wave function among each other, which is possible even if everything is localized at a point. So even point-like objects may carry an angular momentum in quantum mechanics, the spin."

In QM it is impossible and is not necessary to impose R = 0 (see my blog) to have a system at rest. On the contrary, one has to put P = 0. It does not mean point-likeness but ubiquity instead.

There is an article by R. Ohanian on spin. But I am afraid it is finally a tautology or so.

I think the angular momentum is fundamental. I think that even in Classical Mechanics a description of anything with help of solely three coordinates R(t) is too primitive. Generally everything is not point-like and rotates, roughly speaking. So the intrinsic angular momentum J is as fundamental as the linear momentum P (as well as color, charge, and flavor ;-).
AnswerKenntnis:
A hint of the special role of angular momentum happens when you look for its conjugate variable. It is angular position, which is adimensional. And then you have that any product of a variable times its conjugate has units of action, which are the same units that angular momentum. So classical mechanics already tell us that something is going on.
(Caveat: that you can have the same units with scalar product and with cross product, and the physical meaning is different. If you have checked the pamphlets from German car and enginemakers, you could have noticed the unit "Nm", newton times meter, and the unit "Joule", used differently.)
AnswerKenntnis:
As for spin and extended particles, I'd say the contrary: it is not contrary to intuition that point particles have some intrinsic angular momentum, because a point looks as if it had some rotation invariance built-in. The surprising thing is that extended objects have this angular momentum, without a point for the rotational symmetry to pivot in.
AnswerKenntnis:
Existence of a spin of a particle is of course an indication that the particle is in fact composed of space-separated parts. This does not mean though that the particle is composed of other particles. 

For example, at least a part of the electron's spin is currently known to be in fact the orbital momentum of the quantum vacuum fluctuations which are involved by electron's core into rotation. This part is known as anomalous angular momentum of electron.

Another example is photon where the spin can be explained as an order in which the energy contained in electric and magnetic fields rotates around the axis laid along the direction of the photon's propagation.
AnswerKenntnis:
There is more to it than Spin being intrinsic angular momentum. An electron has an "internal degree of freedom" - to be left handed or right handed, and it can leave point A with RH spin and arrive at B with LH spin. Thus Pauli needs two complex components in his equation. (unlike a photon which arrives with the same spin although it too has LH and RH, so there is no internal degree of freedom ). This is distinct from the spin-vector which defines a direction in space. The two-valuedness comes from rotation being about a bivector which can point up or down along the axis of rotation. One can do spatial rotations either way - and electrons seem to make the distinction - as if there are two kinds, but everything else is the same mass and charge, so we say it is the same particle, with opposite spins. So it seems that there is no necessary connection to either relativity (except for fixing up the Thomas factor in the Pauli eq ) or QFT. Hamilton had the algebra to make the classical distinction between left and right - it is built into quaternion algebra, but he did not see it as a mechanical property of particles - but heck, he did not see the Maxwell equation either.
QuestionKenntnis:
Common false beliefs in Physics
qn_description:
Well, in Mathematics there are somethings, which appear true but they aren't true. Naive students often get fooled by these results. 

Let me consider a very simple example. As a child one learns this formula $$(a+b)^{2} =a^{2}+ 2 \cdot a \cdot b + b^{2}$$ But as one mature's he applies this same formula for Matrices. That is given any two $n \times n$ square matrices, one believes that this result is true: $$(A+B)^{2} = A^{2} + 2 \cdot A \cdot B +B^{2}$$ But eventually this is false as Matrices aren't necessarily commutative. 

I would like to know whether there any such things happening with physics students as well. My motivation came from the following MO thread, which many of you might take a look into:


http://mathoverflow.net/questions/23478/examples-of-common-false-beliefs-in-mathematics
AnswersKenntnis
AnswerKenntnis:
Amazingly, Wikipedia has an article titled "List of common misconceptions".  There is a (short) section dedicated to Physics, which mentions:


The role of the Coriolis effect in bathtubs and sink drains
The role of angular momentum in bicycle stability
The "equal time" fallacy in explaining the lift developed by an airfoil
Glass isn't actually a high viscosity fluid
Composition of air
"Lightning never strikes twice"


The Astronomy section has some good ones too:


When a star collapses into a black hole, its gravitational pull does not actually increase.
Meteorites are not actually hot when they land; usually they are cold.  (I would add: the heating of meteors is more due to the compression of the air in front of them than to 'friction with the air' as commonly believed.)


Some that I would add:


"Once something is in orbit it is free from Earth's gravity."  Even educated people get tripped up on this one; the internet is rife with people suggesting we just "nudge" the International Space Station into lunar orbit.  At a much more basic level of misunderstanding, there is the idea that astronauts are "weightless" because they are far away from the earth.
"There is a high tide on the opposite side of the earth from the moon/sun because the earth 'shields' the ocean from the gravitational pull."
AnswerKenntnis:
A mistake that I often come across and that is so easy to make: people somehow have a visceral belief that heavy objects fall faster than light ones. Setting aside of course problems of air resistance, this is obviously false, but it seems to be so counterintuitive and I think it is somehow tied to our intuitive understanding of mass as inertia. Since higher mass means higher inertia. People understand this intuitively as it takes more force to push a fat guy than a thin one. But they don't see that gravity is a force proportional to mass, so that more inertia is paralleled by more gravity as well. With as a result the same gravitational acceleration for light and heavy bodies.

I've even noticed the mistake being made by professional physicists in colloquial conversations.

EDIT: I just found this article today, shedding a new light on why misconceptions in physics or science in general are so common and so hard to get rid of.
AnswerKenntnis:
"Summer is when the Earth is closest to the sun, and winter is when it's furthest away."

It's true that the Earth's orbit is slightly elliptical, but the effect of this, as far as seasons, is very small. For one thing, this wouldn't explain why the sun rises and sets at different times in different seasons, and if this were true, the whole planet would have summer at the same time.

The seasons are actually caused by the tilt of the Earth relative to its orbit around the Sun.
AnswerKenntnis:
I would say that for most people, the quadratic scaling of kinetic energy with speed is somewhat of a mystery. 

People don't understand how if you go twice as fast, a car accident actually four times as energetic, hence the high number of reckless drivers and deadly accidents.
AnswerKenntnis:
Classical mechanics is boring and mostly solved

...especially in case of fluid dynamics (-;
AnswerKenntnis:
I will give some meta-false beliefs: these are beliefs held by the general public, which happen to be true, which are hyper-corrected by many physicists with bogus corrections based on the urge to appear smart:

Electrons move slowly down a wire


The belief: the electrons move lightning fast down a wire. 
the hypercorrection: in the completely obsolete Drude model, electrons move slowly. In this model, you imagine the current is carried by a classical gas of electrons, and you divide the total current by the density of all electronic charge to get the drift velocity. This predicts a completely bogus drift velocity of a few cm/s, which is total nonsense, because only electrons near the Fermi surface contribute to the conductivity. Nevertheless, you see this hypercorrection repeated endlessly (it appears here too).
The best answer: the electronic wavefunctions are spread out in a metal. The correct notion of electron velocity is the Fermi velocity, which is enormous typically, because the wavelength is about 1 atomic radius. While it isn't the same as the speed of electricity going down the wire (which is the speed of the field perturbations, some significant fraction of the speed of light), it is enormously high. Impurities which can scatter electrons will alter this speed, but not as much as the naive hypercorrection says.


The atom is mostly empty space


The belief: the atom is full of stuff, that's why stuff is hard when you push against it.
The hypercorrection: In the totally obsolete Rutherford-Bohr model, the atom is mostly empty space, the tiny pointlike electron orbiting a nucleus which contains most of the mass.
The best answer: But it is the electrons' wavefunction which tells you whether something is empty space or not. A region filled with electronic wavefunction feels hard to the touch, because two electrons can't be compressed into the same space without squeezing their wavefunction to have very high spatial variations, by the exclusion principle. Atoms are full of electronic wavefunction, and are therefore not empty space, at least not by any reasonable definition.


There is nothing mystical about measurement in quantum mechanics


The belief: the measurement problem in the standard quantum mechanics suggests that consciousness is somehow involved in measurements
hypercorrection: decoherence explains all that! Quantum mechanics is no different than determinism as far as enlightenment values are concerned.
The best answer: Decoherence tells you why you don't have interference between classically different worlds, or histories, and its an important part of the story if quantum mechanics is exact. But it does not tell you why you "percieve" one consistent history as a world. You need a dictionary between physics and perception. Since this dictionary is fundamental, and weird, and philosophical, it is important to explain that this is not an output of physics, but an input, which links mathematical theory with explicit sense-data.


There is no such thing as a centrifugal force


The belief: when things are rotating, they are pushed out by a centrifugal force.
The hypercorrection: There is no centrifugal force. There is a centripetal force (centripetal was a made up word to replace centrifugal) which pulls you in.
The best answer: This is obviously true from the point of view of the inertial frame, there is no centrifugal force, but if you are looking at it from the point of view of the rotating object, then there is. It all depends on your choice of reference frame.


The big bang happened everywhere at once


The belief: the big bang means that the universe started at some definite point and got bigger from there.
The hypercorrection: the big bang happened everywhere at once, and it is just wrong to think of it as happening at some single point in an extended model of space-time. If the universe is open, the big-bang was infinite in extent.
The best answer: There are three important caveats: 1. In FRW models, the bang-point is a singularity, so its outside space and time, and it is impossible to determine if it is "really" a single point or "really" everywhere at once, so it's just a meaningless question. 2. In the Newtonian "big bang" model, where you imagine the universe is now filled with particles which have a speed away from you linearly proportional to the distance from you, everything does come out from a single point! All the newtonian world lines converge on your current position. That's true even though the universe is spatially homogenous (the reason its not a paradox is that Galilean boosts are nontrivially mixed with translations). 3. the best picture, in my view, is the holographic picture, where you are surrounded by a horizon which was smaller in the past. This view is similar to the Newtonian big bang, in that everything came from a small region bounded by a dS cosmological horizon. This is mathematically equivalent to everything else, except throwing away the stuff outside the horizon that can't be observed.


I would like to admit that I was a little flabbergasted when a lay-person told me that everything in a Newtonian big-bang comes from a single place. That was completely counterintuitive.
AnswerKenntnis:
Quantum mechanics is way too strange so it can't possibly be a correct description of the real world. Right? I think nothing else needs to be said about this.

Or maybe on a second thought, some more concrete beliefs and their solution are in order:


The physical world has to be deterministic (it doesn't).
Every possible question that can occur to you must have a precise answer by measurement (we observe only what we can, not what we want to).
The collapse of the wavefunction is in contradiction with finite speed of light (no information is being transmitted).
AnswerKenntnis:
A couple of space and sci-fi derived misconceptions:


An orbiting satellite needs propulsion and that orbiting is different from free fall.
You can actually see a laser beam in free space! I've seen this in my experimental physics class a few years back, before keychain lasers were common.
AnswerKenntnis:
That if an object is moving, there must be a force propelling it in that direction. 
Students very commonly think that forces cause objects to have velocity, rather than the fact that forces cause objects to change velocity.
AnswerKenntnis:
Some I have heard:


Heavier objects fall faster (this is just plain wrong.) However, bigger and smaller objects in a typical Earth environment would fall at different rates due to air resistance, but actual mass has no influence.
Two cars colliding at 60 mph is the same as one car colliding into a wall at 120 mph. I think MythBusters did something on this.
Electrons travel really fast around a circuit: actually, they travel really slow but it is similar to Newton's cradle in that a small movement in one ball can transfer the energy to the last one almost instaneously.
The laws of thermodynamics have been broken by some guy in a garage with some magnets. Well, no, they still seem to be intact, and a lot of modern science depends on them!
That an oscilloscope trace travels faster than the speed of light on expensive high speed analog scopes (~1-2GHz.) This is not quite true: although the beam may sweep the surface of the CRT faster than c (due to the relatively small movement at the neck of the CRT), the trace cannot communicate information faster than c. 
More related to chemistry, but the fact that water can have "memory" and all that homeopathic nonsense that con arti^H^H^H^H^H^H^H^H homeopaths spurt out.
AnswerKenntnis:
The misconceptions about special relativity and quantum mechanics are quite well-known. A lot of the posts above discuss them in detail. So rather than doing that I'll list some misconceptions from general(say high school) physics:


When a body rests on a surface the upward contact force acting on it is reaction to its weight. This is obviously wrong as action and reaction act on different bodies.
There's a lot of misconceptions about non-inertial(pseudo) force. My physics teacher once said that non-inertial forces arise only when the body is in contact with an accelerating frame.
Nothing can move faster than light. Of course it's false unless you add the phrase "in vacuum". The Cherenkov radiation happens when some charged particle moves in a medium with speed greater than the speed of light in that medium. 
Friction always has to act in the opposite direction of overall motion. Actually friction provides the necessary force for rolling without which no vehicle would ever run. The correct formulation is friction opposes the instantaneous motion of the point of contact.
Light always travel in straight lines. Even without gravitational bending if we simply have a medium with a variable refractive index light will follow a curve through it. It's a nice application of Snell's law.
Newton's second law provides a definition of force. It's a very widespread misconception unfortunately even among professional physics students. This strips Newton's second law of any physical content and forces(pun intended) it to become a tautology. Of course the actual content of the law is that the force is given by some other law(say gravitational or em) and it equals ma. For a persuasive discussion on this see the first volume of Feynman's Lectures on Physics. (I am very sorry that I forgot the chapter or page number).
Newton's first law is derivable from second law. The proof goes as follows : F=ma. If F=0 then a=0 since $m~{}\neq 0$ QED. The problem is without first law there is no notion of an inertial frame and the laws become pointless.
In special relativity the hypothesis of the constancy of the speed of light in vacuum(c) with respect to all observers is redundant because it can be derived from the principle of relativity. Of course c may vary without contradicting the principle of relativity. In fact, in Newtonian mechanics c is observer dependent it respects the principle of relativity. The constancy of c hypothesis gives the Lorentz transformation whereas in Newtonian mechanics we have the Gallilean transformation. If you are still not convinced then look at this formulation of special relativity without the second hypothesis. Google doubly special relativity.
AnswerKenntnis:
If you somehow manage to BREAK a law of physics, the universe will vanish!
AnswerKenntnis:
As a tutor, I frequently have conversations like this:

"So we worked out that if when I toss my pen up at 2m/s, it will go 20cm high.  How high will it go if I toss it up at 4m/s?"

"40 cm."

"Well, okay, let's check by working through that equation again..."

[We find out that the answer is 80 cm]

"So when I throw it up twice as fast, it goes four times as high, because it takes twice as long to get to the top, but is also going twice as fast."

"Okay."

"Now what if I throw it up three times as fast?  How many times as high will it go?"

"Six."

This isn't a misconception about kinetic energy, so much as a lack of comprehension about what scaling is.  When students are missing this concept, almost all of physics is more difficult to discuss.
AnswerKenntnis:
You need something more to get something more

This is about emergent macroscopic properties of microscopical laws. Some people can't understand that statistics is powerful enough to make seemingly random heap of molecules suddenly show macroscopic properties like being solid or being magnetic and they think some hand of god is required to make this happen.

The best illustration of a contradiction to this principle are living organisms. They consist of nothing else than few physical laws all the way down and large number of molecules. All that was needed was statistics and natural selection.



I will agree though that it is quite amazing all of our nature can spontaneously emerge just from particles given enough space and time.
AnswerKenntnis:
The most common misconceptions are about gravity:

(1) Gravity turns off at space-shuttle orbit distance because the astronauts are weightless

Gravity is at about 80% strength compared to the surface of the Earth.  The astronauts are weightless because the shuttle is in free-fall (orbit).  If there was no gravity, the shuttle could not orbit.

(2) Gravity is generated by the spinning Earth.  If the Earth stopped spinning, gravity would turn off.

Gravity is generated by virtue of the Earth's mass and the mass of the object; the two exert a mutual pull.  There are some smaller effects associated with the spinning Earth (e.g. the Coriolis effect), but gravity would still work fine if the Earth stopped spinning.

There is another great misconception about the force of impact between a truck and a small car in a collision:

(3) The truck exerts a greater force of impact on the car, than the car on the truck

While the damage can be certainly unequal, the forces are.
AnswerKenntnis:
If you are riding a bicycle and you turn the front wheel to the left then the bicycle will steer to the left.
AnswerKenntnis:
I hear from time to time people highly educated and skilled in Physics (unlike those believing that heavy objects fall faster than light ones) making the following claim:


  ... quantum mechanics indicates that certain physical quantities can take only a countable set of discrete values. Consequently, many current approaches to foundational questions in physics and cosmology advocate novel discrete or 'digital' pictures of nature.


("Is Reality Digital or Analog" essay contest at FQXi)

The discrete spectra of some quantum observables do not imply/suggest that nature, in particular spacetime, is fundamentally discrete. The spectrum of a continuous operator acting on Hilbert spaces [which is a topological (vector) space, hence is continuous], often has a discrete part. This has nothing to do with spacetime being discrete. If it will (eventually) turn out to be discrete, it will be for other reasons.
AnswerKenntnis:
Historically the concept of absolute velocity was commonly believed until the time of Galileo Galilei in the early 17th century. As a naive child, before studying basic physics, it is surprisingly easy to believe in this even these days!

The idea of absolute velocity states that all velocities are fixed with respect to an absolute frame of reference. Galileo showed that velocity is relative to your frame of reference, a principle known as Galilean relativity. This was later fully quantified by Isaac Newton, who also proposed that acceleration is invariant with respect to inertial frames.
AnswerKenntnis:
Why do I have to learn this law when they change it every few years?

This has to do with a fact that some (actually, a lot of) people believe that the progress in physics is done in form of revolution and (in particular) that one day we might find laws that will contradict everything we knew up till then.

Well, if one looks closely on history of physics, it should become apparent that progress was always just evolutionary. Even when some idea needed a revolution in the way people think (as with SR and QM) it always turned out to be just a generalization of our previous ideas (so both SR and QM have nice classical limits which coincide with Newtonian mechanics).

Barring the useless philosophical views (like we might live in Matrix or we don't know whether the sun will rise tomorrow for sure) it's pretty certain that our universe is a comprehensible place and our theories are just better and better approximations to the reality. So it will always be useful to learn Newtonian mechanics, even one million years from now.
AnswerKenntnis:
"Burning coal for heating is more efficient than electrical due to thermodynamical losses at coal power plant"

This is false, even though it is true that converting electrical power to heat would be even worse. The correct way to heat with a given amount of heat source is this: 


burn it at high temperature
make work with heat machine between Thigh and the environment
use the work to power an air conditioner to bring heat from Tenvironment into Troom.


This gives efficiency more than 1 (more heat brought into room than heat produced by burning), and net cooling of the surrounding environment.
AnswerKenntnis:
I think one common false belief is that a light mill rotates because photons deposit more momentum on the shiny side (where they are reflected) than on the black side (where they are absorbed).

I find it quite astonishing to see that many people think so despite the fact that a light mill spins to the opposide direction than predicted by that explanation.
AnswerKenntnis:
Here's another list of false beliefs. These are held by science popularizers. Whether they actually believe these beliefs, or just utter them for the purpose of getting more viewers, is an unanswerable question:

The curved space near massive object can be pictured as a deformed rubber sheet

This one is due to Einstein, unfortunately. You put balls on a rubber sheet, and you see that they roll towards each other. The reason this is a terrible explanation is because you have the Earth's gravity doing the pulling, not the curved space. The actual geodesics on a curved space like the rubber sheet are repelled by the central mass. The reason things attract in relativity is because of the time-dilation factor, and this is the dominant effect. It is just as easy to explain things correctly, in terms of time slowing down near a massive object, and world-lines trying to maximize their proper time with given fixed endpoints, but popularizers never do this.

A variable speed of light can replace inflation.

This appeared in a recent popular show, and it is based on the following bogus idea: if light moved faster at early times, then all the universe could have been in communication! The reason this is false is because no matter how the speed of light is imagined to vary, one can recoordinatize space-time in terms of the intersections of light cones, and unless these lightcones split instead of merge, you get the same communication paradox--- new regions coming into causal contact are coming into causal contact for the first time.

Mesons and Baryons are made of quarks like atoms are made of protons, neutrons and electrons.

This is insidious, because its true for heavy mesons. But it's much more false than true for pions and protons and all the excitations at lower than 1GeV, because of the vacuum condensates. There is no reasonable model of light pions which does not take into account their Goldstone nature. This type of explanation also leaves out Nambu and Skyrme, both of whom were unjustly ignored for too long.

String theory is a theory of strings

This picture is not good for someone who doesn't already have a sense of string theory, because if you start out making home-made models of relativistic strings, you will never get anything like the correct string theory. The strings you naively picture would not have the special light-cone interactions that strings do in the Mandelstam picture, and they would not obey Dolen Horn Schmidt duality. They would just be conglomerations of point particles held together by rubber bands. They would have the wrong spectrum, and they would be full of ghosts.

The only proper way to say what strings are is to say right off the bat that they are S-matrix states, and that they are designed to be an S-matrix theory with linear Regge trajectories. They have a string picture, but the constraint that exchanging strings in the S-channel is dual to exchanging them in the T-channel is all-important, just as it was all-important historically. Without this, even with the Nambu action, you are at a loss for how to incorporate interactions. It is not obvious that the interactions are by topology unless you know Dolan Horn Schmidt.

It is also important for realizing that string interactions are somewhat holistic (that they become local on the light cone is the surprise, not the other way around). You add them order by order in perturbation theory by demanding unitarity, not by asking what happens when two strings collide in the usual sense. These "strings" are strange new things born of 1960s Chew-isms, and their closest cousins are flux lines in gauge theory, or fishnet Feynman diagrams, not a collection of point masses held together by spring-like forces.

This is also insidious, because Chew, Mandelstam, Dolen, Scherk, and all that generation developed the greatest physical theory the world will ever see, and their reward was: "You're fired". (in Scherk's case, "You're crazy"). Then they were heckled for thirty years, while their work was appropriated by a new generation, who described them as the deluded misguided Chew-ites who discovered something great by accident.

There is more than a snowball's chance in hell for large extra dimensions

The idea that there are large extra dimensions was very popular in 2000, but it's completely preposterous. Large extra dimensions bring the planck mass down to about a TeV, giving neutrinos generic majorana masses which are in the KeV-MeV range, so you need to fine tune. They lead to essentially instantaneous proton decay, and huge CP violations in strong interactions, so you need to fine tune some more. To avoid proton decay, there is a clever mechanism due to Arkani-Hamed and Schmalz which puts the quarks and leptons in different places in the extra dimensions. This idea is appealing only at a superficial first glance, because it requires that the SU(2) and U(1) of the standard model be extended in the extra dimensions, which affects their running immediately. The theory predicts unambiguously and model-independently that proton decay suppression requires huge electroweak running at around a TeV. That's a signal you haven't seen a hint of at 100GeV collisions. Come on. In addition, how do you stabilize large dimensions? It's the same fine-tuning as before, so the number of problems has gone up.

A low Planck scale would completely demolish the predictivity of string theory. You can squeeze a lot of stuff into large dimensions. In my opinion, it is this brand of string theory that the critics correctly criticize as fundamentally non-predictive.
AnswerKenntnis:
One widespread belief (I think due to popular books such as Hawking's) is that GTR can never ever, ever be quantized and you always obtain infinities and blah blah. Well, it can, in many situations and in many theories. What is actually meant is that GTR is not a renormalizable quantum field theory in the naive way. But this specification is never explicitly pointed out so people get a false impression that quantum gravity is something completely out of the realms of current physics. Well, surprise, surprise, it's not. We can quantize many gravitational effects (such as waves), we understand that black holes have entropy, we understand they produce Hawking radiation and eventually disappear, etc. And assuming that string theory is correct we can predict whole deal more about it.
AnswerKenntnis:
Some sport instructors would tell you:

Running on a treadmill is easier because you only have to jump, while on the street you also push forward

I suggested then that running in a train should be the easiest, by this line of thought. However, it is true that starting to run (or, accelerating) is indeed easier. Or the air resistance, unless there's a slowest forward wind. Or, the randomly changing slope. Also, lack of the air conditioner. That pretty much summarizes the difference.
AnswerKenntnis:
As an instructor, I have great difficulty teaching Newton's 3rd law: "For every action there's an opposite and equal reaction." This is very basic and very old physics but it's hard to teach. A typical example of the false reaction force answer is the following:


  I hold an apple in my hand. The earth
  pulls down on the apple with a
  gravitational force. What is the
  reaction force to this?


The answer most of them will give is that the reaction force is "my hand pushing up on the apple." Arggghhhh! Of course the reaction force is "the apple pulling up on the earth."

Students fail to realize that the opposite and equal reactions have to be between the same pair of objects. That is, forces arise as pairs. I wish they'd just rename the law so that it makes it more clear that the reaction force has to operate between the same pair of objects.

I demonstrate the law by holding a long spring in my hands and telling them that forces are like this spring. When it applies a force on one end it applies a force on the other (assume massless spring). Next quarter I'm going to try some more extreme measures on this, clearly I'm failing.
AnswerKenntnis:
While the other answers are absolutely correct, they are very subtle in the way they appear. However one very large false-belief in electrical science is that current is taken to be "flowing" from the + terminal to the - terminal in DC current.

While it doesn't really matter which way you choose since you are dealing with close to light speed current, electrons are actually moving from - terminal to + terminal(do not get me wrong electrons are NOT moving anywhere near light speed while drifting. They are moving in the order of centimeters per second.). Hence the current actually moves from - to +.

And it would be interesting to note that not a soul in the professional area of electrical science considers the current from - to +, as it would cause inconsistency with his/her colleagues.
AnswerKenntnis:
"Long hair grows slower" is due to biological effects

In fact, this is a purely mathematical phenomenon. The bigger average length, the more decrease is caused by each hair fall. This leads to a differential equation $$\frac{dL}{dt} = K - \alpha L$$ that has a solutions which decay exponentially to an equilibrium at $$K/\alpha$$.
AnswerKenntnis:
the belief that cartoon-universe rules apply to falling objects:

as embodied by the statement that, "if you are trapped in a falling elevator, you can avoid destruction by jumping at the last moment, so that when the elevator hits, you are in the air, and only fall the last inch or two."
AnswerKenntnis:
It really bugs me. I've even heard from some quantum mechanics lecturers that they think quantum entanglement implies faster than light communication!
AnswerKenntnis:
The concept that quantum mechanics undermines determinism. The Schrodinger wave equation evolution is completely deterministic. The results of measurements are probabilistic, but this does not mean that the various superposed states do not have causes. This is not the same thing as a hidden variable theory. The probabilities are deterministic.
T'Hooft has some interesting ideas on a determinism underlying QM (not the same thing as saying the wave equation is deterministic).  I am not arguing that qm is in all senses deterministic, but it isn't completely non-deterministic either.
QuestionKenntnis:
What causes insects to cast large shadows from where their feet are?
qn_description:
I recently stumbled upon this interesting image of a wasp, floating on water:



Assuming this isn't photoshopped, I have a couple of questions:


Why do you see its image like that (what's the physical explanation; I'm sure there is an interesting one)?
Why are the spots surrounding the wasp's legs circle shaped? Would they be square shaped if the 'feet' of the wasp were square shaped?
AnswersKenntnis
AnswerKenntnis:
The mechanism at play here is surface tension. The cohesion of the molecules of water is what keeps the wasp afloat. Due to this cohesion, the surface of the water behaves like a membrane and is curved inwards. The light rays that would be refracted from the perfectly flat surface are now incident at an altered angle and are reflected or refracted by altered angles around the tip of the wasp's legs, hence the shadow.
The curvature of the surface traces the shape of the object that touches the surface. As you can see though, the area of the shadow is much larger than the wasp's legs' tips. The shape of the shadow will therefore always be rounded. The radii of the curvature can also be calculated, given the difference in pressure between air and water.
AnswerKenntnis:
That is a truly amazing picture! I am by no means an expert, but I have an idea.

When the wasp stands on the water, it is curved down slightly. The light that hits these parts will then be bent more outwards than if it just hit regular water. This happens at every side of the circle, so the light is always bent out, and doesn't reach the bottom at those points.

Therefore, you get this effect.

Also, to answer your second question, the feet are so small compared to the shadow that they play no big role in the shape.
AnswerKenntnis:
This is a great example of how nice it often is to reason about refraction stuff using Fermat's principle.

Let's reduce all this to 2 dimensions. The surface tension produces something like this:

Now if we want to now were a light "ray" needs to go to get from some light source, we just need to find the way that takes it the least time. Light is slower in water, so it wants to go as far as possible in air ΓÇô of course, only if that's not too much longer to go. So far from the insect, a light ray would just enter the water straight perpendicular, since that minimises both the total waylength and the path in water.


However, right below the insect foot, that won't work ΓÇô the foot itself is not translucent* ΓÇô and, more importantly, a little way left or right from right below the foot the quickest path will still be right through the foot, since any other path will require the light to travel substantially more through water, while the total path length is only a little shorter. 


so all these rays are "invisible". Whether that works out this way depends on how far we are away from right-below-the-foot, so that makes a circular shadow, even when the foot itself has another shape.


*Actually, it is kind of translucent I suppose, but we now it won't get hit by a lot of light total. So if that bit of light has to be spread over a whole lot of ground, there won't be much intensity down there.
