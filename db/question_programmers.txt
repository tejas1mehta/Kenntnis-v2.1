QuestionKenntnis:
What technical details should a programmer of a web application consider before making the site public?
qn_description:
What things should a programmer implementing the technical details of a web application consider before making the site public?  If Jeff Atwood can forget about HttpOnly cookies, sitemaps, and cross-site request forgeries all in the same site, what important thing could I be forgetting as well?

I'm thinking about this from a web developer's perspective, such that someone else is creating the actual design and content for the site.  So while usability and content may be more important than the platform, you the programmer have little say in that.  What you do need to worry about is that your implementation of the platform is stable, performs well, is secure, and meets any other business goals (like not cost too much, take too long to build, and rank as well with Google as the content supports).  

Think of this from the perspective of a developer who's done some work for intranet-type applications in a fairly trusted environment, and is about to have his first shot and putting out a potentially popular site for the entire big bad world wide web.

Also, I'm looking for something more specific than just a vague "web standards" response.  I mean, HTML, JavaScript, and CSS over HTTP are pretty much a given, especially when I've already specified that you're a professional web developer.  So going beyond that, Which standards?  In what circumstances, and why?  Provide a link to the standard's specification.
AnswersKenntnis
AnswerKenntnis:
The idea here is that most of us should already know most of what is on this list.  But there just might be one or two items you haven't really looked into before, don't fully understand, or maybe never even heard of.

Interface and User Experience


Be  aware that browsers implement standards inconsistently and make sure your site works reasonably well across all major browsers.  At a minimum test against a recent Gecko engine (Firefox), a WebKit engine (Safari and some mobile browsers), Chrome, your supported IE browsers (take advantage of the Application Compatibility VPC Images), and Opera. Also consider how browsers render your site in different operating systems.
Consider how people might use the site other than from the major browsers: cell phones, screen readers and search engines, for example. — Some accessibility info: WAI and Section508, Mobile development: MobiForge.
Staging: How to deploy updates without affecting your users.  Have one or more test or staging environments available to implement changes to architecture, code or sweeping content and ensure that they can be deployed in a controlled way without breaking anything. Have an automated way of then deploying approved changes to the live site. This is most effectively implemented in conjunction with the use of a version control system (CVS, Subversion, etc.) and an automated build mechanism (Ant, NAnt, etc.).
Don't display unfriendly errors directly to the user.
Don't put users' email addresses in plain text as they will get spammed to death.
Add the attribute rel="nofollow" to user-generated links to avoid spam.
Build well-considered limits into your site - This also belongs under Security.
Learn how to do progressive enhancement.
Redirect after a POST if that POST was successful, to prevent a refresh from submitting again.
Don't forget to take accessibility into account.  It's always a good idea and in certain circumstances it's a legal requirement.  WAI-ARIA and WCAG 2 are good resources in this area.
Don't make me think


Security


It's a lot to digest but the OWASP development guide covers Web Site security from top to bottom.
Know about Injection especially SQL injection and how to prevent it.
Never trust user input, nor anything else that comes in the request (which includes cookies and hidden form field values!).
Hash passwords using salt and use different salts for your rows to prevent rainbow attacks. Use a slow hashing algorithm, such as bcrypt (time tested) or scrypt (even stronger, but newer) (1, 2), for storing passwords. (How To Safely Store A Password). The NIST also approves of PBKDF2 to hash passwords", and it's FIPS approved in .NET (more info here). Avoid using MD5 or SHA family directly.
Don't try to come up with your own fancy authentication system. It's such an easy thing to get wrong in subtle and untestable ways and you wouldn't even know it until after you're hacked.
Know the rules for processing credit cards. (See this question as well)
Use SSL/HTTPS for login and any pages where sensitive data is entered (like credit card info).
Prevent session hijacking.
Avoid cross site scripting (XSS).
Avoid cross site request forgeries (CSRF).
Avoid Clickjacking.
Keep your system(s) up to date with the latest patches.
Make sure your database connection information is secured.
Keep yourself informed about the latest attack techniques and vulnerabilities affecting your platform.
Read The Google Browser Security Handbook.
Read The Web Application Hacker's Handbook.
Consider The principal of least privilege. Try to run your app server as non-root. (tomcat example)


Performance


Implement caching if necessary, understand and use HTTP caching properly as well as HTML5 Manifest.
Optimize images - don't use a 20 KB image for a repeating background.
Learn how to gzip/deflate content (deflate is better).
Combine/concatenate multiple stylesheets or multiple script files to reduce number of browser connections and improve gzip ability to compress duplications between files.
Take a look at the Yahoo Exceptional Performance site, lots of great guidelines, including improving front-end performance and their YSlow tool (requires Firefox, Safari, Chrome or Opera). Also, Google page speed (use with browser extension) is another tool for performance profiling, and it optimizes your images too.
Use CSS Image Sprites for small related images like toolbars (see the "minimize HTTP requests" point)
Busy web sites should consider splitting components across domains.  Specifically...
Static content (i.e. images, CSS, JavaScript, and generally content that doesn't need access to cookies) should go in a separate domain that does not use cookies, because all cookies for a domain and its subdomains are sent with every request to the domain and its subdomains.  One good option here is to use a Content Delivery Network (CDN).
Minimize the total number of HTTP requests required for a browser to render the page.
Utilize Google Closure Compiler for JavaScript and other minification tools.
Make sure thereΓÇÖs a favicon.ico file in the root of the site, i.e. /favicon.ico. Browsers will automatically request it, even if the icon isnΓÇÖt mentioned in the HTML at all. If you donΓÇÖt have a /favicon.ico, this will result in a lot of 404s, draining your serverΓÇÖs bandwidth.


SEO (Search Engine Optimization)


Use "search engine friendly" URLs, i.e. use example.com/pages/45-article-title instead of example.com/index.php?page=45
When using # for dynamic content change the # to #! and then on the server $_REQUEST["_escaped_fragment_"] is what googlebot uses instead of #!. In other words, ./#!page=1 becomes ./?_escaped_fragments_=page=1. Also, for users that may be using FF.b4 or Chromium, history.pushState({"foo":"bar"}, "About", "./?page=1"); Is a great command. So even though the address bar has changed the page does not reload. This allows you to use ? instead of #! to keep dynamic content and also tell the server when you email the link that we are after this page, and the AJAX does not need to make another extra request.
Don't use links that say "click here". You're wasting an SEO opportunity and it makes things harder for people with screen readers.
Have an XML sitemap, preferably in the default location /sitemap.xml.
Use <link rel="canonical" ... /> when you have multiple URLs that point to the same content, this issue can also be addressed from Google Webmaster Tools.
Use Google Webmaster Tools and Bing Webmaster Tools.
Install Google Analytics right at the start (or an open source analysis tool like Piwik).
Know how robots.txt and search engine spiders work.
Redirect requests (using 301 Moved Permanently) asking for www.example.com to example.com (or the other way round) to prevent splitting  the google ranking between both sites.
Know that there can be badly-behaved spiders out there.
If you have non-text content look into Google's sitemap extensions for video etc. There is some good information about this in Tim Farley's answer.


Technology


Understand HTTP and things like GET, POST, sessions, cookies, and what it means to be "stateless".
Write your XHTML/HTML and CSS according to the W3C specifications and make sure they validate.  The goal here is to avoid browser quirks modes and as a bonus make it much easier to work with non-standard browsers like screen readers and mobile devices.
Understand how JavaScript is processed in the browser.
Understand how JavaScript, style sheets, and other resources used by your page are loaded and consider their impact on perceived performance. It is now widely regarded as appropriate to move scripts to the bottom of your pages with exceptions typically being things like analytics apps or HTML5 shims.
Understand how the JavaScript sandbox works, especially if you intend to use iframes.
Be aware that JavaScript can and will be disabled, and that AJAX is therefore an extension, not a baseline.  Even if most normal users leave it on now, remember that NoScript is becoming more popular, mobile devices may not work as expected, and Google won't run most of your JavaScript when indexing the site.
Learn the difference between 301 and 302 redirects (this is also an SEO issue).
Learn as much as you possibly can about your deployment platform.
Consider using a Reset Style Sheet or normalize.css.
Consider JavaScript frameworks (such as jQuery, MooTools, Prototype, Dojo or YUI 3), which will hide a lot of the browser differences when using JavaScript for DOM manipulation.
Taking perceived performance and JS frameworks together, consider using a service such as the Google Libraries API to load frameworks so that a browser can use a copy of the framework it has already cached rather than downloading a duplicate copy from your site.
Don't reinvent the wheel. Before doing ANYTHING search for a component or example on how to do it. There is a 99% chance that someone has done it and released an OSS version of the code.
On the flipside of that, don't start with 20 libraries before you've even decided what your needs are. Particularly on the client-side web where it's almost always ultimately more important to keep things lightweight, fast, and flexible.


Bug fixing


Understand you'll spend 20% of your time coding and 80% of it maintaining, so code accordingly.
Set up a good error reporting solution.
Have a system for people to contact you with suggestions and criticisms.
Document how the application works for future support staff and people performing maintenance.
Make frequent backups! (And make sure those backups are functional) Ed Lucas's answer has some advice.  Have a restore strategy, not just a backup strategy.
Use a version control system to store your files, such as Subversion, Mercurial or Git.
Don't forget to do your Acceptance Testing.  Frameworks like Selenium can help.
Make sure you have sufficient logging in place using frameworks such as log4j, log4net or log4r. If something goes wrong on your live site, you'll need a way of finding out what.
When logging make sure you capture both handled exceptions, and unhandled exceptions. Report/analyse the log output, as it'll show you where the key issues are in your site.


Lots of stuff omitted not necessarily because they're not useful answers, but because they're either too detailed, out of scope, or go a bit too far for someone looking to get an overview of the things they should know. Please feel free to edit this as well, I probably missed some stuff or made some mistakes.
QuestionKenntnis:
I'm graduating with a Computer Science degree but I don't feel like I know how to program
qn_description:
I'm graduating with a Computer Science degree but I see websites like Stack Overflow and search engines like Google and don't know where I'd even begin to write something like that. During one summer I did have the opportunity to work as a iPhone developer, but I felt like I was mostly gluing together libraries that other people had written with little understanding of the mechanics happening beneath the hood.

I'm trying to improve my knowledge by studying algorithms, but it is a long and painful process. I find algorithms difficult and at the rate I am learning a decade will have passed before I will master the material in the book. Given my current situation, I've spent a month looking for work but my skills (C, Python, Objective-C) are relatively shallow and are not so desirable in the local market, where C#, Java, and web development are much higher in demand. That is not to say that C and Python opportunities do not exist but they tend to demand 3+ years of experience I do not have. My GPA is OK (3.0) but it's not high enough to apply to the large companies like IBM or return for graduate studies.

Basically I'm graduating with a Computer Science degree but I don't feel like I've learned how to program. I thought that joining a company and programming full-time would give me a chance to develop my skills and learn from those more experienced than myself, but I'm struggling to find work and am starting to get really frustrated.

I am going to cast my net wider and look beyond the city I've grown up in, but what have other people in similar situation tried to do? I've worked hard but don't have the confidence to go out on my own and write my own app. (That is, become an indie developer in the iPhone app market.) If nothing turns up I will need to consider upgrading and learning more popular skills or try something marginally related like IT, but given all the effort I've put in that feels like copping out.

EDIT: Thank you for all the advice. I think I was premature because of unrealistic expectations but the comments have given me a dose of reality. I will persevere and continue to code. I have a project in mind, although well beyond my current capabilities it will challenge me to hone my craft and prove my worth to myself (and potential employers). Had I known there was a career overflow I would have posted there instead.
AnswersKenntnis
AnswerKenntnis:
Best way to learn to program is to write programs.

Two suggestions :


develop a game
develop a web site


Algorithms, while useful, and should be understood, actually play second fiddle to software design. TDD / Design Patterns / Architecture / Refactoring / Unit Testing / The process of putting code together / etc tend to be far more important skills.

Also, far better to do this in your own time.  Don't wait to work this stuff out on the job.  I find the people who tend to do better are the ones who early in their careers put the effort in to develop their skills in their own time.  Usually because they are genuinely passionate about software development


One more thing is to "Read books and samples" and don't be ashamed to ask. If you want to learn you should ask :)
AnswerKenntnis:
I felt like I was mostly gluing together libraries that other people had written


While I understand why you feel like this wasn't "real programming", the truth is that integration work makes up a significant percentage of the typical workload for a corporate programmer.  Your experience might be a little more valuable than you think :)
AnswerKenntnis:
First, thank you for an immensely honest question. There are a number of ways to tackle the issues at hand. Here are a few tips, which I considered very helpful for me in the past and still continue to use them to broaden my knowledge.


Learn, Learn and Learn some more. This is probably the single most important tip I can give you. Never stop learning. Knowing one language is good, knowing multiple is even better. Having knowledge of other languages will make you a better programmer and will make it easier to tackle certain tasks and will help you gain a better knowledge of common data structures.
Start small.
Start a hobby project in your spare time. Don't do something you can accomplish fairly easily. Take on a project wherein you have no idea where to begin. Throw yourself in the deep end. The benefits of this is that you will learn things you never knew existed and when you do complete it, you will feel an immense pride and satisfaction. This is what keeps me going.
Have a genuine passion for what you do. Although people will disagree with me on this one. I do not believe you can excel in this field if you simply consider it a 9-5 job. There has to be a passion to do it.
Help out other people on SO! The best way to understand is to try to teach it to other people.
Study other peoples programs and try to figure out how they work, then implement similar techniques in your own programs. Try to read it and get a understanding of it, then do it yourself based of that understanding, rather than copy and paste.
Keep at it. Things can get very frustrating at times, but very rewarding when finished. If you do not understand something, take a break, clear your thoughts and try again. Ask us at SO! We are a willing bunch :)
Never stop learning new technologies.
Read some books. I understand being a student, you would have done a tonne of reading. Here are a couple of practical books that you might find handyΓÇª hopefully
-- The Pragmatic Programmer: From Journeyman to Master
-- Code Complete: A Practical Handbook of Software Construction
AnswerKenntnis:
Start a personal project.  The trouble with school is that the most complicated thing you did there was a project that took 15 weeks to a year and involved a couple other people.  The problem domain was well-understood (your professor didn't give you any tasks that didn't fit neatly into your semester.)  This is not a luxury the real world affords.

If you have to do something major, start-to-finish, that you can be passionate about, your brain will start to wrap around the process.  As long as this is just a career and you don't have a love for it, you'll still feel like you haven't made it yet.
AnswerKenntnis:
Start on one of those in-demand languages,using a project as K. Nicholas says.
Don't measure yourself by StackOverflow.  That will discourage you unnecessarily.
AnswerKenntnis:
I can't help with your job situation, but I hope I can help you develop your skills and also put your feelings about your own skills into perspective.


  I'm graduating with a Computer Science degree but I don't feel like I know how to program.


It's possible that your instructors have something to be ashamed of.  It's also possible that your feeling about not knowing how to program are natural and appropriate to this stage of your life and education.  Here are some ideas that may help:


Many employers don't care what courses are on your transcript or even what your GPA is.  Instead they want to know what you can do, and especially what have you built.  For a good job, what you have built is more important than the technology (C, C#, what have you) on your resume.
If you didn't get the chance to build a number of interesting projects during your education, shame on your instructors.  But you can build those projects now.  Scour web sites for interesting problems.  Our second-semester students are just finishing "Song Search"—we pulled a huge amount of lyrics for a web site, they build an inverted index, you feed it keywords and it shows you lyrics that contain those words, in context.  It's not Google but it's made on the same principles with similar data structures, and you can start building it now.



  I've worked hard but don't have the confidence to go out on my own and write my app.


Maybe you haven't worked hard on the right kinds of problems?  It's good to find problems that


Are open-ended
Have more than one good solution
Have plenty of bad solutions


If you tackle these sorts of problems, you learn to make choices, to live with the consequences, and if things don't work, to go back and revisit your choices.  You'll learn more from your failures than from your successes, but you'll gain more confidence from your successes than your failures.

Good problems—with properties like the ones I list above—are like gold, except that if you get a good problem from somebody else, they don't lose anything.  Scour the web for good problems, and practice, practice, practice.  If the Euler problems are where you have to start, well they are OK for beginners.  But soon you will want to build small or medium-size projects that you think are really cool.  If you are excited about something you've built, that will impress potential employers.  If you are not excited, it's hard to hire you.

Peter Norvig reports that it takes ten years to become an expert.  Of course you don't feel like an expert right after getting your degree.
I will let you in on a little secret: Most members of the Harvard faculty (I was one for eight years) feel like they don't really belong at Harvard, they don't know enough, and it must have been some mistake that they were hired.  These kinds of feeling are very, very common for people making the transition from school to the workplace, or from one kind of job to another.  So common there's a name for it: "the impostor syndrome."

Even though you have your degree, your university will still talk to you.  If you had any really good professors, they probably still care about you.  They certainly care that one of their students has graduated with a B average and yet feels she hasn't mastered the basic skills of her trade.  So seek out one or two of the most energetic, most sympathetic professors from your program, and get some help finding good problems.  Then put yourself in charge of your skills, your knowledge, and your feelings about them.
Build a little somethingi every day, and don't waste any of your precious building time on anything that isn't really cool.  Eventually, I promise, you will recapture a sense of excitement about programming, and following that, you will be able to build confidence in yourself as well.
AnswerKenntnis:
There was a scene on an old Law & Order in which a DA complains that she learned nothing about how to do her job at law school, that she didn't learn anything about the real world.  The professor to whom she's complaining replies, "It's a law school, not a lawyer school."

The same applies to computer science.  Perhaps you didn't learn how to contribute immediately to some project you see online, but you probably developed the foundation you need to be successful in the long run.

First, get a job, any job.  Become self-sufficient.  Particularly in the current economy, I would never fault any candidate for working at a bookstore or whatever while they look for more appropriate employment.  I do have questions for people who sit around doing nothing.

Find a project, any project.  There are many applicable projects on github.com for instance.

The good news is that a lot sooner than you think, no one will care where you went to school, what your GPA was, or anything like that.

Hang in there!  It can be tough going, but you'll be glad of the experience one day.
AnswerKenntnis:
Have you looked at ProjectEuler? I taught myself Python by doing the problems on that site. :] If you're after learning Java or C#, you can try that out. Also, I recommend trying your hands on GUI programming as well.

Edit:

Here's a great topic on SO for many links you can look into for coding practice:

http://stackoverflow.com/questions/662283/websites-like-projecteuler-net
AnswerKenntnis:
I am guessing from your user name that you are a woman... If not, feel free to ignore this, or adapt it to your own point of view.

In my experience, women graduating from college in computer science consider themselves much less competent than their male counterparts with similar skills. One might say that women are (or, to be exact, women I know are) simply more honest relative to their own shortcomings, but in the end, they have more trouble selling themselves to employers.

(I spent most of grad school feeling like a total failure compared to my peers, and ended up graduating top of my class. )

So my advice would be to stay honest with yourself and keep on working hard on developing your skills. But do not downplay what you do know when looking for a job, and don't restrain yourself from applying for "reach" jobs. 


Find a mentor who can give you a real assessment of your value.
Conduct fake interviews, and learn how to project confidence, energy and passion. 
When you do land an interview, talk about your personal projects and ask technical questions. Your drive to learn and your energy is your best asset.


Added as an afterthoughts:


When you do land your first job, don't be afraid of negotiating your salary. Too many women just accept the first offer gratefully, and never get what they deserve.
AnswerKenntnis:
A lot of people have said that you should start a personal project. In my opinion, this is the best advice on here. I would add some things I didn't see when I read the other answers...


Pick something in an area that you are passionate about. The best
place to find this is maybe in your
interests outside of computer
science. That could be a non-profit
you're involved in, a hobby that
you're passionate about, a sport that
you do. 
Find a collaborator. Coding alone is hard, so another thing that would help enormously is
if you found a buddy to collaborate
with you on this project. This makes
it so much more fun and keeps you motivated. In his recent
blog post Jeff Attwood talks about
this exact thing in his experience of
building SO.
http://www.codinghorror.com/blog/2010/05/on-working-remotely.html
Pick something modest (initially). The ideal project
would start with something small. If
the goal out of the gate is too
ambitious then it will become too
daunting. Having done development for
PCs, phones, embedded systems and the
web, I would say that the web is best
place to look for something
achievable that other people could
start using immediately.
If possible, pick something that other people will use. Even if you only have ten "customers", the feeling of having other people use the thing that you have built is like a drug. Incredibly satisfying. Learning from customers and responding to them is also such a valuable learning experience.


If this project is a labor of love that you happily work on deep into the night, and then leap out of bed the next morning to get back to it, then good things will follow. You will learn, the confidence will grow. And once you have something out there that people can see, it becomes the beginning of your portfolio. Nothing impresses programmers and (decent) hiring managers than something real.
AnswerKenntnis:
A few comments, from the perspective of someone who has been a developer for 20+ years:


  I see websites like Stackoverflow and
  search engines like Google and don't
  know where I'd even begin to write
  something like that.


They are the product of teams, mostly building on libraries and infrastructure (.net, java, asp.net, etc) produced by other teams, and backed by experience and resources. That you, individually, don't know where to begin to do something similar is completely understandable. Don't worry about this.


  During one summer I did have the
  opportunity to work as a iPhone
  developer, but I felt like I was
  mostly gluing together libraries that
  other people had written with little
  understanding of the mechanics
  happening beneath the hood.


A lot of development work is now like that, I'm afraid. But there is a lot of scope for doing interesting work 'on top' of those libraries. And don't worry about finding algorithms difficult - you'll almost certainly never have to implement a quicksort, linked list, or whatever during your career. That's what libraries are for.


  Basically I'm graduating with a
  Computer Science degree but I don't
  feel like I've learned how to program.


Knowing how to program, and knowing how to function as a professional developer are two very different things. You just need some experience, preferably working with other developers on real-world systems. Try to add either C# or Java to your skillset - there isn't much difference between them so the knowledge is transferrable. Beware of becoming too specialised too soon. You may have to accept that you won't earn much money immediately, so keep your personal costs low for a while if you can.

Start working on the project you mentioned, but as well as increasing your programming knowledge, try to use it was a way to get experience of related skills like version control, unit and integration testing, and even writing simple documentation. These sort of skills are what distinguish a developer from a programmer, and are a good showcase for a prospective employer. There are lots of free tools available (the express editions of Visual Studio, github, nunit, Google apps) that can help.

From what you've written it sounds like your CS degree has taught you how to think about technical problems. You also seem to have a good level of self-knowledge, including about your current technical limits and experience. Use these as advantages. Now isn't a good time to be entering the job market, but if you work hard at it you'll be okay. Don't worry, learn, get experience, stay up-to-date, try to do things you enjoy.

Good luck!
AnswerKenntnis:
Programming isn't all about your understanding of algorithms or your GPA during college. Programming is about having the ability to think outside the box, desire and willingness to learn and most important of all, creativity.  

On a personal note, I had just graduated college last May and I had a terrible GPA. I had focused more on my social life than academia and I paid the price.

However, during my recent job interview out of college, (which had took me less than a year to land) I showed off my creativity, passion for learning and analytical skills, which had helped me get the job.
AnswerKenntnis:
Note:  I expect this post to be downvoted. This isn't an answer to the original poster but an observation on most of the comments I've read above.

The answers I'm reading above are really scaring me. There seems to be an almost universal feeling that comp.sci degrees are useless or not relevant, or don't teach you anything.  Have comp.sci degrees really become that bad, or is this all just hot air from developers without degrees trying to justify their lack thereof?

Seriously people, what are they teaching people in university nowadays?  When you do a CS degree (at least when I was a student) by the time you graduate you would have a good grasp of compiler design, comp. complexity, formal methods and logic, a whole zoo of data structures and algorithms, basic operations research (LP etc), databases, cyptography and security, scheduling algorithms, network protocols, internet development, OS and kernel design, parallel algorithms and data structures, numerical algorithms and a whole lot more.  The whole lot is typically topped up by a huge programming project that forms part of the final dissertation.  Don't tell me that comp.sci graduates don't get enough practise programming.

I also appreciate the fact that some of the more traditional comp.sci degrees omit some aspects of the software development cycle,  you probably won't hear much talk about TDD, unit testing.  But let's be  brutally honest: it isn't really rocket science, is it?  You won't hear about SCRUM or function point analysis either. Teaching detailed courses on such basic issues would be pretty redundant, though many degrees nowadays do have a course on system design where they do give students an idea of how these things work.

Granted, you might not be an expert in .NET, PHP or the other current industry standards but that should be irrelevant. In 5-10 years time the languages of choice will have changed anyway. Just because you learn to drive in a Peugeot doesn't mean you shouldn't be able to drive a Fiat, and the same should apply for programming languages.   It should take a few months practice with a few good books to be competent to work in any language.

Surely if anything is a waste of time its these costly "certifications" which basically involve sitting for an electronic multiple choice exam and ask you about (usually pointless) minutiae of a particular architecture or language.  They tend to measure a programmer's competence by his/her knowledge of certain pathological programming cases.  

I have worked in the I.T industry in various companies and I have also been involved in the interview process for other software developers.  We did set out a few of the usual fizzbuzz questions, but there were aimed mainly at people without formal training or trainees.  I have never ever met a comp.sci graduate not capable of answering a basic and not-so-basic programming questions.

Note I live in Europe and know only about European university degrees.  However I'm pretty sure that the university standard in the U.S is much higher than that in the EU.
AnswerKenntnis:
Your humility will serve you well.  The Beginner's Mind is helpful for all kinds of learning, no matter how much education and experience we have.

Work through exercises, as others have suggested - at Project Euler and elsewhere.

Work out solutions to help others here on SO.  The exercise of understanding the question, determining what you know that can apply, and finally articulating an answer will help build your confidence, as the exercises build your skills.

Stick with it; you'll be fine.
AnswerKenntnis:
Graduating with a comp sci degree no more makes you a great programmer than graduating from a music program makes you a great musician.

There's no substitute for  practice, practice, practice and experience. Program 8 hours a day and in 5 years you might have that understanding that you lack right now.
AnswerKenntnis:
Such a candid question, great answers - I'm gonna chime in briefly :)

The answers so far made me lol a bit - they potentially celebrate our own greatness a tad ironically. I come to SO because of poor documentation and bugs in frameworks. There is obviously other gold here but it is worth retaining your humility, even if you do become a great dev - and I say that from the perspective of personal historical(?) arrogance.

Keep in mind that you may not (are probably not) being hired by a techie though better HR people and managers will use a techie to assess you.

Employers have a plan for their employees, try and perceive that plan the whole way through and slot yourself into it respectfully, perhaps giving it a bit of personal spin and enhancement. Difficulty and opportunity can arise when an employer doesn't really have a specific plan - asking good questions and helping them specify the plan can really make you stick out in these situations.

Business people can be (rightfully) paranoid about devs patronizing them as we often have to manage their perceptions a bit to help them with decisions they don't understand - and I say that, tongue in cheek, to show you the perspective NOT to have or develop of your non-tech co-workers. I humbly think that understanding that this is often the essence of relationships between techs and non-techs is important - and it's hard to avoid, what we do is obtuse.

Being professional, open-minded and respectful does get jobs - if you're fuzzy on what professional is, I'm sure there are places to work it out on the web - I wish somebody had pointed that out to me when I started. :)

The final thing that I would say is that, as you get better at development and architecture, and you already sound like you've stepped onto this path, you may find your professional code can become intensely dis-satisfying, even if it is the appropriate solution.

I'm not sure what the solution is there but try to find an outlet and be less emotionally involved in your work, it'll help you pace yourself and live a better life - be especially careful about trying to put in extra effort to make something "right" - a lot of the time you will be creating complication for your co-workers and you'll almost never be able to put in the amount of time you need to realise it within the timeframe of your project. A symptom of this is "going dark" - when you don't want to explain what you want to do to anyone. Many of the best devs can explain the essence what they're doing to a lay person - this is a great skill to learn and I've found it intellectually liberating to practice.

Heh, and when estimating, to start with, divide your task up into bits, total up the time and then double it (it's called, divide, conquer, march home ;)

Good luck! I left school expecting to be a ski instructor and ended up a lead dev. I'm sure you'll do well at whatever you end up doing too.
AnswerKenntnis:
Yes, it's reasonably normal- most schools, even prestigious ones, do a great job of teaching computer science and a terrible job of teaching software development.  This is getting slowly better, but still has a long way to go.

Anyway, it sounds like you're doing most of the right things:


Program outside of work
Read books on software development (Code Complete, Design Patterns, Mythical Man-Month, etc).
Keep learning new technologies- school should have taught you how to learn languages, not the languages themselves.  Learn new frameworks, IDE's, apis, libraries, build tools, etc.
Hang around on sites like SO and here.  Interacting with experienced people on a regular basis is one of the few semi-shortcuts to wisdom.
AnswerKenntnis:
Really great question. I'm sure there are a lot of people who are going through exactly what you've described in all walks of life and job markets.

First - no more worrying about things that are out of your control. You are not allowed to stress or even think about school, what you did or didn't learn or how well you did academically. 

Second - specialize. There will always be demand for people who are really good at what they do even if what they do happens to be obscure. You need to pick your favorite programming language and resolve to completely master that language and ΓÇ£make it your own.ΓÇ¥ You already have a lot of great advice about how to improve programming skills but at the end of the day nothing compares to do finding an authoritative book on the subject and locking yourself in your room for a few days while you do nothing but read every page and write out every example the book gives. 

Third - advertise yourself. In this history of computer science this step has never been easier than it is today. The answers that you give and the questions that you ask on SO are your resume. Take it upon yourself to become the leading authority on SO for that language you've decided to master. Take responsibility for any question that comes through this server with your tag on it, even if it means hours of extensive research and coming up with late answers. Search the archives and read through every question ever asked on SO on your topic. Fix misinformation, provide your own answers and variations to answers and combine a few existing answers into one better answer. Flooding SO with an endless stream of your comments, answers and edits, should be your raison d'etre (plus its fun because you get reputation points).

Fourth - work on your public image. Self-confidence is essential for landing the best job. Companies want people who are great coworkers, collaborators and communicators. If you think this might be a problem, tell your friends that you need them to boost your ego and shower you with endless complements then go to your local library and pick up one of these.
AnswerKenntnis:
Okay, so saw this hacker news and I was like "HOLY CRAP THAT'S ME!!"

So I graduated last December with a degree in CS and felt that somehow I had managed to pass my classes and get a degree without actually earning it.  It felt like all my classmates had incredible coding skills, and that all I could do was make basic Java programs.  Learning how to code and becoming a good coder just takes a ton of time.  There is sooooo much stuff out there to learn and my advice is just to take it a little at time.  You'll learn things as you go.  Like everyone else has sad, the best way to learn is by doing especially with coding.  Not to mention there is SO much about building applications that they don't even to bother tell you about in school.  Don't feel bad, just know that there is a lot out there for you to learn, and realize that it's gonna take time to learn it.  

I think you should definitely look past the city where you grew up.  There are a ton of companies out there hiring CS grads, and not all of them require or expect you to be a great programmer right out of school.  There were plenty of interviews I went on where I didn't get asked any coding questions (there were also a lot that asked me to solve algorithms and to code).  Apply at companies that care more about your ability to learn than what you already know (in my experience bigger companies care more about this because it won't impact them as much if you aren't able to crank out code right away).  

I don't know if coding is something you want to do and pursue, but there are also plenty of jobs that are non technical that require a technical background.   You could look into those options.  

I wouldn't focus on learning the ins-and-outs of one language like C# or Java and focus on the principles of programming.  You should be able to transfer good coding skills and apply them in any language (this is important for longevity in your career..Java isn't gonna hot forever).  On the other hand, knowing the "hot skills" can help land a job.  Interviewing skills are REALLLLLY important.  Take every interview you get, even if it's just for practice.  

Anyways don't think big companies like IBM are out of your reach.  I felt the same as you a few months ago, and I work at IBM now.  My GPA was not that great either. I still suck at programming but I know I'll get better with time.  Just be confident in your ability to learn!
AnswerKenntnis:
Wow, what a lot of answer before this one.

How about a completely different response...

Lets reframe your problem in terms of being a human.

"I've graduated conception class and I'm just about to be born. My mother is crowning and I'm due to be delivered tomorrow morning. But when I look at athletes like Usain Bolt I think "how can I ever run that fast?".

But here you are, you are a fully fledged human, you've gone through school, you can walk, talk and if you've done a computing course you're pretty intelligent to boot. Nothing to be ashamed of. 

Now, did Usain Bolt get born and then suddenly start putting in insane times on the track, or was it a large amount of getting on with life, finding out what he was interested in, followed by an incredible amount of application and then suddenly "Bam!" he was there. 

More likely the latter. So don't give yourself a hard time for not knowing what you should be doing. It takes time to work out what is right for you. That is for you, not what your parents think, not what your peers think. What is right for you. What lights your fire. What makes you think *yes this is cool" even when everyone else thinks that is pants.

Its not uncommon to start (and/or) leave your degree having no idea what you want to do. 

For me, I was good at school at the O levels (a UK qualification) but I couldn't give a damn about the A levels (required to get into University to do a degree). So I went to a lesser place to do my degree (and as a result not the degree I would have preferred). But I stuck it out (still not having a clue what I wanted as a career) and learned a lot about electronics and stuff that most software guys never learn. During this time I freelanced as a computer games writer. 

The end result? After gaining my degree (with commendation) I walked into a job with a job interview at a higher salary than those doing straight electronics and I didn't have change my hair or wear a suit. What? Well at 23 that stuff matters (looking back now, at age 44 I kind of laugh at it, but thats the vapidity of youth for you).

That company went bust and then I started getting more serious. But it probably wasn't for another few years before I suddenly found (by accident) what really interested me. Turned out to be low level software tools. Should have been obvious - all the computer games I wrote were written in assembly, I always liked the low level stuff that no one else could get their heads around. But to see that as a future, that takes more maturity and no surprise that it didn't really manifest until my mid-20s.

I'm often impressed (and amazed) that young people, often still in their teens, seem to know what they want to do. But the real question is, are they still doing it at age 27? Or have they changed course because their early ideas were not correct for them?

In terms of how do I do something as impressive as THAT (whatever that is, Google, SO)? Well, you do it in chunks, just like you do software and everything else in life. You start with the basics, get experience in it. If you're good enough you carry on and get more experience, etc, or you abandon it because you realise that you're not good enough or that its as boring as hell (thats why I don't do comms even though its always been a gold mine. For me, its boring!).

By all means, look at your peers, look at your elders, examine their choices and interests. But examine your own as well. Often what at first sight seems mad/bonkers is actually the real deal. Doing something that you find interesting (rather than just paying the bills) is SO much more rewarding. 

Yeah I know as a 23 year old you're focussed on the Audi TT and the cool flat and thus tempted by money rather than sensible career choices, but seriously at some point you'll realise the cool car isn't all its cracked up to be - that girl should like you for who you are, not what you drive.

Seriously, think about it. The not so cool solution may just be the right answer.
AnswerKenntnis:
In addition to the great advice given by others, I would add participating in developer events in your area. Look for meet-ups, user groups, bar camps, code camps, etc. This will help you network with other developers, get job leads, keep up with new technologies, and provide a realistic peek at the skill levels of other developers.
AnswerKenntnis:
Follow the 10,000 hour rule.

In order to become a master of something, you need to practise for at least 10,000 hours.  So spend 10,000 hours programming, and you will become a master of it.  If you have not spent 10,000 hours programming and you don't feel like a master of the craft, don't be disheartened, just spend more time coding.

Also note that it will probably take you around 3 and a half years to do this if you program for 8 hours a day.  If you only program during business hours it will probably take you about 4 years.  If you haven't spent this much time during your 3/4 year degree programming, then you probably won't feel like a master.
AnswerKenntnis:
Just a couple of thoughts, if I may.

Wendy says that one can graduate with a CS degree, yet have little idea of the practice of programming. Isn't that hugely troubling, even horrifying? It's as disturbing as the fraud that submerged English departments in the '70s: literature is not a collection of ideas that one can understand and integrate, but rather a bunch of 'texts' that the student must abstract and 'deconstruct' ( http://www.answers.com/topic/deconstruction ). Happily, that fetid tide is ebbing, maybe because the instructors who wiped out in its weedy surf are retiring now. 

Years ago -- decades ago -- my first course in CS taught me assembly language (before C was invented) using actual problems like sorting, hashing, and searching (and, yes, recursion). My second course taught the design and realization of a real live working compiler. I was a part-time student at MIT and those two courses were all I needed to begin getting paid as a programmer; and to become just somewhat productive two or three months later. 

So this morning, hearing Wendy's cry, I'm thinking that surely MIT, of all places, cannot have diluted its offerings and deluded its students with (in the context of programming practice) pretty-much-useless crap. But when I look at MIT's EE/CS curriculum, I see that's just what's happened:

http://student.mit.edu/catalog/m6a.html

I particularly notice that the the department uses Python as a/the teaching language! I mean, really! It looks like a CS degree at MIT means to qualify a student to become a teacher of CS at MIT. Talk about recursion!    

Then I came across this contribution to Coding Horror ( http://www.codinghorror.com/blog/2006/07/separating-programming-sheep-from-non-programming-goats.html ) and thought it very much to the point of this conversation:

"I'm a latecomer to this discussion [about predicting success in programmer candidates], but in my experience as a late-blooming 30 year old CS undergraduate senior, I've found the programming classes to be useless, and less badly taught as un-taught. I've only been to a community college and then the University of Illinois in Chicago, but the introductory programming classes were:

"1. Object-oriented, which left students with little or no understanding of procedural methods, and

"2. Weed-out classes. The classes consisted primarily of descriptions of different types of problems and the mathematics behind them, rather than ayntax and structure, for which people were told to just read the book.

"Code was barely directly acknowledged until the Data Structures core, and then it still depended on which instructor you got, some being very code light and some being nicely code heavy. You could tell that it was a big temptation for teachers to be code light at this point, because if they concentrated on code, they would also have to concentrate on teaching students who had been in a computer science course for two years how to program.

"Since coding is a hobby for many young people, I think that educational institutions have relied on that to establish their expected learning curves, leaving people who had little to no experience programming when they entered school no choice but to cheat like crazy, spend all of their spare time studying code, or switch majors. And it isn't a necessarily a deficit in abstract thinking in my experience, because everyone I know who dropped out of CS ended up in Electrical Engineering, which is nothing to shake a stick at on the abstract front. They still don't know how to program, while doing math that I can't make heads nor tails of. Most absurd memories:

"1. Java as the required programming language. I'm not going to bash Java here, but wouldn't it be nice for students to have to learn their own garbage collection? And wouldn't pointers be a nice thing to learn, even if we never decided to program in a language with them again?

"2. Taking a core class on operating systems theory, after being being deluged with Java, and finding out it was in C (of course) without even one C class on campus?

"Of course, I'm thirty, and also one of those people who always programmed, so I had no problems, but I saw plenty of people who I knew were better at abstraction than me (from Calculus, DiffEQ, and physics classes before) and their total agony at trying to finish a program that had been stacked against them."

Just a couple of data points but, as others have said, haunting.

-- Pete
AnswerKenntnis:
Firstly, hang in there!  

Secondly, here are some things that helped me:


Keep your job search up beyond your region of interest.  Definitely be ok to re-locate.  Great chance to get out and see a new place!
Because experience is low, I think interviewers want to make sure you're someone that will be excited and energetic about working and solving problems.  So I made sure I was interested in coding for the company.  Which I was ;)
Ask your interviewer questions.  Research the company and have some material ready during the interview.  What design patterns do you use?  Why X technology instead of Y technology?  I feel this rounds you out as a person during the interview and gives you a chance to take a break.
Code for fun at home!  It doesn't have to be successful, but just write code that maybe utilizes a technique you've read about or a technology like a database.


Thirdly, I was in a similar boat as you when I graduated so again, hang in there and keep searching.  Your first job is out there.
AnswerKenntnis:
Very few people graduate any discipline being an expert at something they have merely studied. Computer science isn't in any way special in that regard. Nothing beats empirical experience and you only get that from developing fully-fledged software for real clients, with all the demands, time-constraints, changes and teamwork this involves.
AnswerKenntnis:
I found this wicked site the other day http://99designs.com/
Under website design or other design you can probably find some software related projects.

This would be a great way to get coding, develop some new skill, meet some new people who may be potential employers and you may even make some money.

I have found there is great value to employers in showing that even though you didn't have a job you weren't sitting on your arse. Show that you got out there and did some projects, preferably ones you can show off at an interview.
AnswerKenntnis:
Don't worry. Rome was not built in one day.

For each P in PeopleYouKnow
Try
  ask/call P for a Job apply for job;
Catch Denial As Exception
  don be worry;
  //you'll get a job later
End Try

Finally
 If you haven't found a job yet
  For each programmingJobAd in internet

 Try
    apply for job;
    Follow up;
  Catch Denial As Exception
   don be worry;
  //you'll get a job later
 End Try
end
//Keep trying.
//find a bug from this code.


EDIT: #! diff A B

3c3
<   ask/call P for a Job apply for job;
---
>   ask/call P for a Job job; if job is available apply for job;
9a10
>  //(sic)
14c15
<     apply for job;
---
>     apply for programmingJobAd;
20c21
< end
---
> End
AnswerKenntnis:
First, don't worry that you can't code google. Google was made over a number of years by a lot of very experienced programmers. That's like finishing a visual arts degree, and wondering how you could make The Last Supper.

For job hunting, don't sweat the requirements. Just call them, and say that you don't have the years, but you would still like to apply. If they really want the experience, ask if there are more junior positions available - they might be able to create a new position just for you. Most jobs are created for a specific person. Make sure you contact the project manager, not the HR department. HR doesn't usually create new jobs, they often just screen applicants for existing jobs. Google is your friend in this case ;)

Don't try to code a web app (like google or stack overflow) unless you want to invest about 6 months. It's a huge learning curve. You need to learn to manage a VCS, run a web server, HTML JS and CSS coding, a database system, and the web app language. It's brutal. Most of these technologies aren't transferable unless you want to do web work.

If you do want to do web apps, you might look at installing a simple web app (like this django-based IP to country lookup app - http://www.coulix.net/blog/2006/aug/17/ip-country-flags-django-comments/). You could shop around for a $90 dreamhost discount code (so you can work on a real web server), and try to set things up. Don't worry too much about security or performance (but do use ssh) - it's just a learning project.

If you want to do stuff on the desktop, you could have a look at pygame.
AnswerKenntnis:
You can read all about programming, but you won't learn how to really program until you well.. start programming! 

I would recommend you start a personal project. What's something that you want to create? a game? A blog? It doesn't matter. Just make something! 

Then, after doing some real coding for a bit(few weeks or months) I recommend trying to contribute to open source projects. The personal project helps you to figure out how to program when the goal is not already laid out for you(knowing how to actually design something is not often taught in school). Contributing to existing projects teaches you to work in a team and to follow code standards. 

I wouldn't waste my time reading a whole lot else. I'd say at most you should probably read about 20% of the time and write code the other 80%. (of course, by time, I mean your time set aside for programming related things)
AnswerKenntnis:
Well, here's MY two cents... coupled with quotes from a few other sources.


  A surprisingly large fraction of
  applicants, even those with masters'
  degrees and PhDs in computer science,
  fail during interviews when asked to
  carry out basic programming tasks.
  
  -- Dan Kegel via Jeff Atwood 
  
  http://www.codinghorror.com/blog/2007/02/why-cant-programmers-program.html


This is a problem that I've seen plenty of, even down to the point where people with years of "experience" on their resume not being able to perform simple tasks. Personally, I think the educational institutes are largely responsible for this, from the faculty who don't know the subject matter or can't teach it to the counselors who should be steering students that only want to get into a particular for the money without a reasonable interest in the field itself.

From the sound of your question, you're coming out of college feeling like you just got screwed out of a few years of your life to get a piece of paper. That's how a lot of hiring agencies view a degree these days for programmers. They want people who have proven that they can program, not just someone who has a piece of paper. But the fact that you're asking this question in the first place shows me a desire to actually learn to program.

One of the best things you can do to learn has been pointed out several times already, which is programming stuff. Unfortunately, while this absolutely will increase your knowledge and understanding of the technology, it's not incredibly likely to help you find work unless it's something you can put on your resume.

I suggest that you start by building yourself a few applications with specific purpose, figure out what you can do and what will challenge you to move forward, then go to some places where you can pick up freelance work. Don't expect to get paid a huge amount for freelance work you pick up on the web, you're looking to add it to your resume, not your wallet. Once you've got 3-5 items that you can point to and say "I built that" or "I designed that functionality for them", then list all that stuff out, and find a hiring agency (they're EVERYWHERE... the headhunters) and work with a few of their people to streamline your resume so it highlights your education and resourcefulness in finding work to do, and minimizes the aspect of your lack of experience.

The biggest thing to remember when you're talking to potential employers (that I see people mess up all the time), and something that ANYONE IN ANY FIELD should pay attention to, is that while you absolutely don't want to sell yourself short or minimize what you CAN do, don't OVERSELL yourself and make them believe you can do things that you have no clue how. Employers always have a knack for asking you to do things you don't know how to, and it's your job at that point to figure it out, but if you tell them you already can, you're inevitably going to eat your words later.

Best of luck to you!
QuestionKenntnis:
What is the single most effective thing you did to improve your programming skills?
qn_description:
Looking back at my career and life as a programmer, there were plenty of different ways I improved my programming skills - reading code, writing code, reading books, listening to podcasts, watching screencasts and more.

My question is: What is the most effective thing you have done that improved your programming skills? What would you recommend to others that want to improve?

I do expect varied answers here and no single "one size fits all" answer - I would like to know what worked for different people.
AnswersKenntnis
AnswerKenntnis:
In no specific order...


Working with people far smarter than myself
Always listening to what others have to say, regardless if they're junior, intermediate, senior or guru.  job title doesn't mean anything.
Learning other frameworks/languages, and seeing how they do things, and compare that to stuff that I already know
Reading about patterns, best practices, and then examining my old stuff and applying those patterns where necessary
Pair programming
Disagreeing with everything Joel says. ;)
AnswerKenntnis:
Deciding TO be a 'Jack-of-all-Trades'

Fairly early in my career, I was an expert with a particular database and programming language.  Unfortunately, that particular database lost the 'database wars', and I discovered that my career options were ... limited.  After that I consciously decided that I would never let myself become boxed in like that again.  So I studied everything I could get my hands on:  Windows, Unix, C, C++, Java, C#, Perl, Python, Access, SQL Server, Oracle, Informix, MySQL, etc.  Whatever tools and technologies are new or unusual, I became the 'go-to-guy' -- "Ask Craig, if he doesn't know it, he'll learn it."  As a result I've worked on all sorts of projects, from embedded systems for environmental telemetry to command and control systems for missile defense.

The only problem I've ever had is with companies that insist on pidgeon-holing me into a specialty, when my specialty is being a generalist. [EDIT: Also known as a Polymath or Renaissance Man or multi-specialist.]

Something to keep in mind ... what's the half-life of knowledge in high tech?  It tracks with Moore's Law:  half of everything you know will be obsolete in 18-24 months.  An expert who chooses the wrong discipline can easily be undermined by the press of technology; a generalist only has to add some more skills and remember the lessons of the past in applying those skills.
AnswerKenntnis:
I always thought of my self as a pretty hot-shot programmer.  Then a new guy, call him Aaron, was hired into our team.  Aaron was obviously much better than me in most areas.  He was younger than me, too.   He made me realize I hadn't really improved much in the past years.  I was an ad-hoc hacker, and a mediocre one at that.

This alerted me to consciously try to improve myself and especially the quality of code I write.

Aaron lead me to learn a lot of things.  He taught me how most of the code I write will have to be maintained and extended for at least several years, so I should write the code with that in mind.  I should write automatic tests for my code.  Aaron was always talking about how I should never stop at the first working version, but refactor and refine until the code is elegant.  I've discovered that the languages and tools I was using had a lot of room for improvement.  

The most important thing I learned from Aaron was to never stop learning. 

After a couple of years, Aaron left the company.  I felt empty.  The past years with him had lifted me to whole new levels of skill, and I realized I was now much better than the rest of the team.  They were still writing bad code, and doing the same mistakes as before.  I tried to teach them, but they had no interest to learn.  In fact, they were annoyed that someone would be so arrogant to tell them what mistakes they were doing.

So, a few months later, I left the company as well.  I moved to a smaller company with a very talented team.  Everyone there wanted to learn more, and I loved it.

I'm glad I met Aaron.  Without him, I'd probably still be working at the old company with the old gang, going nowhere, and thinking too much of myself.
AnswerKenntnis:
Two things:


Read code written by different people. 
Write documentation for code written by other people. 


Writing code is extremely easy; every other person I know can do that. But reading someone else's code and figuring out what it does was a whole new world to me.
AnswerKenntnis:
Hit the gym regularly.

Seriously, my brain works a whole lot better when I'm in shape.  Problems become easier and less overwhelming, goofing off is much less of a temptation, and working through things step-by-step doesn't seem like such an arduous task.
AnswerKenntnis:
Programming.  Working on interesting projects.  There is NOTHING like getting in and working on stuff.  Especially under pressure.  I always tell anyone who asks me how to program - just find a cool project (even if you have to make it up) and work on it.
AnswerKenntnis:
Took a part-time job tutoring CS students at my university.  It really forces you to understand something at a completely different level when you have to explain it to someone else.
AnswerKenntnis:
I'm a big fan of the "learn one programming language every year" system.  One year gives you enough time to get past the "okay, I know the syntax, so now I know the language" bias, and forces you to go a little farther and understand what's beneficial in that language, and program in a style native to that language (By which I mean, you don't end up writing java applications using Ruby syntax).  Each language will change the way you think about programming-  I knew how to use recursion, but thinking in recursion didn't happen until I took a class on prolog (I imagine a functional language like ML would have the same effect).
Start a Pet project.  My personal equation for a good pet project is, something you have experience with + something you don't = app you would find useful.  For instance, Migratr (my own caffeinated-weekend-turned-ongoing project) started out as "I know c#, but I've never coded against a web API.  And I want to move all my photos to Zooomr".  It could just as easily have been "I've coded against web API's before, but I don't know C#"


Publishing your pet project is an amazing educational experience in itself.  Suddenly all the things practically nobody teaches but everybody's supposed to know (for me it was setting up your own testing system, getting the most out of version control systems, how to pace yourself when nobody else is setting your deadlines, how to interact with your users and how to know when to say "no" to feature requests), all that stuff bubbles to the surface and forces you to self-educate on a level you weren't before-  at least not by idly reading flamewars on dzone about the pros/cons of the "foo" vs "bar" way of doing things.

Doing these two things covers both ends of the spectrum.  Learning a new language will make you a better coder.  The pet project will make you a better developer:P
AnswerKenntnis:
Taught myself assembly.  Did it on an old 6502 chip when I was 13? 14?  Too long ago.  But I can't think of anything that will improve your development more than getting down to the bit level.  

Learning assembly gives you insight into the way computers 'think' on a fundamentally lower level, and the elegance at this level is surprising...  there are no wasted motions, no 'disposing' of data.  Developing at this level will teach you efficiency and hone your critical thinking and logic skills.  It will also cure you of any sloppy habits you have fairly quickly!

The 65xx chip had three registers (the accumulator, X, and Y) and no machine level instructions for multiply or divide.  I remember coding a routine to calculate battle damage, looking through the book, and suddenly realizing that I would have to write my own math library.  Spent a couple of weeks scribbling 1's and 0's all over my notebook, trying to figure out what 'divide' and 'decimal places' really meant.  

I've studied C++, pascal, .NET, many others since then... but none of them have taught me as much, intrigued me as much, or left me with the sense of 'wow' that assembly on my old commodore did.
AnswerKenntnis:
Looking back at old things I wrote and realizing just how bad they were.
AnswerKenntnis:
Read


books, not just websites
for self-improvement, not just for the latest project
about improving your trade, not just about the latest technology
read code, not just you are working on.


Just develop the appetite for reading.
AnswerKenntnis:
Programming.

Seriously, there are books, there are coding katas, there are sites like this, but I believe that the best way to improve as a developer is to work on real project, with real fickle customers with real, ever-changing requirements with real engineering problems.  There's no substitute for experience.
AnswerKenntnis:
I think the most important thing you can do is make a conscious effort to improve.  There's no single silver bullet, you have to keep looking for new sources of information, new experiences, and more practice.

And the second most important thing, think about what you're doing, why you're doing it, and how you can do it better.  Same thing with previous projects.  Look back at what you've done, and how you might do it differently now.  Think about what could have been done better, or where you could still improve on it.

I see two great examples of this at work every day.  I have one coworker who loves to learn, and wants to be the best developer he can.  He's uses any downtime to read blogs, read books, discuss programming techniques, and ask tons of questions.  He's also very noticeably improved in just the past year.  Another coworker does his job, and does it fairly well.  But that's all he does.  He sticks with what he knows, doesn't make much effort to improve, doesn't work on any projects outside of his existing ones, and after 4 years, he has the exact same skill set and programming ability that he had when I met him.
AnswerKenntnis:
Many people have suggested writing code. I'd have to say that reading other people's code is much more beneficial.
AnswerKenntnis:
Pair-programmed with very diverse and opinionated people
AnswerKenntnis:
The basic things that helped me as a programmer:


Learned Touch Typing. 
Learned to overcome shyness and ask
questions.


Typing for a programmer is essential. Everyone has had a "programmer" coworker who typed using exactly two fingers and had to look at the keyboard for everything. Not fun. Learning to touch type give a huge boost to your productivity as a programmer.

And if you don't ask, no one is gonna tell you.
AnswerKenntnis:
Contributing to/participating in open-source projects was by far the biggest thing for me.
AnswerKenntnis:
You can read all the books, code, and open source projects you like, but you need to understand the end-user aspect of software development. You need to step out of the echo chamber. So I'll address a couple non-technical points that will help your technical career.


Step away from the keyboard and interact with the end-user and see, through their eyes, how they use the software. End users are typically not technical, so they see software as a magical piece of work, while you see software as a logical set of steps. The two worlds are completely different. So what seems easy and logical to you may seem cryptic and intimidating to others.
Test, test, test. A lot of the software I've seen in large corporations use test cases.  Hell, they use JUnit, xUnit, and all the other unit testing languages out there. But the problem I've seen is that most programmers never see what their software looks like in Production. Learn how users (or systems, if these are batch jobs) interact with your application, library, or interface to find out what kind of abhorrent information they throw at it. This will help you generate good test cases and stop assuming your program will always be fed the correct set of data.
AnswerKenntnis:
Learned Scheme.
AnswerKenntnis:
Writing code and lots of it.
AnswerKenntnis:
Learning regular expressions .
AnswerKenntnis:
Competing in TopCoder Algorithm contests.
AnswerKenntnis:
Go all out: create your own project, your milestones, your resources, dependencies, requirements, and test plan.  It will force you not only to improve your programming skills to operate within specific parameters, but will also serve to highlight exactly where you most need to improve.  Make regular updates about your progress, whether through a blog or more formal project updates, so that you can see exactly where you've been and where you hope to go.
AnswerKenntnis:
Quit my last job.
AnswerKenntnis:
I think constantly questioning what you are doing is the biggest thing.  Never think that your code is perfect, always strive to improve it.  

It seems like I've had 2 or 3 times when I thought my code was perfect, then realized I had a long way to go.

I guess the biggest thing was when I started seeing my code itself as consumed by other programmers and not a machine.  It's easy to write code your machine can process, but it's tough writing DRY, understandable code.

And I don't mean just understanding "What does this line do", I mean making it trivial to figure out "How does this class fit in with all the other classes" while making the classes interface so well-formed that it's virtually impossible to misuse it.
AnswerKenntnis:
They say that 70% of good code is error checking and handling. When I started programming that way, my code got a lot better. Thinking about what can go wrong and then handling it right away has made a huge difference. It feels like doing all that checking is just getting in the way of getting the code up and running, but it shortens the time from start to finish by a factor of 2 to 4.


  Just who are these people "they" and where do "they" live?
AnswerKenntnis:
My coding skill improved a lot when I started wondering before implementing something how am I going to document this thing.

"Thing" here should have all the possible granularity. From the method to the whole product. For instance at the method level it prevents adding a method in the API that doesn't fit, or is unclear, before actually writing it. And if I really need to implement a method I cannot document (easily), it's a sign there is a design problem somewhere...

Automatically, the attitude "if I can't explain it, I don't write it" filters out bad code and  conversely once I know how to document correctly a thing, it becomes simpler and cleaner to implement.
AnswerKenntnis:
Constantly learn and practice what you learn.

By means of: 


Personal Projects: Ever since I started programming I have been doing personal projects.  Ranging from little games, image processing, steganography, implementing file type specifications, implementing various protocols from scratch, or implementing various programs over time.
Reading books:  I decided to read and follow through various books in my spare time.   There are a lot of well written books by experts just sitting around waiting to be read.  The depth you can get from a book is unmatched by for example reading various forum posts.
AnswerKenntnis:
This is usually my chronological order of learning any new technology:


Regularly read good blogs (Atwood, Martin Fowler, etc.), Keep up-to-date with technology news, Follow stuff about interesting new technology. These steps will let me decide if I find anything interesting to further explore.
Read the right book or any other resource to learn for your level (e.g. for beginners if you want to learn design patterns, I would suggest 'Head First Design Patterns'). I have also specific preferences for books.
Roll out a toy project or two using the thing I learned. I don't worry about the usefulness of the project. My intention is just to exploit my learning. (e.g. A calculator project for OOP would be fine)
I would see if I could use the stuff at work. (e.g. Though we don't use subversion at work, I use it as my local repository, I used Ruby for a task which would otherwise be too monotonous, and time consuming)
This is the best part which I think most people miss out. Knowledge sharing sessions.Give a session or two to fellow team members for example. I believe teaching is one of the best ways to really learn the technology. I guarantee your level of understanding of the technology will become multi-fold, whether you audience gets it or not. :-)
AnswerKenntnis:
Hack on some open source project for a few months; the larger the better. When you're interacting with some highly opinionated, geographically diverse people who don't know you, you can't help but learn from your mistakes far faster - I think it's a certain embarassment factor. Plus, if you identify one or two really smart people, then you can glean valuable insight, if not pure knowledge, from them.
QuestionKenntnis:
My boss decided to add a "person to blame" field to every bug report. How can I convince him that it's a bad idea?
qn_description:
In one of the latest "WTF" moves, my boss decided that adding a "Person To Blame" field to our bug tracking template will increase accountability (although we already have a way of tying bugs to features/stories). My arguments that this will decrease morale, increase finger-pointing and would not account for missing/misunderstood features reported as bug have gone unheard.

What are some other strong arguments against this practice that I can use? Is there any writing on this topic that I can share with the team and the boss? I find this sort of culture unacceptable to work in but want to try and change it before jumping ship. Any input is appreciated.
AnswersKenntnis
AnswerKenntnis:
Tell them this is only an amateurish name for the Root Cause field used by professionals (when issue tracker does not have dedicated field, one can use comments for that).

Search the web for something like software bug root cause analysis, there are plenty of resources to justify this reasoning 1, 2, 3, 4, ....




  ...a root cause for a defect is not always a single developer (which is the main point of this field)...


That's exactly why "root cause" is professional while "person to blame" is amateurish. Personal accountability is great, but there are cases when it simply lays "outside" of the dev team.

Tell your boss when there is a single developer to blame, root cause field will definitely cover that ("coding mistake made by Bob in commit 1234, missed by Jim in review 567"). The point of using the term root cause is to cover cases like that, along with cases that go out of the scope of the dev team.

For example, if the bug has been caused by faulty hardware (with the person to blame being someone outside of the team who purchased and tested it), the root cause field allows for covering that, while "single developer to blame" would simply break the issue tracking flow.

The same applies to other bugs caused by someone outside of the dev team - tester errors, requirements change, and management decisions. Say, if management decides to skip investing in disaster recovery hardware, "blaming a single developer" for an electricity outage in the datacenter would just not make sense.
AnswerKenntnis:
Another probable result for such a policy is that people won't report bug if they think they may be the "person to blame", so it will actually reduce the number of bug reported by the team.
AnswerKenntnis:
The main argument I would use against it is to ask what problem he's trying to solve. There are almost certainly better ways of solving the same problem.

For one thing, is there really only ever one person to blame? If there is, you're doing something else wrong. A good process takes a piece of work through an analyst, a programmer, a reviewer and a tester before it gets to production. If you're not doing all of these stages, maybe that's the solution to the problem your boss is trying to solve. If you are then which one is to blame? It might be none of them, it could be legacy code that's to blame.

It's no good having people back-biting and pointing fingers, trying to avoid a black mark which won't go away once it's set. It solves nothing. Very few people are maliciously negligent. You need to do a proper retrospective, see what went wrong and what you can do to make sure it doesn't go wrong again.

From that you will clearly see if one person is regularly at fault and that may be a different problem to deal with.

The trick to stopping a manager set on creating accountability is to offer it freely, but in a way that actually makes sense to you.
AnswerKenntnis:
The root cause for a fielded defect is never a single person.  Perfectly conscientious people will make errors, and a process that expects them to be infallible is unreasonable.  If you are not verifying changes to production systems before deployment, either manually or through automated testing, then bugs are inevitable.   

Wrong:


  Bob forget to check input and the program crashed dividing by zero.


Right:


  Code vulnerable to a divide by zero error was not detected before deployment.
      New test cases have been added to verify proper handling of invalid input.  Code was corrected and all new test cases are passing.
AnswerKenntnis:
There are at least three problems with that field.

The first one is that blaming people isn't good for morale. Ok. But maybe he doesn't care about morale and wants to fire bad developers. Hard to argue against.

The second one is that getting that field right is going to be hard and a pretty big time sink. It's more complex than just finding out who wrote the bad code. And any potentially information that is hard to figure out can be sandbagged/cheated on. But maybe he's prepared to pay that cost and audit the information. Fine.

The more fundamental issue is that this field is not going to be a good metric to take action on. Sure, he'll have a nice ranking of whose code causes the most defects. But guess who'll be on top of that list? Probably the company founder, or maybe a top developer who has a very low defect rate but is so productive he writes a disproportionate portion of the code. So he'll either end up firing his best developer, or have him slow down so much he's not his best developer any more. And they guy who writes a line of code a month - preferably comments - will probably get rewarded for his low defect numbers.

Yet another software metric failure.
AnswerKenntnis:
Change "Person to blame" to "Person to praise"

The main person to fix the bugs gets their name on it.
AnswerKenntnis:
If you're up for a little civil disobedience, get the team to agree to put a list of all the developers in that field for each bug. If that won't fit, write "I'm Spartacus!" instead. The point, of course, is that you're all responsible for all the bugs, and you're not happy about having to point out the individual who created any one bug.

Another option: play along. Don't do anything in particular -- just do good work and fill in the field as accurately as you can for a few months. Then explain to the boss that assigning blame for each bug makes everyone on the team unhappy and uncomfortable. Tell him that you all feel that there's little correlation between bugs created and anything else (skill, effort, sanity). (It'll help if you can run some numbers that show that there really isn't a correlation.)

Gandhian Civil Disobedience: Put your name on every field(unless other developers step up and put their name for their bugs), and accept blame for every bug whether it's yours or not. Nothing will render that field or the idea of blaming someone more useless than this. If your boss asks why is your name on every field, then you can explain "because I don't think development is a blame game, if you really need people to blame and crucify, then crucify me for everything and let my team work peacefully."
AnswerKenntnis:
Simple answer.  

The "Blame" field will be used for nothing more than scapegoating and fingerpointing, morale will plummet, team trust will be destroyed and everyone will be trying to find ways to prove that something is not their fault instead of fixing it.  People will also be more inclined to keep quiet about bugs instead of reporting them, because they don't want a colleague to get into trouble.  It's completely counter productive. 

What's more important, victimising somebody for making an honest mistake, or getting the problem fixed as quickly as possible?  

Your boss seems to think bugs are a sign of laziness or sloppiness.  They're not.  They're a fact of life.  How many patches does Microsoft push out in a year?
AnswerKenntnis:
The fact that you consider leaving the company over this feature should be a strong signal in itself. At least if there's more of you thinking the same way. If that doesn't worry him, I'd say there's good reasons to start looking for other ships anyways.
AnswerKenntnis:
I once had a boss implement a system very similar to this, and although it wasn't programming (it was print design for a daily newspaper) the concept and appropriate response are the same.

What she did was instead of adding a 'person to blame' field on our paperwork she gave each of the designers a set of colored stickers. Each designer got a different colored sticker and was instructed that for any design worked on or even touched the sticker must be added to that design's paperwork.

The boss' stated goal for "the sticker initiative" was to establish the source of all our department's errors (mistakes in paperwork, misfilings, bad copy, essentially the print equivalent of bugs)

What we did was gave each of the other designers a quarter of our stickers so that we each had all the colors, and instead of putting just our color on each design we put all four of the designers' colors.

Don't just write your name in the [Blame] box- put everyone's name that is on the team/project, and make sure the whole team does the same.

We worked together against her orwellian bitchiness and as a result we actually ended up catching each others mistakes and talking to each other about it and ultimately had a significant reduction in errors. She was a sh*t manager though, and instead of recognizing that her initiative ended up uniting us and increasing productivity she got all butthurt and disbanded the sticker system and declared it a failure and formally reprimanded all of us.
AnswerKenntnis:
Maybe you should look at it as "Who is in the best position to fix the bug?" A part of me also feels, you broke it, you fix it. There should be some accountability.

I don't agree with keeping some sort of score. Some people create more bugs because they work on more complex parts of the code. If lines of code isn't a useful metric, I doubt bugs per lines of code is any better. Code will never get checked in.

At some point a manager should know who is doing their job and who isn't, as well as, who does it better because the rest of the team does.
AnswerKenntnis:
Your actual question was about how to change the culture before you leave the company, by convincing your boss that adding a person to blame field for bug reports is a bad idea.  But of course changing the culture requires him to really understand why this is a bad idea.

This is a tall order.  Besides the issue of saving face after changing his mind, there is the basic problem that people who think about solutions primarily in terms of individual blame are usually pretty set in that mindset.  

You asked for writing on this topic, and Peopleware comes to mind.  It's highly regarded and talks in general terms about how to manage people doing creative work where the output is difficult to measure.  The problem is that you reading it won't help much, your boss would have to read it, and believe at least some of it.

Oddly, since the problem here is more about people than bug reports, it very possibly belongs more on Workplace than Programmers.  But the success of software projects is usually pretty heavily traceable to human social interaction, so the real answers are often mostly about things that transcend software.

My only other, half serious, suggestion is to say (or convince a co-worker to say, since you plan to leave) that you are willing to take on full responsibility for the project's success, and your name should always go in the field, since even if someone else made the mistake directly, you have assumed responsibility for making sure everyone on the team does quality work.

It's nonsense of course, how could you ever back that up, but some people (especially people who are big on blame) really eat that stuff up.  Ronald Reagan used to publicly accept personal responsibility every time any member of his administration was caught in a scandal (and there were quite a few) and it actually worked out pretty well politically every time.  The best part for you is that the responsibility generally comes with no actual consequences, they just think you are a stand up guy for taking the responsibility.

Or maybe that's not how it will go down.  It makes no sense at all to me so it's hard for me to predict when it will work, but I have witnessed it work when it seemed to have no business doing so (in the workplace, not just the Reagan example).
AnswerKenntnis:
It is strange that no one mentioned this before:
Adding such a functionality to the bug tracker would motivate employees to try to game the system.

This is a common problem for approaches like what the question presented, among other similar ideas (paying by number of code lines, paying by number of bugs). This will encourage many to focus on getting a good score, instead of solving problems related to the software they are working on.

For example, trying to submit a bug report with a wording to lessen one's own blame, and shove it on someone else can lead to developers misunderstanding the cause of the problem (or the work being given to another developer who does not know that section of a code as good as the one who was working on it the most and who was the primary cause of the bug) leading to more time and effort to fix the problem.
AnswerKenntnis:
This sounds a lot like when Scott Adams pointed out the failed wisdom of a Bug Bounty when the Pointy Haired Boss in Dilbert.  Wally announced he was going to go and 'write him a new Mini Van".  

Dilbert comic strip for 11/13/1995 from the official Dilbert comic strips archive.

I recall once when Snow Skiing that someone pointed out that 'not falling" was not the sign of a good skiier; but often the sign of one that doesn't try anything (or doesn't really ski at all).

Bugs can be introduced in code by poor programming and poor design; but, they can also come as the consequence of writing lots of difficult code.  Dinging people who produce the most bugs is as likely to Ding poor developers as highly productive ones.

It sounds like your boss may be frustrated with the number of defects.  Are the people in your group passionate about quality?  Creating a 'what' field for the cause rather than a 'who' field could be more productive. Ex: Requirements Change, Design Flaw, Implementation Flaw, etc.  Even this will fail unless there is a group by-in to improving the quality of the product.
AnswerKenntnis:
It will wind up punishing his most prolific programmer. Odds are, one or two people might be the best employees who have worked on the most projects. If you have, in a 10-person department, one coder who is just a fountain of output and he's written 60% of the interface code, then 60% of the bugs will be in his code.

Explain that this system would make it look like the person who writes the most code is the worst programmer, and the person who writes the least code is the best programmer.
AnswerKenntnis:
People don't go to work intent on making mistakes, and any strategy set in place, to specifically attach blame for what may or may not have been human error is ridiculous - not to mention extremely unprofessional.

At the very least, a "responsible party" assigned to take charge and "fix" the issue, or come up with a plan to track and/or prevent similar events from occurring, would be good. Sometimes the solution is nothing more than additional training. I've worked for a number of companies where it was part of your job description, to get a "company paid/company time" education. One place even built an entire "training center", that the local college "borrows" on occasion, for their industrial technologies courses.

I have worked in a manufacturing environment for the last 20 years, where programming mistakes don't just cause errors, they physically destroy things, and/or worse, they get people hurt. However, one constant in every field of manufacturing that stands strong, is that there is never, under any circumstances, someone to blame. Because it's a defect in the system, plain and simple - not a defect in the people. Look at it this way - the use of spell checker - a highly effective tool, for those less fortunate in the area of textual virtuosity, or maybe just those a little over worked... but by no means a method of blame or accountability.

A work environment, no matter what kind, or for what purpose it serves, is a system. A system made up of individual components, that if properly "in tune", works in total harmony - or some semblance of such.

Suggested reading on your boss' part: The 7 Habits of Highly Effective People

It sounds like he could use a little humility, if not a reality check. He's just as much a part of the team, as everyone else, and he needs to realize that - or it just won't work, and in the end, he will be holding the bag regardless.

Suggested reading and/or research on your part:

Look into 5 why analysis, root cause analysis... anything thing that puts you in a better position to offer a solution, not a problem. And your disagreement with your boss, is just that, a problem, not a solution. Offer him something better, something that makes sense, and even be prepared to allow him to take credit for the idea.

Right now it doesn't seem like he is prepared to fix anything, because he does not have a firm understanding of what is broken, if there is anything broken at all - other than his "I'm the boss" mentality.

Good Luck! I hope you manage to get through this, in a manner that's acceptable for all, especially in these times.

EDIT: Personally, from my own experience... "Go ahead, blame me. Because sure enough, I'll fix it, and down the road, when it happens again, who will be there to save the day? yep, you guessed it... me, with a big ole grin."
AnswerKenntnis:
Tell him "blame" is negative. Change it to "person to fix" then at least it is framed in a positive manner, and the same job still gets done. How can people work if they are being "blamed"?!
AnswerKenntnis:
Tell your boss that developing in a team needs social skills. He might even nod.

But the problem is, that this is something developers are extremely bad with. Adding tools that suggest blaming is more important than proper problem analysis is counter-productive.

Instead you need incentives to improve social skills, and the communication infrastructure you have should support that. For example, coin it positively: Name a person who is responsible for a ticket, who takes care of it.

Also start with code reviews so you can learn from each other. That spares the blames later on.
AnswerKenntnis:
It looks like your boss has not a deep understanding of software and maybe he does not intend to, either. So he has a different language, a different culture.

Quitting a job for a problem like this, before even trying to step forward for a solution is just being a quitter. Quitting is quitting. Do not quit until he makes you sure that you can never understand each other. To be sure about that, you should first try.

Since he does not know our language, and he is the boss, the first step here would be trying to talk to him in his language. What do I mean with language? Let's think together:

We software people, most of us love the job we do, we have a deep connection with what we are doing. Otherwise it does not work and one cannot go on in this business for a long time without loving it or being a complete ...you fill the blanks...

He, however, sees things much differently. With every bug report, while most of us get excited to make the thing work better (no, even if it is sometimes really very stressful, we love problems, just admit it!), he sees it as a failure, a measure of being unsuccessful. First thing he should want to understand is that bugs are good. Bugs makes clients love the company. (Now this is his language) When a customer reports a bug, or when we find one ourselves, after it is solved, it is much better than the situation it never happened. Bugs create customer loyalty (I am serious!), bugs create a great excuse to communication between the consumer and the producer of the software.

To "increase the profit of bugs" you should offer making bug reports even more open. With every bug report and its quick, clean, good solution, customers feel and see that "wow, these guys are awesome! They work really hard. Look at these things they are solving. We were not even aware that software was such a complex thing!" blah blah and blah...

Make your move, talk in his language. Bugs are great for a software company, not a problem. They make us a living.

For team ethics, efficiency, or any kind of talk you would make might work the opposite way you intended. If you want to quit, he will think "aha, my solution started to work from the very first day! Bad links have already started to drop by themselves before they are exposed!" He believes in his idea of finding the bad boys in the company and it is very difficult to convince him otherwise. Especially when you might be one of those bad boys!

So, focus on his real problem: Bugs. Show him that bugs can be very useful. Without any problems, a relationship is boring. Everything that does not kill you makes you stronger. Every bug is a great opportunity that you can use to increase the customer happiness.

This is just one thing you can tell. Think about his concerns and you will find many other items to add to your list. The GOLDEN KEY is to offer an alternative thing instead of fighting with his idea!
AnswerKenntnis:
If you are doing Agile, and it sounds like you are from the features/stories comment. The person to blame would be the QA person that let the bug slip through, or the Product Owner / Customer that accepted the feature/story as complete with the bug in it.

I did typesetting back in the day, here is my take on it.


  This is like blaming a typesetter for misspellings and other things
  that a proofreader should have found but missed. The typesetter made
  the misspelling, but the proofreader missed it, so it is the
  proofreader to blame for the mistake making to print, not the
  person that made the error in the first place.


In an Agile environment it is the QA persons responsibility to catch errors ( bugs ) and it is the Product Owners responsibility to not accept things that aren't correct. This is two levels of proof readers that should insulate the developers from things that get released, which is the only way anything should be classified as a bug in an Agile environment.
AnswerKenntnis:
To give the boss some credit, the concept of "blame assignment" is already baked into tools like SVN, and appropriate use of the data can be constructive to developers in "finding out who to talk to" while debugging, e.g.: http://www.codinghorror.com/blog/2007/11/who-wrote-this-crap.html 

While I agree with gnat's response above that a Root Cause field is a good thing, this isn't the same information, and "denormalizing" the field to sometimes assign the previous developer name(s) for the affected source, and sometimes have a technical description (e.g. "didn't scale to 10000 users") will just muddy the waters.  I'd advocate for keeping a Root Cause field clearly a technical description (e.g. even when a clear programmer error, have it capture details such as "IndexOutOfRange Exception when fooData=999")  This can potentially provide some useful feedback when reviewed en masse, and allow for some corrective actions to be taken to resolve entire classes of issues with architectural or framework changes (e.g. improving custom container classes, top-level exception handing)

That said, adding a Person To Blame field clearly can send a very bad message and destructive message to a software team that management wants to single out and punish individual developers who break code most often.  I suspect manager believes that this public scrutiny will cause developers to be more careful and self-regulating to avoid getting their names on this "wall of shame", and doesn't understand why developers would feel threatened by this, especially if it is added generically to every bug report.

The problems with adding this as a bug field/potential metric are easy to start enumerating:


Bugs are highly variable in difficulty to resolve, and a simple statistic of bug count/developer will not reflect this.
Developers are highly variable in capability " " " " " " " " " " "
Many software systems have components that need refactoring, however refactoring legacy components (particularly if the legacy base has no/limited unit testing facilities) will initially introduce bugs.  Developers are likely to be discouraged from this "good" activity, if there's a stigma/fear associated with generating new bugs (even if these are trivial to resolve and the end result a major improvement in the system.)
Testers may file a highly variable number of bugs related to the same issue, resulting in highly skewed bug counts/developer, unless a more detailed analysis is done.


That's just the tip of the iceberg.  Combine these with the finger-pointing of who expected what API behavior, incorrect expected results in tests, and "earlier in the chain" issues with incorrect/missing requirements, and it should be obvious that a metric like this is doomed as valueless (unless the goal is to damage morale and cause a mass exodus.)

Back to the boss viewpoint, it's OK that he/she wants to find out if there are developers who are breaking code repeatedly, and to try and do something (hopefully constructive) about that.  Trying to get this information by adding a field to bug reports is not going to provide meaningful information for the reasons listed above.  In my experience, this information can be learned by being plugged in with the team, participating in most of the team meetings, integrating (carefully) information learned in occasional one-on-one meetings with team members, and getting familiar with the subsystems in the code (even if they can't read code.)
AnswerKenntnis:
For accountability I wouldn't want a person to blame field,  I would want a Person who knows the code field or a person who can fix field, so that I would know where to send the support ticket.

This would speed up the process fixing the bug itself and give accountability, kind of like killing two birds with one stone. I would personally bring this up to him and let him decide if this would help to increase morale and accountability without making anyone feel like they failed.  Extreme testing doesn't catch all bugs, otherwise there wouldn't be bug reporting.
AnswerKenntnis:
I think your manager is trying to solve a problem with the wrong solution. I think maybe there is an issue that too many bugs are being released and your manager wants the developers to take more ownership and accountability towards the code that they write.

Using test driven development and setting up a continuous integration server (like Jenkins) will help to solve this problem, without introducing the "blame game". A continuous integration server is important to this because when someone commits code that "breaks the build" an email goes out to the team showing the person to blame. Since this code hasn't been released to a production environment, this type of blame is more proactive and encouraging (and fun!).

The result is that developers will take more ownership, feel more confident, and there will be less bugs in production code.
AnswerKenntnis:
Just let it go. Your boss will discover on his own that it causes a problem, if it does. 

Lets be blunt, you have an opinion and so does he. He is your manager and his opinion is the one that wins. 

Yes you can go to war over this issue, but is it really worth it? I doubt it lasts more than 3 months before it falls into disuse.

But actively sabotaging this or screaming about it just uses up political capital that is better saved for asking for that extra time off, next raise, promotion, or when a really critical design decision is going to be made.

At that moment, when it really counts you do npt want the boss to remember that you were the person who actively sabotaged his idea for "person to blame".

Respect the office even if you do not respect the decision.

Save the stress and table pounding for decisions that are going to be much longer lasting.
AnswerKenntnis:
Point out that if a single person's mistake causes a bug to end up in production, then there's something wrong with your methodology, or your over-all way of developing software.  Point out that preventing bugs getting to production is the responsibility of the entire team.  

Using either of these two arguments, see if you can persuade your boss that having the "whom to blame" field as a single-selection field would be misleading; and that therefore, it's necessary to make sure that the "whom to blame" field is a multiple-selection field.  Once you've achieved this, then ensure that for every bug, everybody's name is in the field.  Your boss will eventually see that any reporting on the field is futile.
AnswerKenntnis:
If my boss did that the following would happen, in this exact order:

1) I would immediately start looking for a new job.

2) Every time a bug is reported with a person to blame, my boss's name would appear there, and a comment as to why a bad process in the team is responsible for it. And CC that to his boss (preferably in a batch). Do you have unit tests? If not then it means that the dev process is broken, thus the bug. Do you have constant automated integration testing with all external systems? Then the dev process is broken, thus the bug. Do you have the ability to make every environment identical in production via script to not allow human error? Then the dev process is broken, thus the bug. Is one developer terrible? Then the hiring criteria is badm thus the boss's fault. Are all developers making stupid mistakes due to lack of rest because they are working 12 hrs a day? Then the dev process is broken. As you can see 100% of the bugs are the fault of the boss.

As a side note: Every good development manager is aware of what I wrote above. And Agile strategies are meant to point out to the boss or his/her supperiors why dev is slowing down: Look, we're spending 50% of our time fixing bugs. Lets look at strategies to reduce them so we can spend 40% of the time fixing bugs, and then revisit this problem getting it to 30%. etc.

Unfortunately it sounds like you don't have a good manager because of the field. So I suggest doing (1) and not bringing it up to the manager (except on your exit interview)
AnswerKenntnis:
Just make sure the first bug report in the system describes a bug in the bug reporting system called "person to blame" and put your boss's name in the person to blame field.
AnswerKenntnis:
Email him this SO question. If he's open to reason, the comments here will provide sanity checks for his reasoning. If he's not reasonable, you're unlikely to convince him with reasons that make sense. Additionally, he'll be able to read the reasons outside of a conversation (which can sometimes be more convincing because of the removed motivation to "be right" in the heat of a conversation).

You could also try to turn it around. The field could be "possible steps to avoid a similar bug from occurring", or something shorter to that effect. Then you could pool up solutions and vote on which ones to implement to make your workplace better. Perhaps a solution-oriented approach is more productive and likely better received (provided that there's actual follow through on the revisiting of suggestions).
AnswerKenntnis:
This field will not fix the current issue/bug.It causes more effort. More time equals more costs.Less money for the boss.

Perhaps a revision for your "Definition of Done (DoD)" will help to consider a better quality in your process.
AnswerKenntnis:
I see two possibilities here: He wants to be able to punish people who make mistakes, or he just hasn't thought it through.  Let him know that the perception to everyone will be that he intends to punish those who make mistakes.  Ask him if that's the culture that he wants to encourage.

my boss decided that adding a "Person To Blame" field to our bug tracking template will increase accountability

In my experience, when management wants to "make people more accountable," what they mean is they want to be able to exact punishment for failure.  Whether it's to fire poor performers, or just let them get dinged in the yearly salary review ("Sorry, Bob, you had 17 bugs flagged as your fault, and that's over the limit of 15"), it's punishment.

He'll probably say "Oh, no, we don't want that," so then ask him how that data will get used.  Remind him that you don't add data points to a database unless you're going to use it.   Does he want to either select on a given criteria ("Show me all the open bugs in the report creator subsystem"), so that you can work on things, or to be able to get aggregate data ("Which subsystem has had the most bugs"), so that you can do post-mortem analysis.  Does he envision some sort of tote board of failure where people can be publicly humiliated?  

So which does he intend?  Does he want to be able to say "Show me all the bugs that are Bob's fault?"  Why?  Or does he want to be able to say "Show me who is at fault most of the time?"  Why?  The first one is not meaningful, and the second is only punitive.  Or the third option is that he has no real reason.

I admit there is the possibility that he could be looking for those programmers on the team who need help in improving their skills.  If so, there are better ways to capture this information that don't create a culture of finger-pointing.
QuestionKenntnis:
Which hashing algorithm is best for uniqueness and speed?
qn_description:
Which hashing algorithm is best for uniqueness and speed? Example (good) uses include hash dictionaries. 

I know there are things like SHA-256 and such, but these algorithms are designed to be secure, which usually means they are slower than algorithms that are less unique. I want a hash algorithm designed to be fast, yet remain fairly unique to avoid collisions.
AnswersKenntnis
AnswerKenntnis:
I tested some different algorithms, measuring speed and number of collisions. 

I used three different key sets:


A list of 216,553 English words (in lowercase)
The numbers "1" to "216553" (think zip codes, and how a poor hash took down msn.com)
216,553 "random" (i.e. type 4 uuid) GUIDs


For each corpus, the number of collisions and the average time spent hashing was recorded.

I tested:


DJB2
DJB2a (variant using xor rather than +)
FNV-1 (32-bit)
FNV-1a (32-bit)
SDBM
CRC32
Murmur2 (32-bit)
SuperFastHash


Results

Each result contains the average hash time, and the number of collisions

Hash           Lowercase      Random UUID  Numbers 
=============  =============  ===========  ==============
Murmur            145 ns      259 ns          92 ns
                    6 collis    5 collis       0 collis
FNV-1a            152 ns      504 ns          86 ns
                    4 collis    4 collis       0 collis
FNV-1             184 ns      730 ns          92 ns
                    1 collis    5 collis       0 collis*
DBJ2a             158 ns      443 ns          91 ns
                    5 collis    6 collis       0 collis***
DJB2              156 ns      437 ns          93 ns
                    7 collis    6 collis       0 collis***
SDBM              148 ns      484 ns          90 ns
                    4 collis    6 collis       0 collis**
SuperFastHash     164 ns      344 ns         118 ns
                   85 collis    4 collis   18742 collis
CRC32             250 ns      946 ns         130 ns
                    2 collis    0 collis       0 collis
LoseLose          338 ns        -             -
               215178 collis


Notes: 


The LoseLose algorithm (where hash = hash+character) is truly awful. Everything collides into the same 1,375 buckets
SuperFastHash is fast, with things looking pretty scattered; by my goodness the number collisions. I'm hoping the guy who ported it got something wrong; it's pretty bad
CRC32 is pretty good. Slower, and a 1k lookup table


Collisions actually happen?

Yes. I started writing my test program to see if hash collisions actually happen - and are not just a theoretical construct. They do indeed happen:

FNV-1 collisions


creamwove collides with quists


FNV-1a collisions


costarring collides with liquid
declinate collides with macallums
altarage collides with zinke
altarages collides with zinkes


Murmur2 collisions


cataract collides with periti
roquette collides with skivie
shawl collides with stormbound
dowlases collides with tramontane
cricketings collides with twanger
longans collides with whigs


DJB2 collisions


hetairas collides with mentioner
heliotropes collides with neurospora
depravement collides with serafins
stylist collides with subgenera
joyful collides with synaphea
redescribed collides with urites
dram collides with vivency


DJB2a collisions


haggadot collides with loathsomenesses
adorablenesses collides with rentability
playwright collides with snush
playwrighting collides with snushing
treponematoses collides with waterbeds


CRC32 collisions


codding collides with gnu
exhibiters collides with schlager


SuperFastHash collisions


dahabiah collides with drapability
encharm collides with enclave
grahams collides with gramary
...snip 79 collisions...
night collides with vigil
nights collides with vigils
finks collides with vinic


Randomnessification

The other subjective measure is how randomly distributed the hashes are. Mapping the resulting HashTables shows how evenly the data is distributed. All the hash functions show good distribution when mapping the table linearly:



Or as a Hilbert Map (XKCD is always relevant):



Except when hashing number strings ("1", "2", ..., "216553") (e.g. zip codes), where patterns begin to emerge in most of the hashing algorithms:

SDBM:



DJB2a:



FNV-1:



All except FNV-1a, which still look plenty random to me:



In fact, Murmur2 seems to have even better randomness with Numbers than FNV-1a:




  When i look at the FNV-1a "number" map, i think i see subtle vertical patterns.  With Murmur i see no patterns at all. What do you think?




The extra * in the above table denotes how bad the randomness is. With FNV-1a being the best, and DJB2x being the worst:

      Murmur2:
       FNV-1a: 
        FNV-1: *
         DJB2: **
        DJB2a: **
         SDBM: ***
SuperFastHash: 
          CRC: *******************
     Loselose: **************************
                                        *
                                 *************
                        ******************************
          ******************************************************************


I originally wrote this program to decide if I even had to worry about collisions: I do.

And then it turned into making sure that the hash functions were sufficiently random.

FNV-1a algorithm

The FNV1 hash comes in variants that return 32, 64, 128, 256, 512 and 1024 bit hashes. 

The FNV-1a algorithm is:

hash = FNV_offset_basis
for each octetOfData to be hashed
    hash = hash xor octetOfData
    hash = hash * FNV_prime
return hash


Where the constants FNV_offset_basis and FNV_prime depend on the return hash size you want:

Hash Size    Prime                       Offset
===========  =========================== =================================
32-bit       16777619                    2166136261
64-bit       1099511628211               14695981039346656037
128-bit      309485009821345068724781371 144066263297769815596495629667062367629
256-bit
    prime: 2^88 + 2^8 + 0x3b = 309485009821345068724781371
    offset: 100029257958052580907070968620625704837092796014241193945225284501741471925557
512-bit
    prime: 2^344 + 2^8 + 0x57 = 35835915874844867368919076489095108449946327955754392558399825615420669938882575126094039892345713852759
    offset: 9659303129496669498009435400716310466090418745672637896108374329434462657994582932197716438449813051892206539805784495328239340083876191928701583869517785
1024-bit
    prime: 2^680 + 2^8 + 0x8d = 5016456510113118655434598811035278955030765345404790744303017523831112055108147451509157692220295382716162651878526895249385292291816524375083746691371804094271873160484737966720260389217684476157468082573
    offset: 1419779506494762106872207064140321832088062279544193396087847491461758272325229673230371772250864096521202355549365628174669108571814760471015076148029755969804077320157692458563003215304957150157403644460363550505412711285966361610267868082893823963790439336411086884584107735010676915


See the main FNV page for details.

As a practical matter:


32-bit UInt32,
64-bit UInt64, and
128-bit Guid can be useful


All my results are with the 32-bit variant.

FNV-1 better than FNV-1a?

No. FNV-1a is all around better. There was more collisions with FNV-1a when using the english word corpus:

Hash    Word Collisions
======  ===============
FNV-1   1
FNV-1a  4


Now compare lowercase and uppercase:

Hash    lowercase word Collisions  UPPERCASE word collisions
======  =========================  =========================
FNV-1   1                          9
FNV-1a  4                          11


In this case FNV-1a isn't "400%" worse than FN-1, only 20% worse.

I think the more important takeaway is that there are two classes of algorithms when it comes to collisions:


collisions rare: FNV-1, FNV-1a, DJB2, DJB2a, SDBM
collisions common: SuperFastHash, Loselose


And then there's the how evenly distributed the hashes are:


outstanding distribution: Murmur2, FNV-1a, SuperFastHas
excellent distribution: FNV-1
good distribution: SDBM, DJB2, DJB2a
horrible distribution: Loselose




Update 

Murmur? Sure, why not



Update 

@whatshisname wondered how a CRC32 would perform, added numbers to the table.

CRC32 is pretty good. Few collisions, but slower, and the overhead of a 1k lookup table.

snip all erroneous stuff about CRC distribution - my bad



Up until today I was going to use FNV-1a as my de facto hash-table hashing algorithm. But now I'm switching to Murmur2:


Faster
Better randomnessification of all classes of input


And I really, really hope there's something wrong with the SuperFastHash algorithm i found; it's too bad to be as popular as it is.

Update: From the MurmurHash3 homepage on Google: 


  (1) - SuperFastHash has very poor collision properties, which have been documented elsewhere.


So I guess it's not just me.

Update: I realized why Murmur is faster than the others. MurmurHash2 operates on 4 bytes at a time. Most algorithms are byte by byte:

for each octet in Key
   AddTheOctetToTheHash


This means that as keys get longer Murmur gets its chance to shine.



Update

GUIDs are designed to be unique, not random

A timely post by Raymond Chen reiterates the fact that "random" guids are not meant to be used for their randomness. They, or a subset of them, are unsuitable as a hash key:


  Even the Version 4 GUID algorithm is not guaranteed to be unpredictable, because the algorithm does not specify the quality of the random number generator. The Wikipedia article for GUID contains primary research which suggests that future and previous GUIDs can be predicted based on knowledge of the random number generator state, since the generator is not cryptographically strong.


Randomess is not the same as collision avoidance; which is why it would be a mistake to try to invent your own "hashing" algorithm by taking some subset of a "random" guid:

int HashKeyFromGuid(Guid type4uuid)
{
   //A "4" is put somewhere in the guid. 
   //i can't remember exactly where, but it doesn't matter for 
   //the illustrative purposes of this pseudocode
   int guidVersion = ((type4uuid.D3 & 0x0f00) >> 8);
   Assert(guidVersion == 4);

   return (int)GetFirstFourBytesOfGuid(type4uuid);
}


Note: Again, i put "random guid" in quotes because it's the "random" variant of guids. A more accurate description would be Type 4 uuid. But nobody knows what type 4, or types 1, 3 and 5 are. So it's just easier to call them "random" guids.
AnswerKenntnis:
Here is a list of hash functions, but the short version is:


  If you just want to have a good hash function, and cannot wait, djb2 is one of the best string hash functions i know. It has excellent distribution and speed on many different sets of keys and table sizes


unsigned long
hash(unsigned char *str)
{
    unsigned long hash = 5381;
    int c;

    while (c = *str++)
        hash = ((hash << 5) + hash) + c; /* hash * 33 + c */

    return hash;
}
AnswerKenntnis:
If you are wanting to create a hash map from an unchanging dictionary, you might want to consider perfect hashing https://en.wikipedia.org/wiki/Perfect_hash_function - during the construction of the hash function and hash table, you can guarantee, for a given dataset, that there will be no collisions.
AnswerKenntnis:
CityHash by Google is the algorithm you are looking for. It is not good for cryptography but is good for generating unique hashes. 

Read the blog for more details and the code is available here. 

CityHash is written in C++. There also is a plain C port. 

About 32-bit support:


  All the CityHash functions are tuned for 64-bit processors. That said, they will run (except for the new ones that use SSE4.2) in 32-bit code. They won't be very fast though. You may want to use Murmur or something else in 32-bit code.
AnswerKenntnis:
The SHA algorithms (including SHA-256) are designed to be fast.

In fact, their speed can be a problem sometimes. In particular, a common technique for storing a password-derived token is to run a standard fast hash algorithm 10,000 times (storing the hash of the hash of the hash of the hash of the ... password).

#!/usr/bin/env ruby
require 'securerandom'
require 'digest'
require 'benchmark'

def run_random_digest(digest, count)
  v = SecureRandom.random_bytes(digest.block_length)
  count.times { v = digest.digest(v) }
  v
end

Benchmark.bmbm do |x|
  x.report { run_random_digest(Digest::SHA256.new, 1_000_000) }
end


Output:

Rehearsal ------------------------------------
   1.480000   0.000000   1.480000 (  1.391229)
--------------------------- total: 1.480000sec

       user     system      total        real
   1.400000   0.000000   1.400000 (  1.382016)
AnswerKenntnis:
It depends on data You are hashing. Some hashing works better specific data like text. Some hashing algorithms were specificaly designed to be good at specific data.

Paul Hsieh once made fast hash. He lists source code and explanations. But it was already beaten. :)
AnswerKenntnis:
Java uses this simple multiply-and-add algorithm:


  The hash code for a String object is computed as

 s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]

  
  using int arithmetic, where s[i] is the i​-th character of the string, n is the length of the string, and ^ indicates exponentiation. (The hash value of the empty string is zero.) 


There are probably much better ones out there but this is fairly widespread and seems to be a good trade-off between speed and uniqueness.
AnswerKenntnis:
First of all, why do you need to implement your own hashing? For most tasks you should get good results with data structures from a standard library, assuming there's an implementation available (unless you're just doing this for your own education). 

As far as actual hashing algorithms go, my personal favorite is FNV. 1

Here's an example implementation of the 32-bit version in C:

unsigned long int FNV_hash(void* dataToHash, unsigned long int length)
{
  unsigned char* p = (unsigned char *) dataToHash;
  unsigned long int h = 2166136261UL;
  unsigned long int i;

  for(i = 0; i < length; i++)
    h = (h * 16777619) ^ p[i] ;

  return h;
}
QuestionKenntnis:
Where did the notion of "one return only" come from?
qn_description:
I often talk to Java programmers who say "Don't put multiple return statements in the same  method." When I ask them to tell me the reasons why, all I get is "The coding standard says so." or "It's confusing." When they show me solutions with a single return statement, the code looks uglier to me. For example:

if (blablabla)
   return 42;
else
   return 97;


"This is ugly, you have to use a local variable!"

int result;
if (blablabla)
   result = 42;
else
   result = 97;
return result;


How does this 50% code bloat make the program any easier to understand? Personally, I find it harder, because the state space has just increased by another variable that could easily have been prevented.

Of course, normally I would just write:

return (blablabla) ? 42 : 97;


But the conditional operator gets even less love among Java programmers. "It's incomprehensible!"

Where did this notion of "one return only" come from, and why do people adhere to it rigidly?
AnswersKenntnis
AnswerKenntnis:
This notion of Single Entry, Single Exit (SESE) comes from languages with explicit resource management, like C and assembly. In C, code like this will leak resources: 

void f()
{
  resource res = acquire_resource();  // think malloc()
  if( f1(res) )
    return; // leaks res
  f2(res);
  release_resource(res);  // think free()
}


In such languages, you basically have three options: 


Replicate the cleanup code.
Ugh. Redundancy is always bad. 
Use a goto to jump to the cleanup code.
This requires the cleanup code to be the last thing in the function. (And this is why some argue that goto has its place. And it has indeed ΓÇô in C.) 
Introduce a local variable and manipulate control flow through that.
The disadvantage is that control flow manipulated through syntax (think break, return, if, while) is much easier to follow than control flow manipulated through the state of variables (because those variables have no state when you look at the algorithm). 


In assembly it's even weirder, because you can jump to any address in a function when you call that function, which effectively means you have an almost unlimited number of entry points to any function. (Sometimes this is helpful. Such thunks are a common technique for compilers to implement the this pointer adjustment necessary for calling virtual functions in multiple-inheritance scenarios in C++.) 

When you have to manage resources manually, exploiting the options of entering or exiting a function anywhere leads to more complex code, and thus to bugs. Therefore, a school of thought appeared that propagated SESE, in order to get cleaner code and less bugs. 



However, when a language features exceptions, (almost) any function might be exited prematurely at (almost) any point, so you need to make provisions for premature return anyway. (I think finally is mainly used for that in Java and using (when implementing IDisposable... finally otherwise) in C#; C++ instead employs RAII.) Once you have done this, you cannot fail to clean up after yourself due to an early return statement, so what is probably the strongest argument in favor of SESE has vanished. 

That leaves readability. Of course, a 200 LoC function with half a dozen return statements sprinkled randomly over it is not good programming style and does not make for readable code. But such a function wouldn't be easy to understand without those premature returns either. 

So in languages where resources are not or should not be managed manually, there is little or no value in adhering to the old SESE convention. OTOH, as I have argued above, SESE often makes code more complex. So it is a dinosaur that (except for C) does not fit well into most of today's languages. Instead of helping the understandability of code, it hinders it. 



So why do Java programmers stick to this? I don't know, but from my (outside) POV, Java took a lot of conventions from C (where they make sense) and applied them to its OO world (where they are useless or outrightly bad), where it now sticks to them, no matter what the costs. (Like the convention to define all your variables at the beginning of the scope.) 

But programmers stick to all kinds of strange notations for irrational reasons. (Deeply nested structural statements ΓÇô "arrowheads" ΓÇô were, in languages like Pascal, once seen as beautiful code.) Applying pure logical reasoning to this seems to fail to convince the majority of them to deviate from their established ways. So the best way to change such habits is probably to teach them early on to do what's best, not what's conventional. You, being a programming teacher, have it in your hand. :)
AnswerKenntnis:
"Single Entry, Single Exit" was written when most programming was done in assembly language, FORTRAN, or COBOL.  It has been widely misinterpreted, because modern languages do not support the practices Dijkstra was warning against.

"Single Entry" meant "do not create alternate entry points for functions".  In assembly language, of course, it is possible to enter a function at any instruction.  FORTRAN supported multiple entries to functions with the ENTRY statement:

      SUBROUTINE S(X, Y)
      R = SQRT(X*X + Y*Y)
C ALTERNATE ENTRY USED WHEN R IS ALREADY KNOWN
      ENTRY S2(R)
      ...
      RETURN
      END

C USAGE
      CALL S(3,4)
C ALTERNATE USAGE
      CALL S2(5)


"Single Exit" meant that a function should only return to one place: the statement immediately following the call.  It did not mean that a function should only return from one place.  When Structured Programming was written, it was common practice for a function to indicate an error by returning to an alternate location.  FORTRAN supported this via "alternate return":

C SUBROUTINE WITH ALTERNATE RETURN.  THE '*' IS A PLACE HOLDER FOR THE ERROR RETURN
      SUBROUTINE QSOLVE(A, B, C, X1, X2, *)
      DISCR = B*B - 4*A*C
C NO SOLUTIONS, RETURN TO ERROR HANDLING LOCATION
      IF DISCR .LT. 0 RETURN 1
      SD = SQRT(DISCR)
      DENOM = 1.0 / (2*A)
      X1 = (-B + SD) / DENOM
      X2 = (-B - SD) / DENOM
      RETURN
      END

C USE OF ALTERNATE RETURN
      CALL QSOLVE(1, 0, 1, X1, X2, *99)
C SOLUTION FOUND
      ...
C QSOLVE RETURNS HERE IF NO SOLUTIONS
99    PRINT 'NO SOLUTIONS'


Both these techniques were highly error prone.  Use of alternate entries often left some variable uninitialized.  Use of alternate returns had all the problems of a GOTO statement, with the additional complication that the branch condition was not adjacent to the branch, but somewhere in the subroutine.
AnswerKenntnis:
I think the best justification is, single return statements make debugging easier. I remember millions of times I had to reduce the function into single return just to print out the return value at a single point.

  int function() {
     if (bidi) { print("return 1"); return 1; }
     for (int i = 0; i < n; i++) {
       if (vidi) { print("return 2"); return 2;}
     }
     print("return 3");
     return 3;
  }
AnswerKenntnis:
In theory a single exit point sounds great. In complex coding it is not. Consider this case:

if (condition1)
   don'tcontinue1 
else 
   do elsethings1
endif
//condition2 check
if (condition2)
   don'tcontinue2  
else    
   do elsethings2
endif
//conditionN check...


if you have 5 conditions, and you want to use a single exit you either have to:

A-Nest the if statements 

B-Introduce a flag

C-Write 1 complex if upfront

D-Use a goto as a single exit point

All of the above don't lead to better code (except for (D), more is on an early paper found here: GOTO Considered Harmful)

Early exit is not as harmful as it looks unless it has side effects. An example of side effect is not cleaning up or releasing resources. For example this is harmful:

myConnection.Open();
If condition1
   return
else
   do things
endif
myconnection.Close();


In the above case, the connection will not be closed in case condition1 is true, which is a bad thing.

So in my opinion, it is not that bad to use early exits as long as the practice is used with care.
AnswerKenntnis:
"Single Entry, Single Exit" originated with the Structured Programming revolution of the early 1970s, which was kicked off by Edsger W. Dijkstra's letter to the Editor "GOTO Statement Considered Harmful".  The concepts behind structured programming were laid out in detail in the classic book "Structured Programming" by Ole Johan-Dahl, Edsger W. Dijkstra, and Charles Anthony Richard Hoare.

"GOTO Statement Considered Harmful" is required reading, even today.  "Structured Programming" is dated, but still very, very rewarding, and should be at the top of any developer's "Must Read" list, far above anything from e.g. Steve McConnell.  (Dahl's section lays out the basics of classes in Simula 67, which are the technical foundation for classes in C++ and all of object-oriented programming.)
AnswerKenntnis:
I'd say: Write methods as short as possible (I always strive for no more than 15 lines, though that maybe impossible sometimes, due to deadlines e.g.) because then it really doesn't matter which style you use. Your code will always be readable, no matter if you use single/multiple exit points, no matter if you declare variables in the smallest scope possible or at the beginning of a method (yuk, how I hate that).
AnswerKenntnis:
One of the reason for the existence of the sigle exit point guideline, in my opinion, is people tend to write long and deeply nested methods and when you have such methods, multiple returns actually reduce the code readability (which implies maintainers are more likely to introduce bugs by accident). 

My high school programming teacher taught me the single exit point standard, and I had been following it religiously without really knowing why. 

After I read the book Clean Code and practice the the advice of writing short and simple methods I found that multiple returns is actually not bad, especially in the case of argument validation. 

I used to write code like this:

public String getSomeValue(String arg) {

  String valueToReturn = null; 

  if (arg==null) {
     valueToReturn = "";
  }  else {
     ...
     ...
     ...
     valueToReturn = "some value";
  }

  return valueToReturn;
}


And now I would write the above like:

public String getSomeValue(String arg) {

  if (arg == null) {
    return "";
  }

  ...
  ...
  ...
  valueToReturn = "some value;

  return valueToReturn;
}


What I like about the second style are:


The "meat" of the method is not nested inside the if block, so I get one free indentation (I try to avoid having more then 4 levels of indentation in each method).
The return value of corner case arguments or invalid arguments is more obvious. 


I think as long as the code is not too complex, and you don't hide the returns in deeply nested blocks then multiple returns is ok. I really want to jump off a building when I see code like this:

if (somecondition) {
  if (somcecondition2) {
     if(somecondition3) {
       if(somecindition4) {
         ....
           if (somecondition10) {
               ...
               ...
               ...
               //20 lines of code after
               return "hello world";
            }
         }
      }
      ...
      ...
      ...
      return "hello monday";
   }
}
...
...
...
return "hello earth";
AnswerKenntnis:
It could be my lack of knowledge of Java, but are the parenthesis around the ternary operator test required? If not, don't use them.

For me (professionally anyway), I usually go with "when in Rome..." and follow the coding standard. At the end of the day, there's not much worse than inconsistencies scattered throughout a code base. If you have the luxury of a standard, I'd rather follow the standard to ensure consistency (at least on my own part) than do something that may make sense to me but confuses someone else on my team, who might make me take time out of my day to explain my changes to them.

Having said that of course, if I feel strongly about something in the standard that should be changed, then I make sure that I have concrete reasoning as to why what's currently there bothers me and exactly how my proposal makes everyone's life easier. If the lead decides that's what's best for the team and the standard and chalks up time to educate everyone on the change, cool. If not, oh well, I tried and I'll just stick to using professionally failed convention in my own code (unless my mind was changed during a discussion with a group of peers).

As far as multiple exit points go, my only gripe with them really is if they're anywhere but the beginning of a function or at the end (personally I actually only like the exit points at the end and structuring my logic to accommodate, but after having a few discussions with peers, I've found that the former doesn't make me vomit in my mouth anymore.. Well, as much as it used to anyway ;)). Finding nested return statements can be a bit of a pain and really should be avoided unless there's absolutely no option (and even then I'd suggest refactoring to fix what you have).
AnswerKenntnis:
There is a factor often overlooked in this discussion and that is the future of the code.

In most commercial code, one person writes it, possibly dozens of people enhance it over a period of perhaps several years and then one person has the job of debugging it by which time it is as snarled up as it possibly can be.

So take your two pieces of code, one single-exit and one multi-exit and add some real bulk to each one but maintain the exitedness of each. Now tell me how valuable the code is! I can guarantee the single-exit code will still be single-exit and simple to the point of trivial to trace while the other will have cost many hours of debugging.

I do not object to early-exiting when parameter and state checking so long as it is both early and cheap. There are also times when I will code a multi-exit method because that is what it truly is but I will never create a multi-exit method just because it is quicker to write or slightly shorter.

Imagine yourself still in the same company ten years down the line and a junior programmer comes to you, shows you your own code and asks you why it is so complex. You can either blame all those other juniors who have modified it to death or take responsibility for your own short-cut.
AnswerKenntnis:
I say in a memory-managed language like Java, multiple exits make a lot of sense.

If you write a side-effect free methods, it is logically impossible to justify not returning as soon as you have your answer.

Let's say you have a comparator that compares on multiple criteria.  The method compares on a first criteria and breaks ties on a second criteria.  If there is still a tie, it compares on a third criteria.

In this case, you can return as soon as you have a non-zero comparison result or you can wrap the rest of the method in if statements.  It is clear that returning as soon as you have a non-zero is most desirable.  The case of comparison on first criteria, stands out and is easy to understand.  The fact that looking at the second criteria is only necessary to look at in the case of a tie is also obvious.  

By returning as soon as possible, you have reduced nesting and reduced the amount of code needed to understand the most basic case.

There are cases where you might not want multiple returns.  For example, it can be bad to have a return that is deeply nested in a long method where it is not obvious.  But long methods with deep nesting are not ideal and you can still reduce the confusion with good commenting.

So the only reason to stay in a method after you have the result is that your code has side effects.  However, having side effects is not the ideal.  I would personally write a side effect free method and another method that takes the result and does whatever else.
AnswerKenntnis:
I wrote a blog post on this topic a while back.

The bottom line is that this rule comes from the age of languages that don't have garbage collection or exception handling. There is no formal study that shows that this rule leads to better code in modern languages. Feel free to ignore it whenever this will lead to shorter or more readable code. The Java guys insisting on this are blindly and unquestioning following a outdated, pointless rule.

This question has also been asked on Stackoverflow
AnswerKenntnis:
Consider the fact that multiple return statements are equivalent to having GOTO's to a single return statement. This is the same case with break statements. As thus, some, like me, consider them GOTO's for all intents and purposes.

However, I don't consider these types of GOTO's harmful and will not hesitate to use an actual GOTO in my code if I find a good reason for it.

My general rule is that GOTO's are for flow control only. They should never be used for any looping, and you should never GOTO 'upwards' or 'backwards'. (which is how breaks/returns work)

As others have mentioned, the following is a must read
GOTO Statement Considered Harmful
However, keep in mind that this was written in 1970 when GOTO's were way overused. Not every GOTO is harmful and I would not discourage their use as long as you don't use them instead of normal constructs, but rather in the odd case that using normal constructs would be highly inconvenient.

I find that using them in error cases where you need to escape an area because of a failure that should never occur in normal cases useful at times. But you should also consider putting this code into a separate function so that you can just return early instead of using a GOTO... but sometimes that's also inconvenient.
AnswerKenntnis:
From a C# perspective...

I had an instance recently in an environment where many people are changing code, and checking in and out of source control.

I had a file checked out, with an if statement that set two properties of an object if both values were not null.

if (value1 != null && value2 != null) {
    object.Value1 = value1;
    object.Value2 = value2;
}


I added some code south of the if statement, to set some further properties.

if (value1 != null && value2 != null) {
    object.Value1 = value1;
    object.Value2 = value2;
}

object.Value3 = "This is necessary.";


Meanwhile, someone else picked up their refactoring tool and went through all the warnings about indented code. They allowed their tool to "invert if to reduce nesting". The if statement then returned if either of the values was null.

if (value1 == null || value2 == null)
    return;

object.Value1 = value1;
object.Value2 = value2;


Then I merged this new code in.

if (value1 == null || value2 == null)
    return;

object.Value1 = value1;
object.Value2 = value2;

object.Value3 = "This is necessary.";


The upshot is that if value1 or value2 are null, object.Value3 will never be set, and suddenly loads of unit tests fail. More work for me.

That's why I encourage people not to return from inside if blocks.

If there is a validation issue, e.g.

public void MyMethod(MyObject object) {
    if (object == null)
        {
            // invalid state
        }

    ...

}


then I would recommend throwing an exception, rather than simply returning (or returning a default value)
AnswerKenntnis:
One reason to use only one exit point at the end of the function is that indentation always has the same meaning.  This helps make reading code faster and less error prone.

A new style for every language seems to make sense though.  Take the following Ruby code:

def get_new_animal_action(animal)
  if animal.type == :cat
    if distance_from_home(animal.address) > DRIVE_LIMIT
      :buy_shipping_container
    else
      :drive_animal_home
    end
  else
    :call_bob
  end
end


Since the 'if' statements act like functions with a return value, we have the clarity of C with a single exit point and the lightness of multiple returns.
AnswerKenntnis:
There is a case where one return only isn't comfortable. For example when you catch an error or extraordinary case, and should immediately return.
For example:

function some_fun( a ) {
    if ( !a ) { result = false; } 

    var result;

    // some big piece of code
    // with many levels of indention
    // and so on
    if ( /*  */ ) {
        // some code
        result = 1;
    } else {
        result = 2;
    }

    return result;
}


I think it's much prettier than such an example:

function some_fun( a ) {
    var result;

    if ( !a ) {
        result = false;
    } else {
        // some big piece of code
        // with many levels of indention
        // and so on
        if ( /*  */ ) {
            // some code
            result = 1;
        } else {
            result = 2;
        }
    }

    return result;
}


So there is one more indention level...
AnswerKenntnis:
There are three reasons why I consider early return code to be better in general.


Why carry on when you know the answer? You are also forcing readers to follow the entire function just in case the result variable gets modified down the line. 
It makes exit conditions clear. You immediately define the contract of your function, stating what kind of input will make the function give useful output. This is only useful if you define your exit conditions in the beginning of the function, wich is preferable anyway.
There is no need to write extra code to dodge further modification to the result. No need to pepper a function with "else if" clauses or wrap large sections of code in if statemetns just because the first condition has already calculated the result. This minimizes nesting and complexity.


There is only one catch, and that is that really long functions and randomly placed exit conditions are a pain to work with, but the long function is the culprit, not the early return.
AnswerKenntnis:
Multiple returns can also be more efficient. If you have a code that checks for something and is ready to return before going through the rest of the operations, then why introduce a second variable to track its state?

for example:

function doStuff(variable) {
    if (variable == 5)
       return true;


    for(var i = 0, ret = 0; i < variable; i++){
       ret += variable;
    }
    if (ret == 5)
       return true;

    //do 10 more operations
}


This is faster then putting the extra check to see if you need to continue with the operation or not like in this example:

function doStuff(variable) {
    var value = true;
    var done = false;

    if (variable == 5) {
       value = true;
       done = true;
    }

    if (!done){

       for(var i = 0, ret = 0; i < variable; i++){
          ret += variable;
       }
       done = ret == 5;
       value = ret == 5;  
    }
    if (!done){
       //do 10 more operations
    }

    return value;
}


From the above 2 examples, which is easier to read and understand? Not only is the first one easier to understand, it also avoids the multiple of extra checks required by the second example. 

Of course this is sometimes still required for cleanup code, however why not simply put that in a different method all together?
AnswerKenntnis:
While I agree that single point of exit is sort of ridiculous in Java there is some merit to making sure all conditions are met and not exiting prematurely. In functional programming this far easier as the languages including the control constructs are generally expression based. That is the expression always exits to one value.

Unfortunately Java is not expression based but you can simulate it by using local final variables:

final String something;
switch (...) {
// switch has to set something
}


Consequently this is a bad single point of exit that I see often just to avoid typing else:

String s = "default";
if (blah) 
    s = "something";
return s;


While the code above seems OK it can get confusing if you add to the method.

final String s;
if (blah) {
    s = "something";
}
else {
    s = "default";
}
return s;


The above is better but obviously better is just to return in each branch (ie no local variable s.):

if (blah) {
    return "something";
}
else {
    return "default";
}


The other nice thing about expression based variable setting is that its generally easier to refactor as most IDEs allow you to extract methods from control statements. Using final makes this much easier to guarantee.

My point is its far better to write your code to be expression based instead of focusing on single point of exit.
AnswerKenntnis:
This is somewhat connected to Guard Clause refactoring.

http://stackoverflow.com/a/8493256/679340

Guard Clause will add returns to the beginning of your methods. And it makes code a lot cleaner in my opinion.
AnswerKenntnis:
Just because a feature is available in a programming language, doesn't means it suits your programming needs. This topic is one of the reasons I prefer (structured or object oriented) pascal over c style programming languages.

(* structured or object pascal*)
function test(var x: integer): integer;
begin
  // result is automatically declared      

  if (x > 0)
  begin
    Result := DoSomething(x);
  end
  else
  begin
    Result := DoSomethingElse(x);
  end;

  // can debug here, with code or Debugger Tool
  // WriteLn("Result: ", Result);
end;

/* c, c++, or similar */
int test(int x)
{

  if (x > 0)
  {
    return DoSomething(x);
  }
  else
  {
    return DoSomethingElse(x);
  }

  // cannot know the value of the result
}


Since I have to use a lot of c style programming languages at work, I use this trick, which It ca be debug easily.

int test(int x)
{
  // copied from pascal
  int Result = 0;

  if (x > 0)
  {
    Result = DoSomething(x);
  }
  else
  {
    Result = DoSomethingElse(x);
  }

  // can debug here, with code or Debugger Tool
  // cout << "Result: " << Result << "\n"

  return Result;
}    


Cheers.
AnswerKenntnis:
It's always easy to link Fowler.

One of the main examples that go against SESE are guard clauses:

Replace Nested Conditional with Guard Clauses


  Use Guard Clauses for all the special cases

double getPayAmount() {
    double result;
    if (_isDead) result = deadAmount();
    else {
        if (_isSeparated) result = separatedAmount();
        else {
            if (_isRetired) result = retiredAmount();
            else result = normalPayAmount();
        };
    }
return result;
};  

  
                                                                                                           

double getPayAmount() {
    if (_isDead) return deadAmount();
    if (_isSeparated) return separatedAmount();
    if (_isRetired) return retiredAmount();
    return normalPayAmount();
};  

  
  For more inforamtion see page 250 of Refactoring...
QuestionKenntnis:
I've inherited 200K lines of spaghetti code -- what now?
qn_description:
I hope this isn't too general of a question; I could really use some seasoned advice.

I am newly employed as the sole "SW Engineer" in a fairly small shop of scientists who have spent the last 10-20 years cobbling together a vast code base. (It was written in a virtually obsolete language: G2 -- think Pascal with graphics). The program itself is a physical model of a complex chemical processing plant; the team that wrote it have incredibly deep domain knowledge but little or no formal training in programming fundamentals. They've recently learned some hard lessons about the consequences of non-existant configuration management. Their maintenance efforts are also greatly hampered by the vast accumulation of undocumented "sludge" in the code itself. I will spare you the "politics" of the situation (there's always politics!), but suffice to say, there is not a consensus of opinion about what is needed for the path ahead.

They have asked me to begin presenting to the team some of the principles of modern software development. They want me to introduce some of the industry-standard practices and strategies regarding coding conventions, lifecycle management, high-level design patterns, and source control. Frankly, it's a fairly daunting task and I'm not sure where to begin. 

Initially, I'm inclined to tutor them in some of the central concepts of The Pragmatic Programmer, or Fowler's Refactoring ("Code Smells", etc). I also hope to introduce a number of Agile methodologies. But ultimately, to be effective, I think I'm going to need to hone in on 5-7 core fundamentals; in other words, what are the most important principles or practices that they can realistically start implementing that will give them the most "bang for the buck". 

So that's my question: What would you include in your list of the most effective strategies to help straighten out the spaghetti (and prevent it in the future)?
AnswersKenntnis
AnswerKenntnis:
Foreword

This is a daunting task indeed, and there's a lot of ground to
cover. So I'm humbly suggesting this as somewhat comprehensive guide
for your team, with pointers to appropriate tools and educational
material.

Remember: These are guidelines, and that as such are meant to
adopted, adapted, or dropped based on circumstances.

Beware: Dumping all this on a team at once would most likely
fail. You should try to cherry-pick elements that would give you the
best bang-for-sweat, and introduce them slowly, one at a time.

Note: not all of this applies directly to Visual Programming
Systems like G2. For more specific details on how to deal with these,
see the Addendum section at the end.



Executive Summary for the Impatient


Define a rigid project structure, with:

project templates,
coding conventions,
familiar build systems,
and sets of usage guidelines for your infrastructure and tools.

Install a good SCM and make sure they know how to use it.
Point them to good IDEs for their technology, and make sure they know how to use them.
Implement code quality checkers and automatic reporting in the build system.
Couple the build system to continuous integration and continuous inspection systems.
With the help of the above, identify code quality "hotspots" and refactor.


Now for the long version... Caution, brace yourselves!



Rigidity is (Often) Good

This is a controversial opinion, as rigidity is often seen as a force
working against you. It's true for some phases of some projects. But
once you see it as a structural support, a framework that takes away
the guesswork, it greatly reduces the amount of wasted time and
effort. Make it work for you, not against you.

Rigidity = Process / Procedure.

Software development needs good process and procedures for exactly the
same reasons that chemical plants or factories have manuals,
procedures, drills and emergency guidelines: preventing bad outcomes,
increasing predictability, maximizing productivity...

Rigidity comes in moderation, though!

Rigidity of the Project Structure

If each project comes with its own structure, you (and newcomers) are
lost and need to pick up from scratch every time you open them. You
don't want this in a professional software shop, and you don't want
this in a lab either.

Rigidity of the Build Systems

If each project looks different, there's a good chance they also
build differently. A build shouldn't require too much research or
too much guesswork. You want to be able to do the canonical thing and
not need to worry about specifics: configure; make install, ant,
mvn install, etc...

Re-using the same build system and making it evolve over the time also
ensures a consistent level of quality.

You do need a quick README to point the project's specifics, and
gracefully guide the user/developer/researcher, if any.

This also greatly facilitates other parts of your build
infrastructure, namely:


continuous integration,
continuous inspection.


So keep your build (like your projects) up to date, but make it
stricter over time, and more efficient at reporting violations and bad
practices.

Do not reinvent the wheel, and reuse what you have already done.

Recommended Reading:


Continuous Integration: Improving Software Quality and Reducing Risk (Duval, Matyas, Glover, 2007)
Continuous Delivery: Release Sotware Releases through Build, Test and Deployment Automation (Humble, Farley, 2010)


Rigidity in the Choice of Programming Languages

You can't expect, especially in a research environment, to have all
teams (and even less all developers) use the same language and
technology stack. However, you can identify a set of "officially
supported" tools, and encourage their use. The rest, without a good
rationale, shouldn't be permitted (beyond prototyping).

Keep your tech stack simple, and the maintenance and breadth of
required skills to a bare minimum: a strong core.

Rigidity of the Coding Conventions and Guidelines

Coding conventions and guidelines are what allow you to develop both
an identity as a team, and a shared lingo. You don't want to err
into terra incognita every time you open a source file.

Nonsensical rules that make life harder or forbid actions explicity
to the extent that commits are refused based on single simple
violations are a burden. However:


a well thought-out ground ruleset takes away a lot of the whining
and thinking: nobody should break under no circumstances;
and a set of recommended rules provide additional guidance.



  Personal Approach: I am aggressive when it comes to coding
  conventions, some even say nazi, because I do believe in having a
  lingua franca, a recognizable style for my team. When crap code
  gets checked-in, it stands out like a cold sore on the face of an
  Hollywood star: it triggers a review and an action automatically.
  In fact, I've sometimes gone as far as to advocate the use of
  pre-commit hooks to reject non-conforming commits. As mentioned, it
  shouldn't be overly crazy and get in the way of productivity: it
  should drive it. Introduce these slowly, especially at the
  beginning. But it's way preferable over spending so much time fixing
  faulty code that you can't work on real issues.


Some languages even enforce this by design:


Java was meant to reduce the amount of dull crap you can write with
it (though no doubt many manage to do it).
Python's block structure by indentation is another idea in this
sense.
Go, with its gofmt tool, which completely takes away any debate
and effort (and ego!!) inherent to style: run gofmt before
you commit.


Make sure that code rot cannot slip through. Code
conventions, continuous integration and continuous
inspection, pair programming and code reviews are your
arsenal against this demon.

Plus, as you'll see below, code is documentation, and that's
another area where conventions encourage readability and clarity.

Rigidity of the Documentation

Documentation goes hand in hand with code. Code itself is
documentation. But there must be clear-cut instructions on how to
build, use, and maintain things.

Using a single point of control for documentation (like a WikiWiki or
DMS) is a good thing. Create spaces for projects, spaces for more
random banter and experimentation. Have all spaces reuse common rules
and conventions. Try to make it part of the team spirit.

Most of the advice applying to code and tooling also applies to
documentation.

Rigidity in Code Comments

Code comments, as mentioned above, are also documentation. Developers
like to express their feelings about their code (mostly pride and
frustration, if you ask me). So it's not unusual for them to express
these in no uncertain terms in comments (or even code), when a more
formal piece of text could have conveyed the same meaning with less
expletives or drama. It's OK to let a few slip through for fun and
historical reasons: it's also part of developing a team
culture. But it's very important that everybody knows what is
acceptable and what isn't, and that comment noise is just that:
noise.

Rigidity in Commit Logs

Commit logs are not an annoying and useless "step" of your SCM's
lifecycle: you DON'T skip it to get home on time or get on with the
next task, or to catch up with the buddies who left for lunch.  They
matter, and, like (most) good wine, the more time passes the more valuable
they become. So DO them right. I'm flabbergasted when I see co-workers
writing one-liners for giant commits, or for non-obvious hacks.

Commits are done for a reason, and that reason ISN'T always clearly
expressed by your code and the one line of commit log you
entered. There's more to it than that.

Each line of code has a story, and a history. The diffs can tell
its history, but you have to write its story.


  Why did I update this line? -> Because the interface changed.
  
  Why did the interface changed? -> Because the library L1 defining it
  was updated.
  
  Why was the library updated? -> Because library L2, that we need for
  feature F, depended on library L1.
  
  And what's feature X? -> See task 3456 in issue tracker.


It's not my SCM choice, and may not be the best one for your lab
either; but Git gets this right, and tries to force you to write
good logs more than most other SCMs systems, by using short logs and
long logs. Link the task ID (yes, you need one) and a leave a
generic summary for the shortlog, and expand in the long log: write
the changeset's story.

It is a log: It's here to keep track and record updates.


  Rule of Thumb: If you were searching for something about this
  change later, is your log likely to answer your question?


Projects, Documentation and Code Are ALIVE

Keep them in sync, otherwise they do not form that symbiotic entity
anymore. It works wonders when you have:


clear commits logs in your SCM, w/ links to task IDs in your
issue tracker,
where this tracker's tickets themselves link to the changesets in
your SCM (and possibly to the builds in your CI system),
and a documentation system that links to all of these.


Code and documentation need to be cohesive.

Rigidity in Testing


  Rules of Thumb:
  
  
  Any new code shall come with (at least) unit tests.
  Any refactored legacy code shall come with unit tests.
  


Of course, these need:


to actually test something valuable (or they are a waste of time
and energy),
to be well written and commented (just like any other code you check in).


They are documentation as well, and they help to outline the contract
of your code. Especially if you use TDD. Even if you don't, you
need them for your peace of mind. They are your safety net when you
incorporate new code (maintenance or feature) and your watchtower
to guard against code rot and environmental failures.

Of course, you should go further and have integration tests, and
regression tests for each reproducible bug you fix.

Rigidity in the Use of the Tools

It's OK for the occasional developer/scientist to want to try some new
static checker on the source, generate a graph or model using another,
or implement a new module using a DSL. But it's best if there's a
canonical set of tools that all team members are expected to know
and use.

Beyond that, let members use what they want, as long as they are ALL:


productive,
NOT regularly requiring assistance
NOT regularly adjusting to your general infrastructure,
NOT disrupting your infrastructure (by modifying common
areas like code, build system, documentation...),
NOT affecting others' work,
ABLE to timely perform any task requested.


If that's not the case, then enforce that they fallback to defaults.



Rigidity vs Versatility, Adaptability, Prototyping and Emergencies

Flexibility can be good. Letting someone occasionally use a hack, a
quick-n-dirty approach, or a favorite pet tool to get the job done
is fine. NEVER let it become a habit, and NEVER let this code
become the actual codebase to support.



Team Spirit Matters

Develop a Sense of Pride in Your Codebase


Develop a sense of Pride in Code

Use wallboards

leader board for a continuous integration game
wallboards for issue management and defect counting

Use an issue tracker / bug tracker



Avoid Blame Games


DO use Continuous Integration / Continuous Inspection games: it
fosters good-mannered and productive competition.
DO keep track defects: it's just good house-keeping.
DO identifying root causes: it's just future-proofing processes.
BUT DO NOT assign blame: it's counter productive.


It's About the Code, Not About the Developers

Make developers conscious of the quality of their code, BUT make them
see the code as a detached entity and not an extension of
themselves, which cannot be criticized.

It's a paradox: you need to encourage ego-less programming for a
healthy workplace but to rely on ego for motivational purposes.



From Scientist to Programmer

People who do not value and take pride in code do not produce good
code. For this property to emerge, they need to discover how valuable
and fun it can be. Sheer professionalism and desire to do good is not
enough: it needs passion. So you need to turn your scientists into
programmers (in the large sense).

Someone argued in comments that after 10 to 20 years on a project and
its code, anyone would feel attachment. Maybe I'm wrong but I assume
they're proud of the code's outcomes and of the work and its legacy,
not of the code itself or of the act of writing it.

From experience, most researchers regard coding as a necessity, or at
best as a fun distraction. They just want it to work. The ones who are
already pretty versed in it and who have an interest in programming
are a lot easier to persuade of adopting best practices and switching
technologies. You need to get them halfway there.



Code Maintenance is Part of Research Work

Nobody reads crappy research papers. That's why they are
peer-reviewed, proof-read, refined, rewritten, and approved time and
time again until deemed ready for publication. The same applies to a
thesis and a codebase!

Make it clear that constant refactoring and refreshing of a codebase
prevents code rot and reduces technical debt, and facilitates future
re-use and adaptation of the work for other projects.



Why All This??!

Why do we bother with all of the above? For code quality. Or is it
quality code...?

These guidelines aim at driving your team towards this goal. Some
aspects do it by simply showing them the way and letting them do it
(which is much better) and others take them by the hand (but that's
how you educate people and develop habits).

How do you know when the goal is within reach?

Quality is Measurable

Not always quantitatively, but it is measurable. As mentioned, you
need to develop a sense of pride in your team, and showing progress
and good results is key. Measure code quality regularly and show
progress between intervals, and how it matters. Do retrospectives to
reflect on what has been done, and how it made things better or worse.

There are great tools for continuous inspection. Sonar being
a popular one in the Java world, but it can adapt to any technologies;
and there are many others. Keep your code under the microscope and
look for these pesky annoying bugs and microbes.



But What if My Code is Already Crap?

All of the above is fun and cute like a trip to Never Land, but it's
not that easy to do when you already have (a pile of steamy and
smelly) crap code, and a team reluctant to change.

Here's the secret: you need to start somewhere.


  Personal anecdote: In a project, we worked with a codebase
  weighing originally 650,000+ Java LOC, 200,000+ lines of JSPs,
  40,000+ JavaScript LOC, and 400+ MBs of binary dependencies.
  
  After about 18 months, it's 500,000 Java LOC (MOSTLY CLEAN),
  150,000 lines of JSPs, and 38,000 JavaScript LOC, with dependencies
  down to barely 100MBs (and these are not in our SCM anymore!).
  
  How did we do it? We just did all of the above. Or tried hard.
  
  It's a team effort, but we slowly inject in our process
  regulations and tools to monitor the heart-rate of our product,
  while hastily slashing away the "fat": crap code, useless
  dependencies... We didn't stop all development to do this: we have
  occasional periods of relative peace and quiet where we are free to
  go crazy on the codebase and tear it apart, but most of the time we
  do it all by defaulting to a "review and refactor" mode every chance
  we get: during builds, during lunch, during bug fixing sprints,
  during Friday afternoons...
  
  There were some big "works"... Switching our build
  system from a giant Ant build of 8500+ XML LOC to a
  multi-module Maven build was one of them. We then had:
  
  
  clear-cut modules (or at least it was already a lot better, and we
  still have big plans for the future),
  automatic dependency management (for easy maintenance and updates, and
  to remove useless deps),
  faster, easier and reproduceable builds,
  daily reports on quality.
  
  
  Another was the injection of "utility tool-belts", even though we
  were trying to reduce dependencies: Google Guava and Apache Commons
  slim down your code and and reduce surface for bugs in your code
  a lot.
  
  We also persuaded our IT department that maybe using our new tools
  (JIRA, Fisheye, Crucible, Confluence, Jenkins) was better than the
  ones in place. We still needed to deal with some we despised (QC,
  Sharepoint and SupportWorks...), but it was an overall improved
  experience, with some more room left.
  
  And every day, there's now a trickle of between one to dozens of
  commits that deal only with fixing and refactoring things. We
  occasionally break stuff (you need unit tests, and you better write
  them before you refactor stuff away), but overall the benefit
  for our morale AND for the product has been enormous. We get there
  one fraction of a code quality percentage at a time. And it's fun
  to see it increase!!!


Note: Again, rigidity needs to be shaken to make room for new and
better things. In my anecdote, our IT department is partly right in
trying to impose some things on us, and wrong for others. Or maybe
they used to be right.  Things change. Prove that they are better
ways to boost your productivity. Trial-runs and prototypes are
here for this.



The Super-Secret Incremental Spaghetti Code Refactoring Cycle for Awesome Quality

       +-----------------+      +-----------------+
       |  A N A L Y Z E  +----->| I D E N T I F Y |
       +-----------------+      +---------+-------+
                ^                           |
                |                           v
       +--------+--------+      +-----------------+
       |    C L E A N    +<-----|      F I X      |
       +-----------------+      +-----------------+


Once you have some quality tools at your toolbelt:


Analyze your code with code quality checkers.

Linters, static analyzers, or what have you.
Identify your critical hotspots AND low hanging fruits.

Violations have severity levels, and large classes with a large number
 of high-severity ones are a big red flag: as such, they appear as
 "hot spots" on radiator/heatmap types of views.
Fix the hotspots first.

It maximizes your impact in a short timeframe as they have
 the highest business value. Ideally, critical violations should
 dealt with as soon as they appear, as they are potential security
 vulnerabilities or crash causes, and present a high risk of inducing a
 liability (and in your case, bad performance for the lab).
Clean the low level violations with automated codebase sweeps.

It improves the signal-to-noise ratio so you are be able to
 see significant violations on your radar as they appear. There's often
 a large army of minor violations at first if they were never taken care
 of and your codebase was left loose in the wild. They do not present a
 real "risk", but they impair the code's readability and maintainability.
 Fix them either as you meet them while working on a task, or by large
 cleaning quests with automated code sweeps if possible. Do be
 careful with large auto-sweeps if you don't have a good test suite
 and integration system. Make sure to agree with co-workers
 the right time to run them to minimize the annoyance.
Repeat until you are satisfied. 

Which, ideally, you should never be, if this is still an
 active product: it will keep evolving.


Quick Tips for Good House-Keeping


When in hotfix-mode, based on a customer support request:


It's usually a best practice to NOT go around fixing other issues,
as you might introduce new ones unwillingly.
Go at it SEAL-style: get in, kill the bug, get out, and ship your
patch. It's a surgical and tactical strike.

But for all other cases, if you open a file, make it your duty to:


definitely: review it (take notes, file issue reports),
maybe: clean it (style cleanups and minor violations),
ideally: refactor it (reorganize large sections and their neigbors).



Just don't get sidetracked into spending a week from file to file and
ending up with a massive changeset of thousands of fixes spanning multiple
features and modules - it makes future tracking difficult. One issue in
code = one ticket in your tracker. Sometimes, a changeset can impact multiple
tickets; but if it happens too often, then you're probably doing something wrong.



Addendum: Managing Visual Programming Environments

The Walled Gardens of Bespoke Programming Systems

Multiple programming systems, like the OP's G2, are different beasts...


No Source "Code"

Often they do not give you access to a textual representation of
your source "code": it might be stored in a proprietary binary
format, or maybe it does store things in text format but hides
them away from you. Bespoke graphical programming systems are
actually not uncommon in research labs, as they simplify the
automation of repetitive data processing workflows.
No Tooling

Aside from their own, that is. You are often constrained by their
programming environment, their own debugger, their own
interpreter, their own documentation tools and formats. They are
walled gardens, except if they eventually capture the interest
of someone motivated enough to reverse engineer their formats and
builds external tools - if the license permits it.
Lack of Documentation

Quite often, these are niche programming systems, which are used
in fairly closed environments. People who use them frequently sign NDAs
and never speak about what they do. Programming
communities for them are rare. So resources are scarce. You're
stuck with your official reference, and that's it.


The ironic (and often frustrating) bit is that all the things these
systems do could obviously be achieved by using mainstream and general
purpose programming languages, and quite probably more
efficiently. But it requires a deeper knowledge of programming,
whereas you can't expect your biologist, chemist or physicist (to name
a few) to know enough about programming, and even less to have the
time (and desire) to implement (and maintain) complex systems, that
may or may not be long-lived. For the same reason we use DSLs, we have
these bespoke programming systems.


  Personal Anecdote 2: Actually, I worked on one of these
  myself. I didn't do the link with the OP's request, but my the
  project was a set of inter-connected large pieces of
  data-processing and data-storage software (primarily for
  bio-informatics research, healthcare and cosmetics, but also for
  business intelligence, or any domain implying the tracking of
  large volumes of research data of any kind and the preparation of
  data-processing workflows and ETLs). One of these applications was,
  quite simply, a visual IDE that used the usual bells and whistles:
  drag and drop interfaces, versioned project workspaces (using text
  and XML files for metadata storage), lots of pluggable drivers to
  heterogeneous datasources, and a visual canvas to design pipelines
  to process data from N datasources and in the end generate M
  transformed outputs, and possible shiny visualizations and complex
  (and interactive) online reports. Your typical bespoke visual
  programming system, suffering from a bit of NIH syndrome under the
  pretense of designing a system adapted to the users' needs.
  
  And, as you would expect, it's a nice system, quite flexible for its
  needs though sometimes a bit over-the-top so that you wonder "why
  not use command-line tools instead?", and unfortunately always
  leading in medium-sized teams working on large projects to a lot of
  different people using it with different "best" practices.


Great, We're Doomed! - What Do We Do About It?

Well, in the end, all of the above still holds. If you cannot extract
most of the programming from this system to use more mainstream tools
and languages, you "just" need to adapt it to the constraints of your
system.

About Versioning and Storage

In the end, you can almost always version things, even with the
most constrained and walled environment. Most often than not, these
systems still come with their own versioning (which is unfortunately
often rather basic, and just offers to revert to previous versions
without much visibility, just keeping previous snapshots). It's not
exactly using differential changesets like your SCM of choice might,
and it's probably not suited for multiple users submitting changes
simultaneously.

But still, if they do provide such a functionality, maybe your
solution is to follow our beloved industry-standard guidelines above,
and to transpose them to this programming system!!

If the storage system is a database, it probably exposes export
functionalities, or can be backed-up at the file-system level. If it's
using a custom binary format, maybe you can simply try to version it
with a VCS that has good support for binary data. You won't have
fine-grained control, but at least you'll have your back sort of
covered against catastrophes and have a certain degree of disaster
recovery compliance.

About Testing

Implement your tests within the platform itself, and use external
tools and background jobs to set up regular backups. Quite probably,
you fire up these tests the same that you would fire up the programs
developed with this programming system.

Sure, it's a hack job and definitely not up to the standard of what is
common for "normal" programming, but the idea is to adapt to the
system while trying to maintain a semblance of professional software
development process.

The Road is Long and Steep...

As always with niche environments and bespoke programming systems, and
as we exposed above, you deal with strange formats, only a limited (or
totally inexistant) set of possibly clunky tools, and a void in place
of a community.

The Recommendation: Try to implement the above guidelines outside
of your bespoke programming system, as much as possible. This ensures
that you can rely on "common" tools, which have proper support and
community drive.

The Workaround: When this is not an option, try to retrofit this
global framework into your "box". The idea is to overlay this
blueprint of industry standard best practices on top of your
programming system, and make the best of it. The advice still applies:
define structure and best practices, encourage conformance.

Unfortunately, this implies that you may need to dive in and do a
tremendous amount of leg-work. So...

Famous Last Words, and Humble Requests:


Document everything you do.
Share your experience.
Open Source any tool your write.


By doing all of this, you will:


not only increase your chances of getting support from people in
similar situations,
but also provide help to other people, and foster discussion
around your technology stack.


Who knows, you could be at the very beginning of a new vibrant
community of Obscure Language X. If there are none, start one!


Ask questions on Stack Overflow,
Maybe even write a proposal for a new StackExchange Site in the
Area 51.


Maybe it's beautiful inside, but nobody has a clue so far, so
help take down this ugly wall and let others have a peek!
AnswerKenntnis:
The very first step would be introduction of a Version Control System (SVN, Git, Mercurial, TFS, etc.). This is must to have for a project that will have re-factoring.

Edit: regarding VSC - Every source control package can manage binaries, although with some limitations. Most of the tools in the market has the ability to use a custom difference viewer and editor, use this capability. Binary source files is not an excuse not to use version control. 

There is a similar post on how to deal with legacy code, it might be a good reference to follow - Advice on working with legacy code
AnswerKenntnis:
When I have to work with spaghetti code, the first thing I work on is modularization.  Find places where you can draw lines and extract (more or less) independent pieces of the codebase.  They probably won't be very small, due to a high degree of interconnectedness and coupling, but some module lines will emerge if you look for them.

Once you have modules, then you're not faced with the daunting task of cleaning up an entire messy program anymore. Now, instead, you have several smaller independent messy modules to clean up.  Now pick a module and repeat on a smaller scale.  Find places where you can extract big functions into smaller functions or even classes (if G2 supports them).

This is all a lot easier if the language has a sufficiently strong type system, because you can get the compiler to do a lot of the heavy lifting for you.  You make a change somewhere that will (intentionally) break compatibility, then try to compile.  The compile errors will lead you directly to the places that need to be changed, and when you stop getting them, you've found everything.  Then run the program and test everything!  Continuous testing is crucially important when refactoring.
AnswerKenntnis:
I don't know if this is an option to you, but I would start trying to convincing them to hire more professional developers. This way they could concentrate in domain problems (I'm sure they have enough there).

I believe they are very smart people, but becoming a good developer demands a lot of time. Are they ready to spend so much time in a activity that isn't their main business? IMHO, this isn't the way to achieve the best results.
AnswerKenntnis:
Wow. Sounds like you have a really big challenge ahead of you! I'd do something along the following lines:


First of all: Prioritize. What do you want to achieve first? What is the most important for the current state of the project? What will you get the most from vs how much time it'll take to get there.
Ensure that you have a version control system. Git or Mercurial for instance.
Get some kind of continuous integration system (e.g. Jenkins) up and running.
Get a bug tracking system up and running. Mantis is quite nice in my opinion.
Look into static code analysis (if something is available for the language you're currently working with). 
Try to achieve as much consistency in anything from naming of variables to general code conventions and guidelines in the code base.
Get the system under test. This is extremely important for a big legacy system like this in my opinion. Use test cases to document existing behavior, no matter if the behavior feels weird or not (usually there's a reason as to why the code looks a certain why, might be good or bad, or both ;P). Michael Feathers Working Effectively With Legacy Code is an excellent resource for this.
AnswerKenntnis:
They say that the first step in solving a problem is admitting that you have one. With that in mind, you might start by generating a dependency graph that illustrates the vast tangle that is your current code base. Good tool to generate dependency diagram? is a few years old but contains some pointers to tools that can help create such graphs. I'd go with one great big, ugly graph that shows as much as possible to drive the point home. Talk about problems that result from too many interdependencies and maybe throw in a line from Buckaroo Banzai:


  You can check your anatomy all you want, and even though there may be
  normal variation, when it comes right down to it, this far inside the
  head it all looks the same. No, no, no, don't tug on that. You never
  know what it might be attached to.


From there, introduce a plan to start to straighten out the mess. Break the code into modules that are as self-contained as possible. Be open to suggestions as to how to do that -- the folks you're talking to know the history and the functionality of the code better than you do. The goal, however, is to take one big problem and turn it into some number of smaller problems which you can then prioritize and start to clean up.

Some things to focus on:


Create clean interfaces between modules and start to use them. Old code may, of necessity, continue to not use those nice new interfaces for a while -- that's the problem you're starting to solve. But get everybody to agree to use only the new interfaces going forward. If there's something they need that's not in the interfaces, fix the interfaces, don't go around them.
Look for cases where the same functionality has been repeated. Work toward unification.
Remind everybody from time to time that these changes are meant to make life easier, not more difficult. Transition can be painful, but it's for a good purpose, and the more everybody is on board the faster the benefits will come.
AnswerKenntnis:
After looking into Gensym G2 for a bit it looks like the way to approach this problem is going to be highly dependent upon how much of the code base looks like this:



or this:



versus this, courtesy of 99 Bottles of Beer:

beer-bottles()

i:integer =99;
j:integer;
constant:integer =-1;

begin
for i=99 down to 1
    do
    j = (i+constant);
        if (i=1) then begin
            post"[i] bottle of beer on the wall";
            post" [i] bottle of beer";
            post" Take one down and pass it around ";
            post" No bottle of beer on the wall"; 
        end 
        else begin
            post"[i] bottles of beer on the wall";
            post" [i] bottles of beer";
            post" Take one down and pass it around ";
            if (i=2) then 
                post" [j] bottle of beer on the wall"
           else
                post" [j] bottles of beer on the wall"; 
           end
    end
end


In the case of the latter you are working with source code which is effectively a known quantity and some of the other answers offer some very sage advice for dealing with it.

If most of the code base is the latter, or even if a sizable chunk is, you are going to be running into the interesting problem of having code that likely cannot be refactored due to being extremely specialized, or worse yet, something that looks like it may be removable, but unless it is properly documented, you don't know if you are removing critical code (think something along the lines of a scram operation) that doesn't appear to be so at first glance. 

Although obviously your first priority is going to be getting some sort of version control online, as pointed out ElYusubov, and it does appear that version control has been supported since version 8.3. Since G2 is a combination of a couple different language methodologies you would likely find it to be most effective to use the version control that is provided with it as opposed to trying to find something else and getting it to work.

Next, although some would likely advocate for starting to refactor, I'm a strong advocate of making sure you fully understand the system you are working with before you start touching any of the code, especially when dealing with code and visual diagrams that developed by developers with out formal training (or background) in software engineering methodologies. The reasoning for this is several fold, but most obvious reason is that you are working with an application that potentially has over 100 person-years worth of work put into it and you really need to make sure you know what it is doing and how much documentation there is in it. As you didn't say which industry the system is deployed to, based upon what I have been reading about G2 it appears it is safe to assume that it is likely a mission critical application that may even hold a potential for also having life safety implications. Thus, understanding exactly what it is doing is going to be very important. It there is code that is not documented work with the others on the team to make sure that documentation is put into place to make sure people can determine what the code does. 

Next start wrapping unit tests around as much of the code base and visual diagrams that you can. I must admit to some ignorance with regards to how to do this with G2 but it might almost be worth creating your own testing framework to get this in place. This is also an ideal time to start introducing the other members of the team to get them use to some of the more rigorous engineering practices involved with code quality (i.e. all code must have unit tests and documentation). 

Once you have unit tests in place on a fair amount of the code, you can start approaching refactoring on manner such as that suggested by haylem; however, remember to keep in mind that you are dealing with something that is meant for developing expert systems and refactoring it might be an uphill battle. This is actually an environment where there is something to be said for not writing extremely generic code at times. 

Finally, make sure you pay close attention to what the other team members say though, just because the code and diagram quality is not the best doesn't necessarily reflect poorly upon them. Ultimately, for the time being they are likely to know more about what the application does than you which is why it is all the more important for you to sit down and make sure you understand what it does before making sweeping changes as well.
AnswerKenntnis:
Usually the complaints you hear upfront has nothing to do with the important problems. After all, it is entirely normal to hear of these complaints in any software projects. 

Hard to understand code? Check. Massive code base? Check. 

The real problem is that people leave, and when the new person joins the organisation, there is a typical disorientation. Furthermore, there is an issue of unrealistic expectations and code-quality issues. 

Here is what I would tackle, in order:


Backups, both the server and local version
Set up bug tracker
Set up versioning system
Set up FAQ/Wiki
First debriefing of all scientist/programmers

Remind them of the 80/20 rule. 20% of the bugs are responsible for 80% of the problems.
Focus on the biggest issues, and keep enhancement requests et c off. 
The purpose here is not to scare people with a big list, but a list of small achievable wins. After all, you have to prove your worth as well.

Set up build system

Start working on getting reliable builds (this may take a while)
identify and name every project
identify cyclic dependencies
if there are binaries from some open-source projects, try to obtain sources 

Identify how the G2 code can be modularized, e.g. APIs, services
Identify how the G2 code can be tested, documented.
Set up code-review system
Second debrief 

What does an ideal module look like? 
How to cordon off old code
How to wrap new module around old code (See the Strangler Pattern http://agilefromthegroundup.blogspot.com.au/2011/03/strangulation-pattern-of-choice-for.html )

Identify a crack team of better programmers and work with them to wrap their modules. 
Code reviews are there at this stage to improve communication, and documentation. Keep it easy at this stage. Sort out any process issues.
Roll out the system to other programmers. Let the crack team members become peer mentors to the rest. Remember that scaling is the issue here. You are effectively in a management role.
AnswerKenntnis:
Questions like these are the whole reason the Software Carpentry project exists.  

For the last 14 years, we've been teaching scientists and engineers basic software development skills: version control, testing, how to modularize code, and so on. All our materials are freely available under a Creative Commons license, and we run a couple of dozen free two-day workshops every year to help people get started.   

Based on that, I think the best starting point is probably Robert Glass's excellent (short) book Facts and Fallacies of Software Engineering: its evidence-based approach is a good way to convince scientists that what we're telling them about good programming practices is more than just opinion.
As for specific practices, the two that people are most willing to adopt are version control and unit testing; once those are in place, they can tackle the kind of systematic refactoring that Michael Feathers describes in Working Effectively With Legacy Code.
I no longer recommend The Pragmatic Programmer (lots of exhortation, hard for novices to put into practice), and I think McConnell's Code Complete is too much to start with (though it's a great thing to give them six months or a year in, once they've mastered the basics).

I would also highly recommend Paul Dubois' excellent paper "Maintaining Correctness in Scientific Programs" (Computing in Science & Engineering, May-June 2005), which describes a "defense in depth" approach that combines a dozen different practices in a logical, coherent way.
AnswerKenntnis:
I think first of all you have to clear your situation. What do they want from you?


It is very unlikely that they want you to learn an ancient language, because this now seems a dead end: there is a decreasing chance to find anyone knowing or wanting to learn G2, so the knowledge will be buried in the collapsing heap of code when the current scientists leave or the all-patched code fails more and more often.
Are the scientists (or some of them) ready to learn a new language and lots of programming paradigms? Or do they want to separate the programming and scientific activity in the long term, and perhaps have some more programmers if needed? This seems a rational and more efficient separation of expertise.


I think the core requirement here is to "save the knowledge in the system", so you have to go and excavate it!

The first task is to write a documentation. 

Analyze the structure and the requirements as if this would be a new task, but with the help of an existing system. They will be pleased because you ASK instead of TEACHING first - and you will quickly get enough, but more organized background knowledge from a programmer's point of view: "what's going on here?" The docs (system static structure, workflow, components, problems) will be immediately valuable for them, and will perhaps show up more relevant information to them than to you (some of the guys may have "AHA!" and start fixing some codes immediately)...

You should then start asking where they want to go? 

If they are ready to move away from G2, what system do they want to see (platform, language, interface, general structure)? You might start writing an external wrapper around the system if at all possible, having the target structure, but keeping the original components, thus slowly starting a kind of framework that allows new components to be implemented in a this target environment. You have to find the core services (persistent data connections and "toolkits": core calculation, drawing, ... libraries), and so you provide them a familiar environment in a new platform and language, which allows the transition either by you or them: take the old codes one by one, reimplement (and CLEAN!) them in the new environment. When that is ready, they know the new language; and the service layer (mostly made by you, sorry) is ready to host the new components.

If they don't move, you must learn G2, and create the modular framework there, into which you or they should move the components (with cleaning). Anyway, the language is just a serialization of data and algorithm tree...

While analyzing and writing the docs, read, use and advertise GoF Design patterns! :-)

... my 2 cents
AnswerKenntnis:
I just got done doing a series of presentations on Robert Martin's SOLID principles for my co-workers. I don't know how well these principles translate to G2, but since you're looking for 5-7 core fundamentals, these seem like a well-established set to start out with. If you want to round it up to 7, you could start with DRY and throw in Fail-Fast.
AnswerKenntnis:
The only production problem sounds like a change management issue.  If that is the case and the software otherwise performs it purpose the first advice I would give is to resist the urge to do too much too quickly.

Source control, refactoring, more trained devs are all good suggestions, but if this is the first time you have had to deal with this sort of issue moving slowly and making controlled changes cannot be emphasized enough.

The urge to shred through the mess will be great at times, but until you have reverse engineered enoughof it that you know you can test your replacement version adequately you need to be very careful.
AnswerKenntnis:
The most important principles for working in such a situation are:


Be patient. A hole which took 20 years to dig isn't going to be filled in in a few weeks.
Be positive. Resist the temptation to complain and grumble.
Be pragmatic. Look at a positive change you can accomplish in a day, and do that, today.  Got a version control system yet? Implement it and train people. Then look and see if you can automate testing (Unit testing or something similar). Rinse. Repeat.
Be a model.  Show (don't just tell) people how agile works by being agile.  The first three points above are the keys to being a Good Guy, which is the predecessor to being an effective Agile guy.  In my opinion, people who are admirable developers aren't only smart, they're also good, model employees and colleagues.
Map your territory.  I have a technique for mapping giant legacy codebases. I clone the repo, make a working copy, and then I try to change something, and see what else breaks.  By investigating coupling (via global state, or broken APIs, or a lack of consistent API or any abstractions or interfaces to program against) and by reading the code that breaks when I change things, I discover the cruft, I ask questions that lead to insights from the rest of the team (Oh we added that because Boss X 5 years ago demanded that, it never worked!). Over time, you'll get a mental map of the territory. Once you know how big it is, you'll know enough to make your map and get home.  Encourage others to map the territory of your giant codebase, and to build the technical knowledge of the team. Some people balk at "documentation" because it isn't agile.  Whatever.  I work in scientific environments too, and documentation is king for me, agile manifestos be damned.
Build little apps.   When working with a legacy codebase, I find I get ground down to a pulp. I get my spirit back by building little helper apps.  Maybe those apps are going to help you read, and understand and modify that giant G2 codebase.  Maybe you can make a mini IDE or parser tool that will help you work in your environment.  There are many cases where Meta-programming and Tool-building will not only help you break out of the giant deadlocks that legacy codebases impose on you, they also give your brain the ability to fly unrestricted by your G2 language. Write your tools and helpers in whatever language you can do them fastest and best in. For me, those languages include Python, and Delphi.  If you're a Perl guy, or you actually LIKE programming in C++ or C#, then write your helper tools in that language. teach the rest of the team to build little helper-apps and tools, and "componentry" and you'll eventually see that your legacy codebase isn't so daunting after all.
AnswerKenntnis:
Revision control: show the domain experts the benefit of being able to revert, see who changed what, etc. (This is tougher with all-binary files, but if the contents are indeed code, surely there is some sort of G2-to-text converter that can enable diffs.)
Continuous integration and test: get the domain experts involved in creating end-to-end tests (easier, since they must already have inputs and expected outputs somewhere) and small unit tests (harder, because spaghetti code probably involves lots of global variables) that cover nearly all functionality and use cases.
Refactor common code into reusable routines and components. Non-software people with no revision control probably copy-and-paste 100s of lines at a time to make routines. Find them and refactor them, showing that all the tests pass and the code has gotten shorter. This will also help you to learn its architecture. If you're lucky by the time you have to start making the hard architectural decisions, you might be down to 100KLOC.


Politically, if you find resistance from the old timers in this process, hire a consultant to come in and give a talk about good software methodology. Make sure you find a good one whose views you agree with, and get management to buy off on the necessity of the consultant even if the domain experts don't. (They should agree - after all, they hired you, so evidently they realize that they need software engineering expertise.) This is a money-wasting trick, of course, but the reason is that if you - the new hotshot youngster programmer - tell them they need to do something, they may ignore it. But if management pays a consultant $5000 to come in and tell them what they need to do, they'll put more faith in it. Bonus points: get the consultant to advise twice as much change as you really want, then you can be the "good guy" and side with the domain experts, compromising to only change half as much as the consultant suggested.
AnswerKenntnis:
"The program itself is a physical model of a complex chemical processing plant
  ..."
  
  "Since G2 is like not-code, but rather automated code written by some gadawful GUI..." ΓÇô Erik Reppen 


Assuming your software's main goal is to simulate (perhaps optimize, run parameter estimations upon) a complex chemical plant, or parts of one... then I'd like to throw out a rather different suggestion:

You might do well to consider using high-level mathematical modelling language to extract the essence, the core mathematical models, out of hand-coded software.

What a modelling language does is to decouple the description of the problem from the algorithms used to solve the problem. These algorithms are generally applicable to most simulations/optimisations of a given class (e.g. chemical processes) in which case they really shouldn't be re-invented and maintained in-house. 

Three commercial packages used widely in your industry are: gPROMS, Aspen Custom Modeller, and (if your models don't include phenomena distributed along spacial domains) there are Modelica-based software packages, such as Dymola.

All of these packages support "extensions" in one way or another, so that if you have parts of your models that require custom programming, they can be encapsulated into an object (e.g. a .DLL) that can be referenced by the equations in the model.  Meanwhile, the bulk of your model remains succinct, described in a form easily readable by the scientists directly. This is a much better way to capture your company's knowledge and IP.

Most of these programs should also allow you to 'start small' and port small parts (sub-models) of your monolithic code into their format, by being called externally. This may be a good way to maintain a working system and validate it one piece at a time.

Full disclaimer: I worked as a software engineer at the company behind gPROMS for 8 years. In that time I saw (and occasionally incorporated) examples of custom software (e.g. coming from academia) which had started small and tidy, implementing some clever solution or algorithm, but then exploded over the years with extensions and modifications - without the helpful guidance of a software engineer to keep it clean. (I'm a big fan of multi-disciplinary teams.) 

So I can say with some experience that certain key choices made poorly early on in a software's development (like a language or key library) tend to stick around and cause pain for a long long time... They've already 'shaped' the software around them. It sounds to me like you could be facing many many years of pure code cleaning here. (I'm hesitant to use numbers but I'm thinking 10+ person years, maybe much more if you can't get the code ported from G2 to something that supports good automated refactoring tools like Eclipse/Java quick-smart.)  

While my default status is to "refactor and keeping a working system", I also think once a problem gets "too big", then a more radical change/re-write becomes faster overall. (And possibly brings additional benefits, like jumping to a more modern technology.) I say that with some experience porting to a new software platform, but from what I gather it's even more dramatic with a port to a mathematical modelling package. 

To give some perspective, you might be quite amazed at the size reduction. E.g. the 200,000 LoC could actually be represented in something like 5,000 lines of equations (OK I'm guessing here, but I could try and get you an actual testimonial from friends in the business); plus a few relatively small function modules written in something like C (for example, physical-property calculations - although again off the shelf packages may exist depending on your chemical process).  This is because you literally just throw away the algorithmic solution code and let a general-purpose 'stack' of mathematical solvers do the hard work. Once you have simulations running, you can do much more with them, like optimising the process - without changing a line of code.

Finally I would say: if the only reliable documentation of the various mathematical models (and algorithms) is the code itself, you will want the help of the scientists and original authors to help extract those models out, ASAP, not years down the track when some of them may have moved on. They should find that a mathematical modelling language a very natural way to capture those models - they may even (shock horror) enjoy (re)writing it. 



Finally, since my answer might be off the mark, I'd just like to add one more book to the list of good books already referenced here: Clean Code by Robert Martin. Full of simple (and justified) tips that are easy to learn and apply, but which could make a world of difference to people developing new code at your company.
AnswerKenntnis:
I would throw down the following:


There is one programmer here. Screw politics. They know their trade. You know yours. Mark that territory even if you have to piss on it. They're scientists. They can respect that sort of thing or should since they're pretty much constantly doing the same themselves. Through whatever means you can, mark the boundaries now. This is what I'll fix. This is what I can't be responsible for.
Scientists write/test the algorithms. Scientists who want to can write their own algorithms in 1-3 languages everybody can agree on for you to convert to core code. That puts testing their stuff on them. Beyond that, they're going to have to help you isolate the important science stuff vs. the good-god-knows-what they did for architecture. The codebase is hosed. There's lots of slash and burn that's going to need to be done. Give them options to hand you working versions of stuff that employs what they know best so you can do what you do best. Stick their knowledge in a box that they're responsible for but that you can work with. Ideally when things go well on a mega-refactor conversations will be more about what sorts of exciting things you can do with interface rather than what tentacle Z9 of drBobsFuncStructObjThingMk2_0109 did when it pissed off global var X19a91.
Use an event-driven-friendly language with first class functions if you can. When all else fails, triggering an event or tossing a callback to some object with an interface and state mechanism that actually makes sense can be a huge time-saver when you're knee-deep in code that makes no bloody sense and quite possibly never will. Scientists seem to like Python. Not hard to glue lower-level math-intensive C stuff with that. Just sayin'
Look for somebody who has solved this or a similar problem. Spend some serious time researching. These guys heard about G2 from somebody.
Design Patterns. Adapters. Use 'em. Use 'em a lot in situations like this.
Learn what you can of the science. The more you know, the better you can determine intent in the code.
AnswerKenntnis:
Do the analysis first.

I would do some analysis before deciding what to teach.  Figure out where the biggest pain points are.  Use those to prioritize what practices to go over.

Introduce only a few changes at a time (in a similar situation I did 2-3 practices every 2 weeks).

I would limit the practices to ~3 depending on level of change to there programming style of SDLC; until they start to get comfortable with them (I would push to introduce 1 new change every ~1-2 weeks as they get more comfortable with the idea of learning new approaches).  It's also a good idea to identify what the criteria for success is.  What the practice should accomplish (even if it's a soft goal like team morale).  That way you can show if it's effective or not.


Why limit the number of changes?


Even if you assume these people want to be better programmers and are open to learning there are limits to how much and how fast people can learn new concepts and apply them; especially if they don't have CS foundation or have participated in a Software Development Life Cycle previously.  

Add a weekly wrap-up meeting to discuss how the practices affected them.

The meeting should be used to discuss what went well and what needs work.  Allow them to have a voice and be collaborative.  Discuss and make plans to address problems they are having and to preview the next changes coming up.  Keep the meeting focused on the practices and their application.  Do a little evangelizing on the benefits they should start to see from applying the practices.

Certain practices take precedence.

Proper use of a version control system (IMO) trumps everything else.  Close behind are lessons in modularization, coupling/cohesion and feature/bug ticket tracking.

Remove practices that don't work.

Don't be afraid to get rid of practices that don't work.  If there is a high cost and little to no benefit, remove the practice. 

Improvement is a process.

Convey that sustained, consistent improvement is a process.  Identify the biggest pain points, apply a solution, wait/coach and then repeat.  It will feel agonizingly slow initially until you build up some momentum.  Keep everyone focused on the improvements that are coming and the improvement that are already successful.
AnswerKenntnis:
Sounds like the first step you have is to sell to the team the need to invest in new software methodology.  Per your statement, there is no concensus in the team, and you'll need it to be able to plow ahead with a slow "upgrade" of the code.

I would (if I can) personally take the hard lessons learned, and introduce each of the key concepts that you want to as the solution to the problem in the software industry.

E.g. two developers had different copies and ended up deploying a hybrid untested release -> Introduce version control, branching, and testing.

Somebody removed a few lines of code that they didn't understand and caused an outage -> introduce DDD.

If the hard lessons aren't being shared with you in sufficient detail, then just show your own examples of how things went wrong when this discipline wasn't adhered to.
AnswerKenntnis:
Source code control is step #1 as already stated many times. While the people you work with may not be professional developers and they won't respond to a lot of enterprise or agile mumbo jumbo. They're not low level code monkeys either and trying to treat them like that by forcing them to do things 'your way' will not fly.

You've got to survey whats out there. If they haven't used source code control then just identifying the right versions of the code (if possible) and what all the possible deliverables are is going to take a long time. Then you're going to have the task of teaching your colleagues how to use source code control and convince them that its worth their time. Start with the benefits! 

While you're doing that, find other low hanging fruit and fix those problems. 

Above all else, listen to what they have to say and work on improving their situation. Don't worry about trying to put your stamp on what they do.

Good luck!
QuestionKenntnis:
I'm a manager. How can I improve work relationships and communication with programmers?
qn_description:
A little background first. I'm a project manager at medium-sized company. I started as a CS major and had a little exposure to programming, but after a few months I knew it's not my path, so I switched over to management. That proved to be a good decision, and after graduating I've worked in software management at various companies (for 5 years now).

Recently, we had a very painful project. It was the worst of the worst, with many mistakes both on our side and on the customers side and just barely ending it without losses. It has led to many frustrating situations, one of which escalated to the point where one of our senior developers left company after a vocal argument with us (the management). This was a red flag for me: I did something terribly wrong. (for the record, the argument was about several mistaken time estimates)

I searched many places for answers and a friend pointed me to this site. There are many questions here about frustrations with management. I can understand that the general bad experiences lead to a general reluctance against "those guys in the suits". 

I'm that guy in the suit. It may not look like it, but all I want is a successful project, and with limited resources it takes painful decisions. That's my job. One of the things the aforementioned senior developer complained about was work equipment. Frankly, I had no idea that the computers we had were not suited for working. After this, I asked many programmers and the general consensus was that we need better machines. I fixed that since then, but there was obviously a huge communication gap between me and the programmers. Some of the most brilliant developers are the most shy and silent people. I know that, and it was never a problem during an interview. People are different, and have strengths at different areas. 

The case of the underpowered PCs is just one of the many that led me to thinking that there is a communication issue. How can I improve communication with programmers without being intimidating and repetitive? 

What I'm hoping is that people don't complain about good things. If you love your workplace and love (or at least like :)) your manager, please tell me about them. What are they doing right? Similarly, if you hate it, please describe in detail why. I'm looking for answers about improving communication because I think that is my problem, but I might be wrong.
AnswersKenntnis
AnswerKenntnis:
Wow!  Thanks for asking.  Technically, like you, I guess I'm management, since I spend much more time communicating and leading teams than I do writing code.... but here's my take from both ends of the management horizon.  Whether I'm a developer or a manager working for another manager, here's some stuff that helps in my communication with my management:


Why?  is a very important question - almost any factual answer has a "why" behind it and that "why" may well be more important than the actual question.  There's a few tangents to this:

The Developer Why - Developers will have a lot of answers that make absolutely no sense to management.  I certainly did, and one of the ways I got into management was being really good at explaining the teams "whys" in terms management could understand.  If you don't have a "speaker for the geeks" on hand, you can learn to speak geek by restating their answers to the why question in more commonly understood metaphors.  Keep at it until you both understand and agree on what's going on.
The Management Why - your "why" is just as important.  Why do you need the time estimates?  What are you using them for?  How screwed will we be as a company if they are too high or too low?  This is stuff that you as a manager probably have keen insights into, but this is all voodoo to the developer.  The trick is, the developer may not ask.  Management has asked him for something, and he's going to do the best he can to be accurate, timely and thoughtful - but if he doesn't know the "why" he may optimize in ways you would rather he didn't.  Offer your why, and ask for him to do the same thing - restate the answer in his own terms.  Similarly - go into the why of the business - often developers care but have no direct knowledge of how the business development works - having someone volunteer this information is both motivating and enlightening.



About time estimates specifically - I've had to do a ton, and I have absolutely profited from telling my development team "I need this because I am going to ask for more money to pay our salaries, I will trust what you tell me and I will use your numbers... that means that if you low ball on me, we are all screwed because I will not be able to ask for money a second time - we have to live with what we propose".  With that context, developers changed from low estimates that attempted to show me how confident and brilliant they were to high estimates that came a lot closer to real expectation setting.


No one is wrong - The "why" question goes a long with a corollary - asking "why" instead of saying "that's outrageous!  No way!" keeps the conversation flowing.  Sometimes there is a severe disconnect between what someone things is being asked and what the asker is asking.  My best management has been horribly surprised by my answers, and when surprised, they blink in astonishment and then ask "why do you say that?".  They do not say (immediately) - "you need to change it".  I have reduced numbers on proposals to meet a competitive goal, but only after talking intensely about how we could change the scene and create a different context for the question. 
listen for ambient noise, word choice and the space between the words - Here's a bunch of things I have liked and stolen to use myself:


hang out in the work area, do something productive of your own (don't try to get in developer work, they know you're not a developer) and just listen.  How does the team solve problems?  What are their big problems?  You will never hear the real skinny on their direct assessment of you or management at large, but you may get a really good sense of where problem areas are.  Make sure you're doing something of your own that is productive.  No one likes spying, but unless morale is so low that you can't be near them with out everyone evacuating, productivity within proximity should be tolerable.  
look for word choices - they are often as important as the words themselves.  When I've used particularly positive or negative words, my management frequently asks me why I chose those words when it's a situation they aren't familiar with.
look for pauses, gaps and body language.  If there's a power distance between yourself and the developers (and it sounds like there is) they may not want to contradict or confront you.  But the basic instinct to say "hey, you're wrong" usually manifests itself somewhere.  

open up to as many communication mediums as possible - be ready to chat in person, on the phone, by email, by IM - anything and everything to establish the flow of communication.  People are so diverse that just one trick won't work.  And I see it as the manager's job to be the multi-format communicator, not the developer's.
make it worthwhile talking to you If someone tells you about a problem and maybe a possible solution, he should and probably accept that you are the manager and therefor may decide in favor of a different solution, or no solution at all because you don't think it worth the trouble. But after the third time this happens, especially if it happens without an explanation about 99% of the people will stop telling you anything. 


And here's one that's incredibly hard for me, but has worked great when I can do it - be aware of the difference between introverts and extroverts.  Chances are that you are an extrovert - that's why your job seemed good and a development position did not.  Developers are, for the most part, introverts.  "Introvert" does not mean "can't communicate", but it means that their pattern, process and velocity are significantly different and the urge to communicate incessantly is virtually non-existent.  Plan in the time and quiet (but collocated) space to let introvert based thoughts to come out.  Many of my introvert friends tell me they are just waiting for me to "shut up for like 5 minutes" so they can put together a thought and respond.  Here's a few great articles on the same thing - 5 things extroverts should know about introverts and Rands in Repose on the Nerd Cave - a particularly developer-tastic example of what's great for introverts.  Rands is pretty fantastic, by the way.  He's a geek himself, so he comes at it from a developer-focus, which can be off putting if that's not your style, but he's funny and has some really good insights on team development.

I think the #1 things I have loved about my favorite managers was:


they were as deeply committed and excited about the project as I was (if not more)
I never had a doubt that they had my back - I knew with certainty that when they were in front of the next level of authority that I (or my peers) would never be the scape goat.  It would always be a group failure, if there was failure.
I was given ownership of something significant and appropriate for my skills at the time, but with enough resources to expand my skills and get the job done.
they saw me as both an individual and as part of the team - they were actively engaged in knowing my strengths and weaknesses and working to help me play on my strengths and augment my weaknesses.
they were aware of my personal goals and interested incorporating them as much as they could
they were upfront when making me happy couldn't and wouldn't be a priority.  There's a real value in hearing "I know you hate this type of work, but I need you to do it - here's how it won't be forever...".
there was always time in a week (maybe not at the instant) to explain the big picture
there was near constant feedback and status with no finger pointing but plenty of recognition for individual work.
there was always the truth.  If something was sensitive and couldn't be discussed, they said so point blank.  If something was uncertain, they gave a level of confidence.
AnswerKenntnis:
The easy part first: there's a technical red flag in your post:  I shudder at your mention of "mistaken estimates" - it's an estimate, it CANNOT BE MISTAKEN, that's why it's called an estimate.  It can be off a little, it can be off a lot, that's why it's called an estimate.   If you are taking estimates as gospel, that's going to be one of the main problems that "your" developers are having with your style.

However, I 100% agree with you about the difficulty of communicating with developers.  I had a revelation several years back, as a developer in a project management training.  I saw several of my fellow developers absolutely railing against management after an open atmosphere of discussion was created (management was present here).  Everything that was wrong was the fault of the managers in the eyes of the developers.
The kicker was that management had no idea about nearly all of the huge issues that were raised that day.  Developers were assuming that it was all so obvious that management couldn't possibly have missed it, and management was assuming that developers would tell them what they need.

Therefore, IMHO the first part of the answer must be to let your developers know that you are not psychic, and in fact probably downright stupid when it comes to the technical side.  You are relying on, counting on and trusting them to come to you in a timely fashion.  You are there to make their life easier, not harder.

And whatever you do, do NOT ask them questions you don't actually want to know the answer to - referring to the "mistaken estimates" above ;-).  That's a developer's kryptonite.
AnswerKenntnis:
There's a lot of good stuff here but I feel the following need to be said..
..Sorry to be harsh.. But this need to be said:


5 Years as a PM, and to not know what kind of PC a developer needs, is outrageous.
To have a developer quit because of bad working conditions as your first real red flag, is insane.


What I think you have is a Trust issue, more so than a communication issue. No one tells what is wrong because they don't trust what you will do with that info, and may even fear it will be used against them. The dev that told you about all these issues did so because there was no consequences in doing so, he was quitting.

Personally I would never hire a PM who did not have some development experience in his background. I think on your next project, you should dedicate a small portion of your time as part of the Dev. team. Say 8 hours a week, working as Jr developer under the team lead. 

This will really open your eyes to the dynamics of a development team, because right now, you are not even part of that team, you are an outsider. The fact that your dev quitting came a shock to you, proves it. Everyone on the team knew he was unhappy. And not one of them told you :


  "We are going to lose our best guy if you don't do something"


That should be red flag #2.
AnswerKenntnis:
As a manager I'm sure you heard about kaizen, one of the tenets of the Toyota Production System meaning continuous improvement. 

You admitted you have a problem, so, that's a great start.

Kaizen has five primary elements:


Quality Circles: Groups which meet to discuss quality levels concerning all aspects of a company's running.
Improved Morale: Strong morale amongst the workforce is a crucial step to achieving long-term efficiency and productivity, and kaizen sets it as a foundational task to keep constant contact with employee morale.
Teamwork: A strong company is a company that pulls together every step of the way. Kaizen aims to help employees and management look at themselves as members of a team, rather than competitors.
Personal Discipline: A team cannot succeed without each member of the team being strong in themselves. A commitment to personal discipline by each employee ensures that the team will remain strong.
Suggestions for Improvement: By requesting feedback from each member of the team, the management ensures that all problems are looked at and addressed before they become significant.


(Source)

If you take a long look at this a constant over those elements is the emphasize on teamwork and feedback.

An example, you say you had an argument over time estimates. Are you the responsible for those time estimates? Do you talk with the team about that? I'm sorry but I've seen managers just pull out a number from their as-. One crucial thing: never haggle over time estimates your team gives to you. A lot of managers imagine that if they can meet shorter time deadlines if their team works harder. This is a lie. Nine women can't have a baby in one month, remember that.

So, my first advice:

Listen: start with a simple e-mail to the team: "What's the best way for the development team to give feedback to the management about the management performance?". Iterate with the team and implement the consensus. Your task is to allow the team to develop great software, not to herd them. Keep this in mind.

Honesty and Loyalty: If no ones answers when you ask something, it's because they don't believe you will listen or, worst yet, they believe you will punish them for that. So be honest. If someones says you suck, take this as a feedback and do not take revenge. Understand their reasons, improve on it. 

And, at last, although I've seen some critiques about it here, I'd like to recommend you to read a book called The Mythical Man-Month: Essays on Software Engineering. The book is about Fred Brooks experience at IBM while managing the development of OS/360. While a few things here and there may be dated, it's the starting step to understand how the development process of complex software works. S.Lott cites the Agile Manifesto, which I'm not particularly keen on it, but it's also worth reading.
AnswerKenntnis:
Read this:

http://agilemanifesto.org/


  
  Individuals and interactions over processes and tools
  Working software over comprehensive documentation
  Customer collaboration over contract negotiation
  Responding to change over following a plan
  


In many cases, the organization (i.e., your boss) demands that you 


follow a clearly broken process to it's logical conclusion leading to a "death march".  This leads in turn to arguments and resignations.
create excessive, low value, unused documentation.
engage in complex change control a/k/a contract negotiation.  Every user change requires an elaborate ritual to "quality" and "prioritize" the change.  Really, it's all about the "freeze" -- preventing change.
follow the plan irrespective of the consequences.  Some organizations value on-time delivery of broken (or even useless) software.  It's the plan that's valued, not the solution of a business problem.


It may be that the issue isn't your personal communication skills.  

It may be that the entire development "environment" or "methodology" is fatally broken, and bad feelings are just a symptom of general bad practices.
AnswerKenntnis:
To me this sounds like you never talked with the devs in an easy atmosphere. Your talks with them were merely of official nature? That's too bad. 

Why don't you get to know them personally and thus get to know what's good and what's bad about the company, their workplace and their projects? Respect them and earn their respect, by showing sincere interest and appreciation for their work.

If they trust you and needn't fear about being pawn offers, they will most likely even tell you unattractive truths.

I am a Team-Lead and my team trusts me. I protect them. I take all the blame and I share the fame with them. Our management protects me. That boosts the morale. We had a really hard project and an almost evil customer, nearly impossible to finish but we did it eventually, because we talk about everything in a very frank and open manner.
AnswerKenntnis:
Clap! Clap! Clap! You certainly should be a good person, for you have come out in open to see what can be done to get better at your job.

Please find below what I have witnessed in a great manager, and have personally adopted when I lead the team as a senior member. 


M entor more than manage.
A llow team members to voice their thoughts and concerns. Be all ears
to it. Take the constructive ones.
N ever betray team members by playing divisive politics. This
back-fires sooner and silently.
A nger not. Never have grimaces on your face when you are with your
team, come what may. This one is really difficult.
G enuinely and openly appreciate the winner for his/her good work. In
the same breadth, very softly and tactically cricicize the work not
person for any wrongs, to the person who is responsible, in isolation
and not in open.
E ncourage ownership and leadership in every individual. This boosts
the morale and commitment of the person, because he would feel
respected.
R oam around with your team once in a while. This one
induces/increases bonding within team members.


Wish you good luck in your sincere endeavour :)
AnswerKenntnis:
In general, the guys in the trenches start feeling mutinous when they feel their gripes aren't being heard by people who can and will fix the situations. When they don't even feel they can gripe without risking their standing in the company, that's even worse.

I'm a Theory-Y kinda guy, and most "knowledge workers" tend to be; Given a chance, we like our work, and want to do it well. However, the downside of a Theory-Y workplace is that it may not be immediately apparent there's a problem, because people, wanting to do well and thus not wanting to make waves, will find ways around that problem, or simply ignore it. This leads to pent-up frustration, that eventually blows up in the entire team's face. A shop run by a Theory-X manager would probably have guys that complain much earlier; the employees are only in it for the money, so if the job sucks more than usual they'll gripe.

As for what you can do, in an environment with seniors and leads in the room doing the job, they are your best asset. Talk to them. You might schedule 30 minutes a week for "two-ways", where the leads give you updates and air concerns about the day-to-day of the project, and you give them updates on the business side and plan with them to resolve concerns before they become problems that seriously affect the team.

In Agile, at the end of each "sprint" or "iteration" (which is a unit of development work usually lasting between one and three weeks), the entire team, from the most junior members up to the PM, has a "retrospective". They look back at what they did, what went right, what didn't, and identify things to keep doing and things to change. There are several formats, and you can invent your own, but the result of the retro should be that people feel their voice was heard, and that things will change as a result.

Talking about Agile; my first job was for a small company, and by "small" I mean the whole firm couldn't field a softball team. There were four programmers when I started, and that dwindled to two before I left. The founder, President, CEO and 95% stakeholder in the company ruled it with an iron fist, and he was the sole source of planning in the organization, meaning there wasn't much. The Boss was a workaholic and expected everyone else to be as well; Everything you had to give was no more or less than his expectation, and for this he paid an entry-level salary to people who'd worked for him for a decade.

I left that company and began work for another firm that did things VERY differently; they practiced the SCRUM Agile basic methodology, with daily standups, pair programming, sprint teams and retrospectives. For one day every two weeks at the beginning of each sprint, we did nothing but plan out the next two weeks' work. For a big chunk of another day, we did nothing but look back on what we'd done and find ways to improve as a team. There were developers sitting next to me who were Microsoft MVPs, getting the job done, and encouraging and complementing what I was doing.

Night. And. Day. The main difference? First, I did not feel like I was expected to kill myself for the project; a fundamental tenet of Agile is the sustainable pace of development. Second, I had a voice in deciding how I would be expected to do my work. I had to do the work, but if I had "heartburn" over what I was going to be expected to pull off in the next sprint, I could voice that opinion and it would be heard and given weight. If I felt there was a better way, I could say so and it would be entertained.

As far as finding solutions and resolving problems, you must be careful not to look like you're working from the top down. For computers; say your RMR (recurring monthly revenue) only allows the company to upgrade four computers ever two weeks. The first four computers should not all go to the leads and seniors; they should go to the people with the slowest computers. If you give bonuses to the team, don't just give them to "our valuable seniors and leads, without whom this wouldn't have been possible"; EVERYONE in your dev team made it happen. If a junior has a complaint, hear him out; just because he's a junior doesn't mean he doesn't know anything.
AnswerKenntnis:
Improving relations:
Just had a hard project? Give them a break afterwards. Vacation time to go unwind, get their breath.
Coders bill of rights These things are a given. Basic things that should go without saying. If you violate these, you are abusing your codemonkeys.
The Joel test I agree with most of them. But some really depend. If you're missing some, you're probably making life for your programmers harder then it needs to be.
Technical debt. So you can push for completion, but you have to realize that by cutting corners you incur technical debt that will take more time at a latter date to fix. If that date comes up BEFORE the end of the project, you've really screwed up.
RSA animate: Motivation This is a fantastic bit that really shows how to motivate knowledge workers. 
Free day to code what they want. From the RSA video. Don't remember the name, but the company it mentions has a short explanation of it on their site. Seems like a good idea to me. In most shops there are things that everyone knows is busted, but nobody has time to fix, and it's not a high priority. This lets them tackle technical debt. It also lets them show off their brilliant ideas.

And for the love of god, try to keep a 40 hour work week. Also, flextime. Flextime can make up for a whole world of bullshit. For me at least.

Improving communication between programmers and bosses
That's tougher for me. That whole shmoozing thing is more of a boss skillset then a programmer's focus. I could say some basic things like time estimates are only estimates. Walking on water and fulfilling requirements are easy if they're frozen. Maybe asking the shy programmers to give a presentation on their project at a code review or something. Practice makes perfect, ya? But I'd bow to others' advice on this one.

"Similarly, if you hate it, please describe in detail why."
Well that's going to open up the floodgates here. And if I wasn't using an openID that could obviously be linked to me, I'd probably vent too. If you really want a giant list of what not to do, ask on a forum that's more friendly to anonymous posting.
AnswerKenntnis:
I've always found that people in general will not treat you any better than you treat them (though they may treat you worse).  I personally (I'm a developer) respond to honesty with honesty, to respect with respect, to trust with trust, and so on.  You should have an informal chat with your team in a neutral atmosphere, and tell them what you just told us.  The point that gets forgotten in the toxic "us versus them" environments is that it should all be "us".  Both management and workers need to know that, and the enterprise must support that.

Good luck.
AnswerKenntnis:
You now have a proven record of not only accepting feedback, but acting on it. You've demonstrated that you have influence with higher decision makers and you can get things done for your team. That makes a big difference. Programmers my be more introverted, but we like to talk about programming. An informal environment is nice, but everyone still needs to be professional. Allow people to vent some, but make sure the discussions are productive and not just a bitch session.
AnswerKenntnis:
From my experience, the most significant factor for me to like or dislike my manager is if he/she understands development in general and understands the work I am doing. Some positive results are listed here:


I don't have to waste too much time to justify why would a change take so long or cannot be implemented. Well, technically, any change can be implemented and upper management usually demands the implementation any way. But at least in such a situation, your direct manager is on your side, asking for more time (instead of pushing you or burning you out).
I know I have a better chance to be supported in case of a bad situation (a WTF hack, a production issue, etc.). Usually, non-technical person tend to blame developer for such a situation while a good manager UNDERSTANDS what's really going on and supports his/her developer (not just know or willing to take it that way for being nice).
I know my work and performance are to be evaluated by a proper person.


In my opinion, if you don't do programming any more and usually in a tight project schedule or budget, the chance for your developers to like is very low. If that's the case, you better move up quickly and have some one else to be the direct manager. Sorry I sounded bad in this paragraph but that's the way I see it. Your seems to be a good person and deserves some truth.
AnswerKenntnis:
I am also one of the guys in a suit, and have been for more than 15 years. I was a software developer when I started out, and I still write code when I get a chance. So I think I can talk for both sides and I have a bit of experience in these situations. I also have credentials, such as "Employee of the year", elected by staff, that make me confident in handling these situations.

What I witness very often is that there are substantial differences in the value systems and operational methods/approaches taken between management and developers.

For a lot of developers, sincerity, honesty and otherwise a flexible work environment are very high on the list. Unfortunately the very same values are not very high on the list of top management. And this leads inevitably to huge clashes, particularly if middle management (you and I) decide to completely take the top management side. The only way out of this (from my point of view) is by taking a firm stand in front of your team, backing them all the way, and building a trust relationship through open communication and, most importantly, by doing what you are saying (which is often the opposite of what you get from top management, where politics completely overpower sincerity).

At the same time you yourself need to stay operational, so you have to find a way to communicate to top management in a language they understand and play their game. That is the true challenge of middle management.
AnswerKenntnis:
I believe that with developer happiness productivity all comes down to the little things. The math works out pretty awesome for management. Give me a donut(-25 cents) in the morning and I will work twice as hard all day(+many dollars). It is not that we actively sabatoge things by working slow when we are dissatisfied, it is that we are working on extremely complicated systems and it is extremely difficult to focus when we are frustrated about something. It is really probably better that we do not code as much when we are angry.

Estimates, however, must be addressed by themselves. Every issue I have can be solved by handing me a donut, with the exception except unrealistic estimates. True or false this is how a developer sees it: Management has everything to gain (like a new boat) by a shorter estimate, while developers have everything to lose (like a month's worth of sleep). Management is in charge though, so they win the estimate war every time. I think the estimate system works best when developers decide the deadline (it's hard enough for us to give an accurate estimate, so how would a manager?), but management positively motivates them to be ambitious, with the understanding that no developers get railroaded for being a bit off.
AnswerKenntnis:
Consider what kind of reaction do you give a programmer that may have a question, comment or concern.  Is there a, "What do you want now?" or "Why are you bothering me with this?" kind of a response?  How well are you at encouraging the developers to voice concerns and comments?  That is merely a starting point though.

Secondly, be careful of where you are trying to have these discussions.  I doubt I'd be very open discussing my work machine with someone in the next cube if I knew my manager was within earshot of hearing the whole thing.  If you want people to give open and honest feedback, there has to be some privacy given to knowing their answers aren't going to be publicly broadcast or used against them.

Third, consider what kind of Emotional Intelligence skills do you have.  Emotional Intelligence for Project Managers: The People Skills You Need to Achieve Outstanding Results by Anthony Mersino would be a book recommendation I got yesterday from a Lunch and Learn about EQ.  If you really want to get deep into psychology here there are various personality profile tools that could be used,e.g. Enneagram, social styles, and MBTI.

Lastly, consider what is the culture in your company.  Are mistakes something swept under the rug?  Are complaints a big no-no that could get someone in trouble really easily?  What behaviors are rewarded or encouraged and which are tolerated and accepted?  While some of this is observation, some of it may also require some conversations that should be held either away from the office or in a room where there isn't likely to be eavesdropping.  You will likely be repetitive in trying to use this in the beginning.  That isn't a bad thing if you are trying to establish a new practice and get people on board with speaking up if the culture was primarily one where everyone just knew to "suck it up."  This may be more touchy-feely than other answers but this would be what I'd give for an answer if I was asked about this where I work.
AnswerKenntnis:
Do the devs feel that you are their advocate?  By that I mean that they know they are free to share with you their concerns/frustrations without being beaten up?  Do they feel that you fight for them?  Do they feel that you appreciate their work?  Do they feel that you genuinely want them to succeed in their career?

If they feel appreciated, you'll probably have better communication.
AnswerKenntnis:
As a developer I'm a major nerd and lack social skills and I make no apologies about it. After all, I'm the talent, and you've hired me for my talent. If you needed social butterfly's to do the job you'd have a room full of project managers instead of developers.

I know that some developers are very socially astute, but I think the median leans toward introverted nerd.

When someone requests me to do something I make absolutely no inferences and do EXACTLY what is requested. It seems like with some project managers I always end up with problems because they expect me to make inferences about their project which I absolutely will not do, so sometimes things don't turn out as they expect even though they turn out to be exactly what they have requested. I think the reason that this happens with some project managers is because they don't deliver high quality HLDs, BRDs and place too much value in the social aspect of project management rather than what's in black and white. 

I think that this is where worlds collide. I think in the world of project management the social skills and quality of personal candor are important factors, but to developers like me it means absolutely nothing. It doesn't impress me to yammer on about how important this or that task is. I don't even want to go out for lunch or beers like some people here have suggested. 

What I really want are good, high quality HLDs and BRDs. I want schedules and deadlines that are achievable, and if new designs or plans are introduced, I want the schedule and deadlines to be re-adjusted. I've worked on projects where the requirements seem to change on the fly -- to me this is my red flag that I'm dealing with poor quality project leadership. As a developer the worst thing you can do is bring me new project requirements on a daily basis, especially after we already have a schedule or have made schedule commitments. It's intolerable when new requirements are brought in, without compensation to deadlines. Working more hours, working late, I have no problem with that, but it's not something that's always quantitative with development. Some changes can take a few extra hours, some changes could take weeks for proper R&D, testing, QA etc... It's not always like baking a cake, sometimes a single change to the requirements can be like changing the whole recipe. I've witnessed project managers melt down and have temper tantrums on conference calls because their deadlines wont permit the extra development, yet they're asking for development which was not in their initial project requirements. 

I can only give my own personal bias and experience as an example so please don't infer that I'm speaking in behalf of all developers. I only see these things through the microcosm of my own career but this post describes exact conditions that caused me to throw in the proverbial towel.
AnswerKenntnis:
How often are you talking to your developers? I don't mean project status meetings, questions about deliverables or other topics that you bring to them - how often do you sit down with one of your programmers, ask them how things are going, and just listen.

A lot of the other answers are really good - you should look into agile development. You need your developers to learn and grown in their roles. But if you're not actually listening to what your developers are (or are not!) saying, then you need to take care of that first.

Good reference on one-on-ones - http://www.randsinrepose.com/archives/2010/09/22/the_update_the_vent_and_the_disaster.html
AnswerKenntnis:
Short and sweet. Excel at what you do - this will engender communication.

What does excelling mean to your developers ? .. Read, re-read, yes even study PeopleWare
AnswerKenntnis:
All great ideas and comments in the posts above!

Here's an idea: send your I.T. staff to communication workshops at your local community college - paid by the company, of course.

Make sure a) the workshops have a good reputation, and b) not to send your employees together. They will tend to stick together and not mingle with the other class members, not only reducing the value of the workshops, but causing disruption to the others.

Productive team oriented communication is a skill that anyone can learn, but is a subject matter I feel is lacking in most scholastic paths. 

This idea is in no way a magic bullet, but it is a good foundation piece of the puzzle. Not only will your associates learn to communicate better with each other, but the bonus will be, that they will start to understand and respect YOUR work better, too (communication being at the core of PM).

Just my 2 bits :)
AnswerKenntnis:
Just to make an answer out of a recommendation that has been coming up in a few answers already. Michael Lopp  (a.k.a. rands) has been writing about managing developers and "getting in their heads" on his blog, Rands in Repose, and in a book, Managing Humans (book sources). The book contains mostly edited content from his posts prior to 2007, and provides a good way to catch up with the management-related parts of his blog (he also talks about e.g. gambling, and whether you want to read that is another matter). His writing is generally great & often humorous, so there is little risk in reading him.
AnswerKenntnis:
Take the team out for beer (and you're buying).
AnswerKenntnis:
I'm late to the party, but just saw this question.

One thing I don't see very well addressed is this:

Grunts don't ever tell the whole truth to the suits. Rands says this in DNA but doesn't address it head-on, he's on a different topic.

You are wearing a suit, and you sign the checks. You represent the interest of the company. You are not representing the engineers. If you did, you'd not be signing their checks, they'd be signing yours. 

This is of course not news to you or the engineers. But when an engineer knows that bring up certain issues - problems - with his workplace would cause significant conflict  - the risk/reward tradeoff does not favor the engineer. Engineers are paid to produce product, not kick off workplace culture fights. Getting involved in those is a fast track to doing the job wrong.

So part of the management task is to provide a way for the engineers to be open about problems, without incurring corporate politics & career backlash. It's nice to have raises, after all, and there are other companies, if this one doesn't feel congenial.
AnswerKenntnis:
I'm surprised noone has mentioned this great book which is dealing exactly with your question and subject - "Peopleware: Productive Projects and Teams" by DeMarco and Lister. From the editorial : the major issues of software development are human, not technical . The first three reviews on amazon would easily be enough to convince me to buy this book if I was in your situation.
AnswerKenntnis:
A lot of the answers here have very good points, but I would just like to throw in a couple of resources that might help. I've been in a few situations that either crumbled in on themselves into a giant mess or were solved very efficiently because of communication between the people involved. Three books have helped my personally improve my communication skills, especially in high-stress situations where there is a lot on the line:


Difficult Conversations: How to Discuss What Matters Most
Crucial Conversations: Tools for Talking When Stakes Are High
Crucial Confrontations: Tools for Resolving broken promises, violated expectations, and bad behavior


From reading your question, I think you see the value of communication. I personally feel that communication is more important to a manager or leader than business or technical skills. The people that you are leading should have the hard skills that you need to get the majority of the project done. A good technical leader or project manager should be able to focus on communication, whether it is within the team or between the team and the customer or the team and the business entity (or even a combination of those three).
AnswerKenntnis:
Read a couple great novels which give insight into the perspectives of the technical workforce:


Hackers, Steven Levy - http://en.wikipedia.org/wiki/Steven_Levy
Microserfs, Douglas Coupland - http://en.wikipedia.org/wiki/Douglas_Coupland
Cathedral and the Bazaar, Eric Raymond - http://en.wikipedia.org/wiki/Cathedral_and_the_Bazaar


These are as important as any typical memoir on management (Drucker et al).
AnswerKenntnis:
I have done various roles over many many years - developer, senior developer, technical lead etc.

From your question - it's quite obvious that your developers don't tell you stuff because they don't think you can help.

This may be because of 2 reasons.


They don't think you have the power to fix things. However, I think this is unlikely because then you probably would know it & also the developers would have whined about it to you.
You are the kind of person who when a developer comes to you with a problem, does one or more of the following things

When they come to you with problems, you tell them - I like solutions not problems.
You listen to them nicely & then task them with fixing the problem. You give them pep talk about what an honour it is for them be given the responsibility to fix the problem. Over time your guys understand that when they go to you with a problem, they will end up with extra work, so they don't come to you with problems.
You deny it's a problem. You give convincing reasons for this. But as this keeps happening, over time your guys learn that there is no point approaching you with a problem.
You say "Yes, I understand". You say this kind of stuff happens occasionally & there is nothing one can do. If this is a pattern, then again your guys understand it.



If it's any or all of the above, then you need to rectify it.
AnswerKenntnis:
The thing I hate most is people getting in-between me, the developer, and the end user. The best managers let me do this and change our solution to match what I think the user wants or could do with.

The worst practice to me is often dress up as "good" - usually the manager has themselves, or a BA or someone write specs which the develoeprs have to interpret and implement, with timescales agreed beforehand. 

If its a customised solution often even the developers wont know how long it will take and usually the customer hasn't a clue what is best for them. That is why itereative development is great. Isn't the way most deals are done though so a good manager will fight to work as above.

Finally some developers aren't good at communication and can't relate to customers. They are perhaps best suited to problems with clear requirements, especially clear technical requirements. Perhaps you need developers which are better communicators and want to work to solve busieness problems not pure technical ones.
AnswerKenntnis:
It is very easy to keep the team happy

Try to listen to them many time their question has answers also in it. i would encourage team member to come with problems and the probable solution.

Team outing is a good idea (might be some game plan)

if your project require some late nights and weekend work and you think that you actually dont add a lot of value to the team still it would be a good idea to come some time with something to eat and appreciate the team about their efforts and if possible arrange some PTO forthem 

do a bimonthly 1:1 with every team member to make sure that they are comfortable.

Last but not least it might be a good idea for you to understand the project functionally and somewhat technically as well.

Please let me know if you have more question
AnswerKenntnis:
I'm also (french so forgive my english) software manager who have a scientific engineering background but not specifically IT software background originally. So I have no special affinity with coding at the begining but I've been a statistical quality engineer from the school of Deming which has a huge different teaching to every "modern" schools that followed though they pretend to inherit from Deming. The worst is 6 sigma, lean was better but unfortunately what happened is this http://leanandkanban.wordpress.com/2011/05/13/what-did-deming-really-say:

ΓÇ£Originally Six Sigma was derived from Toyota Quality Management (TQM) by Motorola to achieve six sigma levels of quality, and then through Allied Signal and GE it morphed to projects by Black Belts based on statistics to become a cost-reduction programΓÇöevery project needs a clear ROI. In other words, we denigrated the program from a leadership philosophy to a bunch of one-off projects to cut costs. It was a complete bastardization of the original, and it rarely led to lasting, sustainable change because the leadership and culture were missing.

ΓÇ£A similar thing happened to lean when it got reduced to a toolkit (e.g., value-stream mapping, KPI boards, cells, kanban).

ΓÇ£Lean and Six Sigma in no way reflect the original thinking of excellent Japanese companies or their teachers like Deming.ΓÇ¥

Today the Agile movement is like lean (see Jeff Sutherland course and his honoring of Deming http://blogs.forbes.com/stevedenning/2011/05/27/jeff-sutherland-the-21st-century-will-be-the-century-of-scrum/), it's better than Waterfall but it's still very far from the original teaching of Deming because instead of reading Deming in his original text, gurus just re-packages him approximately never quoting his whole 14 principles of Management, and sells tools and seminars that have little value by themselves. 

Now when it comes to software field, problems are there are people on one hand who know the general principles but have no real ideas how to really apply them and on the other hand people who are writing softwares but ignore the principles because they just listen to fake gurus who sold them tools without telling them the real principles and they should in fact build their own management tools.

So for me the software project manager should make effort to go deeper into day to day operationality of software coding, not just doing planning in Microsoft Project (or  burdown chart with Agile) or nice presentation in Powerpoint to upper management but have some values also for the developer team. When the developer team have problems even if it is technical problems, the project manager can help to orient the diagnostic with an external eyes. He can look at code, even if he doesn't understand the tiny detail, he can ask some naive questions which may trigger developers to realize they didn't think about that clue (I have numerous personal examples but it's too lengthy so I will rather write an article on my blog).

An other stuff is that I try to have a general awareness of the evolution in the field like new frameworks, new architecture paradigms by reading technical articles. I will participate in some integration tests, write some documentations myself (stuffs programmers hate so I do for them, they would of course feed me with the core), anything I am able to do practically for helping the team.

In general developers feel like they're doing the hard work, and it is true, often I do tell them that I'm doing the easy part by staying in abstraction, nevertheless I do try to help concretely when needed - because micro-management is not good either as it can generate interference feelings.

As conclusion: eliminate slogans with developers (that's in fact one of the 14 principles of Deming), show them you do care about the project concrete software, not about documents or your meeting with upper management only.
QuestionKenntnis:
Is the use of "utf8=Γ£ô" preferable to "utf8=true"?
qn_description:
I have recently seen a few URIs containing the query parameter "utf8=Γ£ô". My first impression (after thinking "mmm, looks cool") was that this could be used to detect a broken character encoding. 

So, is this a better way to resolve potential problems with character encoding, or is it just a developer having fun with a hack?
AnswersKenntnis
AnswerKenntnis:
By default, older versions of IE (<=8) will submit form data in Latin-1 encoding if possible. By including a character that can't be expressed in Latin-1, IE is forced to use UTF-8 encoding for its form submissions, which simplifies various backend processes, for example database persistence.

If the parameter was instead utf8=true then this wouldn't trigger the UTF-8 encoding in these browsers.
AnswerKenntnis:
Another approach is to specify the encoding in the head: <meta charset="UTF-8">. And use JavaScript's encodeURI(), and PHP's urlencode() before sending the get/post data.
QuestionKenntnis:
What is the Mars Curiosity Rover's software built in?
qn_description:
The Mars Curiosity rover has landed successfully, and one of the promo videos "7 minutes of terror" brags about there being 500,000 lines of code. It's a complicated problem, no doubt. But that is a lot of code, surely there was a pretty big programming effort behind it. Does anyone know anything about this project? I can only imagine it's some kind of embedded C.
AnswersKenntnis
AnswerKenntnis:
It's running 2.5 million lines of C on a RAD750 processor manufactured by BAE. The JPL has a bit more information but I do suspect many of the details are not publicized. It does appear that the testing scripts were written in Python. 

The underlying operating system is Wind River's VxWorks RTOS. The RTOS in question can be programmed in C, C++, Ada or Java. However, only C and C++ are standard to the OS, Ada and Java are supported by extensions. Wind River supplies a tremendous amount of detail as to the hows and whys of VxWorks.

The underlying chipset is almost absurdly robust. Its specs may not seem like much at first but it is allowed to have one and only one "bluescreen" every 15 years. Bear in mind, this is under bombardment from radiation that would kill a human many times over. In space, robustness wins out over speed. Of course, robustness like that comes at a cost. In this case, it's a cool $200,000 to $500,000.

An Erlang programmer talks about the features of the computers and codebase on Curiosity.
AnswerKenntnis:
The code is based on that of MER (Spirit and Opportunity), which were based off of their first lander, MPF (Sojourner). It's 3.5 million lines of C (much of it autogenerated), running on a RA50 processor manufactured by BAE and the VxWorks Operating system. Over a million lines were hand coded.

The code is implemented as 150 separate modules, each performing a different function. Highly coupled modules are organized into Components that abstract the modules they contain, and "specify either a specific function, activity, or behavior." These components are futher organized into layers, and there are "no more than 10 top-level components."

Source: Keynote talk by Benjamin Cichy at 2010 Workshop on Spacecraft Flight Software (FSW-10), slides, audio, and video (starts with mission overview, architecture discussion at slide 80). 



Someone on Hacker News asked "Not sure what means that most of the C code is auto generated. From what?"

I'm not 100% sure, although there probably is a separate presentation in that year or a different year that describes their auto-generation process. I know that it was a popular topic in general at the FSW-11 conference.

Simulink is a possibility. It's a MATLAB component popular among mechanical engineers, and therefore most navigation & control engineers, and allows them to 'code' and simulate things without thinking they're coding.

Model-based programming is definitely a thing that the industry is slowly becoming aware of, but I don't know how well it's catching on at JPL or if they would have chosen to use it when the project started.

The third and most likely possibility is for the communication code. With all space systems, you need to send commands to the flight software from the ground software, and receive telemetry from the flight software and process it with the ground software. Each command/telemetry packet is a heterogeneous data structure, and is is necessary that both sides are working from the exact same packet definition, and format the packet so it is correctly formatted on the one side, and parsed on the other side. This involves getting a whole lot of things right, including data type, size, and endianness (although the latter is usually a global thing, you could have multiple processors onboard with different endianness).

But that's just the surface. You need lots of repetitive code on both sides to handle things like logging, command/telemetry validation, limit checking, and error handling. And then you can do more sophisticated things. Say you have a command to set a hardware register value, and that value is sent back in telemetry in a particular packet. You could generate ground software that monitors that telemetry point to ensure that when this register value is set, eventually the telemetry changes to reflect the change. And of course, some telemetry points are more important than others (e.g. main bus current), and are designated to come down in multiple packets, which involves extra copying on the flight side and data de-duplication on the ground side.

With all that, it's much easier (in my opinion) to write one collection of static text files (in XML, csv, or some DSL/what-have-you), run them through a perl/python script, and presto! Code!

I do not work at JPL, so I cannot provide any detail that is not in the video, with one exception. I've heard that the autogenerated C code is written by Python scripts, and the amount of autocoding in a project varies greatly depending on who the FSW lead is.
AnswerKenntnis:
I'm putting these up for references:


Mars Code | JPL Laboratory for Reliable Software
The Evolution of the Mars Science Laboratory Flight Software 
Curiosity's FSW Architecture: A Platform for Mobility and Science , Slides
Monitoring the Execution of Space Craft Flight Software 
UML State Chart Autocoding for the Mars Science Laboratory (MSL) Mission, Slides
Process Improvement for Defect Removal for MSL , Slides
Mars Science Laboratory Flight Computer Needs


MSL also seems to pack the Electra Software Defined Radio, which runs RTEMS
QuestionKenntnis:
Why do game developers prefer Windows?
qn_description:
Is it that DirectX is easier or better than OpenGL, even if OpenGL is cross-platform? Why do  we not see real powerful games for Linux like there are for Windows?
AnswersKenntnis
AnswerKenntnis:
Many of the answers here are really, really good. But the OpenGL and Direct3D (D3D) issue should probably be addressed. And that requires... a history lesson.

And before we begin, I know far more about OpenGL than I do about Direct3D. I've never written a line of D3D code in my life, and I've written tutorials on OpenGL. So what I'm about to say isn't a question of bias. It is simply a matter of history.

Birth of Conflict

One day, sometime in the early 90's, Microsoft looked around. They saw the SNES and Sega Genesis being awesome, running lots of action games and such. And they saw DOS. Developers coded DOS games like console games: direct to the metal. Unlike consoles however, where a developer who made an SNES game knew what hardware the user would have, DOS developers had to write for multiple possible configurations. And this is rather harder than it sounds.

And Microsoft had a bigger problem: Windows. See, Windows wanted to own the hardware, unlike DOS which pretty much let developers do whatever. Owning the hardware is necessary in order to have cooperation between applications. Cooperation is exactly what game developers hate because it takes up precious hardware resources they could be using to be awesome.

In order to promote game development on Windows, Microsoft needed a uniform API that was low-level, ran on Windows without being slowed down by it, and most of all cross-hardware. A single API for all graphics, sound, and input hardware.

Thus, DirectX was born.

3D accelerators were born a few months later. And Microsoft ran into a spot of trouble. See, DirectDraw, the graphics component of DirectX, only dealt with 2D graphics: allocating graphics memory and doing bit-blits between different allocated sections of memory.

So Microsoft purchased a bit of middleware and fashioned it into Direct3D Version 3. It was universally reviled. And with good reason; looking at D3D v3 code is like staring into the Ark of the Covenant.

Old John Carmack at Id Software took one look at that trash and said, "Screw that!" and decided to write towards another API: OpenGL.

See, another part of the many-headed-beast that is Microsoft had been busy working with SGI on an OpenGL implementation for Windows. The idea here was to court developers of typical GL applications: workstation apps. CAD tools, modelling, that sort of thing. Games were the farthest thing on their mind. This was primarily a Windows NT thing, but Microsoft decided to add it to Win95 too.

As a way to entice workstation developers to Windows, Microsoft decided to try to bribe them with access to these newfangled 3D graphics cards. Microsoft implemented the Installable Client Driver protocol: a graphics card maker could override Microsoft's software OpenGL implementation with a hardware-based one. Code could automatically just use a hardware OpenGL implementation if one was available.

In the early days, consumer-level videocards did not have support for OpenGL though. That didn't stop Carmack from just porting Quake to OpenGL (GLQuake) on his SGI workstation. As we can read from the GLQuake readme:


  Theoretically, glquake will run on any compliant OpenGL that supports the
   texture objects extensions, but unless it is very powerfull hardware that
   accelerates everything needed, the game play will not be acceptable.  If it
   has to go through any software emulation paths, the performance will likely
   by well under one frame per second.
  
  At this time (march ΓÇÖ97), the only standard opengl hardware that can play
   glquake reasonably is an intergraph realizm, which is a VERY expensive card.
   3dlabs has been improving their performance significantly, but with the
   available drivers it still isnΓÇÖt good enough to play.  Some of the current
   3dlabs drivers for glint and permedia boards can also crash NT when exiting
   from a full screen run, so I donΓÇÖt recommend running glquake on 3dlabs
   hardware.
  
  3dfx has provided an opengl32.dll that implements everything glquake needs,
   but it is not a full opengl implementation.  Other opengl applications are
   very unlikely to work with it, so consider it basically a ΓÇ£glquake driverΓÇ¥.


This was the birth of the miniGL drivers. These evolved into full OpenGL implementations eventually, as hardware became powerful enough to implement most OpenGL functionality in hardware. nVidia was the first to offer a full OpenGL implementation. Many other vendors struggled, which is one reason why developers preferred Direct3D: they were compatible on a wider range of hardware.
Eventually only nVidia and ATI (now AMD) remained, and both had a good OpenGL implementation.

OpenGL Ascendant

Thus the stage is set: Direct3D vs. OpenGL. It's really an amazing story, considering how bad D3D v3 was.

The OpenGL Architectural Review Board (ARB) is the organization responsible for maintaining OpenGL. They issue a number of extensions, maintain the extension repository, and create new versions of the API. The ARB is a committee made of many of the graphics industry players, as well as some OS makers. Apple and Microsoft have at various times been a member of the ARB.

3Dfx comes out with the Voodoo2. This is the first hardware that can do multitexturing, which is something that OpenGL couldn't do before. While 3Dfx was strongly against OpenGL, NVIDIA, makers of the next multitexturing graphics chip (the TNT1), loved it. So the ARB issued an extension: GL_ARB_multitexture, which would allow access to multitexturing.

Meanwhile, Direct3D v5 comes out. Now, D3D has become an actual API, rather than something a cat might vomit up. The problem? No multitexturing.

Oops.

Now, that one wouldn't hurt nearly as much as it should have, because people didn't use multitexturing much. Not directly. Multitexturing hurt performance quite a bit, and in many cases it wasn't worth it compared to multi-passing. And of course, game developers love to ensure that their games works on older hardware, which didn't have multitexturing, so many games shipped without it.

D3D was thus given a reprieve.

Time passes and NVIDIA deploys the GeForce 256 (not GeForce GT-250; the very first GeForce), pretty much ending competition in graphics cards for the next two years. The main selling point is the ability to do vertex transform and lighting (T&L) in hardware. Not only that, NVIDIA loved OpenGL so much that their T&L engine effectively was OpenGL. Almost literally; as I understand, some of their registers actually took OpenGL enumerators directly as values.

Direct3D v6 comes out. Multitexture at last but... no hardware T&L. OpenGL had always had a T&L pipeline, even though before the 256 it was implemented in software. So it was very easy for NVIDIA to just convert their software implementation to a hardware solution. It wouldn't be until D3D v7 until D3D finally had hardware T&L support.

Dawn of Shaders, Twilight of OpenGL

Then, GeForce 3 came out. And a lot of things happened at the same time.

Microsoft had decided that they weren't going to be late again. So instead of looking at what NVIDIA was doing and then copying it after the fact, they took the astonishing position of going to them and talking to them. And then they fell in love and had a little console together.

A messy divorce ensued later. But that's for another time.

What this meant for the PC was that GeForce 3 came out simultaneously with D3D v8. And it's not hard to see how GeForce 3 influenced D3D 8's shaders. The pixel shaders of Shader Model 1.0 were extremely specific to NVIDIA's hardware. There was no attempt made whatsoever at abstracting NVIDIA's hardware; SM 1.0 was just whatever the GeForce 3 did.

When ATI started to jump into the performance graphics card race with the Radeon 8500, there was a problem. The 8500's pixel processing pipeline was more powerful than NVIDIA's stuff. So Microsoft issued Shader Model 1.1, which basically was "Whatever the 8500 does."

That may sound like a failure on D3D's part. But failure and success are matters of degrees. And epic failure was happening in OpenGL-land.

NVIDIA loved OpenGL, so when GeForce 3 hit, they released a slew of OpenGL extensions. Proprietary OpenGL extensions: NVIDIA-only. Naturally, when the 8500 showed up, it couldn't use any of them.

See, at least in D3D 8 land, you could run your SM 1.0 shaders on ATI hardware. Sure, you had to write new shaders to take advantage of the 8500's coolness, but at least your code worked.

In order to have shaders of any kind on Radeon 8500 in OpenGL, ATI had to write a number of OpenGL extensions. Proprietary OpenGL extensions: ATI-only. So you needed an NVIDIA codepath and an ATI codepath, just to have shaders at all.

Now, you might ask, "Where was the OpenGL ARB, whose job it was to keep OpenGL current?" Where many committees often end up: off being stupid.

See, I mentioned ARB_multitexture above because it factors deeply into all of this. The ARB seemed (from an outsider's perspective) to want to avoid the idea of shaders altogether. They figured that if they slapped enough configurability onto the fixed-function pipeline, they could equal the ability of a shader pipeline.

So the ARB released extension after extension. Every extension with the words "texture_env" in it was yet another attempt to patch this aging design. Check the registry: between ARB and EXT extensions, there were eight of these extensions made. Many were promoted to OpenGL core versions.

Microsoft was a part of the ARB at this time; they left around the time D3D 9 hit. So it is entirely possible that they were working to sabotage OpenGL in some way. I personally doubt this theory for two reasons. One, they would have had to get help from other ARB members to do that, since each member only gets one vote. And most importantly two, the ARB didn't need Microsoft's help to screw things up. We'll see further evidence of that.

Eventually the ARB, likely under threat from both ATI and NVIDIA (both active members) eventually pulled their head out long enough to provide actual assembly-style shaders.

Want something even stupider?

Hardware T&L. Something OpenGL had first. Well, it's interesting. To get the maximum possible performance from hardware T&L, you need to store your vertex data on the GPU. After all, it's the GPU that actually wants to use your vertex data.

In D3D v7, Microsoft introduced the concept of Vertex Buffers. These are allocated swaths of GPU memory for storing vertex data.

Want to know when OpenGL got their equivalent of this? Oh, NVIDIA, being a lover of all things OpenGL (so long as they are proprietary NVIDIA extensions), released the vertex array range extension when the GeForce 256 first hit. But when did the ARB decide to provide similar functionality?

Two years later. This was after they approved vertex and fragment shaders (pixel in D3D language). That's how long it took the ARB to develop a cross-platform solution for storing vertex data in GPU memory. Again, something that hardware T&L needs to achieve maximum performance.

One Language to Ruin Them All

So, the OpenGL development environment was fractured for a time. No cross-hardware shaders, no cross-hardware GPU vertex storage, while D3D users enjoyed both. Could it get worse?

You... you could say that. Enter 3D Labs.

Who are they, you might ask? They are a defunct company whom I consider to be the true killers of OpenGL. Sure, the ARB's general ineptness made OpenGL vulnerable when it should have been owning D3D. But 3D Labs is perhaps the single biggest reason to my mind for OpenGL's current market state. What could they have possibly done to cause that?

They designed the OpenGL Shading Language.

See, 3D Labs was a dying company. Their expensive GPUs were being marginalized by NVIDIA's increasing pressure on the workstation market. And unlike NVIDIA, 3D Labs did not have any presence in the mainstream market; if NVIDIA won, they died.

Which they did.

So, in a bid to remain relevant in a world that didn't want their products, 3D Labs showed up to a Game Developer Conference wielding presentations for something they called "OpenGL 2.0". This would be a complete, from-scratch rewrite of the OpenGL API. And that makes sense; there was a lot of cruft in OpenGL's API at the time (note: that cruft still exists). Just look at how texture loading and binding work; it's semi-arcane.

Part of their proposal was a shading language. Naturally. However, unlike the current cross-platform ARB extensions, their shading language was "high-level" (C is high-level for a shading language. Yes, really).

Now, Microsoft was working on their own high-level shading language. Which they, in all of Microsoft's collective imagination, called... the High Level Shading Language (HLSL). But their was a fundamentally different approach to the languages.

The biggest issue with 3D Labs's shader language was that it was built-in. See, HLSL was a language Microsoft defined. They released a compiler for it, and it generated Shader Model 2.0 (or later shader models) assembly code, which you would feed into D3D. In the D3D v9 days, HLSL was never touched by D3D directly. It was a nice abstraction, but it was purely optional. And a developer always had the opportunity to go behind the compiler and tweak the output for maximum performance.

The 3D Labs language had none of that. You gave the driver the C-like language, and it produced a shader. End of story. Not an assembly shader, not something you feed something else. The actual OpenGL object representing a shader.

What this meant is that OpenGL users were open to the vagaries of developers who were just getting the hang of compiling assembly-like languages. Compiler bugs ran rampant in the newly christened OpenGL Shading Language (GLSL). What's worse, if you managed to get a shader to compile on multiple platforms correctly (no mean feat), you were still subjected to the optimizers of the day. Which were not as optimal as they could be.

While that was the biggest flaw in GLSL, it wasn't the only flaw. By far.

In D3D, and in the older assembly languages in OpenGL, you could mix and match vertex and fragment (pixel) shaders. So long as they communicated with the same interface, you could use any vertex shader with any compatible fragment shader. And there were even levels of incompatibility they could accept; a vertex shader could write an output that the fragment shader didn't read. And so forth.

GLSL didn't have any of that. Vertex and fragment shaders were fused together into what 3D Labs called a "program object". So if you wanted to share vertex and fragment programs, you had to build multiple program objects. And this caused the second biggest problem.

See, 3D Labs thought they were being clever. They based GLSL's compilation model on C/C++. You take a .c or .cpp and compile it into an object file. Then you take one or more object files and link them into a program. So that's how GLSL compiles: you compile your shader (vertex or fragment) into a shader object. Then you put those shader objects in a program object, and link them together to form your actual program.

While this did allow potential cool ideas like having "library" shaders that contained extra code that the main shaders could call, what it meant in practice was that shaders were compiled twice. Once in the compilation stage and once in the linking stage. NVIDIA's compiler in particular was known for basically running the compile twice. It didn't generate some kind of object code intermediary; it just compiled it once and threw away the answer, then compiled it again at link time.

So even if you want to link your vertex shader to two different fragment shaders, you have to do a lot more compiling than in D3D. Especially since the compiling of a C-like language was all done offline, not at the beginning of the program's execution.

There were other issues with GLSL. Perhaps it seems wrong to lay the blame on 3D Labs, since the ARB did eventually approve and incorporate the language (but nothing else of their "OpenGL 2.0" initiative). But it was their idea.

And here's the really sad part: 3D Labs was right (mostly). GLSL is not a vector-based shading language the way HLSL was at the time. This was because 3D Labs's hardware was scalar hardware (similar to modern NVIDIA hardware), but they were ultimately right in the direction many hardware makers went with their hardware.

They were right to go with a compile-online model for a "high-level" language. D3D even switched to that eventually.

The problem was that 3D Labs were right at the wrong time. And in trying to summon the future too early, in trying to be future-proof, they cast aside the present. It sounds similar to how OpenGL always had the possibility for T&L functionality. Except that OpenGL's T&L pipeline was still useful before hardware T&L, while GLSL was a liability before the world caught up to it.

GLSL is a good language now. But for the time? It was horrible. And OpenGL suffered for it.

Falling Towards Apotheosis

While I maintain that 3D Labs struck the fatal blow, it was the ARB itself who would drive the last nail in the coffin.

This is a story you may have heard of. By the time of OpenGL 2.1, OpenGL was running into a problem. It had a lot of legacy cruft. The API wasn't easy to use anymore. There were 5 ways to do things, and no idea which was the fastest. You could "learn" OpenGL with simple tutorials, but you didn't really learn the OpenGL API that gave you real performance and graphical power.

So the ARB decided to attempt another re-invention of OpenGL. This was similar to 3D Labs's "OpenGL 2.0", but better because the ARB was behind it. They called it "Longs Peak."

What is so bad about taking some time to improve the API? This was bad because Microsoft had left themselves vulnerable. See, this was at the time of the Vista switchover.

With Vista, Microsoft decided to institute some much-needed changes in display drivers. They forced drivers to submit to the OS for graphics memory virtualization and various other things.

While one can debate the merits of this or whether it was actually possible, the fact remains this: Microsoft deemed D3D 10 to be Vista (and above) only. Even if you had hardware that was capable of D3D 10, you couldn't run D3D 10 applications without also running Vista.

You might also remember that Vista... um, let's just say that it didn't work out well. So you had an underperforming OS, a new API that only ran on that OS, and a fresh generation of hardware that needed that API and OS to do anything more than be faster than the previous generation.

However, developers could access D3D 10-class features via OpenGL. Well, they could if the ARB hadn't been busy working on Longs Peak.

Basically, the ARB spent a good year and a half to two years worth of work to make the API better. By the time OpenGL 3.0 actually came out, Vista adoption was up, Win7 was around the corner to put Vista behind them, and most game developers didn't care about D3D-10 class features anyway. After all, D3D 10 hardware ran D3D 9 applications just fine. And with the rise of PC-to-console ports (or PC developers jumping ship to console development. Take your pick), developers didn't need D3D 10 class features.

Now, if developers had access to those features earlier via OpenGL on WinXP machines, then OpenGL development might have received a much-needed shot in the arm. But the ARB missed their opportunity. And do you want to know the worst part?

Despite spending two precious years attempting to rebuild the API from scratch... they still failed and just reverted back to the status quo (except for a deprecation mechanism).

So not only did the ARB miss a crucial window of opportunity, they didn't even get done the task that made them miss that chance. Pretty much epic fail all around.

And that's the tale of OpenGL vs. Direct3D. A tale of missed opportunities, gross stupidity, willful blindness, and simple foolishness.
AnswerKenntnis:
I found it strange that everybody's focusing on user base, when the question is 'game developers', not 'game editors'.

For me, as a developer, Linux is a bloody mess. There are so many versions, desktop managers, UI kits, etc... If I don't want to distribute my work as open source, where the user can (try to) recompile so it fits his unique combination of packages, libraries and settings, it's a nightmare!

On the other hand, Microsoft is providing (most of the time) incredible backward compatibility and platform stability. It is possible to target whole range of machines with one closed-source installer, for instance computers running Windows XP, Vista and 7, 32 and 64 bits flavors, without proper DX or VC redistributables installed, etc...

One last thing, PLEASE EVERYBODY ON THE INTERNET STOP COMPARING OPENGL AND DIRECTX! Either compare Direct3D vs OpenGL or don't do this. DirectX provides input support, sound support, movie playing, etc etc that OpenGL doesn't.
AnswerKenntnis:
It's because there are more Windows users on the planet than Linux and Mac. The truth is that people make things for whichever has the biggest market.
The same goes with mobile phones: Android and iPhone have awesome games but Windows Mobile and Symbian don't...
AnswerKenntnis:
Because Windows has over 90% market share, and Linux (since you specifically asked about Linux) has a reputation for having lots of users who don't like to pay for software. Whether or not that's true or how true it is is irrelevant; the perception is there and it influences people's decisions.
AnswerKenntnis:
Because Windows is backed by a huge organization, that more than a decade ago decided they want game development to happen on their platform.

This wasn't true for the Mac, and it isn't true now. Not even for iOS. Apple doesn't provide tools for iOS game development. But it's a huge market (there's more iPhones out there, than there was PCs in 1995) with relatively little competition, so people do it anyhow.

As for Linux, there's not even some sort of central institution, that could set any sort of priorities. The direction in which Linux is going, is more less determined by a bunch of very good, yet slightly unworldly programmers.  

To create a PC game today, you need a lot of 2d/3d artists, game designers, scriptors, actors, testers and what not. As to the actual programming, you might simply use an actual game engine (CryEngine, Unreal Engine, Quake Engine, Source Engine). So you might be able to do the whole thing without any actual programmers.

Because of that, and because of the nature of businesses, programmers have little say in which platform is chosen. And typically, managers look for support, which is something Microsoft claims to offer, and to deal with things that are somehow seizable to their thought patters, which open source is not.

For that reason, most commercial end-user software development is done on windows.
I work for a company, that creates flash games, and is thus not bound to a particular platform. However, we all develop on windows, because most of the tools we use aren't available for Linux.
AnswerKenntnis:
As some have already said, the most important part is the user-base. 95% of PC users use Windows. PC gamers use almost exclusively Windows. Even these who use Mac or Linux most often run Windows games through some virtualization or emulation (with very, very few exceptions).

But demographic is not everything. I wouldn't underestimate the part that Microsoft is doing to make the platform more attractive for game developers. Basically you get fully featured set of tools for free, most importantly XNA Game Studio. This allows not only development for Windows, but also for Xbox360. And with the latest edition even for WP7 phones. Obviously since it's Microsoft tool, it uses DirectX, not OpenGL.
AnswerKenntnis:
Ewwww, I don't. I use Linux almost exclusively. I dual-boot to Windows to make Windows builds, and use the Mac for the Mac builds, but that's it.

The trick is a cross-platform framework we've developed over the years. Our games are built on top of that, and behave identically in Linux/OpenGL, Mac/OpenGL, and Windows/Direct3D (and soon in iOS/OpenGL).

Admittedly my company doesn't do AAA titles, so it may not apply to these, but we do make top casual games (see website - CSI:NY, Murder She Wrote and the two upcoming 2011s are examples of titles using important licenses, The Lost Cases of Sherlock Holmes 1 and 2 were quite successful as well)

I wouldn't give up gedit+gcc+gdb+valgrind for anything else.
AnswerKenntnis:
Inertia. If you've used Windows in the past then switching to something else is a hassle. If you're on Windows DirectX is easier and more likely to work well than OpenGL.
Market share. The market share of Windows on the desktop is bigger than that of OS X which in turn is bigger than that of Linux. Money talks.
Brand. DirectX is better known than things like SDL (which is the sort of thing you would need to replicate some of DirectX's features that go beyond OpenGL).
Less confusion. Will the user's Linux only support up to OpenGL 1.4 or OpenGL 2+? Can you use an OpenGL 2.0 tutorial like An intro to modern OpenGL. Chapter 1: The Graphics Pipeline on your version Linux?


These days Linux is more of a curio when it comes to games development and most developers would be better off fiscally doing an OS X port version before a Linux version (see things like Steam). Even then the console market is worth more than these two platform combined for games...

If you wanted to mono platform DirectX is fine. If you want to be cross platform there's a strong chance you will have to go with OpenGL on at least some of the other platforms.
AnswerKenntnis:
The answer is obvious. The objective of writing a game is to make money. More end users run Windows, therefore there is a bigger market and you would expect to make more money from a Windows game than a Linux game. It's that simple.

If ever you ask yourself the question 'Why does someone do...', just remember that money makes the world go round.
AnswerKenntnis:
Tools, tools, tools.

That's what it comes down to.  Develop on Windows and you get access to some of the best development tools on the planet.  Nothing comes even remotely close to Visual Studio's debugger, the DirectX Debug Runtimes are awesome, PIX is awesome, and comparable equivalents just don't exist on other platforms/APIs.  Sure, there is some good stuff there; I'm not saying that the tools on other platforms are bad, but those that MS provide are just so far ahead of the pack (honourable exception: Valgrind) that it's not even funny.

Bottom line is that these tools help you.  They help you get stuff done, they help you be productive, they help you focus on errors in your own code rather than wrestle with an API that never quite behaves as documented.
AnswerKenntnis:
So I've gone over all these answers, and as a game developer who has code on console games that have been on Walmart shelves, I have a very different answer.

Distribution.

See, if you want to be on a Nintendo console, you have to get Nintendo's permission, buy from Nintendo's factories, pay Nintendo's overheads, negotiate with Walmart, deal with warehousing, you need money up front to manufacture, to print boxes, to ship, to do all the insurance, et cetera.

If you want onto the XBox, sure there's XBLA, but you still need Microsoft's blessing, you have to wait your turn in line, it's tens of thousands of dollars just to release a patch, etc.

On iOS, you still need Apple's okay, and they can (and do) capriciously pull you.

On Steam, you still need Valve's permission or greenlight, and lots of money.

.

On Windows?  You set up a website and a download button.

.

I'm not saying the other platforms aren't valuable.  But there is so *much* horrible stuff going on when you're trying to develop a game, that to me, the promise of being able to just slap a binary on a site and focus on the work - at least to get started - really lowers a lot of potential failure barriers.

"We can do the XBLA port later, when things are stable" mentality.

And to a lesser extent sure this is fine for Linux too, and if seven customers is good, you can start there.  

But Windows has three massive advantages: genuinely open development, genuinely open deployment, and a very large, very active customer base which is interested in quirky stuff.

It's hard to imagine where else I'd rather start.
AnswerKenntnis:
I think you should read more about the History of DirectX and This Article.

I think MS chose DX over openGL because they like to lock people into using their own OS.
AnswerKenntnis:
A lot has to do with politics and control. 
In the late 90s, SGI and MS actually agreed to combine efforts:

http://en.wikipedia.org/wiki/Fahrenheit_graphics_API

SGI invested heavily in the project, MS did not. SGI needed MS more than MS needed SGI. The rest is history.

D3D and OpenGL are two very different APIs, it is up to the developer to choose which is right for your needs.
AnswerKenntnis:
Simply because Linux failed horribly as a desktop system. As somebody pointed out earlier Linux is a mess for developers (different libraries, ui toolkits, etc) 

Another problem is freetardism and the lack of support for proprietary software. Nvidia always provides fine (proprietary) drivers for Linux however Ubuntu and other distros are not shipping it. There is also no binary driver interface available for Linux as for Windows. (There is a text file called binaryApiNonsense.txt or something in the Kernel sources) Having said that only Nvidia hardware is properly supported under Linux. You can play most of ID softwares games using Nvidia hardware under Linux.

Next thing development tools. MSFT provides excellent C++ support and the Visual Studio debugger is better then gdb with regard to C++. Last but not least, further tools are missing such as Photoshop. Also .net allows you to quickly create gui tools. Many game studios code their tools for internal use using the .net framework.

I almost forgot: The graphics system is horribly, back in the days they just ported X11 over because it was the easiest thing which worked. They failed to properly design and implement a modern graphic system which OSx and Win have.
QuestionKenntnis:
I don't program in my spare time. Does that make me a bad developer?
qn_description:
A lot of blogs and advice on the web seem to suggest that in order to become a great developer, doing just your day job is not enough. For example, you should contribute to open source projects in your spare time, write smartphone apps, etc. In fact a lot of this advice seems to suggest that if you don't love programming enough to do it all day long then you're probably in the wrong career.

That doesn't ring true with me. I enjoy my work, but when I come home from the office I'm not in the mood to jump straight back onto the computer and start coding away until bedtime. I only have a certain number of hours free time each day, and I'd rather spend them on other hobbies, seeing friends or going outside than in front of the computer.

I do get a kick out of programming, and do hack around outside of work occasionally. I'm committed to my personal development and spend time reading tech blogs and books as a way to keep learning and becoming better. But that doesn't extend so far as to my wanting to use all my spare time for coding.

Does this mean I'm not a 'true' software developer at heart? Is it possible to become a good software developer without doing extra outside your job? I'd be very interested to hear what you think.

Update: thanks everyone for your comments & answers. A lot of good thoughts and advice!
AnswersKenntnis
AnswerKenntnis:
IMO this attitude comes from people that have horrible, soul sucking jobs, combined with piss-poor time management skills. If you're basically typing web forms all day, go out and get a more challenging job, or start your own. 

Here's the thing. A concert musician (cellist/pianist/whatever), will practice at most 6 hours per day. Most only practice a few hours per day. at the highest levels

People say program more because you learn more, but that's a smokescreen. 8 hours per day is plenty.

Progress is NOT linear. It's logarithmic:



The only reason that a musician might practice longer than 3 hours, is that they need to squeeze out the extra 1% that those hours gives them. If you think that applies to you, re-solving a problem CS solved 2 decades ago, then you have a prima-donna complex to boot. 

I've worked in pressure cooker companies before, and trust me, the actual amount of work that those guys get done isn't any better than a company like 37signals that places constraints on the amount of work: http://37signals.com/svn/posts/996-why-i-love-working-with-family-people

What ends up happening is that sure, you may be in front of a computer for 10-12 hours, and in the office for 2 more, but that doesn't include the 90 minute lunch you took, the 2 hours you spent browsing discussion forums, and the hour break you had to play one of the many games laid out in the office (foosball, pool, yada...).

Look back at that graph. Now back to me. 

Your mind actually has the opportunity to expand much more if you engage it in some other activity: Learn to play an instrument. Learn a foreign language. Better yet get out and get some exercise, and connect with real live people.

On the logarithmic nature of productivity:


  In the renowned 1993 study of young
  violinists, performance researcher
  Anders Ericsson found that the best
  ones all practiced the same way: in
  the morning, in three increments of no
  more than 90 minutes each, with a
  break between each one. Ericcson found
  the same pattern among other
  musicians, athletes, chess players and
  writers.
  
  For Real Productivity, Less is Truly More


This is actually a well-known principle in the business world, I'm surprised more programmers haven't heard of it. 

Update: More on the Ericsson study.

The whole notion of it taking 10,000 hours / 10 years to become proficient actually comes from the studies done by Ericsson, not from Malcom Gladwell. 

As we all know, you can have 1 year of experience repeated 10 times... so just having your ass in the seat for 10 years doesn't qualify. What does qualify is what Ericsson calls deliberate practice. 

He has found this principle to hold true in athletics, music, writing, chess, and mathematics. He further defines deliberate practice as being so effortful, that even at the highest levels you can only put forth about 4 hours per day. Otherwise you will suffer from overtraining or burnout. Again, he recognizes that there are diminishing returns for deliberate practice, up to about 4 hours. 

On the subject of not having a good/challenging job:

Hogwash. Either get a better job, or here's an idea: Make your current job into something it's not, at least right now. 

One of the best programmers I knew walked into a job as a maintenance programmer on a legacy system that consisted of dozens of programs and hundreds of thousands of lines of code. Most of which had been hacked on over the years so much that you would have to say there wasn't any coherent design to it anymore. 

This was pretty much a go-nowhere, dead-end job. Management wanted you to keep your head down, and just fix the damn bugs. The good developers were working on the greenfield project. People either came here to sit out their remaining days until they retired, or gain a few years of experience before going on to new application development. Whereas most programmers would complain about the lack of career development, or the opportunity to learn new things, or not having exciting projects to work on, or more generally just bitching about no one enabling them, this guy simply sat down, and went about doing the work that needed to be done. 

And over the course of 2 years, he had transformed that system from a buggy hell of spaghetti code to something that was a thing of beauty and functioned like a swiss watch. So complete was the transformation, that the VP of the division started paying more & more attention to the existing project, and started questioning the value of the greenfield project. Although he didn't have a title, the operations people went to him as the de-facto leader of the group. When I left, the VP was talking about creating a new role for him as a systems architect... 

I'm not sure what happened to him after that, but he taught me a couple of very important lessons: 


Your job is what you make it, and there's interesting problems to be solved everywhere. If you hate writing CRUD screens, solve the problem by automatically generating them. 
Don't sit around waiting for opportunities to come to you. Chances are they never will.
AnswerKenntnis:
There is more hidden in this message.

Many enthusiastic programmers like to explore things, to experiment, to follow their ideas, that's how we learn and obtain our skills and vision.

Usually you don't get to follow your interests during work hours. You just do things you are told to do and that's it. Only few of us are lucky enough to do at work what we would do personally even if not paid.

Therefore if you're not doing anything extra, you're not developing to your potential. And that is exactly the problem.
AnswerKenntnis:
It doesn't make you a bad developer, but unfortunately, you still have to compete with the ones who do.

Read this, from Seth Godin's Blog:



Unreasonable

It's unreasonable to get out of bed on a snow day, when school has been cancelled, and turn the downtime into six hours of work on an extra credit physics lab.

It's unreasonable to launch a technology product that jumps the development curve by nine months, bringing the next generation out much earlier than more reasonable competitors.

It's unreasonable for a trucking company to answer the phone on the first ring.

It's unreasonable to start a new company without the reassurance venture money can bring.

It's unreasonable to expect a doctor's office to have a pleasant and helpful front desk staff.

It's unreasonable to walk away from a good gig in today's economy, even if you want to do something brave and original.

It's unreasonable for teachers to expect that we can enable disadvantaged inner city kids to do well in high school.

It's unreasonable to treat your colleagues and competitors with respect given the pressure you're under.

It's unreasonable to expect that anyone but a great woman, someone with both drive and advantages, could do anything important in a world where the deck is stacked against ordinary folks.

It's unreasonable to devote years of your life making a product that most people will never appreciate.

Fortunately, the world is filled with unreasonable people. Unfortunately, you need to compete with them.
AnswerKenntnis:
To answer the question posed: Not programming in your spare time does not make you a bad developer, however, programming in your spare time can make you a better developer.

Programming in your spare time certainly won't hurt your skills, but you shouldn't feel obligated to do it. Programming seems to be a relatively unique field because for many people it is both their job and their hobby, so they enjoy programming in their spare time.
AnswerKenntnis:
No it doesn't make you a bad programmer.  Depending on what you do it may make you a better programmer in the long run.  Early in your career it may influence how fast you learn the variety of skills you need.  However, you may be picking up skills and information that will help in the long term.   Getting some exercise wouldn't hurt either.

There is a fair amount of research that shows performance peeks around a 40 work week.  While we can be production for a period of time working longer hours, in the long run we loose efficiency.  The research I have seen shows people working 80 weeks are about as productive as those working 40 hours a week.

There are a few things you can reflect on off (or on) hours that may help you perform better:


What are you doing?  Can you do it better? Do you need to be doing it?
What are you learning?  What do you need to learn?
What problems are you running into?  Who is best to resolve them?  What can you do?
AnswerKenntnis:
Is it possible to become a good
  software developer without doing extra
  outside your job?


Definitely.  

It may take longer than if you were to spend extra hours honing your skill. I've also found it difficult to put enough time into self-improvement if I'm employed full time and do little programming outside of work hours.

When I was younger, I put in much more time into learning than I do now.  The concepts I use day to day are deeply ingrained at this point, and it seems easier to acquire additional knowledge with this experience under my belt.

"Good" programmers seem to put in the extra hours more because they are obsessive by nature, tend towards the anti-social end of the spectrum, and genuinely enjoy programming and the whole solve problems cycle.
AnswerKenntnis:
In the big scheme of things it's all about finding the right balance in life.

What's important is whether you enjoy programming and whether you keep learning no matter what level you're at. Whether or not you program outside work does not automatically make you a "good" or "bad" programmer.

To give a personal perspective, I've been programming for about 25 years (first as a kid, then professionally). I absolutely love it.

However, I almost never program outside work. This has to do with several things:


I am very lucky in that at work I get to do what I love, every day, all day long.
Quite simply, there are other things in life and there are only so many hours in the day.
AnswerKenntnis:
You can become a good programmer just coding during work, especially if you manage your career well.  However, the greatest programmers code during their off-hours as well.  Having said that, a person who picks good jobs with lots of learning opportunities, uses their time at work well, and so forth will be a better programmer than someone who codes at home because he can't find jobs that provide interesting challenges during the work day.  

While coding at home is valuable, I would say that maximizing the value of your work environment (by learning from coworkers, picking good jobs, managing your career aggressively) is often even more valuable.  The greatest programmers do both, plus have great attitudes toward learning and mentoring others.
AnswerKenntnis:
The real issue is whether a new developer is coding enough (on challenging enough problems) to get through a few hundred thousand lines of practice code and become a decent programmer.

This can in theory be done at work, or on your own time, or both. Practice does have to be somewhat self-directed, so you can learn what you need to learn. That means work that's a little flexible.

Lots of times entry-level jobs are a bunch of bug fixing or other work that isn't going to teach you to be good. What you need to do is write a lot of code from scratch, and that code will probably be bad. It's hard for employers to pay for this. That's why people end up doing an open source project on the side or launching a startup or whatever.

When people say you have to program in your spare time, I think it really means that you have to power through years of meaningful practice early on. You have to do whatever it takes to get the practice and learn to write and maintain a large codebase from scratch. If nobody's paying you to write a probably-disastrous series of projects from scratch, then spare time is the only option.

If you already did the practice when younger and became a good programmer, I don't think you're going to lose the baseline skills if you keep it 9-to-5 later on. Later on it's more a matter of keeping up with new technology which isn't so time-consuming.

However, it's a rare entry-level job that would give you the quantity and quality of practice all by itself to become a good programmer in the first place. A new developer needs to take the initiative to learn, not just hang out in an entry-level job doing entry-level tasks.

It would be really tough to become a good programmer from scratch if you already had a family and were heavily emphasizing work-life balance. Just as it'd be really tough to become a doctor or become a virtuoso musician or anything else in that situation. There's a reason people tend to get through their thousands of hours of practice when they're younger rather than when they're older.
AnswerKenntnis:
If I spent time in the next six months developing a basic game in my spare time and you don't, and supposing all other things are equal (they never are), which of us would know more? Which of us would have more skills and experience?

This effect would be cumulative, as earlier off-hours knowledge would feed into new work and new off-hours knowledge.

And that's why people advise to program in their spare time, insofar as I can tell.
AnswerKenntnis:
Between great and bad there are many levels in between. You can't become a great developer by doing it only 8 or so hours a day, possibly working on problems from a very limited field.

If your job is varied enough, you can become "decent" or "accomplished" or whatever adjective you prefer, but to be a great developer you'd have to be a bit mental, I'm afraid.

Another reason for doing hobby programming is that in a job you often have to adhere to suboptimal standards and make compromises in general and even when you do learn a new thing, you might not be allowed to use it.

And yet another, even more prosaic one is that whatever code you write at work is owned by your employer. You can't return to it and learn from your old mistakes years later, when you're working for someone else.
AnswerKenntnis:
It takes 10,000 hours of effort to become an Expert in anything. [Michael Gladwell, "Outliers"]

So, doing extra work before you get to that level will speed you along the path to real Expert status.

Once you are past that level, then spending more time will have small / diminishing returns.
AnswerKenntnis:
You shouldnt look at it as "Does it make me a bad programmer" because like every skill more practise only makes one better.

So if you want to be better than you currently are and learn new technologies and domains that your work doesnt allow, that is where the extra effort and time spent comes in handy. 

But extra time spending coding (just to prove to someone that you code extra) doesnt necessarily mean you become a good programmer unless you learn what you do and do something different or improve upon what you already know, rather than blindly cut/copy/paste and putting together hacks. Putting extra hours and doing the same thing over and over in no way do I think is going to help you improve.
AnswerKenntnis:
During the day your programming is short bets.  There's money to be earned, you know exactly how much and what for, and you don't have a lot more to gain than a paycheck.  

At night you're working on long bets - crazy ideas that will mainly not work at all.  These are the projects that will catapult you out of the rank and file.  This is how you become the inventor of PHP rather than the code monkey using it.

I interview lots of programmers, and whether they have done interesting oddball things outside of working hours is an instant green light.  It's surprisingly rare.  

Though I will say this: there was one time a guy who did a lot of cool stuff outside of working hours turned out to be a total dud of a hire.  I'm still stumped by that guy.
AnswerKenntnis:
Being a good software developer in heart,depends really upon your heart. You can try to be a great musician and put a lot of time on music,but still that doesnt make you a good musician.You need to enjoy what you do in order to be good in that.Initially i was having the same questions you had, but now because of my likes i have started doing the extra things on software other than the normal work.It improved my performance in the normal work and also provied me with lot of time to involve in other activities not related to software. So the base line is enjoy what you do and you will eventually do what is required.
AnswerKenntnis:
The real question is what you are doing all day.  The analogies to athletes and musicians are usually not applied correctly.  Your day job is not practice, it's performance, unless you have the explicit ability to spend part of your work day in genuine "practice". How do you know? If you never have the time to do anything "right" or learn a new language/technology/framework/etc thoroughly, then you aren't practicing, you're performing. Professional athletes and musicians don't get better during the game or the concert, they get better during focused practice. So, if you're like most, the only time you have for genuine coding "practice" is outside work.  That being said, it's only worthwhile if it's focused.  If you hack all day and then hack all night, you just get tired, not better.

For example, you need to optimize a piece of code at work and you throw the usual profiling tools at it and speed it up maybe 50% and move on to the next task but you think there are other things you might do that would help even more but you don't have the time allowed to try them.  If you take that piece of code home with you and spend a few extra hours really wringing it out, you are learning techniques that make you better the next time you optimize something. You aren't doing the same thing at home as at work.

Practice is about adding skills and honing skills and those activities are necessarily done in an artificial way (it's like isolating a muscle group in the gym, nobody does that in normal physical activity). Building apps at home is not automatically practice.  The work at home should focus on the techniques and the process, not the end product. If an end product is what you need to be motivated, great. Just don't fall into the trap of developing it the same way you do at work.

The coding kata movement is an example of what practice is all about.
AnswerKenntnis:
To answer your question: No, it doesn't make you a bad developer.

However, it might make your professional life down the road a bit harder. If you've got a job where you're constantly learning new technologies that will help you further your career path (whether that be promotions at your current job or new jobs at different companies), well then you're set.

But if you're currently working at a company where the technology used rarely changes, then in 2 or 5 o 10 years you might find it hard to get that next job. If you're coding at home with new technologies and have something to show for it (app, website, open source project, etc), then you'll find it much easier to get that next job.

Of course, you might be completely happy doing the exact same thing for the next 40 years, and as long as your company and the technologies continue to exist, then you're set!
AnswerKenntnis:
There have been times in my life where I did a lot of coding outside of work, and times when I did none because I was working on other things. You should always be learning, but it does not mean you always have to learn about programming in your spare time. If you want to work on some other skill, be it carpentry, needlepoint, gardening etc in your spare time it will help you grow as a person. 

Just don't fall into the trap of thinking you know all you need to know about programming so you can stop learning.
AnswerKenntnis:
This is indeed an interesting post. I have been developing software for the pas 10 years and love my job! I too have a lot of hobbies and try to manage them properly. But I do get a kick out of trying new things spending an hour or two in front of the pc before going to bed. In my case, I am addicted to studying and writing game code. I don't always have time playing games, but I love reading code about games. To be honest, i think the best code I've ever seen came from games and I apply these skills to my code at work. I constantly learn EVERY SINGLE DAY, so IMO, if you are a developer, you'll never reach your peak, there is ALWAYS a better way of doing something....
AnswerKenntnis:
This is a great post.  

I would say, no, it doesn't make you a bad programmer.  If your reviews have been good and you complete your tasks well and on time then you are quite the opposite.  In all honesty, I am envious of you that you get your fill of the code while at work.   

The main reason I program in my spare time is that I don't really care for what I do at work and I use my spare time to do what I like.  What I enjoy doing is video game programming.  There isn't a valid games company to work for where I live (I have a family here now) so I help out with indie games on the side when I can.  It's the only way to cure the itch.  If I did what I loved all day then it would be highly likely that I wouldn't do it in my spare time.
AnswerKenntnis:
My advice will be : Use your commute time to learn stuff, to keep you informed : read IT books, listen to Dev podcast, etc.
Regarding your post-work activities : doesn't change anything. Otherwise, you'll become a boring person.
AnswerKenntnis:
It doesn't make you a bad developer, it might not even make you a kind of "ok" developer it just makes you what you are. I think though, that you'll find most people that are really excellent hackers tend to like to try to new things. At any given time I will be investigating many different languages, databases, web frameworks, robotics, etc. I have two little kids and a wife, mortgage, all that stuff - but I look forward to the times that I can get 2 or 3 hours to hack around with stuff. Sure, C# and .NET is all well and good for earning a paycheck and I actually do I enjoy my work, but messing around with Scala and Lift has been a real blast for me these past few months. There are developers who would classify themselves as "passionate" and those that are just "getting by". I tend to find that developers who constantly challenge themselves tend to advance more quickly than those that don't.
AnswerKenntnis:
Does this mean I'm not a 'true' software developer at heart? 


Absolutely not.  It just means that you like doing things other than coding.  There's nothing wrong with that, and it doesn't make you any better or worse than anyone else, developer or not.


  Is it possible to become a good software developer without doing extra outside your job?


Of course.  Some of that depends on what your work environment is like.  If you've got a boring, soul-sucking, job that sucks up all your time and then some doing boring maintenance work, then being able to use work time to develop your skills will be very limited.  If you have the kind of environment where you have some latitude to do what you want to explore and learn, then take advantage of that opportunity to develop new skills that also help your employer or co-workers out.

If you find something interesting that you want to learn more about and don't have the time or possibility to use it at work, then you have to weigh that against the other demands on your time, be they family, friends, or hobbies.
AnswerKenntnis:
I've been in on a number of interviews to hire programmers recently.  I've found that the candidates who most impress me, both as programmers and as people I wouldn't mind working with, are the ones who spend a significant amount of time on NON-programming pursuits (particularly music, but also pure mathematics, philosophy and literature).  To be sure, most also do some hobby programming, but they have balancing interests as well.
AnswerKenntnis:
If have a job that provides you a completely engaging intellectual experience, chances are that's going to be your mental life -- outside work is for blowing off steam. If your job doesn't engage you, you're much more likely to find intellectual satisfaction after work hours. That's true in any business, not just coding.

Coding in an office saps your creativity, period. At least 8 hours per day, your mind is doing a continuous sprint on someone else's behalf. It's normal to come out of that and not want to get back on a computer. When I worked from an office, it was the same for me -- I'd get home and only want to cook, or go see a movie, hang with friends or just zone in front of the TV. Then from 2001-2004 or so, after the company I worked for tanked and I cracked up, I dropped out and drove a taxi and waited tables. That's when I really started coding in my off hours.

The truth is, I don't think anyone has more than 8 hours a day worth of high-quality coding time in them. A lot of coders only have 2-3, but they might be the most efficient people out there. It's just how you use it, how you spread it out.
AnswerKenntnis:
Is it possible to become a good
  software developer without doing extra
  outside your job?


It's depends on the industry you're working in.
Some are more competitive than others.

If you don't like to learn new stuff on your own, you're likely to stay at your current skill level. That's may be enough ... or not.

It is not about the time dedicated, it is about curiosity, and a thirst for knowledge and mastery. Obviously, it has nothing to do about staying up to date, it is about fundamental -often hard- stuff :  compilers, monads, memory architectures, etc. Things that stretch and expand your brain.

Wanting to Code at home is just an important side effects. 
But don't underestimate it, without doing, learning is often shallow. 

All the best dev I know have theses traits.
AnswerKenntnis:
No, it doesn't make you any less of a programmer than anyone else. You're just more social. I'm envious to be honest.

You don't have to use all of your spare time to write code. My job is pretty simple: server management. I write a lot of code because I don't have to do a lot at work. That doesn't make me better or worse than anyone. If you're passionate about programming, why not do some freelance work? I often put up Craigslist ads for my services and make a good amount of money off of it. It's also a great learning experience since you'll be working with startups and sometimes even large websites. Hope this helps. :)
AnswerKenntnis:
I asked a similar question a couple of days a go. It seems that people seem that we are more passionate about our job if we devote not only all our day time to it, but also all time that we have available for our personal lives.

I do occasionally write some code on my spare time, but I do it for fun, not because I intend to gain some professional growth or recognition. I applaud the people who have the energy for it, but you are more like to burn out by working longer hours and on extra projects than to actually become more expert into anything.

At the end of the day, all that matters are your ability to learn new things (especially on the job), your ability to communicate well, and having good time management skills. That you turn you into a good professional.
AnswerKenntnis:
Is it possible to become a good software developer without doing extra outside your job?


For me it's all about balance.

Although I love programing it is just one side of me, I have other interests.
As I see this if I'm happy (doing other stuff I like doing) and I find my work interesting and fulfilling (which I do) than, on the long term, I'm on the way to become a better software developer.

P.S.
I admit I haven't read all prior posts on this page.
QuestionKenntnis:
Should UTF-16 be considered harmful?
qn_description:
I'm going to ask what is probably quite a controversial question: "Should one of the most
popular encodings, UTF-16, be considered harmful?"

Why do I ask this question?

How many programmers are aware of the fact that UTF-16 is actually a variable length encoding? By this I mean that there are code points that, represented as surrogate pairs, take more than one element.

I know; lots of applications, frameworks and APIs use UTF-16, such as Java's String, C#'s String, Win32 APIs, Qt GUI libraries, the ICU Unicode library, etc.  However, with all of that, there are lots of basic bugs in the processing of characters out of BMP (characters that should be encoded using two UTF-16 elements).

For example, try to edit one of these characters:


𝄞 (U+1D11E) MUSICAL SYMBOL G CLEF
𝕥 (U+1D565) MATHEMATICAL DOUBLE-STRUCK SMALL T
𝟶 (U+1D7F6) MATHEMATICAL MONOSPACE DIGIT ZERO
𠂊 (U+2008A) Han Character


You may miss some, depending on what fonts you have installed.  These characters are all outside of the BMP (Basic Multilingual Plane).  If you cannot see these characters, you can also try looking at them in the Unicode Character reference.

For example, try to create file names in Windows that include these characters; try to delete these characters with a "backspace" to see how they behave in different applications that use UTF-16. I did some tests and the results are quite bad:


Opera has problem with editing them (delete required 2 presses on backspace)
Notepad can't deal with them correctly (delete required 2 presses on backspace)
File names editing in Window dialogs in broken (delete required 2 presses on backspace)
All QT3 applications can't deal with them - show two empty squares instead of one symbol.
Python encodes such characters incorrectly when used directly u'X'!=unicode('X','utf-16') on some platforms when X in character outside of BMP.
Python 2.5 unicodedata fails to get properties on such characters when python compiled with UTF-16 Unicode strings.
StackOverflow seems to remove these characters from the text if edited directly in as Unicode characters (these characters are shown using HTML Unicode escapes).
WinForms TextBox may generate invalid string when limited with MaxLength.


It seems that such bugs are extremely easy to find in many applications that use UTF-16.

So... Do you think that UTF-16 should be considered harmful?
AnswersKenntnis
AnswerKenntnis:
This is an old answer.
  See UTF-8 Everywhere for the latest updates.


Opinion: Yes, UTF-16 should be considered harmful. The very reason it exists is because some time ago there used to be a misguided belief that widechar is going to be what UCS-4 now is.

Despite the "anglo-centrism" of UTF-8, it should be considered the only useful encoding for text. One can argue that source codes of programs, web pages and XML files, OS file names and other computer-to-computer text interfaces should never have existed. But when they do, text is not only for human readers.

On the other hand, UTF-8 overhead is a small price to pay while it has significant advantages. Advantages such as compatibility with unaware code that just passes strings with char*. This is a great thing. There're few useful characters which are SHORTER in UTF-16 than they are in UTF-8. 

I believe that all other encodings will die eventually. This involves that MS-Windows, Java, ICU, python stop using it as their favorite. After long research and discussions, the development conventions at my company ban using UTF-16 anywhere except OS API calls, and this despite importance of performance in our applications and the fact that we use Windows. Conversion functions were developed to convert always-assumed-UTF8 std::strings to native UTF-16, which Windows itself does not support properly.

To people who say "use what needed where it is needed", I say: there's a huge advantage to using the same encoding everywhere, and I see no sufficient reason to do otherwise. In particular, I think adding wchar_t to C++ was a mistake, and so are the Unicode additions to C++0x. What must be demanded from STL implementations though is that every std::string or char* parameter would be considered unicode-compatible.

I am also against the "use what you want" approach. I see no reason for such liberty. There's enough confusion on the subject of text, resulting in all this broken software. Having above said, I am convinced that programmers must finally reach consensus on UTF-8 as one proper way. (I come from a non-ascii-speaking country and grew up on Windows, so I'd be last expected to attack UTF-16 based on religious grounds).

I'd like to share more information on how I do text on Windows, and what I recommend to everyone else for compile-time checked unicode correctness, ease of use and better multi-platformness of the code. The suggestion substantially differs from what is usually recommended as the proper way of using Unicode on windows. Yet, in depth research of these recommendations resulted in the same conclusion. So here goes:


Do not use wchar_t or std::wstring in any place other than adjacent point to APIs accepting UTF-16. 
Don't use _T("") or L"" UTF-16 literals (These should IMO be taken out of the standard, as a part of UTF-16 deprecation).
Don't use types, functions or their derivatives that are sensitive to the _UNICODE constant, such as LPTSTR or CreateWindow().
Yet, _UNICODE always defined, to avoid passing char* strings to WinAPI getting silently compiled
std::strings and char* anywhere in program are considered UTF-8 (if not said otherwise)
All my strings are std::string, though you can pass char* or string literal to convert(const std::string &).
only use Win32 functions that accept widechars (LPWSTR). Never those which accept LPTSTR or LPSTR. Pass parameters this way:

::SetWindowTextW(Utils::convert(someStdString or "string litteral").c_str())


(The policy uses conversion functions below.)
With MFC strings:

CString someoneElse; // something that arrived from MFC. Converted as soon as possible, before passing any further away from the API call:

std::string s = str(boost::format("Hello %s\n") % Convert(someoneElse));
AfxMessageBox(MfcUtils::Convert(s), _T("Error"), MB_OK);

Working with files, filenames and fstream on Windows:


Never pass std::string or const char* filename arguments to fstream family. MSVC STL does not support UTF-8 arguments, but has a non-standard extension which should be used as follows:
Convert std::string arguments to std::wstring with Utils::Convert:

std::ifstream ifs(Utils::Convert("hello"),
                  std::ios_base::in |
                  std::ios_base::binary);


We'll have to manually remove the convert, when MSVC's attitude to fstream changes.
This code is not multi-platform and may have to be changed manually in the future
See fstream unicode research/discussion case 4215 for more info.
Never produce text output files with non-UTF8 content
Avoid using fopen() for RAII/OOD reasons. If necessary, use _wfopen() and WinAPI conventions above.





// For interface to win32 API functions
std::string convert(const std::wstring& str, unsigned int codePage /*= CP_UTF8*/)
{
    // Ask me for implementation..
    ...
}

std::wstring convert(const std::string& str, unsigned int codePage /*= CP_UTF8*/)
{
    // Ask me for implementation..
    ...
}

// Interface to MFC
std::string convert(const CString &mfcString)
{
#ifdef UNICODE
    return Utils::convert(std::wstring(mfcString.GetString()));
#else
    return mfcString.GetString();   // This branch is deprecated.
#endif
}

CString convert(const std::string &s)
{
#ifdef UNICODE
    return CString(Utils::convert(s).c_str());
#else
    Exceptions::Assert(false, "Unicode policy violation. See W569"); // This branch is deprecated as it does not support unicode
    return s.c_str();   
#endif
}
AnswerKenntnis:
Unicode codepoints are not characters!  Sometimes they are not even glyphs (visual forms).

Some examples:


Roman numeral codepoints like "Γà▓".  (A single character that looks like "iii".)
Accented characters like "├í", which can be represented as either a single combined character "\u00e1" or a character and separated diacritic "\u0061\u0301".
Characters like Greek lowercase sigma, which have different forms for middle ("╧â") and end ("╧é") of word positions, but which should be considered synonyms for search.
Unicode discretionary hyphen U+00AD, which might or might not be visually displayed, depending on context, and which is ignored for semantic search.


The only ways to get Unicode editing right is to use a library written by an expert, or become an expert and write one yourself.  If you are just counting codepoints, you are living in a state of sin.
AnswerKenntnis:
There is a simple rule of thumb on what Unicode Transformation Form (UTF) to use:
 - utf-8 for storage and comunication
 - utf-16 for data processing
 - you might go with utf-32 if most of the platform API you use is utf-32 (common in the UNIX world).

Most systems today use utf-16 (Windows, Mac OS, Java, .NET, ICU, Qt).
Also see this document: http://unicode.org/notes/tn12/

Back to "UTF-16 as harmful", I would say: definitely not.

People who are afraid of surrogates (thinking that they transform Unicode into a variable-length encoding) don't understand the other (way bigger) complexities that make mapping between characters and a Unicode code point very complex: combining characters, ligatures, variation selectors, control characters, etc.

Just read this series here http://blogs.msdn.com/michkap/archive/2009/06/29/9800913.aspx and see how UTF-16 becomes an easy problem.
AnswerKenntnis:
I would suggest that thinking UTF-16 might be considered harmful says that you need to gain a greater understanding of unicode.

Since I've been downvoted for presenting my opinion on a subjective question, let me elaborate.  What exactly is it that bothers you about UTF-16?  Would you prefer if everything was encoded in UTF-8? UTF-7? Or how about UCS-4?  Of course certain applications are not designed to handle everysingle character code out there - but they are necessary, especially in today's global information domain, for communication between international boundaries.

But really, if you feel UTF-16 should be considered harmful because it's confusing or can be improperly implemented (unicode certainly can be), then what method of character encoding would be considered non-harmful?

EDIT: To clarify: Why consider improper implementations of a standard a reflection of the quality of the standard itself?  As others have subsequently noted, merely because an application uses a tool inappropriately, does not mean that the tool itself is defective.  If that were the case, we could probably say things like "var keyword considered harmful", or "threading considered harmful".  I think the question confuses the quality and nature of the standard with the difficulties many programmers have in implementing and using it properly, which I feel stem more from their lack of understanding how unicode works, rather than unicode itself.
AnswerKenntnis:
There is nothing wrong with Utf-16 encoding. But languages that treat the 16-bit units as characters should probably be considered badly designed. Having a type named 'char' which does not always represent a character is pretty confusing. Since most developers will expect a char type to represent a code point or character, much code will probably break when exposed to characters beyound BMP.

Note however that even using utf-32 does not mean that each 32-bit code point will always represent a character. Due to combining characters, an actual character may consist of several code points. Unicode is never trivial.

BTW. There is probably the same class of bugs with platforms and applications which expect characters to be 8-bit, which are fed Utf-8.
AnswerKenntnis:
Yes, absolutely.

Why? It has to do with exercising code.

If you look at these codepoint usage statistics on a large corpus by Tom Christiansen you'll see that trans-8bit BMP codepoints are used several orders if magnitude more than non-BMP codepoints:

 2663710 U+002013 ΓÇ╣ΓÇôΓÇ║  GC=Pd    EN DASH
 1065594 U+0000A0 ΓÇ╣ ΓÇ║  GC=Zs    NO-BREAK SPACE
 1009762 U+0000B1 ΓÇ╣┬▒ΓÇ║  GC=Sm    PLUS-MINUS SIGN
  784139 U+002212 ΓÇ╣ΓêÆΓÇ║  GC=Sm    MINUS SIGN
  602377 U+002003 ΓÇ╣ΓÇâΓÇ║  GC=Zs    EM SPACE

 544 U+01D49E ΓÇ╣≡¥Æ₧ΓÇ║  GC=Lu    MATHEMATICAL SCRIPT CAPITAL C
 450 U+01D4AF ΓÇ╣≡¥Æ»ΓÇ║  GC=Lu    MATHEMATICAL SCRIPT CAPITAL T
 385 U+01D4AE ΓÇ╣≡¥Æ«ΓÇ║  GC=Lu    MATHEMATICAL SCRIPT CAPITAL S
 292 U+01D49F ΓÇ╣≡¥ÆƒΓÇ║  GC=Lu    MATHEMATICAL SCRIPT CAPITAL D
 285 U+01D4B3 ΓÇ╣≡¥Æ│ΓÇ║  GC=Lu    MATHEMATICAL SCRIPT CAPITAL X


Take the TDD dictum: "Untested code is broken code", and rephrase it as "unexercised code is broken code", and think how often programmers have to deal with non-BMP codepoints.

Bugs related to not dealing with UTF-16 as a variable-width encoding are much more likely to go unnoticed than the equivalent bugs in UTF-8. Some programming languages still don't guarantee to give you UTF-16 instead of UCS-2, and some so-called high-level programming languages offer access to code units instead of code-points (even C is supposed to give you access to codepoints if you use wchar_t, regardless of what some platforms may do).
AnswerKenntnis:
My personal choice is to always use UTF-8.  It's the standard on Linux for nearly everything.  It's backwards compatible with many legacy apps.  There is a very minimal overhead in terms of extra space used for non-latin characters vs the other UTF formats, and there is a significant savings in space for latin characters.  On the web, latin languages reign supreme, and I think they will for the foreseeable future.  And to address one of the main arguments in the original post: nearly every programmer is aware that UTF-8 will sometimes have multi-byte characters in it.  Not everyone deals with this correctly, but they are usually aware, which is more than can be said for UTF-16.  But, of course, you need to choose the one most appropriate for your application.  That's why there's more than one in the first place.
AnswerKenntnis:
Well, there is an encoding that uses fixed-size symbols. I certainly mean UTF-32. But 4 bytes for each symbol is too much of wasted space, why would we use it in everyday situations?

To my mind, most problems appear from the fact that some software fell behind the Unicode standard, but were not quick to correct the situation. Opera, Windows, Python, Qt - all of them appeared before UTF-16 became widely known or even came into existence. I can confirm, though, that in Opera, Windows Explorer, and Notepad there are no problems with characters outside BMP anymore (at least on my PC). But anyway, if programs don't recognise surrogate pairs, then they don't use UTF-16. Whatever problems arise from dealing with such programs, they have nothing to do with UTF-16 itself.

However, I think that the problems of legacy software with only BMP support are somewhat exaggerated. Characters outside BMP are encountered only in very specific cases and areas. According to the Unicode official FAQ, "even in East Asian text, the incidence of surrogate pairs should be well less than 1% of all text storage on average". Of course, characters outside BMP shouldn't be neglected because a program is not Unicode-conformant otherwise, but most programs are not intended for working with texts containing such characters. That's why if they don't support it, it is unpleasant, but not a catastrophy.

Now let's consider the alternative. If UTF-16 didn't exist, then we wouldn't have an encoding which is well-suited for non-ASCII text, and all the software created for UCS-2 would have to be completely redesigned to remain Unicode-compliant. The latter most likely would only slow Unicode adoption. Also we wouldn't have been able to maintain compability with text in UCS-2 like UTF-8 does in relation to ASCII.

Now, putting aside all the legacy issues, what are the arguments against the encoding itself? I really doubt that developers nowadays don't know that UTF-16 is variable length, it is written everywhere strarting with Wikipedia. UTF-16 is much less difficult to parse than UTF-8, if someone pointed out complexity as a possible problem. Also it is wrong to think that it is easy to mess up with determining the string length only in UTF-16. If you use UTF-8 or UTF-32, you still should be aware that one Unicode code point doesn't necessarily mean one character. Other than that, I don't think that there's anything substantial against the encoding.

Therefore I don't think the encoding itself should be considered harmful. UTF-16 is a compromise between simplicity and compactness, and there's no harm in using what is needed where it is needed. In some cases you need to remain compatible with ASCII and you need UTF-8, in some cases you want to work with work with Han ideographs and conserve space using UTF-16, in some cases you  need universal representations of characters usign a fixed-length encoding. Use what's more appropriate, just do it properly.
AnswerKenntnis:
UTF-8 is definitely the way to go, possibly accompanied by UTF-32 for internal use in algorithms that need high performance random access (but that ignores combining chars).

Both UTF-16 and UTF-32 (as well as their LE/BE variants) suffer of endianess issues, so they should never be used externally.
AnswerKenntnis:
Years of Windows internationalization work especially in East Asian languages might have corrupted me, but I lean toward UTF-16 for internal-to-the-program representations of strings, and UTF-8 for network or file storage of plaintext-like documents. UTF-16 can usually be processed faster on Windows, though, so that's the primary benefit of using UTF-16 in Windows.

Making the leap to UTF-16 dramatically improved the adequacy of average products handling international text. There are only a few narrow cases when the surrogate pairs need to be considered (deletions, insertions, and line breaking, basically) and the average-case is mostly straight pass-through. And unlike earlier encodings like JIS variants, UTF-16 limits surrogate pairs to a very narrow range, so the check is really quick and works forward and backward.

Granted, it's roughly as quick in correctly-encoded UTF-8, too. But there's also many broken UTF-8 applications that incorrectly encode surrogate pairs as two UTF-8 sequences. So UTF-8 doesn't guarantee salvation either.

IE handles surrogate pairs reasonably well since 2000 or so, even though it typically is converting them from UTF-8 pages to an internal UTF-16 representation; I'm fairly sure Firefox has got it right too, so I don't really care what Opera does.

UTF-32 (aka UCS4) is pointless for most applications since it's so space-demanding, so it's pretty much a nonstarter.
AnswerKenntnis:
UTF-16? definitely harmful. Just my grain of salt here, but there are exactly three acceptable encodings for text in a program:


ASCII: when dealing with low level things (eg: microcontrollers) that can't
afford anything better
UTF8: storage in fixed-width media such as files
integer codepoints ("CP"?): an array of the largest integers that are convenient
for your programming language and platform (decays to ASCII in the limit of low
resorces). Should be int32 on older computers and int64 on anything with 64-bit
addressing.  
Obviously interfaces to legacy code use what encoding is needed to make the old
code work right.
AnswerKenntnis:
Unicode defines code points up to 0x10FFFF (1,114,112 codes), all applications running in multilingual environment dealing with strings/file names etc. should handle that correctly.

Utf-16: covers only 1,112,064 codes. Although those at the end of Unicode are from planes 15-16 (Private Use Area). It can not grow any further in the future except breaking Utf-16 concept.

Utf-8: covers theoretically 2,216,757,376 codes. Current range of Unicode codes can be represented by maximally 4 byte sequence. It does not suffer with byte order problem, it is "compatible" with ascii.

Utf-32: covers theoretically 2^32=4,294,967,296 codes. Currently it is not variable length encoded and probably will not be in the future.

Those facts are self explanatory. I do not understand advocating general use of Utf-16. It is variable length encoded (can not be accessed by index), it has problems to cover whole Unicode range even at present, byte order must be handled, etc. I do not see any advantage except that it is natively used in Windows and some other places. Even though when writing multi-platform code it is probably better to use Utf-8 natively and make conversions only at the end points in platform dependent way (as already suggested). When direct access by index is necessary and memory is not a problem, Utf-32 should be used.

The main problem is that many programmers dealing with Windows Unicode = Utf-16 do not even know or ignore the fact that it is variable length encoded.

The way it is usually in *nix platform is pretty good, c strings (char *) interpreted as Utf-8 encoded, wide c strings (wchar_t *) interpreted as Utf-32.
AnswerKenntnis:
Add this to the list:


  The presented scenario is simple (even more simple as I will present
  it here than it was originally!):
   1.A WinForms TextBox sits on a Form, empty. It has a MaxLength set to 20.
  
  2.The user types into the TextBox, or maybe pastes text into it.
  
  3.No matter what you type or paste into the TextBox, you are limited to 20, though it will sympathetically beep at text beyond the 20 (YMMV
  here; I changed my sound scheme to give me that effect!).
  
  4.The small packet of text is then sent somewhere else, to start an exciting adventure.   
  
  Now this is an easy scenario, and anyone can
  write this up, in their spare time. I just wrote it up myself in
  multiple programming languages using WinForms, because I was bored and
  had never tried it before. And with text in multiple actual languages
  because I am wired that way and have more keyboard layouts than
  possibly anyone in the entire freaking universe.   
  
  I even named the
  form Magic Carpet Ride, to help ameliorate the boredom.   
  
  This did not
  work, for what it's worth.   
  
  So instead, I entered the following 20
  characters into my Magic Carpet Ride form:  
  
  0123401234012340123≡áÇÇ  
  
  Uh oh.  
  
  That last character is U+20000, the first Extension B
  ideograph of Unicode (aka U+d840 U+dc00, to its close friends who he
  is not ashamed to be disrobed, as it were, in front of)....
  
  
  
  And now we have a ball game.  
  
  Because when TextBox.MaxLength talks
  about 
  
  Gets or sets the maximum number of characters that can be manually
  entered into the text box.   
  
  what it really means is   
  
  Gets or sets
  the maximum number of UTF-16 LE code units that can be manually
  entered into the text box and will mercilessly truncate the living
  crap out of any string that tries to play cutesy games with the
  linguistic character notion that only someone as obsessed as that
  Kaplan fellow will find offensive (geez he needs to get out more!).
  
  I'll try and see about getting the document updated....
  Regular
  readers who remember my UCS-2 to UTF-16 series will note my
  unhappiness with the simplistic notion of TextBox.MaxLength and how it
  should handle at a minimum this case where its draconian behavior
  creates an illegal sequence, one that other parts of the .Net
  Framework may throw a  
  
  
  System.Text.EncoderFallbackException: Unable
  to translate Unicode character \uD850 at index 0 to specified code
  page.*   
  
  
  exception if you pass this string elsewhere in the .Net
  Framework (as my colleague Dan Thompson was doing).   
  
  Now okay,
  perhaps the full UCS-2 to UTF-16 series is out of the reach of many.
  But isn't it reasonable to expect that TextBox.Text will not produce a
  System.String that won't cause another piece of the .Net Framework to
  throw?   I mean, it isn't like there is a chance in the form of some
  event on the control that tells you of the upcoming truncation where
  you can easily add the smarter validation -- validation that the
  control itself does not mind doing.   I would go so far as to say that
  this punk control is breaking a safety contract that could even lead
  to security problems if you can class causing unexpected exceptions to
  terminate an application as a crude sort of denial of service.   Why
  should any WinForms process or method or algorithm or technique
  produce invalid results?


Source : Michael S. Kaplan MSDN Blog
AnswerKenntnis:
I wouldn't necessarily say that UTF-16 is harmful.  It's not elegant, but it serves its purpose of backwards compatibility with UCS-2, just like GB18030 does with GB2312, and UTF-8 does with ASCII.

But making a fundamental change to the structure of Unicode in midstream, after Microsoft and Sun had built huge APIs around 16-bit characters, was harmful.  The failure to spread awareness of the change was more harmful.
AnswerKenntnis:
Since I cannot yet comment, I post this as an answer, since it seems I cannot otherwise contact the authors of utf8everywhere.org. It's a shame I don't automatically get the comment privilege, since I have enough reputation on other stackexchanges.

This is meant as a comment to the Opinion: Yes, UTF-16 should be considered harmful answer.

One little correction:

To prevent one from accidentally passing a UTF-8 char* into ANSI-string versions of Windows-API functions, one should define UNICODE, not _UNICODE. _UNICODE maps functions like _tcslen to wcslen, not MessageBox to MessageBoxW. Instead, the UNICODE define takes care of the latter.
For proof, this is from MS Visual Studio 2005's WinUser.h header:

#ifdef UNICODE
#define MessageBox  MessageBoxW
#else
#define MessageBox  MessageBoxA
#endif // !UNICODE


At the very minimum, this error should be corrected on utf8everywhere.org.

A suggestion:

Perhaps the guide should contain an example of explicit use of the Wide-string version of a data structure, to make it less easy to miss/forget it. Using Wide-string versions of data structures on top of using Wide-string versions of functions makes it even less likely that one accidentally calls an ANSI-string version of such a function.

Example of the example:

WIN32_FIND_DATAW data; // Note the W at the end.
HANDLE hSearch = FindFirstFileW(widen("*.txt").c_str(), &data);
if (hSearch != INVALID_HANDLE_VALUE)
{
    FindClose(hSearch);
    MessageBoxW(nullptr, data.cFileName, nullptr, MB_OK);
}
AnswerKenntnis:
I've never understood the point of UTF-16.  If you want the most space-efficient representation, use UTF-8.  If you want to be able to treat text as fixed-length, use UTF-32.  If you want neither, use UTF-16.  Worse yet, since all of the common (basic multilingual plane) characters in UTF-16 fit in a single code point, bugs that assume that UTF-16 is fixed-length will be subtle and hard to find, whereas if you try to do this with UTF-8, your code will fail fast and loudly as soon as you try to internationalize.
AnswerKenntnis:
UTF-16 is the best compromise between handling and space and that's why most major platforms (Win32, Java, .NET) use it for internal representation of strings.
AnswerKenntnis:
Someone said UCS4 and UTF-32 were same.  No so, but I know what you mean.  One of them is an encoding of the other, though.  I wish they'd though to specify endianness from the first so we wouldn't have the endianess battle fought out here too.  Couldn't they have seen that coming?  At least utf-8 is the same everywhere (unless someone is following the original spec with 6-bytes).  Sigh.  If you use utf-16 you HAVE to include handling for multibyte chars.  You can't go to the Nth character by indexing 2N into a byte array.  You have to walk it, or have character indices.  Otherwise you've written a bug.  The current draft spec of C++ says that utf-32 and utf16 can have little-endian, big-endian, and unspecified variants.  Really?  If Unicode had specified that everyone had to do little-endian from the beginngin then it would have all been simpler. (I would have been fine with big-endian as well.)  Instead, some people implemented it one way, some the other, and now we're stuck with silliness for nothing.  Sometimes it's embarrassing to be a software engineer.
AnswerKenntnis:
I don't think it's harmful if the developer is careful enough.
And they should accept this trade off if they know well too.  

As a Japanese software developer, I find UCS-2 large enough and limiting the space apparently simplifies the logic and reduces runtime memory, so using utf-16 under UCS-2 limitation is good enough.   

There are filesystem or other application which assumes codepoints and bytes to be proportional, so that raw codepoint number can be guaranteed to be fit to some fixed size storage.  

One example is NTFS  and VFAT specifying UCS-2 as their filename storage encoding.  

If those example really wants to extend to support UCS-4, I could agree using utf-8 for everything anyway, but fixed length has good points like:  


can guarantee the size by length (data size and codepoint length is proportional)
can use the encoding number for hash lookup
non-compressed data is reasonably sized (compared to utf-32/UCS-4)


In the future when memory/processing power is cheap even in any embeded devices, we may accept the device being a bit slow for extra cache misses or page faults and extra memory usage, but this wont happen in the near future I guess...
AnswerKenntnis:
"Should one of the most popular encodings, UTF-16, be considered harmful?"


Quite possibly, but the alternatives should not necessarily be viewed as being much better.

The fundamental issue is that there are many different concepts about: glyphs, characters, codepoints and byte sequences. The mapping between each of these is non-trivial, even with the aid of a normalization library. (For example, some characters in European languages that are written with a Latin-based script are not written with a single Unicode codepoint. And that's at the simpler end of the complexity!) What this means is that to get everything correct is quite amazingly difficult; bizarre bugs are to be expected (and instead of just moaning about them here, tell the maintainers of the software concerned).

The only way in which UTF-16 can be considered to be harmful as opposed to, say, UTF-8 is that it has a different way of encoding code points outside the BMP (as a pair of surrogates). If code is wishing to access or iterate by code point, that means it needs to be aware of the difference. OTOH, it does mean that a substantial body of existing code that assumes "characters" can always be fit into a two-byte quantity ΓÇö a fairly common, if wrong, assumption ΓÇö can at least continue to work without rebuilding it all. In other words, at least you get to see those characters that aren't being handled right!

I'd turn your question on its head and say that the whole damn shebang of Unicode should be considered harmful and everyone ought to use an 8-bit encoding, except I've seen (over the past 20 years) where that leads: horrible confusion over the various ISO 8859 encodings, plus the whole set of ones used for Cyrillic, and the EBCDIC suite, andΓÇª well, Unicode for all its faults beats that. If only it wasn't such a nasty compromise between different countries' misunderstandings.
AnswerKenntnis:
My guesses as to the why the Windows API (and presumably the Qt libraries) use UTF-16:


UTF-8 wasn't around when these APIs were being developed.
The OS needs to do a lookup on the code points to display the glyphs-- if the data is passed around internally as UTF-8, every time it needs to do that for a multibyte character, it would have to convert from UTF-8 to UTF-16/32.  If the bytestream is stored as "wide" chars in memory, it won't need to do this conversion.  So increased memory usage is a tradeoff for decreased conversion work and complexity.


When writing to a stream, however, it's considered best practice to use UTF-8 for the reasons outlined in the Joel article referenced above.
AnswerKenntnis:
This totally depends on your application. For most people, UTF-16BE is a good compromise. Other choices are either too expensive to find characters (UTF-8) or waste too much space (UTF-32 or UCS-4, where each character takes 4 bytes).

With UTF-16BE, you can treat it as UCS-2 (fixed length) in most cases. Characters beyond BMP are rare in normal applications. You still have the option to handle surrogate pair if you choose to, say you are writing an archaeology application.
QuestionKenntnis:
I'm doing 90% maintenance and 10% development, is this normal?
qn_description:
I have just recently started my career as a web developer for a medium sized company. As soon as I started I got the task of expanding an existing application (badly coded, developed by multiple programmers over the years, handles the same tasks in different ways, zero structure)

So after I had successfully extended this application with the requested functionality, they gave me the task to fully maintain the application. This was of course not a problem, or so I thought. But then I got to hear I wasn't allowed to improve the existing code and to only focus on bug fixes when a bug gets reported.

From then on I have had 3 more projects just like the above, that I now also have to maintain. And I got 4 projects where I was allowed to create the application from scratch, and I have to maintain those as well. 

At this moment I'm slightly beginning to get crazy from the daily mails of users (read managers) for each application I have to maintain. They expect me to handle these mails directly while also working on 2 other new projects (and there are already 5 more projects lined up after those). The sad thing is I have yet to receive a bug report on anything that I have coded myself, for that I have only received the occasional lets do things 180 degrees different change requests.

Anyway, is this normal? In my opinion I'm doing the work equivalent of a whole team of developers.

Was I an idiot when I initially expected things to be different?

I guess this post has turned into a big rant, but please tell me that this is not the same for every developer.

P.S. My salary is almost equal if not lower than that of a cashier at a supermarket.
AnswersKenntnis
AnswerKenntnis:
During one of my internships I found I spent a lot of time doing bug fixes. You have to realize that as an entry level employee you aren't going to get the sexiest work, you're going to get the grunt work no one else wants. It's unfortunate, but it's how it is at every job.

Additionally, you have to realize that to a company, having code that works is more important than having code that is clean. From your company's perspective, you changing the existing structure is money wasted on redoing something that is already done and potentally introducing even more errors. Usually these types of companies aren't computer/software companies so no sufficiently high manager has the technical background to know that sometimes you need to do these major overhauls. That said, if your company is run by technically competent people and they understand the value of good code, you may get more leeway, although sometimes you need to choose your battles (the main purpose of a business is still to make money, afterall).

That said, you are not unreasonable in wanting to be able to leave your mark on the software and wanting more meaningful work. It is also unfortunate that you have to deal with so many projects at once while fielding requests from so many different managers.

As a programmer, it is a fact of life that you will spend more time maintaining and modifying other people's code than you will writing your own from scratch. If this is a problem for you then perhaps you should stick to developing as a hobby and pursue a different career. If you are OK with maintaining code but you feel you are not being used effectively or are being overwhelmed, then that is a matter you need to discuss with your manager. If your problems are more serious than that or if you feel like your managers don't know how to effectively manage your skill set then it would be a good idea to consider finding a position at a different company. Given your stated low salary, This is probably your best course of action.
AnswerKenntnis:
Sounds like management has a problem managing your workload and prioritizing tasks. You should talk to your manager and make them understand that you are overloaded and you can't do effective work if everyone keeps bombarding you with requests which they want fulfilled immediately. That makes you jump from one task and project to another, wasting a lot of time switching gears in your mind. For effective software development work, one should be able to immerse oneself into a task and focus fully on it. The more interruptions one gets, the more time is wasted by context switching. Research shows that it takes about 15 minutes to immerse and get to the state of flow where our mind works the most efficiently. If you get interruptions every 15 minutes, you never get to flow, which is a tremendous waste for both you and the company.

So you should try to negotiate a more sensible working mode with your manager. This should include prioritizing the incoming requests and planning ahead to some extent. All user requests should be maintained in a list, ordered by priorities. And the priorities should not be decided by the requester (as naturally everyone thinks his/her request is the most important on earth), neither by you, but by someone with enough business knowledge and overview of the whole range of products you are maintaining (the product owner). Ideally, all incoming requests should be entered into an issue tracker like Jira or Mantis. Or at least mailed to the product owner, not you. And he/she should deal with all the complaints from the users too over "why is my request not ready yet?!", allowing you to focus on the development work. If this is unattainable, you should at least negotiate some windows of time when you look at incoming requests and deal with them, reserving an uninterruptible portion of your time solely for development.

If this is possible, the next step could be to plan ahead to some extent. I.e. estimate the time needed to implement the top priority requests, then schedule your time into sprints, which may be one or more weeks each, and allocate enough tasks to the next sprint to cover your time. You probably want to keep a portion of your time for incoming emergency requests, but the rest can be planned ahead. And you may also prefer to organize work on different projects into separate "streaks", i.e. to work on Project A on Monday, Project B on Tue-Wed, Project C on Thu morning and D on afternoon etc., to further reduce context switching. This way you have a rough idea of what you are to do in the next one or few weeks. Moreover, this gives a roadmap to your clients too: they can see when they get which request implemented. You may or may not want to mention the word "agile" to your manager here - this is basically agile development, but some people oppose Agile without actually knowing what it is :-)

Note that even if your current position seems low valued by your company, the more projects you are maintaining, the more negotiating power you have. Finding and training a new hire to maintain all these projects takes considerable time ( = money) for the company. And you may rightly point out that your code is so much better than the legacy parts of these apps, so it is not a given that they can easily find a candidate of similar capabilities for the same amount of money. Not to mention that if they don't improve working conditions, they will make the next guy get fed up and quit just as fast as you... Try to make them understand that it is in their own best interest to keep you, and to keep you happy where you are :-) This may give you some power to negotiate the above conditions, and/or request a pay raise.

How much negotiating power you have - that is a big question. Your management may or may not be open to these ideas, and may or may not respect you enough to take your pleas seriously. But if you play your cards well, you have a chance. And if they refuse... you can always look for a better workplace. This situation isn't the same for every starter, although (sadly) your experiences are fairly typical. But there really are better workplaces out there. Quality of workplace is only loosely correlated with geographical location, but my feeling is that in Northern Europe you have higher than average chances. So if you can't get your current conditions noticeably improved, you should start looking immediately, before you get completely fed up, and burnt out. It is immensely better to look for a job while you still have one, so you need not accept the first offer just because you need money immediately. Eventually you will find a better place :-)
AnswerKenntnis:
P.S. My salary is almost equal if not lower then that of a cashier at a supermarket.


Heh I wanted to write something about how to negotiate until I read that. Now all I can say is leave! Assuming that's half of what a developer with a degree usually earns. And assuming that things improve and they give you immediately a 10% increase you can figure out yourself how long it takes to get there. It also looks like you do not learn anything on the job and you don't seem to be surrounded by brilliant engineers there, so it's a waste of time.
AnswerKenntnis:
Good question Sir. I was also in a similar situation, and I can tell you that if you stay on that track it never ends. Management will keep shoveling it at you, because... they can.

There are a few additional thoughts that I have on this topic. 


Do what you love. If you don't love it, prepare yourself for making the attempt to find what it is that you might love.
Communication. Communicate clearly your inabilities to deliver to unrealistic expectations. In my similar experience the architect (who did the most shoveling) said, "you have to manage others expectations of you."
Burnout. If you have not experienced burnout, do not tempt fate. It sucks your life and soul out of your mind. Despite your best effort, your working purpose becomes grey, dreary and meaningless. I impart this advice because you must, at all costs, avoid burnout.
Training. Here is the silver lining. Your training and experience, while frustrating, and perhaps underpaid, is in fact, very valuable to your career. This is your saving grace to realize this because you can soak up as much learning as possible and delay any inevitable glass ceiling. 


Focus on what talents and skills you are growing... these will carry you through the next opportunities of your career.
AnswerKenntnis:
You're dealing with multiple issues here... Let's start with the obvious...

Is This Normal?

Hell no. But... is it common? Unfortunately, yes.

Regarding Bug-Fixing Frustration

While that doesn't excuse the rest of the mess you have to deal with and the multiple projects they overload you with, I just want to make a quick note that the "bug-fix" only approach, while frustrating for you as a developer, can be a perfectly sensible approach for the company and its management.

Surface for More Bugs and Costs

The more code you touch, the more likely you are to introduce bugs, even if your intent is to improve it. That means extended dev time, test time, and costs. And if it slips through to a service release with a medium to high-defect, that's a big mess for them.

Noise / Fog in your Logs

From an SCM-perspective, it also makes a bit of sense if you work directly off a service release's branch, as you want to have a clean view of changes relating to bugfixing. If there are 15 commits with thousands of changes surrounding a bugfix that actually required maybe a 1 line code change, it's a tad annoying.

So, being a new hire, it's even more sensible to ask you to refrain from refactoring and/or enhancing the software, and quite OK in my sense to be as "surgical" as possible with your bugfixes. It just keeps headaches at bay.

Can You do Something About It?

Now, it does NOT mean that there would be ways of achieving both sanity of the code and sanity of the involved people's minds. Being junior, they should have someone review your changes, especially bugfixes, and make sure they are approved before making it to the service release builds. That would prevent or limit radical changes, and be safer.

The Project From Hell

Crap Code, Herd of Programmers, Duplication, Crap Architecture

Again, devil's advocate here, but just showing that your initial request contains a few non-consequential bits.

My point is this: I really really really rarely took over a codebase that wasn't in this state. And on the off-chance that I did, they were recently started projects or prototypes that had been kick-started by pretty stellar programmers. But the astonishingly vast majority of them looked like crap, and a scary number of these were actual crap. Even the ones started by good or great programmers can end up being crap, deadlines and other engagements helping...

Welcome to real-life industrial software engineering!

And you know what's fun? It's often way worse in the web-development world. Enjoy! :)

Too Many Projects & Requests, Not Enough Hands & Time

The problem lies here probably in:


your management (maybe unconsciously) abusing you,
your colleagues (maybe unconsciously) abusing you,
your (maybe unknowingly) not covering your ass and fighting your battles enough.


Your managers should be aware that you are working on too many projects to manage. If they aren't, make sure they are ASAP. Make also sure they know it wasn't all a pick-nick in the park and that you felt pressured, and that it needs to stop.

Try to have a look around and make sure your colleagues do not deflect more tasks and projects on you, directly (by really saying "X will be able to take care of that") or indirectly ("I'm not the right person for this, find someone else" -> ends up being you).


  Personal anecdote here: I did an internship a few years back, and just on my very last day, when I got my evaluation, my boss told me, despite being very satisfied with my work overall, that one of the managers had the feeling I had been unloading some "not so fun tasks" on another intern when they would have expected me to pick them up. I was mortified of having them felt let down, and at the idea that I would look like I was slacking off, when my intent was the exact opposite: I was trying to grab the harder tasks and have the other younger intern deal with less hair-pulling issues. Little did I realize that, had I been in his position, I would have been bored by the lack of challenge and probably felt the way he did. The point is, you need to communicate to make sure nobody makes false assumptions about 3 very distinct things:



what you can do,
what you want to do,
and what you are willing to do.


So it's also partly your fault for letting it become this way. But it's normal, it's a lesson everybody needs to learn. It holds in two letters: N - O.

You usually employ it as a prefix for a more lengthy but not so much more charged answer: No, I can't do this. No, I don't know how to do this. No, I'm not sure I'm the right person for this. No, I've never done that.

At first, it's very easy to feel like you can just say "yes, I'll (eventually) do it", and pile things up and get them done, maybe by putting some extra hours in. That's all wrong. You need to understand that your time is, after your skills, your most valuable asset, to you and to your company. If it is misused, it impacts projects, deadlines, and budgets. As simple as that.

Also, it looks a bit worrying that you would have too many people to report to. It's OK to have multiple customers to deal with, and multiple project owners or even principal stakeholders you need to communicate with. But, overall, especially as you're a new hire, you should mostly report to a few managers only (and most likely only your direct manager, and possibly a lead or senior developer). How did it get this way? I don't know. It can be an organizational issue at your company, or it can be the result of you doing a favor and of then being contacted directly, and failing to say "no". Or it can be that your direct manager as issues with dispatching tasks, for all I know (I'm really guessing, but the pattern are recognizable and well-known).

I'd recommend you do the following rather quickly: go talk to your direct manager in person, explain that other managers might be a bit pushy, or (probably less whiny) that you 
have too many stuff piling on from too many people, and that you need his input (and possibly theirs as well) to know which ones to prioritize.

180-degrees Change Requests

These are another big issue. They're probably not your fault, but you can try to help them address it.

"180-degress change requests", as you beautifully and acurately call them, are a clear sign that requirements are fuzzy from the get go, and that nobody tries hard enough to have them chiseled and cleared up over time.

That's usually when someone needs to get on the phone (or better, on their feet), and grab stakeholders by the hand and tell them clearly: "that's where we are, that's where you want us to go, do you confirm we're heading in the right direction?". It's OK to not get clear answers at the beginning, but the more time passes, the clearer they should become, or this project is a disaster waiting to happen.

Usually I'd say grab all the stakeholders within reach, put them in a room, drive them through litigious issues and incrementally try to resolve these - and get priorities while you're at it. However in your case, this may not be your call to make already. But you mention they really gave you the responsibility of the projects; so if that's really the case, then do take responsibility and do that. And DO NOT shy away from saying "we CAN'T do that", or even "we WON'T do that". It's really important to limit the scope of a project.

If there's no scope, there are no clear-cut requirements at the end of the discussion.

E-mail Overload

People tend to behave differently based on the communication medium they use. Personally, though I'm a rather soft-spoken person (and have been working mostly in foreign countries, so I also end up not liking to talk on the phone to much), I'd favor in order of preference based on productivity:


talking to people face-to-face,
talking to people on the phone,
talking to people via IM,
talking to people via email.


E-Mails are nice for tracking, for getting confirmations, for sending notes.

For scheduling, planning and discussing problematic points, they're close to useless. Go knock on the guy's door until he/she opens it and sit down with a notepad and a copy of your documentation to clarify things. Once you're done, then send an email and ask for confirmation. If it comes back with a negative answer or a slightly hidden attempt at sneaking something else in the envelope, go make the siege of your interlocutor's office again.

This is software engineering. It's often more productive when you don't type away on a keyboard, and can actually cut down upfront on the crap you'll need to deal with.

Doing a Team's Worth of Work

Are you doing the equivalent of a team's worth of work? Maybe.

Are you doing the equivalent of your team's worth of work? Probably not.

What I mean is that your team is probably busy working, and you are overworked. And that's the issue: you're overloaded with things that should be pushed out of the current project timelines, or given to someone with time on their hands.


  Was I an idiot when I initially expected things to be different?


No; just new to the party. It's like a first hung-over or relationship. You'll get over it.


  I guess this post has turned into a big rant, but please tell me that this is not the same for every developer.


This is the same for every developer in chaotic organizations, be them startups or well-established giants, and with no experience or confidence to get things to move one bit to tip your chances of survival on the right side of the scale.


  P.S. My salary is almost equal if not lower then that of a cashier at a supermarket.


I did decent salaries on jobs that would appear crappy. It's not the number on the check that counts, it's the context. What you do, your age, where you live and work, etc...

That being said, if you are grossly underpaid, working too much, and not entirely junior, go ask for that raise or get a new job!

It's simple:


if they value what you do, they'll gladly agree to a raise,
if they don't, future in this company doesn't look very rosy (for you, at least, which is what matters), so don't feel bad about leaving.


Be aware that asking for a raise is a good thing, even though you wouldn't be enclined to think so at first. It proves you keep track of what you do, and hints that you keep an eye on other option while still being willing to stay onboard. And it's a good thing to get used to request them, as they are like job interviews or bargaining in general: it's something that requires practice, and they don't fall from the sky if you don't reach for them yourself. Some companies will distribute raises regularly without being requested to, but that's only because they are clever enough to know that it keeps you half-happy and less willing to change, and they want to cut the grass under your foot (most people would feel a bit uneasy about upping a raise offer they've been offered directly).

How to proceed with this request is a bit out of the scope of THIS project right here, so I won't go into the details. But I'd recommend you prepare a record of your SCM commit IDs, of your fixed bugs and achievements, and that you also prepare reports comparing them to the team's overall effort. This way:


you can measure for yourself if you effectively did a lot more than your peers or not,
you can stand your ground if they say your request is not justified.
AnswerKenntnis:
In addition to other people's comments:


Yes it is normal for an entry level employee to be given the jobs nobody else wants to do
You should see this as a building block for your future career


So what should you do? In order to prove youself as a professional developer you need to ensure your work is structured and planned, otherwise you may find it hard to build on the good things you're currently doing - so you should try to do things like the following (if you're not already).


Log your work accurately, for each project.  So if you spend 1 hour fixing a bug on project A, log this time.  This will be useful to show to your manager if you want to discuss workloads.
Write unit tests.  You mention some of the projects you maintain are full of bugs, so my guess is that there are few (if any) unit tests. For each bug report, write a unit test which replicates the bug, then fix the bug.  This will help ensure no regressions occur, improve the quality of the code, and serve as a platform on which to refactor the code if you're given the chance (eg. it could help you convince the stakeholders that rewriting some parts could improve quality without introducing new bugs, due to the unit test suite).
Look for a new job - you work on many projects at once, you have written code from scratch, you have probably experianced the entire project lifecycle - these are all good experience for applying elsewhere
AnswerKenntnis:
Your situation is a bit edgy, yet still normal. But the way you handle it is very bad. Here is what you need to do:


Try to confront your boss. You should have some proof(numbers) how much time bad code-base actually costs. If he does not understand stuff like technical debt , stop mentioning it. It would wreck your head and you could be marked as 'bad attitude' guy. It is not your job to teach your boss to do his work.
Maintain your own backlog (kanban). When new tasks comes put it at end, and tell estimated time of completion. 
Increase your response time, check your email only twice day. Typically before lunch and just before you leave. (email check should not be followed by coding, as it may wreck your head)
Make small code improvement as part of each task. It is simply your job to improve code, skills and tools you are using. It will also boost your moral in long term. 
No project switching during day. Today you are simply working on project X, and you will start other task for Y tomorrow.
Allocate one hour per day for gate keeping. It means small tasks which are trivial to do. If this task takes longer than 10 minutes (no mater what reason is), it goes into backlog and you notify manager it will be delayed. 


Now comes the hard part. Currently managers do not communicate with each other, and just assume you will finish their own project with maximal priority. This brings lot of stress on your head because you are in middle of arguments. You need to force managers to start coordinating your work. At end you should have nice & simple backlog and managers should bully each other without you.

So lets do simple role playing. There are three managers and projects (Xavier with project X, Yvonne with project Y and Zed with project Z). You have backlog for two weeks, 5 days for X and 5 days for Y. 


Z asks you to do some task (1 day)
You respond that it will be done in 11 days. 
Z responds that it is simple task, and should not take more then one day (notice that Z applies small pressure)
You responds that you are currently working for X and latter for Z. You can do her work after that. 
Z responds you should do his task immediately anyway. (increased pressure, direct violation of X and Y territory)
You respond that doing her task would delay work for X and Y. He should ask them first. You also CC X and Y.


Now there are two ends: 


managers will start barking at each other, many emails, probably some meetings.. You should stay away and let winner to assign you task. 
Nothing will happen, Z will ask you two days latter where is his task. You answer that you are working for X currently, and he did not mentioned anything about project Z. Again CC X. 


Now this type of behaviour can get you fired. But your situation is unsustainable and you will probably quit soon anyway. This solution only works if managers are competing against each other (very usual). You should also keep records of your work (backlog), in case somebody would complain, that you are slacking off.
AnswerKenntnis:
Seven years ago I was doing pretty much 100% maintenance work for a while and wrote an article about it: The Art of Maintenance Programming. One part you may find useful:


  
  Get to Like It
  
  
  How can anyone like maintenance programming? Developers dream of being chief architects in their teams, and when they end up maintaining existing software, it feels almost like punishment. It doesnΓÇÖt have to be: maintenance programming has its own set of challenges, and requires a lot of creativity, adaptability, patience, discipline, and good communication. It can be good for your career as well: having bombastic entries like ΓÇ£Architected n-tier 24/7 enterprise applicationΓÇ¥ in your resume looks nice, but employers actually value people who solve problems; and maintenance programming can be a good chance to demonstrate your problem-solving skills.
AnswerKenntnis:
Your problem sounds like sommething you hear more often about. It appears to be a job that could easily fit on the daily wtf. A lot of organisations focus more on sales or feature pushing than on quality. What those companies fail to see is that there is more than external qualities of piece of software. Ward Cunningham coined the term Technical Debt. You might also want to read Steve mcConnel on technical debt. He has some interesting points. In a Google tech talk, Ken Swaber talks about agile software development. A good part is about a story similar to yours. He talks about a software project that has become "braindead" after 10 years of programming by various programmers without ever doing any Refactoring. I think that if you see this video you'll see a lot of similarities to what you describe. Any software system will degrade in quality when it is being expanded on. In fact in order to survice it will have to. The Lehman Laws explain this principle quite well. Ultimately it will boil down to the following question: "How to convince your boss to do refactoring"

How I approached a similar problem:


I confronted my boss and explained that code quality degrades when we keep developing (Lehman Laws)
I tried to explain my boss the concept of technical debt. And that the way he lets you to work is a way that will cost him money in the long run.
In order to actually show him how severe the problem was I have (in my own time) done a static code analysis. Bosses don't understand software but they understand numbers. Although code metrics have their flaws it is good to have something measurable you can talk about. Try to find out what normal readings are for these metrics and you will be surprised when you compare this to your own codebase.
If nothing helps and nothing changes the only thing you can do is explain that a certain new feature will require some rework of other parts of your codebase. Explain that if you have duplicate code and they want to change something that the costs of a change duplicate as well.
A common answer to the previous point will be that no-one has asked and thus not paid for this rework. That "perhaps" this rework is superfluous. You will have to explain that software will always have to change. Like the Lehman Laws say; it will have to change in order to remain in use. If not, other programs who did change will always outlive. It is those who expect change and can adapt to change that will survive. This is what Agile Software Development is all about. (Wikipedia)  


Edit: My boss nowadays uses the concept of technical debt to explain to our customers that we sometimes need to rework parts of the software we build. Just to prove that... if you have a reasonable boss it is possible to change things for the better.
AnswerKenntnis:
Situation you are facing is nearly (if not totally) the same for many freshers.
This happens in the initial phases of career. Here is the catch, we have to overcome this issue and prove our value to the company(be it medium sized or MNC). We should be able to make decisions about what the circumstances demand us to do. So there is nothing harm in putting efforts in the job, provided you get noticed for it and stand as an individual for your work. If you are worth, the company will notice! Adios and best of luck.
AnswerKenntnis:
In my opinion, a company that prohibits refactoring is not worth working for.  Refactoring is an essential software development skill, and version control tools make it very easy to develop changes in isolation without corrupting the 'trunk' (in case a refactor actually does break something).  As Uncle Bob says (paraphrased): "You should haven't to ask to be a professional and do your job right."

Maintenance programming should never mean perpetuating bad code.
AnswerKenntnis:
I have received this email five years ago from one of my friend.

Email body:    


A newly joined trainee engineer asks his boss "what is the meaning of Appraisal?"

Boss: "Do you know the meaning of resignation?"

Trainee: "Yes I do" 

Boss: "So let me make you understand what an appraisal is by comparing it with resignation"

Comparison study: Appraisal and Resignation
|---------------------------------+----------------------------------| 
|       Appraisal                 |       Resignation                |
|---------------------------------+----------------------------------|
|     In appraisal meeting they   |    In resignation meeting they   | 
|     will speak only about your  |    will speak only about your    |
|     weakness, errors and        |    strengths, past achievements  |
|     failures.                   |    and success.                  | 
|---------------------------------+----------------------------------|
|     In appraisal you may need to|    In resignation you can easily |
|     cry and beg for even 2%     |    demand (or get even without   | 
|     hike.                       |    asking) more than 10-20% hike.|
|                                 |                                  |
|---------------------------------+----------------------------------| 
|     During appraisal, they will |    During resignation, they will |
|     deny promotion saying you   |    say you are the core member of|
|     didn't meet the expectation,|    team; you are the vision of   | 
|     you don't have leadership   |    the company how can you go,   |
|     qualities, and you had      |    you have to take the project  |
|     several drawbacks in our    |    in shoulder and lead your     | 
|     objective/goal.             |    juniors to success.           |
|---------------------------------+----------------------------------|
|     There is 90% chance for not |    There is 90% chance of getting|
|     getting any significant     |    immediate hike after you put  |
|     incentives after appraisal. |    the resignation.              |
|---------------------------------+----------------------------------|


Trainee: "Yes boss enough, now I understood my future. For an appraisal I will have to resign ... !!!"
AnswerKenntnis:
If I were you, I would spend some late hours after work rebuilding the application for free. It would probably be a fun task. When you finish it, show it to your boss. If it works and your doing maintenance on it, he should have nothing to worry about. This will make your job much easier and would open the eyes of the higher ups to your potential in the company.

I'm full time college student working a part time internship for $10 an hour. I do QA stuff that's boring, repetitive, and easy. I consider myself to be extremely lucky because I know one day this will open doors to bigger and better places.
AnswerKenntnis:
Try talking with your employers and see if you can solve this out. It sounds like you're way over your head on this, and it has nothing to do with being a bad programmer.

Smaller web companies tend to have a lot of projects going on at the same time, which in many ways leaves you in a though spot. Either try to make the best of your situation, or try to find a new job if you think you can. I promise there are better coding jobs out there, so don't let this first one scare you off.

Good luck, and I hope both you and your co-workers understand the gravity or your efforts!

Personal experience
Like many here I also regognize your situation. I basicly got my first coding job with low salary and had to maintain a lot built code with poor structure. At first I found it funny to learn new stuff, but when I in the end had lots of projects to maintain, new projects to make and a white board which grew bigger and bigger every day with points I wasn't done with I realized that it wasn't working.

After putting up with it for 2 years, I quit and found a couple of months later another coding job which fits me perfectly.

Anyway many times it's not only your projects that might be the issue. I find myself more comfortable on a workplace when I get acknowledged and respected for my work. The problem with the situation that you're in is that your employers might only notice the bugs which arise from created projects, and not the time you take to remove all the other bugs.

Salary
If you want more money, you can often get it. I managed to under 2 years negotiate my salary to a raise of 33%.
Basicly think about the value of your work and how much the company needs you. If they can't afford to give you the salary you've earned, then the company either needs to looks at their expenses or realize that their business isn't working.
And as mentioned by many here, and I agree, you're a very valueable piece of the company puzzle. Hell, you're probably the only one that can solve that puzzle. :)
AnswerKenntnis:
This one is all about money. My guess is that as a starter, you are probably too kind for customers who've already gotten more than they paid for.  

Learn to quote a price for new requests.  This is far from easy and customers will often try you out.  If you can, enlist the help of an experienced project/product manager.

Once you think in terms of money, communicating with management becomes a lot easier.  If your current customers provide full time money, you should not pick up new projects.  But you'll understand that management will still try to bully you into doing them.

If you are indeed valuable to the company, you'll gain bargaining power.  You can ask them to hire more people, get fewer new projects, help you reduce maintenance load, or increase your salary.
AnswerKenntnis:
you start using a issue tracker to keep track of your todo list

this will not only help you with keeping track of what is critical but will help the users and your boss with seeing what your current workload is

also should they ever hire a second developer (or you quit and your replacement now takes up your workload) this will help in managing the workload and you'll be able to avoid stepping on each others toes
AnswerKenntnis:
It should not be the decision of your employer to micromanage you to the extent, that you are prohibited from improving the existing code. Use your own professional judgement. When you are estimating work, I would factor in additional time to allow for some refactoring, if it will increase productivity in the future.

Anyway, it seems like you aren't effectively communicating with your employer. 


You have evidence that refactoring will save money. Draft a proposal for a refactoring project and demonstrate how much time and money the business could save. Outline precisely what changes you would make to the code, and how long it would take.
Keep an accurate log to record how much time you spend on coding, meetings, and answering emails. This will protect you if you fall behind schedule.
Slow down. This may seem a little counter intuitive, but your time will only be abused if you do everything quickly. People will respect your time more if you do less. For example, I would only check email once or twice per day. Otherwise, you risk burnout. 
Considering your pay rate, it isn't worth getting a headache over. Make sure you leave on time every day. Don't put in extra hours without additional compensation. The exception is if there are good advancement options, or if the reputation of the company will greatly boost your resume, then you will just have to suck it up.


Without knowing more, I would just suggest that you try to be more open with your managers. Maybe start increasing your task estimates. Constantly remind them about how busy your workload is. Also, you should meet with your boss and explain that you would like a salary increase within the next six months, and ask how you could improve your performance in order to achieve this pay increase. 

Good luck.
AnswerKenntnis:
Yes, you will always have to maintain applications, written both by you and other people. The only exception is if you write a program which never, ever needs any maintenance. So you better get good at maintenance. 

I think there is a subtle hint in your question of a flaw in your approach. That is, that fixing bugs does not involve improving code. 


  But then I got to hear I wasn't allowed to improve the existing code
  and to only focus on bug fixes when a bug gets reported.


I can't believe someone has told you "you must fix bugs without improving the code." This is both difficult and impossible. What you can't do is re-write the application just because you don't like, or find it hard to understand, the approach used in the existing code-base. 

My advice is to learn to refactor. Any time you are fixing a bug, you have an opportunity to improve at least some of the code. How much of the code-base gets refactored depends on what the bug is and how good or bad the code is. But if you're fixing bugs and knowingly leaving code smells all over the place, then you're not doing your job and you're building up technical debt. 

Some bugs are actually fixed simply by refactoring, and sometimes it's useful to refactor in order to help you understand code. This is because refactoring ought to improve maintainability and coherence of the code. 

When I estimate a bug fix, I will usually make a decision about whether a major refactor will be the best way to do it, and take that into account. The same with unit tests. Both these things should be part of the way you write code, not something optional which involves extra time. 

So you shouldn't be asking "can I improve the code when I fix the bug?" Because you should be anyway. You shouldn't be asking "can I use refactoring to fix the bug?" Because if the code which is causing the application to bug out is in dire need of refactoring, you should do it anyway. You shouldn't be asking "can I write unit tests when I fix this bug?" Because you should write a regression test before you even start trying to fix the bug. 

NB: I feel some attribution for this answer should go to Jeff Atwood, seeing as I linked 3 of his articles.
AnswerKenntnis:
In my experience the academic world or the first 6-12 months of a tech-oriented start-up are the only two reliable areas where you'll encounter a true blank slate. They both carry their own costs but if you love to code and are often horrified by the quality of the code you find out in the wild, you should start pointing your career in one of these directions.
AnswerKenntnis:
Since I can't comment yet because I'm a lurker on this stack, I will just add to the info on here.

1) Since you are just starting out, your salary isn't going to be stellar unless you went to work for a big company like MS, Amazon or something akin. But it shouldn't be that of a grocery employee! Don't put up with that for long, gain experience doing what you are doing and move on when a better opportunity arises.

2) for an entry level gig this is kinda normal, your workload is too high but the type of work is expected. To become a better developer you must learn from others mistakes. The more you see, the better you become. But that implies you are looking for things to learn from, not learning bad habits... 

3) The ratio of maintenance to project work should shift over time, if it isn't that means the company you work for doesn't realize how to keep a good developer, they intend to have you do the same thing day in and day out. Make a yearly goal for yourself as to where you would like to be, salary and job expectations wise, and move accordingly.

4) If you aren't happy, leave! Life is too short to deal with stupid people.

All the best.
AnswerKenntnis:
It sounds like your management fundamentally does not respect you or understand your work load. 

You should not be implementing every feature request that comes through. Your manager should act as a buffer between you and incoming requests (except perhaps the most simple of break/fix requests). Then he or she should sit down with you and determine the feasibility and priority for any approved requests. 

Also, you should be making probably 2x (at least) what they're paying you.

It sounds like they probably really need at least 1 more developer to work alongside you, but with what they're paying you that sounds pretty unlikely.

If they are unwilling to adequately pay you or help you manage your workload, I'd be looking for a new job. You want to work someplace where you're part of a team, and where your management works with you to complete your projects. Get off that sinking ship as soon as you can.

Being a hero on a team of one is only going to burn you out.
AnswerKenntnis:
I'm only a student (still) but that's pretty normal (from my internship experience). That's what you get when working in support and web applications. 

I'd advise you to understand what the client (manager) wants before starting coding. That could be tricky because sometimes they don't know it themselves so work with them till they agree on something. Make sure you both agree on the final solution before you code it.

Also if you are a maintainer you can pretty much change anything in the code - just make sure it doesn't change the behaviour or introduce bugs. I expect managers do not 'allow' you to change anything because they are used to and happy with how it is now and they don't want to pay for any new changes.

Finally, don't worry if you can't handle something because you're doing something else. I'd advise you to let people know that you are overwhelmed with work and their request will take time. If you don't managers would just think you're lazy. Do let them know you already have work and they might hire more people. There is no other way for them to know that the work is too much for a single person.
AnswerKenntnis:
Is your manager aware of all these change requests (maintenance requests)? Does s/he realise that your time is taken up sifting through such requests that you have no authority to sanction? Or do you just make changes everytime a manager asks you?

Seems to me like your first port of call is to put it all on your manager's desk. No one should be coming to you directly. Issues should come through whoever fields such - usually a support team. It is normal that you support your code for a short handover period - usually a week or so. Changes should be costed and charged (transfer charging) in any company that would class itself as "medium size", and this it seems is being bypassed (no wonder they are flooding you - you are like the slut at the prom).

There should be a proper request procedure for both raising issues and change requests. Support/Mainbtenance is about fixing bugs and issues (that fit with the original spec but fail because of a bug in the code or an outside event - such as a power down or corrupt upstream system etc). 

If your company offers none of this and expects you to cope and be responsible for this myriad of random requests, then seriously you need to consider moving on. Pay is always poor at the bottom - in my first programming job (almost 25 years ago) I spent 8 years for the same company and my pay went up little (although it was always much higher than a cashier!). Within 2 years of leaving I had doubled it - and tweo years after that I was taking home over ten times what I started at (but was then an independant contractor). As always, earn you spurs, learn your trade, and jump ship to warmer surroundings.
AnswerKenntnis:
The only way to break this chain is to develop new infrastructures that are designed for flexibility and are full unit+integration tested.

If you manage to sell this to the management (sign other developers and managers on to the concept in 1:1 meetings), then you can slowly reach a state, where most of the applications' code is in the infrastructure and it is easy to expand and maintain, whereas the actual applications are light weighted and can be written pretty quickly.

The development of the infrastructure may enable at first replacing parts of existing applications and after a while (may take a few years) replacing entire applications.

In the long run it can reduce development time of new applications and maintenance time of future existing applications drastically.

The new development will require at least 80% dedication (preferably more) with a team of  more that one developer (a few minds are better than one). All the developers will need to be able to think creatively and to break existing preconceptions.

Try defining and high-level designing such a new infrastructure, then present the definition to your peers and to management.

As of your working conditions, ask to lead a new infrastructure team that deals with these issues (based on your definitions and design) and bring on new developers to maintain the old stuff while you assist them if necessary (up to 10-20% of the time).
If the management agrees to the idea, you can ask to renegotiate your terms. Be prepare to find another job if they refuse. (Remember, your job requires skills, knowledge and experience, you are not as easy to replace as they are paying you to believe.)
AnswerKenntnis:
This is a project management problem. Use some kind of project management to decide what is of highest priority to work on.

a) You need a backlog of items to work on. You put your plan for improving the code in the backlog.

b) All the bugs go into the backlog

c) The backlog gets prioritized.

d) You do it all in priority order.

Bugs may very well be a higher priority, but if once you fix all that you have cycles to spend on new features or on refactoring design.

It's easiest if you just do refactoring improvements incrementally, as you pass over sections that have issues/bugs to fix. Then you can tell management, "I had to fix A, but B was fundamentally broken and I had to do solution C to fix it all so that D will be easier/cheaper in the future" Where A=The bug, B=The anti-pattern, C=Solution, D=Future Gains

If you can't justify work as an investment worth doing, business people will never accept it.
AnswerKenntnis:
Perhaps you're a possition to go to manager and say: "Look I'll be frank with you. My pay is terrible, I could get N times this as an entry level programmer at X. I am doing way to much stuff with A, B and C. I can't maintain this. Honestly, and no offense intended, I'm going to walk out of this room with this either fixed, or with my resignation letter left with you. Now that this is all out in the air. How can we work together to make this right?"
AnswerKenntnis:
This is business as usual. You will be exploited as long as you keep working there it seems. Its in the company's best interest to continue going with this model rather than to make you feel happy in what you are doing. When it comes down to it, they don't really care. Its about creating reliable code for them and if you are a one-man-band, they are certainly making bank of you. Why would they change?

The good news in all this is you are a VIP to them even if they don't know it. What I suggest to do is to line up some more opportunities before jumping ship then grab them by the balls and demand a higher salary. If not move to a better opportunity. In my opinion, you should be finding some more exciting work soon. Aim as high as you can. Once you get to a developer shop, you will be much happier like Google or some fun startup where there is a collaborative developer culture where you will truly feel happy.

What I did personally was use a head hunter contractor organization and quickly got many great experiences under my belt, moving from one to another while still maintaining a steady employment from my contractor. It keeps you from getting bored and challenges you. Eventually, in my spare time I built up some small business which blossomed into actual business then I jumped ship from doing contract work.
QuestionKenntnis:
Overcoming slow problem solving due to increased knowledge of what might go wrong
qn_description:
This has been troubling me for some time, and I'd really appreciate the input of other professionals.

Short background: I started programming when my parents bought me my first computer in 1988 (at age 14, I'm 39 now). I followed a couple of other career paths before finally becoming a professional programmer in 1997. Late bloomer, perhaps, but that's how it was. I'm still happy with my choice, I love programming, and I consider myself good at what I do.

Lately, I've been noticing that the more experience I gain, the longer it takes me to complete projects, or certain tasks in a project. I'm not going senile yet. It's just that I've seen so many different ways in which things can go wrong. And the potential pitfalls and gotchas that I know about and remember are just getting more and more.

Trivial example: it used to be just "okay, write a file here". Now I'm worrying about permissions, locking, concurrency, atomic operations, indirection/frameworks, different file systems, number of files in a directory, predictable temp file names, the quality of randomness in my PRNG, power shortages in the middle of any operation, an understandable API for what I'm doing, proper documentation, etc etc etc.

In short, the problems have long since moved from "how do I do this" to "what's the best/safest way of doing it".

The upshot is that it takes me longer to finish a project than a novice. My version may be rock solid, and as impenetrable as I know how to make it, but it takes longer. 

The "create file" example above was just that, an example. Real tasks are obviously more complex, but less suited for a generic question like this one. I hope you understand where I'm going with this. I have no problem coming up with efficient algorithms, I love math, I enjoy complex subjects, I have no difficulties with concentration. I think I do have a problem with experience, and consequently with a fear of errors (intrinsic or extrinsic).

I spend almost two hours a day reading up on new developments, new techniques, languages, platforms, security vulnerabilities, and so on. The conundrum is that the more knowledge I gain, the slower I am in completing projects.

How do you deal with this?
AnswersKenntnis
AnswerKenntnis:
You're not any slower in completing projects. Previously, you thought your novice projects were done when they really were not. You should sell this quality to clients.

"This company might get it done faster and cheaper, but is it really done? Or will you be hunting bugs for years?"

Beyond that, you need to know and accept the old idiom: "Perfect is the enemy of good."
AnswerKenntnis:
Sounds like it's time for you to join the dark side: management.

I'm not suggesting you give up programming and become a manager. But it seems like all the experience you've quoted up until now has been technical in nature. In simple operation of writing out a file, you can think of 10 different aspects that a less mature developer would never consider. Not necessarily a bad thing, but...

The dark side is all about present value. It's about making minimal investment for maximum gain (cost-benefit analysis). In business everything boils down to how much will this cost me, probability of success, probability of failure, probability of spectacular failure and potential gain. Do the math; act accordingly.

This works just as well when you are a developer: create a temporary file, ignoring permissions and name collisions - 5 min. Net gain, the rest of the team can start working on whatever code depends on the presence of that file. Is it a perfect solution? Absolutely not. Will it get you 99%, 95%, maybe 90%? Yeah, it probably will.

Another question to ask: How do you feel about technical debt? Some people think it must be eliminated. I think those people are wrong. Just like in business, technical debt allows you to borrow "cash" or "time" in order to deliver something sooner.  What's better: A perfect solution in 2 years or a complete hack that a customer can use and purchase in 4 months? Every situation is different, but in some situations if you wait 2 years, your customer will already sign up with your competition.

The key is to manage technical debt the same way a well-run business manages financial debt. If you don't take on enough, you are not getting optimum return on investment. If you take on too much, the interest will kill you.

So my advice: Start evaluating your work based upon whether you're maximizing your value instead of maximizing your thoroughness. And if you practice this, you will develop the same intuition that you already developed in your technical area.

As a side note, I recently started doing the pomodoro technique and it has helped a lot. Instead of going on a tangent of a tangent, focus in small time intervals and then allocate pomodoros for future work/research. It's amazing how many times I made a note to research a topic but an hour later when the time came, I thought to myself, "there's at least 3 other things I could do with my time today which are all more valuable."
AnswerKenntnis:
I had the (likely) same problem many years ago, it lasted for a few years and I overcame it. So maybe it would be of some interest to you to know how I achieved that, even if I'm not sure my way will also apply to you.

You should also have a look here : The Seven Stages of Expertise in Software Engeenering It shows that productivity is in great part a side effect of skill level. It may be that you are still at some point between stage 3 and stage 4 on the technology you're currently using (skill proficiency depends on technology, you can be master of some technologies while still learning others).

Now I start with biographic testimony. 

A bit of context. I'm 47. I started programming at 12 in the 80's. While in high school I also worked as a part time professional game programmer. Basically it didn't got me that much money, just enough to buy hardware, but I enjoyed it and learned much. At 18 I started a formal learning of Computer Science. 

As a result when I turned 20, whenever starting any programming task I knew many ways to solve the given problems and was very conscious of the many parameters and pitfalls at hands, and drawbacks and limits of any method. 

At some points (say about 26 years old) it became really difficult for me to write any program at all. There was so many possibilities opened that I was not able any more to choose between them. For a few years (make it 6) I even stopped programming and became a technical news-writer.

I never totally stopped trying to program nevertheless. And at some point it came back. The key for me was extreme Programming, more specifically the Simplicity principle: "Write The Simplest Thing That Could Possibly Work".

Basically I forced myself not to care about code efficiency (that was my main roadblock, avoid inefficient designs), but just go the easiest way. I also forced me to care less about errors, and delay error handling to a later time, after writing tests raising the error (really that's TDD).

That's something I learned when I was writing. When I do not know what to write, or I knew what I was writing was bad. Just go on. Actually write the bad stuff. I will eventually correct it later. Or if it's really that bad erase it and rewrite it, but it's faster to write things two times that write anything perfect the first time.

Really it is very likely that a code you believe is good at first writing will need as much improvement as a really bad one.

If you follow the Simplicity path you also get an added bonus. You easily accept to remove/change initial design/coding. You get a more flexible mind.

I also got in the habit to put a temporary comment in code, explaining what I was not doing now and intended to do later when the code would be functional in the normal use case. 

I also attended an XP Dojo an practiced code katas with other programmers to internalize the XP practices. It helped.  It made the above formal methods instinctive. Pair programming also helped. Working with young programmers gives some momentum, but with experience you also see what they do not. For instance in my case I often see them engage in overly complicated designs and I know the design nightmare that may lead to. Gone that way. Did that. Had problems.

The paramount point for me is keeping the flow. Being fast is really succeeding in keeping the flow.

Now I'm back as a professional programmer and I believe I'm both better and faster with a deeper understanding of what I'm doing. Practicing TDD I may still be slightly slower than when I was a young bull (and tested nothing), but I also have no fear refactoring and certainly spend much less time debugging (nearly no time at all, make it less than 10% of time).

Summarily: I overcame my codeblock using agile methods (XP), going for simplicity then refactoring, and practicing to make that instinctive. It worked for me. Not sure it can be generalized to anyone else. 

In terms of skill acquisition level, I mostly learned to accept that every time I change technology (learn new language, new framework, etc.) I will go through a stage when I'm slowing down. This is normal and will eventually overcome that. I also can compensate for that with good methodology and general purpose programming skills and It won't be as much a problem.
AnswerKenntnis:
An important part of programming is managing and controlling complexity and for me personally, it is one of the top issues. If ignored then either the frequency of deficiencies surges or, as in your case, the ETA of finished software increases dramatically. 



Software complexity can be controlled and managed from many different levels and ways but a good rule of thumb to get some perspective is this: "Top priority of any software project is customer satisfaction which is a function of their expectations."

In other words, software complexity depends a great deal on how skilled you are at controlling the expectations of your customer. 

When one adopts that view, then two important points become obvious:


customer expectations must be made explicit (in whatever form);
customer expectations can always be modified and that is done through the art of negotiation. 


Your example is a very good one, "simply write it" vs "myriad of other considerations". Think about it - if someone were to write down exhaustive requirements for both variants, can there be an equivalence in described features or not? 

Building an F16 is different from building a Cessna although both of them can fly.
AnswerKenntnis:
The simple answer is: accept it.

In all systems there's trade-offs to be made between reliability, robustness, security, speed, hardware cost, development cost, time to market, you name it. You will not always agree with how the customer makes those trade-offs, but you're not the one making those decisions. All you can do is provide a considered opinion. Software development covers the full gamut, from "my personal web page" to "run the nuclear reactor", and the code has to be written accordingly. If a crash means "reload my personal web page" then who really cares if that happens? But if a crash means "Chernobyl" then your preference for solid code is if anything a bit casual for my liking.

There are some clients who will happily accept "Proof of Concept" level code and run it in their live systems, and often they have systems administrators who are well used to that. IME their systems are usually unavailable for an hour or so in the middle of the night while a bunch of scheduled restarts happen. But they've made the business decision that that is how they want to roll. Ideally because their field is so fast-moving that high-quality code would never be ready, but often because they can't see the value (if a system "just works" they never notice it, therefore that system does not represent value for money). If it bothers you too much, try to avoid those clients.

Keeping up to date is time we all have to spend, or at least should spend. Sometimes that will get you work, other times it just keeps your mind in good shape. Either way, try to enjoy it.
AnswerKenntnis:
The truth is that modern systems are becoming increasingly complex. The computer is now similar to that game "Jenga" where you have all of these pieces relying on many of the others. Pull out the wrong piece and you have an error, pull out a correct/necessary piece and you still may produce an error. The more complex the system the more time you are likely to spend thinking of ways to make your code more robust, and hopefully more secure as well. Speed would be nice, but I think speed takes a back seat a lot these days when you hear in the news that "XYZ" company was hacked into and 6 million customer's credit card numbers were stolen.

Your clients may not want to hear that their program needs to be secure, robust, etc.. But you could tell them that their car does not need seatbelts and airbags nor their house need locks and doors... because who wants to hear all that?

If you are over-thinking you are going about things the right way, except that you need to pick a strategy that seems "concrete" and just go with it.
AnswerKenntnis:
It sounds like your skills would be very useful for very high quality mission critical systems development, like finance/trading related applications, broadcasting, aerospace, defense...

Errors in these sort of applications are very costly and they employ people who think like you as you can cover all the cases.
AnswerKenntnis:
Only thing I can see is: "You are becoming more and more valuable".

As and when you get more experience you learn about things you should avoid, and this is what make you better than others.

One thing you would have noticed that your code would be safer and more maintainable now.


Only thing you need to do is to explain your client why it took time and how it would be useful for them.
You need to show them depth of your knowledge. 
You need to tell them why you did, what you did and how it would matter them and their business.


My suggestion would be to concentrate on this part.
AnswerKenntnis:
when in doubt default to badly quoting Knuth...

"Premature optimization is the root of all evil."

Here is what I would suggest, as it seems like you have a problem that I have from time to time...

What really works for me...


Write the unit tests, as if all the code was done.
document the interface.
implement the interface.


what you have really done:


work through the model layers requirements
really set up the division of work, which objects are responsible for what
get to work in an environment when you can actually step through the working code, which makes things so much faster an more accurate...


also rely on asserts in early development... then figure out which remedies need to be implemented and you wont write code that is unreachable, or hard to test.
AnswerKenntnis:
It sounds like you're aware of your tendency to think about everything that can go wrong.

Experienced Cautious Developers often learn to follow the mantra YAGNI, you ain't gonna need it, when they try to return to a lean, agile and productive workflow after getting too choked up in the weeds of failure-mode-analysis-gone-amok.

However, if you are indeed writing something in a domain where that level of care is no less than what Professionalism demands, then you should realize that your "velocity", your "productivity", in net terms, is measurable by how much good (or harm) you are doing to your company, your customers, and the software suite, or product family you are building or maintaining.

Remember to:


Include total cost of maintenance, total cost of ownership, and total cost of deploying and maintaining solutions when you consider changes in your approach.  Going faster and making more mistakes may or may not make things better.
If you work in a good company, you can probably discuss this in your own team, and with your own supervisor, without it being a Career Limiting Move. If you can't, now is a good time to find that out, and find a new job.
AnswerKenntnis:
I think you should stick to your coding standards, but make sure you are up-front with your clients. Many clients do not know what it takes/costs to build good software. It's part of the professional developer's job to educate them.

Whether you're agile or waterfall, you get some sort of agreement from the client about what they expect the application to do. Too many developers (OK maybe more salespeople) are guilty of sandbagging. "They didn't say they wanted a highly secured website." In most cases, it's because they weren't asked. "Do you mind if your ecommerce site gets hacked?" Of course they care and why would you build it to let anyone penetrate the security? You have to educate them.

Just make sure you're doing only what the client is paying you to do. Part of your service is your experience. Clients expect you to know the pitfalls without them having to ask. It's up to them to include the requirement. You may want to pass on clients that want something cheap.
AnswerKenntnis:
Think about the practical consequences of a bug as compared to all the other problems that need solving.

Consider the following consequences of creating a poorly written piece of code:


Entire database gets dumped every other month. 48 hours of downtime while the backups are restored.
Customer records get cross-linked. $200 worth of orders get shipped to the wrong customers per month.
A order gets stuck in a wrong status once a week. Order ships but warehouse to has to call helpdesk every time it happens.
Once very two weeks or so, the app crashes and user has to re-enter 2 minutes worth of data.
Once a month, the app hangs on startup. User has to kill process and start over.


The first one is obviously unacceptable. #2 - #5 may or may not be, depending on the nature of the business. #2 - #5 need to evaluated in the context of other problems the business is facing.

Ideally, #2 - #5 would never, ever happen. In real life, with conflicting priorities, the people signing your paycheck might want you to be working on other things rather than writing perfect code that never, ever has a problem. 
They aren't going to be impressed if #5 gets fixed at the expense of not fixing a more serious bug in another program.
AnswerKenntnis:
I think you need to learn to decide how much needs to be done for which project. Some project may be trivial and you really don't need to spend all that time in perfecting it. Not everything is gonna need rock-solid encryption nor everything will be scaling to million users.

Writing a program which can scale well for more than a million users will take time and experience which you have now, but if you know that your application is not going to be used by more than 1000 users max, there is no point in spending all that time perfecting it.

I think this is an important stage in your programming career and now you need to goto next level, which has to do with maturity and not programming. You need to be able to correctly decide on how much time and code should be enough for any particular project.

And like everything else, you can attain this as well with practice. So try to decide this before you start a project, sometime even while you are already working on it, with experience you will get past that as well. There may be a few hits and misses in the beginning but with time you will perfect it (decision-making, not code).
AnswerKenntnis:
@Zilk, I am not great programmer and I am been programming since 1998.  Even I am facing this issue now.  But what I realized is ultimately quality matters.  If I die today, somebody should be able to pickup what I am doing now from where I have left.  Such should be the standards of programming (Universal).  

I have moved myself from developer to architect now.  Moving to Management is changing the line.  If you want to continue with your passion you can move to become Architect.  

Initially as Technical Architect-->Solution Architect-->Enterprise Architect-->Chief Architect and so on.  

As an Architect you will be guiding people to success.  People like you who have been programming for decades those years of experience you can utilize to guide others.  

Like a bird higher it flies more land it can see so is your experience.

Let me also tell you programming correct implementation is important than programming a wrong implementation faster.  Recently one of my juniors programmed something wrong and it cost a bank lots of money.  Ofcourse we had delivered on time earlier but that was no use!  Was I given the role to guide even though the same junior would have coded that problem would not have happened.  I am giving this example to stress that giving good guidance is also important.  Some call this job as Consultancy.
AnswerKenntnis:
The solution is to create a collection of libraries with commonly used functions which you can re-use across projects. E.g. I have a StringFunctions.dll .NET library which does stuff like encoding, encryption, decryption, regular expression evaluation, etc. This way, I don't have to continually rewrite things that don't change.

Having a wrapper for file creation tasks also makes lots of sense. Your library could expose a method called GetFile() which does all the checks for you and returns either null or a file (or whatever you deem useful).
AnswerKenntnis:
Another option is: stop writing code, instead sell your expertise in spotting the problems in advance.   

In other words, become a Consultant.   

Many organizations are happy to pay expensive dollars (if not top-dollar) for someone to spot the issues before spending months on creating the code that makes the problems. It is well known that fixing a bug in design, is orders of magnitude cheaper/easier than fixing it after it has been coded, tested, and deployed.   

You won't be writing as much code, and you may likely miss that, but then it seems that the actual lines of code are not your core strength, but in knowing which lines of code should be written - and which shouldn't.   

Focus on your strengths.
(well, if that's what you enjoy...)
AnswerKenntnis:
Don't be too hard on yourself.  You are working in a profession of increasing complexity, that requires more human intelligence, knowledge and experience than ever before.

Computer processing power is slowing - perhaps soon stalling - leading to the need to introduce multi-core chips, gpu powered numerics, parallelism, etc.  There are only so many transistors that can be placed on a chip.

Therefore the big advances presently and in the future are going to come from programmers - advanced algorithms and more efficient code.

If you look at GTA 4 and GTA 5 the differences are astounding.  But they both run on the same hardware.  This is the result of some very intelligent and advanced programming practice that simply was not required, or available, 10 years ago. 

It could also mean that experienced programmers may become more valuable in the future - much like other professions such as engineering where peak earnings typically occur late in the career.
AnswerKenntnis:
My best recommendation for you is: building blocks.

Make a file building block that you can trust always, make one for your API, stop wasting your time writing the same thing over and over. Think about every problem once and fix it once and for all.

Noone will catch up to that, certainly not the novice who spend 80% of their time debugging code that fails for corner cases they don't understand.

Most of all, don't fix problems that cannot happen, such as wrong permissions.

If the permissions are wrong, something is already wrong and you should fix that rather than making your program bullet proof. 

At some point you have to just not shoot yourself in the foot instead of always checking whether you did or not.

Instead of spending time on documentation, spend time making your code self-documenting and as short as possible. Replace all those duplicate-ish functions. Shrink your library, rename things with precision.
AnswerKenntnis:
Knowing all the possible criteria's while developing app is the most important thing in developing a quality product. You are doing good in this. One thing you can do is, You can categorize the requirement into quality level then map the level with the given deadline. In this way to you can meet the project deadline easliy.
AnswerKenntnis:
In the words of Edsger Dijsktra: ΓÇ£If debugging is the process of removing software bugs, then programming must be the process of putting them in. ΓÇ¥ You are just doing less of the latter so you have to do less of the former. It's just a question of learning to spend time coding it right. I am still a relatively young programmer (read 20ish) and I aspire to be able to code something completely right once. An hour of planning and 10 minutes of coding is way better than an 10 minutes of planning, an hour of coding, and three of debugging.
AnswerKenntnis:
Just like you, I started programming at the age of 14, also when I got my first computer (although I had been studying for a few months at that point).
However, I am "only" 33 now. :-)

My suggestion is that, when developing something, you take each one of those worries (file permissions, number of files in a directory, etc.) and then use all of your vast experience to answer a few questions about it, in this spirit:


How long would it take to handle that issue properly in your code?
If you don't handle it properly, how likely it is that this thing will bite you later?
If it does bite you, what are the consequences?


Armed with those answers, such an experienced guy will not have problems to make a wise decision. ;-)

It is the responsibility of "veterans" like you to come up with this type of requirement, and that involves both identifying what can go wrong and deciding which potential problems you should give attention to.
AnswerKenntnis:
You have very vast experience in programming. From your question it seems you are still providing services. I am a freelancer too so i understand your difficulty to make your clients understand the technical delays in the project. As your experience is the person of your caliber should have put your experience into your own product. Now you are like most senior programmer, from my point of  view if you are still providing services than you are wasting your experience and most importantly time. Implement your own ideas make your own products.
AnswerKenntnis:
I hate to bring this up, but you're also hitting that age where you naturally start slowing down. You almost certainly can write more robust and probably more elegant code than all those young guys, but they will always be faster and know all the latest technology.

If you've ever thought about management, this is the time to start migrating. Otherwise, you need to find ways to trade on experience rather than coding speed. Requirements gathering, proposal, design, solution architecture, test strategies, etc, and all the other bits of the SDLC are areas for you to explore. Coding is just one small part of the software development world.

On a recent small project I did in the energy sector, I spent a week gathering requirements and designing/documenting the solution, a day coding it, and a week testing and delivering it. Think about that and where the value is going.

Trade on the fact that your experience means you can look at a given problem and think of ten different solutions, all with their inherent strengths, weaknesses and potential pitfalls, which you understand ahead of time. Get in the position where you're influencing technical decision making long before coders hit keyboards, and you're still there long after the dev team has been disbanded.
QuestionKenntnis:
My customer wants me to record a video of how I develop his software product
qn_description:
Working as a freelancer, I often see strange requests from my customers, some of which can negatively affect my daily work┬╣, and others trying to set some sort of control. I usually encounter those things during preliminary negotiations, so it's easy enough at this state to explain to the customer that I do care about my work and productivity and expect my customers to trust my work.

Things were much harder┬▓ on a project I just accepted, since it's only after the end of the negotiations (the contract being already signed and not mentioning anything about video tracking) and after I started to work on the project that my customer requested that I record a video of all I do on my machine when working on his project, that is, a video which will show that I move the cursor, type a character, open a file, move a window, etc.

I work in my own company, using my own PCs.

I answered to this customer that such request cannot be accepted, since:


Hundreds of hours of work on a dual-screen PC will require a large amount of disk space for the recorded videos. If I don't care about space, I do care about this customer wasting my bandwidth downloading those videos.
Recording a video can affect the overall performance and decrease my productivity (which is not actually true, since the machine is powerful enough to record this video without performance loss, but, well, it still looks like a valid argument).
I can't always remember to turn the video recording on before starting the work, and off at the end.
It may be a privacy concern. What if I switch to my mails when recording the video? What if, to open the directory with the files about this customers project, I first open the parent directory containing the list of all of my customers?
Such video cannot be a reliable source to track the cost of a project (I'm paid by the hour), since some work is done with just a pencil and a paper (which is actually true, since I do lots of draft work without using the PC).


Despite those points, the customer considers that if I don't want to record the video, it's because I have something to hide and want to lie about the real time spent on his project┬│.

How to explain to him that it is not an usual practice for the freelancers to record the videos of their daily work, and that such extravagant requests must be reserved to exceptional circumstancesΓü┤?



┬╣ The most frequent example is to be requested to work through Remote Desktop on a more-than-slow server which uses a more-than-slow Internet connection, or to be forced to use an outdated software as Windows Me without serious reasons as legacy support.

┬▓ In fact, I already did a lot of management and system design related work, which is essential, but usually misunderstood by customers and perceived as a waste of time and money. Observing the concerned customer, I'm pretty sure that he will refuse to pay a large amount of money for what was already done, since there is actually zero lines of code. Even if legally I can easily prove that there was a lot of work on design level, I don't want to end my relation with this customer in a court.

┬│ Which is not as risky as it could be, since I gave to this customer the expected and the maximum cost of the project, so the customer is sure to never be asked to pay more than the maximum amount, specified in the contract, even if the real work costs more.

Γü┤ One case when I effectively record on my own initiative the video of actions is when I have to do some manipulations directly on a production server of a customer, especially when it comes to security issues. Recording those steps may be a good idea to know precisely what was done, and also ensure that there were no errors in my work, or see what were those errors.



Update:

First of all, thank you for all your answers and comments.

Since the question attracted much more attention and had much more answers than I expected, I imagine that it can be relevant to other people, so I add an update. First, to summarize the answers and the comments, it was suggested to (ordered randomly):


Suggest other ways of tracking, as shown in Twitter Code Swarm video, or deliver a "short milestone with a simple, clear deliverable, followed by more complex milestones", etc.
Explain that the video is not a reliable source and can be faked, and that it would be difficult to implement, especially for support.
Explain that the video is not a reliable source since it shows only a small part of the work: a large amount of work is done without using a computer, not counting the extra hours spent thinking about a solution to a problem.
Stick with the contract; if the customer wants to change it, he must expect new negotiations and a higher price.
Do the video, "but require that the customer put [the] entire fee into an escrow account", require a lawyer to video tape all billable time, etc., in other words, "operate in an environment void of trust", requiring the customer to support the additional cost.
Search for the laws which forbid this. Several people asked in what country I live. I'm in France. Such laws exist to protect the employees of a company (there is a strict regulation about security cameras etc., but I'm pretty sure nothing forbids a freelancer to sign consciously a contract which forces him to record the screen while he works on a project.
Just do and send the videos: the customer will "watch a few ten second snippets of activity he won't understand", then throw those videos away.
Say no. After all, it's my business, and I'm the only one to decide how to conduct it. Also, the contract is already signed, and has nothing about video tracking.
Say no. The processes and practices I employ in my company can be considered as trade secrets and are or can be classified.
Quit. If the relation starts like this, chances are it will end badly soon or later. Also, "if he's treating you like a thief - and that is what he's suggesting - then it's just going to get worse later when XYZ feature doesn't work exactly the way he envisioned".


While all those suggestions are equally valuable, I've personally chosen to say to my customer that I accept to do the videos, but in this case, we must renegotiate the contract, keeping in mind that there will be a considerable cost, including the additional fee for copyright release. The new overall cost would be in average three times the actual cost of the project. Knowing this customer, I'm completely sure that he would never accept to pay so much, so the problem is solved.



Second update:

The customer effectively declined the proposal to renegotiate the original contract, taking in account the considerable additional cost.
AnswersKenntnis
AnswerKenntnis:
(Or, the flip-side of my previous advice...)

You stop giving protestations, and say yes.

"Yes, I would be happy to write a new contract for these additional deliverables.  Project-complete tutelege in my proprietary tradecraft is valued at (value of my projected income for the next $N years).  There will also be a licensing fee $Y, for physical file ownership rights.  If you would like to also own the video's content, I'll get back to you shortly with an additional fee for copyright release."

Lest you think that preposterous: seriously, what price makes it worthwhile to risk your business?


A competitor could use that video to criticize, mimic, or undercut your practices.
The client could edit it to make you look dishonest.
You've sacrificed the potential to monetize your business through video tutorials if he chooses to post excerpts of this one for free (or heck, what if he sold them?).


Value of a work product is not equal to the value of (work product + expertise + work processes)

An employer gets to own and direct all of these.  A client only gets to ask "Do you offer__, and if so what do you charge for it?"

So, yep, these are reasonable terms for accommodating an unreasonable request.

BUT unless he accepts those terms and without further howling, I still say a flat "no" is the most persuasive you can possibly be that what he wants is infeasible.
AnswerKenntnis:
You don't explain; not further, not at all.  You just say no.

This is your business, and your choices about how to conduct it are not up for discussion.  The terms of any contract are up for discussion; before signing, that is.

He's giving you multiple big red flags that this contract will be a miserable experience, that it will continue to be so even after you've billed for it, and that he will have nothing but mistrustful things to say to others about your work.  Smile, because you're fortunate that he's communicated this before you're stuck with the jerk.

Thank him graciously for his time, return his deposit, and kick him to the curb.

Really, he's never going to be persuaded of your professionalism no matter what you tell him.
AnswerKenntnis:
I wouldn't deal with this guy, period.  It sounds like simply doesn't understand that much of the job is thought.  If you supplied him with the video he's going to nitpick all the time you spend ignoring him (thinking about the situation.)
AnswerKenntnis:
I think the biggest problem (other than having an insane customer) is that the arguments you make are weak:


  
  Hundreds of hours of work on a dual-screen PC will require a large
  amount of disk space for the recorded videos. If I don't care about
  space, I do care about this customer wasting my bandwidth downloading
  those videos.
  


Disk space and bandwidth really shouldn't be a concern. You'll bill both of those at a significant markup in addition to the hourly rate you already negotiated.


  
  Recording a video can affect the overall performance and decrease my
  productivity (which is not actually true, since the machine is
  powerful enough to record this video without performance loss, but,
  well, it still looks like a valid argument).
  


It's not a valid argument because, as you admit, it's simply not true. It may be a plausible argument, but you're trying to build trust with this client rather than undermine it, right?


  
  I can't always remember to turn the video recording on before
  starting the work, and off at the end.
  


Counterargument: How are you keeping track of the time that you bill? You should be marking time when you start and stop, not trying to figure out how many hours you worked after the fact. Just make the video part of your process.


  
  It may be a privacy concern. What if I switch to my mails when
  recording the video? What if, to open the directory with the files
  about this customers project, I first open the parent directory
  containing the list of all of my customers?
  


You should be able to handle that. Don't switch to your personal e-mail when you're supposed to be working on the project. Use an alias to get to the project.


  
  Such video cannot be a reliable source to track the cost of a
  project (I'm payed by hour), since some work is done with just a
  pencil and a paper (which is actually true, since I do lots of draft
  work without using the PC).
  


Your billing process should be a separate matter. If there's a requirement for video of all billed time, that should absolutely have been part of the original contract. So you're right on this point: the video is not the source of billing.

The best argument, IMO, is simply that recording every second will make you feel like you have someone watching over your shoulder all the time, and that's not something you're comfortable with. If your client doesn't trust you to work and bill in good faith, he or she should pay for what you've done and find a new contractor to finish the work (with the understanding that very few professionals would work under the required conditions).
AnswerKenntnis:
The client doesn't understand software development if he thinks he needs a video of your work. A good programmer will generate the most value for the customer when they don't appear to be doing anything with the computer at all.  Maybe he'd like you to start billing extra for those times when you invariably think of a solution to a problem during your personal time, or while browsing Stack Overflow looking for something else.

The privacy issue (your personal e-mail being recorded) alone is enough to flat out refuse this request.
AnswerKenntnis:
Do it but require that the customer put your entire fee into an escrow account; otherwise, how will you know he will pay you? 

The escrow account should be created by a lawyer who will video tape all billable time spent on the contract.

The client must record all time spent approving the software. Preferably one video file per requirement.

If you're going to operate in an environment void of trust, you may as well go all the way. 

Maybe you can just have a Nanny-Cam taped to the top of your head?
AnswerKenntnis:
"How to explain to him that it is not an usual practice for the
  freelancers to record the videos of their daily work, and that such
  extravagant requests must be reserved to exceptional circumstances"Γü┤


Ask your customer: if you were an employee and not a contractor, would he stand over your shoulder and watch your work all day, every day?  The answer is obviously no.  It is a waste of time (yours and his) to record everything that happens on your screen.

You need to address the root cause of the problem.  Your customer apparently doesn't trust you and thinks you are ripping him off.  Since you've already quoted him a likely and a maximum price, you need to tell him: "You signed a contract based on a known price.  That's how much it is going to cost you regardless of what you perceive my productivity to be."  

Give him an option to buy out your contract based on the time you've spent so far.  If he is that concerned about you ripping him off, he might consider that sunk cost to be worth it.
AnswerKenntnis:
Yeah - absolutely not. My first instinct is to walk away - at such an early stage of the process, if he's treating you like a thief - and that is what he's suggesting - then it's just going to get worse later when XYZ feature doesn't work exactly the way he envisioned. Not doesn't work to spec, doesn't work to what he thought the spec should be.

If you absolutely can't walk away (we all have rent to pay), I'd suggest forcing the client to confront the monetized burden of his request. Create a set-up where all your concerns are solved. If he wants to pay for a dedicated machine, so there's no potential breech of information between other clients, the storage necessary to record said hours upon hours of video, and the administration support costs necessary for processing said video, then perhaps consider it. But if he wants you to document all your work, on video, for free - no way.
AnswerKenntnis:
Why would you bother accepting a contract with such a pesky customer? If they don't trust you before you've committed to the deal it's not going to get better.

It's quite possible your customer has been burned in the past, and that's something you can sympathize with, but you need them to understand that programming is intellectual work, and time at keyboard isn't necessarily reflective of work product or value generated.

I've seen that certain low-end freelancing sites like odesk have software that they encourage freelancers to use which samples occasional frames from your desktop and allow the companies you engage with to see this video, but for me that seems at best a means to prove that you're not spending all your working hours browsing news and entertainment sites. It certainly can't prove whether you're "working" or not until the software can read your mind.

Sane customers will tolerate any of the following as a means to build trust:


A short milestone with a simple, clear deliverable, followed by more complex milestones. Offer the option of canceling future milestones if the cost or results don't meet expectations.
Lawyer-style billing with work items documented with the resolution defined at an agreed increment (6 minute, 15 minute, 30 minute, 1 hour), presented frequently (weekly or monthly). Ability to cancel future work at any time.
An upper limit on hours for any specified work item, after which you agree to discuss any revised estimates based on the new facts that come to light (car mechanic style).


Once you establish a track record with a sane client, you won't need Orwellian monitoring techniques to make them happy. If you present yourself professionally and you can deliver the value the customer needs on a timely basis, nobody will care whether you were typing code at 80 wpm or thinking.

Personally, I prefer to work on projects that timebox deliverables (What can we achieve over the next 3 weeks or 4 weeks), and work on improving velocity as my team gets to know the business problem. In such a case, the customer has the ability the constantly reevaluate whether you're making forward progress or not and whether you're worth the money. I suppose this is why I work as an independent contractor than as a freelancer, and I tend to take on complex business problems rather than things with a "make me a web page" kind of scope, but in my world nobody worries about seeing what's on my screen every waking minute. If a customer had time to review every minute of video generated they'd have a pretty poor performing business.
AnswerKenntnis:
Even if you are working as a freelancer, you need to maintain your work ethics and culture. If possible ask your customer to find another developer. Never entertain such requests.
AnswerKenntnis:
I've run a consulting firm (12 people) and been a freelancer for 16 years.  I've dealt with many, many sizes, shapes, and kinds of firms.

Believe me on this one: any firm that makes such a request has control & trust issues, and this can only end badly.  The relationship is already precarious, I would even say damaged.

If you have the luxury, I'd consider running away from the client & the project as fast as you can.  If financial or contractual concerns take "firing the customer" off the table, I feel for you.  As others have said, this is a new contract: negotiate more money, etc. or just say that you cannot continue under any terms not stipulated in the original contract.

Some of the best business decisions I've made in my life have been when I've fired unreasonable/extremely difficult/impossible to satisfy customers.  Doesn't happen often, especially now (my radar is more finely-tuned than it was when I started), but you have to know when to "cut bait".
AnswerKenntnis:
It's very simple. The answer is no. The negotiations are done.

If he is concerned that you are going to rip him off, then you can find another way to satisfy his concern. With milestones, payment schedule, delivery of source code for him to inspect, etc.

If you can't find a way to satisfy him in a way that makes sense for you, then don't do the job. This world is filled with millions of clients which are less trouble and more sane. Kick him to the curb now before he causes trouble later.

One of the main benefits of freelancing is that you choose who you work with. You don't work with every person who says that they will pay you. You work with who you want to work with. If you don't like them then tell them no.
AnswerKenntnis:
I'd simply argue, that it is not feasible. The biggest concerns are the privacy, and intellectual property of your company and other customers; and the fact, that time spent at the whiteboard, in a meeting, etc. is not recorded.

Sometimes you may need to consult a book, ask someone in IRC or even here, on one of the Stackexchange sites. Sometimes you need to make a call, talk to on-site admins in data centers, etc.

What if the video leaks into the internet, or gets otherwise stolen? In case he doesn't believe you, you could offer code metrics, and have them scrutinized by a 3rd party expert. This however, will add significantly to the cost, if not doubling them.

If he can't understand this, and can't be convinced otherwise, I wouldn't work for him at all. That kind of employee supervision is forbidden by law in Germany.

Before you start working on the project, you'd make a complexity analysis of the main identifiable sections of your project. Those will tell you the man-hours you need to implement or integrate parts of the program. If you stay around that time frame, there is nothing to argue about.

Lawyers, bankers, etc. work in a similar manner. They don't video-blog themselves when they do work for you either...
AnswerKenntnis:
Is there any room for a compromise? 

Maybe you could provide the client with repository logs or a local file history (as provided by Eclipse, I guess there are also stand-alone tools for similar tasks out there). This might satisfy them while not affecting your actual workflow too much...
AnswerKenntnis:
Using a program like AutoScreenShot, you can easily make a (sped-up) video of the development process.

It doesn't take up that much space (one small ~80kb jpeg every 30 seconds), and you don't have to worry about turning it off because keeping it on helps with all sorts of things (proving the time it took to develop, if need be; going through your day to see all the things you wasted time on; etc).

I keep it on all the time.  A month's worth of PC use takes up about 3GB (after which it automatically gets deleted).
AnswerKenntnis:
Many people have come up with valid arguments (including yourself) to try to dissuade the client from requesting this video. But you needn't concern yourself with such things.

Businesses all operate differently. They have different processes and practices they employ to gain an edge, or just generally generate an income. All these practices fall under what is commonly known as "trade secrets" or "classified information" depending on your region.

These secrets are protected by law (check your region for more details) since a business' competitive edge, and thus livelihood, rest on them.

Let's take an example: Apple Inc. They have the most stringent security of any company in existence. They take the protection of their trade secrets with the utmost seriousness. One way they protect their secrets is to issue NDA (non-discolure agreements) to many that do business with them. Even iOS and OS X developers sign these agreements. This ensures that whatever bit of information that is not made publicly available will remain classified. Now imagine asking them to produce video of how they make their products. You'd be laughed out of their offices.

Recording the process by which you write code and design programs most definitely falls under classified information.

You needn't explain yourself to the clientΓÇöthey obviously don't have a good understanding on how business works. Simply inform them that you will not reveal your trade secrets (for obvious reasons) under any circumstances. They can then choose to end the contract or carry on with the work. But what's important here is not that they'll smear your good name or label you "shady" (I'm sure any intelligent person would balk at such an lunatic demand), but that you protect your business.

In closing, the protection of trade secrets are implicitly protected by law. They do not need to be included in your contract or reaffirmed in writing.
AnswerKenntnis:
The problem is, you've chosen with your customer that you will be payed by hours, not by effects. This is very attractive to programmer usually, because he need not to analyze, how much the development will take and he takes no risk of under-estimating the costs.

Otherwise, however, the problem is you will actually earn less if you do your job faster. It gives no motivation to make things quick done. From the customers point of view everything is OK as long as he sees the progress and the total cost will not exchange the amount of what he 'thought it would cost'. 

Propably this amount was exceeded and now the customer thinks you're billing him for more hours you actually works on the project. He pay you for hours, and when he questions the hours specified, you have to prove him that you've worked f.g. 100 hours, and not 50. In fact, pay-for-hours is not as attractive settlement method for developer as it would see on begin.
AnswerKenntnis:
ODesk provides the ability for your customers to see what you're doing. It may be worth investigating as a compromise. 

Personally, I think it's a terrible idea. It's highly unlikely that your customer will watch the video, and if this is the level of micro-management you're getting now, then the amount of micro-management you'll get later is bound to increase. Unless you need this work, I would run away now.
AnswerKenntnis:
Well, I agree with @cczona on both her answers. Also, I'm here to suggest an alternative, besides the video thing:

What if you suggest him that you can keep track of what you've been doing?

I.e. you make a log in (when you start working every day) and a log out (when you stop it, i.e. you could just take note of the hours you spent working that day). Besides, you write a little abstract containing your work-day, your accomplishments and so on, the topics you moved inside the code. Exactly what you did for that day, point approximately how much time you spent in each step. That's a little reasonable and wouldn't cost you so much disk space, nor privacy, nor the negative points listed by @cczona, since you receive your payment by the worked hours.

As a professor, I work with a system where it's mandatory to note the beginning and the ending of the classes, as well as an abstract of them...

If you're up this alternative, and he accepts it, then it's ok. Otherwise, just say farewell.

Good luck!
AnswerKenntnis:
Just to add another view, which is much easier, for your next request in line of this one:

Use a scm (like git) like you always do with your projects (right?). Hand over an export of the full repository, including the history, on delivery. This will be


cheap in terms of time (you already do it)
cheap in terms of disk space
easy to make an overview of (number of commits, dates between commits, graphs etc)
easy to look at steps in "the wrong direction"
easy for you to cover up steps in the wrong direction (merging and rebasing in git terms) but still keep the time log correct


The rest that's on my mind has already been said.
AnswerKenntnis:
a really simple solution.

tell him everything the above people have told you.
BUT,
at the end, tell him, YOU trust him. and you WILL DO exactly as he asked.
and you will not charge him a cent more.

BUT!

you want a video recording of everything he does since YOU start to work on the project,
and until it ends.

basicly you both should have a same time amount recorded.

if he can do that, you will accept he's request.
QuestionKenntnis:
Should I intentionally break the build when a bug is found in production?
qn_description:
It seems reasonable to me that if a serious bug is found in production by end-users, a failing unit test should be added to cover that bug, thus intentionally breaking the build until the bug is fixed. My rationale for this is that the build should have been failing all along, but wasn't due to inadequate automated test coverage.

Several of my colleagues have disagreed saying that a failing unit test shouldn't be checked in. I agree with this viewpoint in terms of normal TDD practices, but I think that production bugs should be handled differently - after all why would you want to allow a build to succeed with known defects?

Does anyone else have proven strategies for handling this scenario? I understand intentionally breaking the build could be disruptive to other team members, but that entirely depends on how you're using branches.
AnswersKenntnis
AnswerKenntnis:
Our strategy is:

Check in a failing test, but annotate it with @Ignore("fails because of Bug #1234").

That way, the test is there, but the build does not break.

Of course you note the ignored test in the bug db, so the @Ignore is removed once the test is fixed. This also serves as an easy check for the bug fix.

The point of breaking the build on failing tests is not to somehow put the team under pressure - it's to alert them to a problem. Once the problem is identified and filed in the bug DB, there's no point in having the test run for every build - you know that it will fail.

Of course, the bug should still be fixed. But scheduling the fix is a business decision, and thus not really the dev's concern... To me, once a bug is filed in the bug DB, it's no longer my problem, until the customer/product owner tells me they want it fixed.
AnswerKenntnis:
Why would you want to allow a build to succeed with known defects?


Because sometimes, you have time constraints. Or the bug is so minor that it isn't really worth delaying the shipment of the product for a few days needed to unit test and fix it.

Also, what's the point in breaking the build intentionally every time you find a bug? If you found it, fix it (or assign it to the person who will fix it), without disturbing your whole team. If you want to remember to fix a bug, then you need to mark it as very important in your  bug tracking system.
AnswerKenntnis:
Tests are there to ensure that you don't (re-)introduce problems. The list of failing tests isn't a substitute for a bug tracking system.  There is some validity in the POV that failing tests aren't only indication of bugs, they are also an indication of development process failure (from carelessness to badly identified dependency).
AnswerKenntnis:
"Break the build" means to prevent a build from completing successfully.  A failing test doesn't do that.  It is an indication that the build has known defects, which is exactly correct.

Most build servers track the status of tests over time, and will assign a different classification to a test that's been failing since it was added vs a regression (test that used to pass and no longer does), and also detect the revision in which the regression took place.
AnswerKenntnis:
I would argue that the failing test should be added, but not explicitly as "a failing test."

As @BenVoigt points out in his answer, a failing test doesn't necessarily "break the build."  I guess the terminology can vary from team to team, but the code still compiles and the product can still ship with a failing test.

What you should ask yourself in this situation is,


  What are the tests meant to accomplish?


If the tests are there just to make everyone feel good about the code, then adding a failing test just to make everyone feel bad about the code doesn't seem productive.  But then, how productive are the tests in the first place?

My assertion is that the tests should be a reflection of the business requirements.  So, if a "bug" has been found that indicates a requirement is not properly met, then it is also an indication that the tests do not properly or fully reflect the business requirements.

That is the bug to be fixed first.  You're not "adding a failing test."  You're correcting the tests to be a more accurate reflection of the business requirements.  If the code then fails to pass those tests, that's a good thing.  It means the tests are doing their job.

The priority of fixing the code is to be determined by the business.  But until the tests are fixed, can that priority truly be determined?  The business should be armed with the knowledge of exactly what is failing, how it is failing, and why it is failing in order to make their decisions on priority.  The tests should indicate this.

Having tests which don't fully pass isn't a bad thing.  It creates a great artifact of known issues to be prioritized and handled accordingly.  Having tests which don't fully test, however, is a problem.  It calls into question the value of the tests themselves.

To say it another way... The build is already broken.  All you're deciding is whether or not to call attention to that fact.
AnswerKenntnis:
In our test automation team, we check in failing tests as long as they fail due to a defect in the product rather than a defect in the test. That way we have proof for the dev team that hey, they broke it. Breaking the build is highly frowned upon, but that's not the same as checking in perfectly compilable but failing tests.
AnswerKenntnis:
Writing a test that you know will fail until the bug is fixed is a good idea, it's the basis of TDD.

Breaking the build is a bad idea. Why? Because it means that nothing can move on until it is fixed. It essentially blocks all other aspects of development.

Example 1
What if your application is vary large, with many components? What if those components are worked on by other teams with their own release cycle? Tough! They have to wait for your bug fix!

Example 2
What if the first bug is hard to fix and you find another bug with a higher priority? Do you also break the build for the second bug? Now you can't build until both are fixed. You have created an artificial dependency.

There is no logical reason why failing a test should stop a build. This is a decision that the dev team needs to make (perhaps with management discussion) weighing up the pros and cons of releasing a build with known bugs. This is very common in software development, pretty much all major software is released with at least some known issues.
AnswerKenntnis:
It depends on the role the tests are supposed to play and how their results are supposed to affect the build system/process adopted. My understanding of breaking the build is the same as Ben's, and at the same time we should not knowingly check in code that breaks existing tests. If those tests came in "later", it might be "okay" to ignore them just to make it less unnecessarily disruptive to others, but I find this practice of ignoring failing tests (so that they appear to pass) rather disturbing (especially so for unit tests), unless there is a way to indicate such tests as neither red nor green.
AnswerKenntnis:
It depends on the bug, of course, but generally if something went wrong that wasn't identified during manual or automated testing, then that implies there is a gap in your coverage. I would definitely encourage figuring out the root cause and slapping a unit test case on top of the problem.

If it's a production issue that is planned for a hot fix from a maintenance branch, you also need to ensure that the fix works on the mainline, and that a developer can't erroneously blow away the fix with an over-zealous merge conflict resolution. 

Additionally, depending on your release policy, the presence of newly updated unit tests can help confirm that a developer has actually fixed the problem, rather than merely changing it [(the problem or the tests?)], although this depends on having encoded the correct requirements in the new unit tests.
AnswerKenntnis:
One problem with adding a know-to-fail test to the build is that your team may get in the habit of ignoring failing tests because they expect the build to fail. It depends on your build system, but if a failing test doesn't always mean "something has just broken" then it's easy to pay less attention to failing tests. 

You don't want to help your team get into that mindset.

So I agree with sleske that you should add the test, but mark it as 'ignored' for the purpose of the automatic build, until the bug is fixed.
AnswerKenntnis:
Another option is to check the failing test into a separate branch in your source control system. Depending on your practices, this may be feasible or not. We sometimes open a new branch for ongoing work, such as fixing a bug that is not trivial.
AnswerKenntnis:
While I think you should in some way 'check in' the bug as test, so that when you fix it it does not happen again, and in some way prioritize it, I think it's best not to break the build (/the tests). The reason is that subsequently build-breaking commits will be hidden behind your broken test. So by checking in a broken test for this bug you demand that the whole team sets their work aside to fix this bug. If that does not happen you might end up with breaking commits which are not traceable as such.

Therefore I would say it's better to commit it as a pending test, and make it a priority in your team not to have pending tests.
QuestionKenntnis:
How do I create my own programming language and a compiler for it
qn_description:
I am thorough with programming and have come across languages including BASIC, FORTRAN, COBOL, LISP, LOGO, Java, C++, C, MATLAB, Mathematica, Python, Ruby, Perl, JavaScript, Assembly and so on. I can't understand how people create programming languages and devise compilers for it. I also couldn't understand how people create OS like Windows, Mac, UNIX, DOS and so on. The other thing that is mysterious to me is how people create libraries like OpenGL, OpenCL, OpenCV, Cocoa, MFC and so on. The last thing I am unable to figure out is how scientists devise an assembly language and an assembler for a microprocessor. I would really like to learn all of these stuff and I am 15 years old. I always wanted to be a computer scientist someone like Babbage, Turing, Shannon, or Dennis Ritchie.



I have already read Aho's Compiler Design and Tanenbaum's OS concepts book and they all only discuss concepts and code in a high level. They don't go into the details and nuances and how to devise a compiler or operating system. I want a concrete understanding so that I can create one myself and not just an understanding of what a thread, semaphore, process, or parsing is. I asked my brother about all this. He is a SB student in EECS at MIT and hasn't got a clue of how to actually create all these stuff in the real world. All he knows is just an understanding of Compiler Design and OS concepts like the ones that you guys have mentioned (i.e. like Thread, Synchronization, Concurrency, memory management, Lexical Analysis, Intermediate code generation and so on)
AnswersKenntnis
AnswerKenntnis:
Basically, your question is "how are computer chips, instruction sets, operating systems, languages, libraries, and applications designed and implemented?"  That's a multi-billion dollar worldwide industry employing millions of people, many of whom are specialists. You might want to focus your question a bit more.

That said, I can take a crack at:


  I can't understand how people create programming languages and devise compilers for it.


It is surprising to me, but lots of people do look at programming languages as magical. When I meet people at parties or whatever, if they ask me what I do I tell them that I design programming languages and implement the compilers and tools, and it is surprising the number of times people -- professional programmers, mind you -- say "wow, I never thought about it, but yeah, someone has to design those things".  It's like they thought that languages just spring up wholly formed with tool infrastructures around them already.

They don't just appear. Languages are designed like any other product: by carefully making a series of tradeoffs amongst competing possibilities. The compilers and tools are built like any other professional software product: by breaking the problem down, writing one line of code at a time, and then testing the heck out of the resulting program.

Language design is a huge topic. If you're interested in designing a language, a good place to start is by thinking about what the deficiencies are in a language that you already know. Design decisions often arise from considering a design defect in another product.

Alternatively, consider a domain that you are interested in, and then design a domain-specific language (DSL) that specifies solutions to problems in that domain. You mentioned LOGO; that's a great example of a DSL for the "line drawing" domain. Regular expressions are a DSL for the "find a pattern in a string" domain. LINQ in C#/VB is a DSL for the "filter, join, sort and project data" domain. HTML is a DSL for the "describe the layout of text on a page" domain, and so on. There are lots of domains that are amenable to language-based solutions. One of my favourites is Inform7, which is a DSL for the "text-based adventure game" domain; it is probably the highest-level serious programming language I've ever seen. Pick a domain you know something about and think about how to use language to describe problems and solutions in that domain.

Once you have sketched out what you want your language to look like, try to write down precisely what the rules are for determining what is a legal and illegal program. Typically you'll want to do this at three levels: 


lexical: what are the rules for words in the language, what characters are legal, what do numbers look like, and so on.
syntactic: how do words of the language combine into larger units? In C# larger units are things like expressions, statements, methods, classes, and so on.
semantic: given a syntactically legal program, how do you figure out what the program does?


Write down these rules as precisely as you possibly can. If you do a good job of that then you can use that as the basis for writing a compiler or interpreter. Take a look at the C# specification or the ECMAScript specification to see what I mean; they are chock-full of very precise rules that describe what makes a legal program and how to figure out what one does.

One of the best ways to get started writing a compiler is by writing a high-level-language-to-high-level-language compiler. Write a compiler that takes in strings in your language and spits out strings in C# or JavaScript or whatever language you happen to know; let the compiler for that language then take care of the heavy lifting of turning it into runnable code. 

I write a blog about the design of C#, VB, VBScript, JavaScript and other languages and tools; if this subject interests you, check it out. http://blogs.msdn.com/ericlippert (historical) and http://ericlippert.com (current)

In particular you might find this post interesting; here I list most of the tasks that the C# compiler performs for you during its semantic analysis. As you can see, there are a lot of steps. We break the big analysis problem down into a series of problems that we can solve individually.

http://blogs.msdn.com/b/ericlippert/archive/2010/02/04/how-many-passes.aspx

Finally, if you're looking for a job doing this stuff when you're older then consider coming to Microsoft as a college intern and trying to get into the developer division. That's how I ended up with my job today!
AnswerKenntnis:
You might find Lets Build a Compiler by Jack Crenshaw an interesting introduction to writing compilers and assembly language.

The author kept it very simple and focussed on building actual functionality.
AnswerKenntnis:
"I would really like to learn this stuff".  If you are long-term serious:


Go to college, specialize in software engineering.  Take every compiler class you can get.  Those people providing the classes are better educated and more experienced than you; its good to have their expert perspectives used to present the information to you in ways you'll never get from reading code.
Stick with math classes through high school and continue in college for all 4 years. Focus on non-standard math: logics, group theory, meta-mathematics.  This will force you to think abstractly.   It will enable you to read the advanced theory papers on compiling and understand why those theories are interesting and useful.  You can ignore those advanced theories, if you forever want to be behind the state of the art.
Collect/read the standard compiler texts: Aho/Ullman, etc.  They contain what the community generally agrees is fundamental stuff.  You might not use everything from those books, but you should know it exists, and you should know why you aren't using it.  I thought Muchnick was great, but it is for pretty advanced topics.
Build a compiler. Start NOW by building a rotten one.  This will teach you some issues.  Build a second one. Repeat.  This experience builds huge synergy with your book learning.
A really good place to start is to learn about BNF (Backus Naur Form), parsers, and parser-generators.   BNF is effectively universally used in compiler land, and you can't realistically talk to your fellow compiler-types if you don't know it.


If you want a great first introduction to compiling, and the direct value of BNF not for just documentation but as a tool-processable metalanguage, see this tutorial (not mine) on building "meta" compilers (compilers that build compilers) based on a paper from 1964 (yes, you read that right) ["META II a syntax-oriented compiler writing language" by Val Schorre. (http://doi.acm.org/10.1145/800257.808896)]
This IMHO is one of the single best comp-sci papers ever written: it teaches you to build compiler-compilers in 10 pages.  I learned initially from this paper.  

What I wrote about above is a lot from personal experience, and I think it has served me pretty well.  YMMV, but IMHO, not by much.
AnswerKenntnis:
Here's an online book/course that you can follow called The Elements of Computing Systems: Building a Modern Computer from First Principles.

Using simulators, you actually build a complete computer system from the ground up.  While many commenters have stated that your question is too broad, this book actually answers it while staying very manageable.  When you're done, you'll have written a game in a high-level language (that you designed), which uses your own OS's functionality, which gets compiled into a VM language (that you designed) by your compiler, which gets translated into an assembly language (that you designed) by your VM translator, which gets assembled into machine code (that you designed) by your assembler, which runs on your computer system which you put together from chips that you designed by using boolean logic and a simple hardware description language.

The chapters:


Course Overview
Boolean Logic 
Combinatorial Chips
Sequential Chips    
Machine Language
Computer Architecture
Assembler
Virtual Machine I: Arithmetic
Virtual Machine II: Control
Programming Language
Compiler I: Syntax Analysis
Compiler II: Code Generation
Operating System
List item


More Fun to Go
AnswerKenntnis:
Take a step back. A compiler is simply a program that translates a document in one language into a document in another language. Both languages ought to be well-defined and specific.

The languages do not have to be programming languages. They can be any language whose rules can be written down. You've probably seen Google Translate; that's a compiler because it can translate one language (say, German) into another (Japanese, perhaps).

Another example of a compiler is an HTML rendering engine. Its input is an HTML file and the output is a series of instructions to draw the pixels on the screen.

When most people talk about a compiler, they are usually referring to a program that translates a high-level programming language (such as Java, C, Prolog) into a low-level one (assembly or machine code). That can be daunting. But it's not so bad when you take a generalist's view that a compiler is a program that translates one language into another.

Can you write a program that reverses every word in a string? For example:

When the cat's away, the mice will play.


becomes

nehW eht s'tac yawa, eht ecim lliw yalp.


That's not a difficult program to write, but you need to think about some things:


What is a "word"? Can you define which characters make up a word?
Where do words start and end?
Are words separated by only one space,    or can there be more--or less?
Does punctuation need to be reversed, too?
What about punctuation inside a word?
What happens to capital letters?


The answers to these questions help the language be well-defined. Now go ahead and write the program. Congratulations, you've just written a compiler.

How about this: Can you write a program that takes a series of drawing instructions and outputs a PNG (or JPEG) file? Maybe something like this:

image 100 100
background black
color red
line 20 55 93 105
color green
box 0 0 99 99


Again, you'll need to do some thinking to define the language:


What are the primitive instructions?
What comes after the word "line"? What comes after "color"? Likewise for "background", "box", etc.
What is a number?
Is an empty input file allowed?
Is it OK to capitalize the words?
Are negative numbers allowed?
What happens if you don't give the "image" directive?
Is it OK to not specify a color?


Of course, there are more questions to answer but if you can nail them down, you have defined a language. The program you write to do the translation is, you guess it, a compiler.

You see, writing a compiler isn't that difficult. The compilers you've used in Java or C are just bigger versions of these two examples. So go for it! Define a simple language and write a program to make that language do something. Sooner or later you're going to want to extend your language. For instance, you may want to add variables or arithmetic expressions. Your compiler will become more complex but you'll understand every bit of it because you wrote it yourself. That's how languages and compilers come about.
AnswerKenntnis:
If you're interested in compiler design, check out the Dragon Book (official title :Compilers: Principles, Techniques, and Tools). It's widely regarded as a classic book on this topic.
AnswerKenntnis:
Don't believe that there's anything magic about a compiler or an OS: there is not. Remember the programs you wrote to count all the vowels in a string, or add up the numbers in an array? A compiler is no different in concept; it's just a whole lot bigger.

Every program has three phases:


read some stuff
process that stuff: translate the input data to the output data
write some other stuff ΓÇô the output data


Think about it: what is input to the compiler? A string of characters from a source file.

What is output from the compiler? A string of bytes that represent machine instructions to the target computer.

So what is the compiler's "process" phase? What does that phase do?

If you consider that the compiler ΓÇô like any other program ΓÇô has to include these three phases, you'll have a good idea of how a compiler is constructed.
AnswerKenntnis:
"Let's build a compiler" was already suggested. There is a "modernized" version using Haskell instead of Turbo Pascal: http://alephnullplex.appspot.com/blog/view/2010/01/12/lbach-1-introduction

Keeping with Haskell, there is a very instructive Scheme interpreter which could give further ideas: Write Yourself a Scheme in 48 Hours
AnswerKenntnis:
Several others have given excellent answers.  I'll just add a few more suggestions.  First, a good book for what you're trying to do is Appel's Modern Compiler Implementation texts (take your pick of C, Java, or Standard ML). This book takes you through a complete implementation of a compiler for a simple language, Tiger, to MIPS assembly that can be run in an emulator, along with a minimal runtime support library.  For a single pass through everything necessary to make a compiled language work, it's a pretty good book1.

Appel will take you through how to compile a language that comes pre-designed, but doesn't spend much time on what various language features mean or how to think about them in terms of their relative merits for designing your own.  For that aspect, Programming Languages: Concepts & Constructs is decent. Concepts, Techniques, and Models of Computer Programming is also a good book for thinking deeply about language design, although it does so in the context of a single language (Oz).

Finally, I mentioned that Appel has his text in C, Java, and Standard ML - if you're serious about compiler construction and programming languages, I recommend learning ML and using that version of Appel.  The ML-family languages have strong type systems are predominantly functional - features that will be different from many other languages, so learning them if you don't already know a functional language will hone your language craft.  Also, their pattern-matching and functional mindsets are extremely well-suited to the kinds of manipulations you need to do often in a compiler, so compilers written in ML-based languages are typically much shorter and easier to understand than compilers written in C, Java, or similar languages.  Harper's book on Standard ML is a pretty good guide to get you started; working through that should prepare you to take on Appel's Standard ML compiler implementation book.  If you learn Standard ML, it will also then be pretty easy to pick up OCaml for later work; IMO, it has better tooling for the working programmer (integrates more cleanly with the surrounding OS environment, produces executable programs easily, and has some spectacular compiler-building tools like ulex and Menhir).



1For long-term reference, I prefer the Dragon Book, as it has more details on the things I'm likely to refer to such as the inner workings of parser algorithms and has broader coverage of different approaches, but Appel's book is very good for a first pass.  Basically, Appel teaches you one way to do things the whole way through the compiler and guides you through it.  The Dragon Book covers different design alternatives in more detail, but provides far less guidance on how to get something working.



Edited: replace incorrect Aho reference with Sethi, mention CTMCP.
AnswerKenntnis:
I had to create a compiler for class in college.

The basics of going about doing this is not as complicated as you'd think. The first step is to create your grammar. Think of the English language's grammar. In the same way you can parse a sentence if it has a subject and predicate. For more on that read about Context Free Grammars.

Once you have the grammar down (the rules of your language), writing a compiler is as simple as just following those rules. Compilers usually translate into the machine code, but unless you want to learn x86, I suggest you maybe look at MIPS or making your own Virtual Machine.

Compilers typically have two parts, a scanner and a parser. Basically, the scanner reads in the code and separates it out into tokens. The parser looks at the structure of those tokens. Then the compiler goes through and follows some rather simple rules to convert it to whatever code you need it to be in (assembly, intermediate code like bytecode, etc.). If you break it down into smaller and smaller pieces, this eventually isn't daunting at all.

Good luck!
AnswerKenntnis:
I'm not an expert, but here is my stab:

You don't seem asking about writing a compiler, just an assembler. This isn't really magic.

Stealing someone elses answer from SO ( http://stackoverflow.com/questions/3826692/how-do-i-translate-assembly-to-binary), assembly looks like this:

label:  LDA #$00
        JMP label


Then you run it through an assembler, and turn into into something like this:

$A9 $00
$4C $10 $00


Only it's all squashed up, like this:

$A9 $00 $4C $10 $00


It's really not magic. 

You can't write that in notepad, because notepad uses ASCII (not hex). You would use a hex editor, or simply write the bytes out programatically. You write that hex out to a file, name it "a.exe" or "a.out", then tell the OS to run it.

Of course, modern CPUs and operating systems are really quite complicated, but that's the basic idea. 

If you want to write a new compiler, here is how it's done:

1) Write an interpreted language using something like the calculator example in pyparsing (or any other good parsing framework). That will get you up to speed on the basics of parsing.

2) Write a translator. Translate your language into, say, Javascript. Now your language will run in a browser.

3) Write a translator to something lower level, like LLVM, C, or Assembly.

You can stop here, this is a compiler. It's not an optimizing compiler, but that wasn't the question. You might also need to consider writing a linker and assembler, but do you really want to?

4) (Insane) Write an optimizer. Large teams work for decades on this.

4) (Sane) Get involved in an existing community. GCC, LLVM, PyPy, the core team working on any interpreter.
AnswerKenntnis:
Petzold's book Code is a great introduction to non-technicals and techies alike starting at first principles. It's highly readable and vast in it's scope without getting bogged down too much.

Now that I've written this, I'm going to have to re-read it.
AnswerKenntnis:
You may want to check this excellent question (and answers) on StackOverflow : Learning To Write a Compiler. It contains a wide list of resources.
AnswerKenntnis:
I can remember a point in my programming career when I was in a similar state of confusion to yours: I had read up on the theory quite a bit, the Dragon book, the Tiger book (red), but still hadn't much of a clue how to put it all together.

What did tie it together was finding a concrete project to do (and then finding out that I only needed a small subset of all the theory).

The Java VM provided me with a good starting point: it's conceptually a "processor" but it's highly abstracted from the messy details of actual CPUs. It also affords an important and often overlooked part of the learning process: taking things apart before putting them together again (like kids used to do with radio sets in the old days).

Play around with a decompiler and the Hello, World class in Java. Read the JVM spec and try to understand what's going on. This will give you grounded insight in just what the compiler is doing.

Then play around with code that creates the Hello, World class. (In effect you're creating an application-specific compiler, for a highly specialized language in which you can only say Hello, World.)

Try writing code that will be able to read in Hello, World written in some other language, and output the same class. Make it so you can change the string from "Hello, World" to something else.

Now try compiling (in Java) a class that computes some arithmetic expression, like "2*(3+4)". Take this class apart, write a "toy compiler" that can put it together again.
AnswerKenntnis:
There are excellent answers in this thread, but I just wanted to add mine as I too once had the same question. (Also, I would like to point out that the book suggested by Joe-Internet is an excellent resource.)

First is the question of how does a computer work ?
This is how: Input -> Compute -> Output.

First consider the ΓÇ£ComputeΓÇ¥ part.
WeΓÇÖll look at how Input and Output works later.

A computer essentially consists of a processor(or CPU) and some memory(or RAM).
The memory is a collection of locations each of which can store a finite number of bits, and each such memory location can itself be referenced by a number, this is called the address of the memory location.The processor is a gadget which can fetch data from the memory, perform some operations based on the data and write back some data back to the memory. How does the processor figure out what to read and what to do after reading the data from memory ?

To answer this, we need to understand the structure of a processor. 
The following is a fairly simple view.
A processor essentially consists of two parts. One is a set of memory locations built inside the processor that serve as itΓÇÖs working memory. These are called ΓÇ£registersΓÇ¥. The second is a bunch of electronic machinery built to perform certain operations using the data in the registers.There are two special registers called the ΓÇ£Program CounterΓÇ¥ or the pc and the ΓÇ£Instruction RegisterΓÇ¥ or the ir. 
The processor considers the memory to be partitioned in three parts. The first part is the ΓÇ£program memoryΓÇ¥, which stores the computer program being executed. The second is the ΓÇ£data memoryΓÇ¥. The third is used for some special purposes, weΓÇÖll talk about it later. 
The Program Counter contains the location of the next instruction to read from the Program Memory. The Instruction Counter Contains a number which refers to the current operation being performed. Each operation that a processor can perform is refered to by a number called the opcode of the operation. How a computer essentially works is it reads the memory location referenced by the Program Counter into the Instruction Register (and it increments the Program Counter so that it points to the memory location of the next instruction). Next, it reads the Instruction Register and performs the desired operation. For example the instruction could be to read a specific memory location into a register, or to write to some register or to perform some operation using the values of two registers and write the output to a third register. 

Now how does the computer perform Input / Output ? IΓÇÖll provide a very simplified answer.
See http://en.wikipedia.org/wiki/Input/output and http://en.wikipedia.org/wiki/Interrupt. for more.
It uses two things, that third part of the memory and something called Interrupts. Every device attached to a computer must be able to exchange data with the processor. It does so using the third part of the memory mentioned earlier. The processor allocates a slice of memory to each device and the device and processor communicate via that slice of memory. But how does the processor know what location refers to what device and when does a device needs to exchange data ? This is where interrupts come in. An interrupt is essentially a signal to the processor to pause what it is currently and save all itΓÇÖs registers to a known location and then start doing something else. There many interrupts, each is identified by a unique number. For each interrupt, there is a special program associated with it. When the interrupt occurs, the processor executes the program corresponding to the interrupt. Now depending on the bios and how the hardware devices are connected to the computer motherboard, every device gets a unique interrupt and a slice of memory. While booting up the operating system with the help of the bios determines interrupt and memory location of each device and sets up the special programs for the interrupt to properly handle the devices. So when a device needs some data or wants to send in some data, it signals an interrupt. The processor pauses what it is doing, handles the interrupt and then gets back to what it is doing. The are many kinds of interrupts, such as for the hdd, keyboard etc. An important one is the system timer, which invokes a interrupt at regular intervals. Also there are opcodes that can trigger interrupts, called software interrupts.

Now we can almost understand how an operating system works. When it boots up, the os sets up a the timer interrupt, so that it gives control to the os at regular intervals. It also sets up other interrupts to handle other devices etc. Now when the computer is running a bunch of programs, and the timer interrupt happens the os gains control and performs important tasks such as process management, memory management etc. Also an os usually provides an abstract way for the programs to access the hardware devices, rather than let them access devices directly. When a program wants to access a device, it calls some code provided by the os which then talks to the device. There is a lot of theory involved in these which deals with concurrency, threads, locks, memory management etc.

Now, one can in theory write a program directly using opcodes. This is what is 
called machine code. This is obviously very painful. Now an assembly language for the processor is nothing but mnemonics for these opcodes, which makes it easier to write programs. A simple assembler is a program that takes a program written in assembly and replaces the mnemonics with the appropriate opcodes.  

How does one go about designing a processor and assembly language. To know that you have to read some books on computer architecture. (see chapters 1-7 of the book refered by joe-internet). This involves learning about boolean algebra, how to build simple combinatorial circuits to add, multiply etc, how to build memory and sequential circuits, how to build a microprocessor and so in.

Now how does one write computer langauges. One could start off by writing a simple assembler in machine code. Then use that assembler to write a compiler for a  simple subset of C. Then use that subset of C to write a more complete version of C. Finally use C to write a more complicated language such as python or C++. Of course to write a language you must first design it ( the same way you desigh a processor). Again look at some textbooks on that.

And how does one write an os. First you target a platform such as x86. Then you figure out how it boots and when will your os get invoked. A typical pc boots this way. It starts up and bios performs some tests. Then the bios reads the first sector of the hdd and load the contents to a specific location in the memory. Then it sets up the cpu to start executing this loaded data. This is the point you os gets invoked. A typical os at this point loads rest of itself memory. Then it initializes the devices and sets up other things and finally it greets you with the login screen.

So to write an os you must write the ΓÇ£boot-loaderΓÇ¥. Then you must write code to handle the interrupts and devices. Then you must write all the code for process-management, device-management etc. Then you must write an api which lets the programs running in your os to access devices and other resources. And finally you must write code that reads a program from disk, sets it up as a process and starts executing it.

Of course my answer is overtly simplified and probably of little practical use. In my defence IΓÇÖm now a graduate student in theory, so I have forgotten a lot of these things. But you can google a lot of these stuff and find out more.
AnswerKenntnis:
For a simple introduction into how compilers work and how to create your own programming language I would recommend the new book http://createyourproglang.com which focuses more on language design theory without having to know about OS/CPU internals, i.e. lexers, parsers, interpreters, etc.

It uses the same tools that were used to create the recently popular Coffee Script and Fancy programming languages.
AnswerKenntnis:
1) Great video lectures from University of Washington:

CSE P 501 Compiler Construction - Autumn 2009
www.cs.washington.edu/education/courses/csep501/09au/lectures/video.html *

2) SICP http://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/
And the book with the same name. This is actually a mandatory for any software engineer out there.

3) Also, about functional programming, Haskell, lambda calculus, semantics (including denotational) and compiler implementation for functional languages.
You can start from 2005-SS-FP.V10.2005-05-24.HDV if you already know Haskell.
Uxx videos are answers. Please follow Vxx videos first.

http://video.s-inf.de/#FP.2005-SS-Giesl.(COt).HD_Videoaufzeichnung

(videos are in English, other courses are in German though.)


new users can only post a maximum of two hyperlinks.
AnswerKenntnis:
Compilers and programming languages (and everything including in building one - such as defining a finite grammar and conversion to assembly) is a very complex task which requires a great deal of understanding about systems as a whole.  This type of course is typically offered as a 3rd/4th year Comp Sci class in University.

I would highly recommend you first get a better understanding of Operating Systems in general and how existing languages are compiled/executed (ie. natively (C/C++), in a VM (Java) or by an interpreter(Python/Javascript)).

I believe we used the book Operating System Concepts by Abraham Silberschatz, Peter B. Galvin, Greg Gagne in my Operating Systems course(in 2nd year).  This was an excellent book which gave a thorough walkthrough of each component of an operating system - a bit pricey but well worth it and older/used copies should be floating around.
AnswerKenntnis:
See Kenneth Louden's book, "Compiler Construction"

http://www.cs.sjsu.edu/~louden/cmptext/

It provides a better hands-on approach to compiler development. 

People learn by doing. Only a small number can see symbols scrawled on the board and jump immediately from theory to practice. Unfortunately, those people are often dogmatic, fundamentalist and the loudest about it.
AnswerKenntnis:
If all you say is true you have the profile of a promising researcher, and a concrete understanding can be obtained only one way: studying. And I'm not saying "Read all these high level computer science books (specially these) written by this genius!"; I mean: you must to be with high level people in order to be a computer scientist like Charles Babbage, Alan Turing, Claude Shannon or Dennis Ritchie. I'm not despising self-taught people (I'm one of them) but there are not many people like you out there. I seriously recommend Symbolic Systems Program (SSP) at Stanford University. As their website says:


  The Symbolic Systems Program (SSP) at
  Stanford University focuses on
  computers and minds: artificial and
  natural systems that use symbols to
  represent information. SSP brings
  together students and faculty
  interested in different aspects of the
  human-computer relationship,
  including... 
  
  
  cognitive science: studying human intelligence, natural languages,
  and the brain as computational
  processes;
  artificial intelligence: endowing computers with human-like
  behavior and understanding; and
  human-computer interaction: designing computer software and
  interfaces that work well with human
  users.
AnswerKenntnis:
I'm going to suggest something a little out of left field: learn Python (or perhaps Ruby, but I have a lot more experience in Python so that's what I'll discuss). And not merely dabble in it, but really get to know it at a deep level.

There are several reasons I suggest this:


Python is an exceptionally well-designed language.  While it has a few warts, it has fewer IMHO than many other languages.  If you are a budding language designer, it's good to expose yourself to as many good languages as possible.
Python's standard implementation (CPython) is open-source and well-documented, making it easier to understand how the language works under the hood.
Python is compiled to a simple byte code which is easier to understand than assembly and which works the same on all platforms Python runs on. So you'll learn about compilation (since Python does compile your source code to byte code) and interpretation (as this byte code is interpreted in the Python virtual machine).
Python has lots of proposed new features, documented in numbered PEPs (Python Enhancement Proposals). PEPs interesting to read to see how the language designers considered implementing a feature before choosing the way they actually did it. (PEPs that are still under consideration are especially interesting in this regard.)
Python has a mix of features from various programming paradigms, so you will learn about various ways to approach solving problems and have a wider range of tools to consider including in your own language.
Python makes it pretty easy to extend the language in various ways with decorators, metaclasses, import hooks, etc. so you can play with new language features to an extent without actually leaving the language. (As an aside: blocks of code are a first-class objects in Ruby, so you can actually write new control structures such as loops! I get the impression that Ruby programmers don't necessarily consider that extending the language though, it's just how you program in Ruby. But it's pretty cool.)
In Python, you can actually disassemble the bytecode generated by the compiler, or even write your own from scratch and have the interpreter execute it (I have done this myself, and it was mind-bending but fun).
Python has good libraries for parsing. You can parse Python code into an abstract syntax tree and then manipulate it using the AST module. The PyParsing module is useful for parsing arbitrary languages, such as ones you design. You could in theory write your first language compiler in Python if you wanted (and it could generate C, assembly, or even Python output).


This investigative approach could go well with a more formal approach, as you will begin to recognize concepts you've studied in the language you're working with, and vice versa.

Have fun!
AnswerKenntnis:
Well, I think your question could be rewritten to be, "What are the core practical concepts of a computer science degree", and the total answer, is, of course, to get your own Bachelor's in Computer Science.

Fundamentally, you create your own programming language compiler by reading a text file, extracting information from it, and performing transformations on the text based off of the information you've read from it, until you have transformed it into bytes that can be read by the loader (c.f., Linkers and Loaders by Levine). A trivial compiler is a fairly rigorous project when done for the first time.

An operating system's heart is the kernel, which manages resources (e.g., memory allocation/deallocation), and switches between tasks/processes/programs.

An assembler is a text->byte transformation.

If you are interested in this stuff, I would suggest writing an X86 assembler, in Linux, that supports some subset of standard X86 assembly. That will be a fairly straightforward entry point and introduce you to these issues. It is not a baby project, and will teach you many things. 

I would recommend writing it in C; C is the lingua franca for that level of work.
AnswerKenntnis:
Another good introductory book is N. Wirth's "Compilerbau" from 1986 (compiler construction) which is about 100 pages long and explains concise, well designed code for the toy language PL/0, including parser, code generator and virtual machine. It also shows how to write a parser that reads in the grammar to parse in EBNF notation. The book is in German but I wrote a summary and translated the code to Python as an exercise, see http://www.d12k.org/cmplr/w86/intro.html.
AnswerKenntnis:
It's a big topic but rather than brush you off with a pompous "go read a book, kid" instead I'll gladly give you pointers to help you wrap your head around it.

Most compilers and/or interpreters work like this:

Tokenize: Scan the code text and break it into a list of tokens. 

This step can be tricky because you can't just split the string on spaces, you have to recognize that if (bar) foo += "a string"; is a list of 8 tokens: WORD, OPEN_PAREN, WORD, CLOSE_PAREN, WORD, ASIGNMENT_ADD, STRING_LITERAL, TERMINATOR. As you can see, simply splitting the source code on spaces won't work, you have to read each character as a sequence, so if you encounter an alphanumeric character you keep reading characters until you hit a non-alphanum character and that string you just read is a WORD to be further classified later. You can decide for yourself how granular your tokenizer is: whether it swallows "a string" as one token called STRING_LITERAL to be further parsed later, or whether it sees "a string" as OPEN_QUOTE, UNPARSED_TEXT, CLOSE_QUOTE, or whatever, this is just one of the many choices you have to decide for yourself as you're coding it.

Lex: So now you have a list of tokens. You probably tagged some tokens with an ambiguous classification like WORD because during the first pass you don't spend too much effort trying to figure out the context of each string of characters. So now read yout list of source tokens again and reclassify each of the ambiguous tokens with a more specific token type based on the keywords in your language. So you have a WORD such as "if", and "if" is in your list of special keywords called symbol IF so you change the symbol type of that token from WORD to IF, and any WORD that is not in your special keywords list, such as WORD foo, is an IDENTIFIER.

Parse: So now you turned if (bar) foo += "a string"; a list of lexed tokens that looks like this: IF OPEN_PAREN IDENTIFER CLOSE_PAREN IDENTIFIER ASIGN_ADD STRING_LITERAL TERMINATOR. The step is recognizing sequences of tokens as statements. This is parsing. You do this using a grammar such as:

STATEMENT           :=   ASIGN_EXPRESSION | IF_STATEMENT

IF_STATEMENT        :=   IF, PAREN_EXPRESSION, STATEMENT

ASIGN_EXPRESSION    :=   IDENTIFIER, ASIGN_OP, VALUE

PAREN_EXPRESSSION   :=   OPEN_PAREN,  VALUE,  CLOSE_PAREN

VALUE               :=   IDENTIFIER | STRING_LITERAL | PAREN_EXPRESSION

ASIGN_OP            :=   EQUAL | ASIGN_ADD | ASIGN_SUBTRACT | ASIGN_MULT

The productions that use "|" between terms means "match any of these", if it there are commas between terms it means "match this sequence of terms"

How do you use this? Starting with the first token, try to match your sequence of tokens with these productions. So first you try to match your token list with STATEMENT, so you read the rule for STATEMENT and it says "a STATEMENT is either a ASIGN_EXPRESSION or an IF_STATEMENT" so you try to match ASIGN_EXPRESSION first, so you look up the grammar rule for ASIGN_EXPRESSION and it says "ASIGN_EXPRESSION is an IDENTIFIER followed by an ASIGN_OP followed by an VALUE, so you lookup the grammar rule for IDENTIFIER and you see there is no grammar ruke for IDENTIFIER so that means IDENTIFIER a "terminal" meaning it doesn't require further parsing to match it so you can try to match it directly with your token. But your first source token is an IF, and IF is not the same as a IDENTIFIER so match failed. What now? You go back to the STATEMENT rule and try to match the next term: IF_STATEMENT. You lookup IF_STATEMENT, it starts with IF, lookup IF, IF is a terminal, compare terminal with your first token, IF token matches, awesome keep going, next term is PAREN_EXPRESSION, lookup PAREN_EXPRESSION, it's not a terminal, what's it's first term, PAREN_EXPRESSION starts with OPEN_PAREN, lookup OPEN_PAREN, it's a terminal, match OPEN_PAREN to your next token, it matches, .... and so on. 

The easiest way to approach this step is you have a function called parse() which you pass it the source code token you're trying to match and the grammar term you're trying to match it with. If the grammar term is not a terminal then you recurse: you call parse() again passing it the same source token and the first term of this grammar rule. This is why it's a called a "recursive descent parser" The parse() function returns (or modifies) your current position in reading the source tokens, it essentially passes back the last token in the matched sequence, and you continue the next call to parse() from there.

Each time parse() matches a production like ASIGN_EXPRESSION you create a structure representing that piece of code. This structure contains references to the original source tokens. You start building a list of these structures. We'll call this entire structure the Abstract Syntax Tree (AST)

Compile and/or Execute: For certain productions in your grammar you have created handler functions that if given an AST structure it would compile or execute that chunk of AST.

So let's look at the piece of your AST that has type ASIGN_ADD. So as an interpreter you have a ASIGN_ADD_execute() function. This function is passed as piece of the AST that corresponds to the parse tree for foo += "a string", so this function looks at that structure and it knows that first term in the structure must be an IDENTIFIER, and the second term is the VALUE, so ASIGN_ADD_execute() passes the VALUE term to a VALUE_eval() function which returns an object representing the evaluated value in memory, then ASIGN_ADD_execute() does a lookup of "foo" in your variables table, and stores a reference to whatever was returned by the eval_value() function.

That's an interpreter. A compiler would instead have handler functions translate the AST into byte code or machine code instead of executing it.

Steps 1 to 3, and some 4, can be made easier using tools like Flex and Bison. (aka. Lex and Yacc) but writing an interpreter yourself from scratch is probably the most empowering exercise any programmer could achieve. All other programming challenges seem trivial after summit-ting this one. 

My advice is start small: a tiny language, with a tiny grammar, and try parsing and executing a few simple statements, then grow from there.

Read these, and good luck!

http://www.iro.umontreal.ca/~felipe/IFT2030-Automne2002/Complements/tinyc.c

http://en.wikipedia.org/wiki/Recursive_descent_parser
AnswerKenntnis:
The computer field is only complicated because it has had time to evolve in many directions.
At its heart it is just about machines that compute.

My favorite very basic computer is Harry Porter's Relay Computer.
It gives a flavor of how a computer works at the base level.
Then you can start to appreciate why things like languages and operating systems are needed.

The thing is, it's hard to understand anything without understanding what needs it.
Good luck, and don't just read stuff.
Do stuff.
AnswerKenntnis:
If you are interested in understanding the essence of programming languages, I would suggest that you work through the PLAI(http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/) book to understand the concepts and their implementation. It will also help you with the design of your own language.
AnswerKenntnis:
I was blessed to be exposed to the PDP-8 as my first assembly language.  The PDP-8 had only six instructions, which were so simple it was easy to imagine them being implemented by a few discreet components, which in fact they were.  It really removed the "magic" from computers.

Another gateway to the same revelation is the "mix" assembly language Knuth uses in his examples. "Mix" seems archaic today, but it still has that DE-mystifying effect.
AnswerKenntnis:
Coursera is offering a free 10-week course on compilers soon. You can also have a look at the lecture videos here. The lecture videos include the following topics:


The Cool Programming Language
Lexical analysis
Finite Automata
Parsing
Top-Down Parsing
Bottom-Up Parsing
Semantic Analysis and Type Checking
Cool Type Checking
Runtime Organization
Code Generation
Opertational Semantics
Local Optimization
Global Optimization
Register Allocation
Garbage Collection
Java


You may also be interested in some podcasts about compiler construction.
AnswerKenntnis:
Go look at http://mikeos.berlios.de/

There is a really simple operating system in x86 assembly.

He has a nice tutorial on how to write a simple os from scratch.
AnswerKenntnis:
If you really have interests in compiler, and never did one before, you could start by designing a calculator for computing arithmetic formulas (a kind of DSL as Eric mentioned). There are many aspects you would need to consider for this kind of compiler:


Allowed numbers
Allowed operators
The operator priorities
Syntax validation
Variable look up mechanism
Cycle detection
Optimization


For example, you have the following formulas, your calculator should be able to calculate the value of x:

a = 1
b = 2
c = a + b
d = (3 + b) * c
x = a - d / b


It's not an extreme difficult compiler to start with, but could make you think more of some basic ideas of what a compiler is, and also help you improving your programming skills, and control the quality of your code (this actually is a perfect problem that Test Driven Development TDD could apply to improve the software quality).
QuestionKenntnis:
Is 4-5 years the ΓÇ£Midlife CrisisΓÇ¥ for a programming career?
qn_description:
IΓÇÖve been programming C# professionally for a bit over 4 years now. For the past 4 years IΓÇÖve worked for a few small/medium companies ranging from ΓÇ£web/ads agenciesΓÇ¥, small industry specific software shops to a small startup. I've been mainly doing "business apps" that involves using high-level programming languages (garbage collected) and my overall experience was that all of the works IΓÇÖve done could have been more professional. A lot of the things were done incorrectly (in a rush) mainly due to cost factor that people always wanted something ΓÇ£nowΓÇ¥ and with the smallest amount of spendable money. I kept on thinking maybe if I could work for a bigger companies or a company thatΓÇÖs better suited for programmers, or somewhere that's got the money and time to really build something longer term and more maintainable I may have enjoyed more in my career. IΓÇÖve never had a ΓÇ£mentorΓÇ¥ that guided me through my 4 years career. I am pretty much blog / google / self taught programmer other than my bachelor IT degree. 

IΓÇÖve also observed another issue that most so called ΓÇ£seniorΓÇ¥ programmer in ΓÇ£my working environmentΓÇ¥ are really not that senior skill wise. They are ΓÇ£seniorΓÇ¥ only because theyΓÇÖve been a long time programmer, but the code they write or the decisions they make are absolutely rubbish! They don't want to learn, they don't want to be better they just want to get paid and do what they've told to do which make sense and most of us are like that. Maybe thatΓÇÖs why they are where they are now. But I donΓÇÖt want to become like them I want to be better. IΓÇÖve run into a mental state that I no longer intend to be a programmer for my future career. I started to think maybe there are better things out there to work on. The more blogs I read, the more ΓÇ£best practicesΓÇ¥ IΓÇÖve tried the more I feel I am drifting away from ΓÇ£my realityΓÇ¥. But I am not a great programmer otherwise I don't think I am where I am now. I think 4-5 years is a stage that can be a step forward career wise or a step out of where you are.

I just wanted to hear what other have to say about what IΓÇÖve mentioned above and whether youΓÇÖve experienced similar situation in your past programming career and how you dealt with it. Thanks.
AnswersKenntnis
AnswerKenntnis:
You open a very interesting question. I wholeheartedly agree with you. I've made similar observations.

I've been programming professionally for several years already and what I have observed is that the amount of good programmers out there, of great developers who love their work and can do it with quality and passion is pretty much close to zero. I probably met only one person who could teach me something. Most of what I know I have learned by myself, reading books and forums, asking in forums and googling for revelation thoughts.

After a while I don't regret this much.

The options to learn in a working environment can often be limited. You don't start things. You don't finish them. You don't design, don't improve, don't refactor, don't think about architecture, you just code and hack things together. It's how most of the shops work. Not only you don't learn anything, it's more likely that you will learn mostly wrong things how NOT to develop software. I've been continuously seeing scary things around me, all those anti-pattern you have heard of. What is worse, I'm forced to do them myself.

I don't know how it happened, but I managed to somehow build an input barrier. I stay open, listen and if I see some potential for self-improvement I research and maybe adopt some technique or idea. But no BS can ever get through. I have worked in badly run projects for a long time, but I have not adopted any of those bad techniques for myself.

I pretty much soon understood that if you wish satisfaction with programming, forget about job and have your own personal project. It's where you can apply all your love, passion and knowledge to do things right with the high quality level. You will learn a great deal of stuff, a myriad of things you would never have been exposed to and challenged with when hacking boring corporate staff. I only do my job for paycheck and get satisfaction with my own personal projects.

One thing I truly don't understand is how this situation is possible nowadays. Software development has matured a lot. It has had good and bad experience. Many successful projects and a great deal of failed ones. There is experience with long-term projects and understanding what long-term effects one or the other organization will bring upon the project. There are numerous studies available and good books written. "Pragmatic Programmer", "Code Complete", "Mythical Man-Month", "Design of everyday things" and others. Why nobody but us, the programmers ever reads them? How it is possible that even after 20 years of working in IT most developers and managers never found a time to read one or the other methodology book. They are written for, but hardly read by, those who need this medication most.

Regarding career perspectives. What I also have noticed in general on the job market for employees, is that employers out there increasingly lose interest in quality work (imagine they had it once) are shopping more and more for the cheapest work craft available. You find it hard to sell your knowledge, experience and understanding of the universe to anyone. It's not in demand. What is in demand is having your projects ruined by the juniors who have no experience and desire to do professional work. Cheap people are used and abused and then thrown out so that the next round begins. Projects are also outsourced to low-wage destination where they are done by people who apparently begin to learn programming just with your project. That's one thing I truly don't understand.

I'm entertaining more and more the idea that I will drop employed programming work at some time in the future. I would very much like to work in my own start-up with my own project. If not that, I'm considering trying freelancing or probably changing the payed job nature. After all, I hardly learn anything during working hours and I don't get any satisfaction at all. I can do anything 9-5 and always have satisfaction with my own personal projects. I learn much from online communities. I receive here attention, support for my ideas and on occasions even recognition I could never get with my job and my work colleagues. Will see where I will be in the future.
AnswerKenntnis:
There was a post by Jeff Atwood on Coding Horror that said that this is supposed to be fun. And part of the job description of being a software developer is enjoying your work. Otherwise it will become a really boring and cumbersome profession for you. 

The fact that you continuously search for ways to improve your work, and you keep getting informed of the new stuff and the best practices shows that you enjoy your work in nature. So my advice to you would be to start looking for a new job.

However I do not think that working for a big company will necessarily improve the working conditions. I think the best environments for software developers are small to mid-sized companies, where the people like what they are doing. The best thing to do would be to search and check-out any companies before you take a decision. Try and make sure that the new company is worth it.
AnswerKenntnis:
First thing : a big warning : if you've been developing for 10 years (like I have) there's nothing else you can do better than developing. So if you want to do something else, something new, do it quickly otherwise it may be too late and you won't be good at doing something else.

Just to share my own point of view : I'm a self driven person, self taught.
I've learned alone pure C, C#, Amiga C developing, Windows, COM, Delphi, PHP, Cinema4D and now Blender and Python. I've almost always worked alone.
Here's the biggest problem I've encountered so far : small companies try to survive and you're one of the ones who keep it alive : it's very stressing but it's also rewarding : you work more, you learn faster, and a lot of stuff quickly, you make products faster (event though you often can't do your job properly). Anyway : too much stress. On the contrary, big companies will always survive but the problem is about people : too many people. It's a jungle, but far worse : nothing is clear : if  the people you work with feel like you're better than they are, they'll try their best not to help you to go up but only to shoot you because they fear for their own career. It's the way it works in France (I don't know for other countries).

To make it short : try to find a middle sized company where you feel like it's your place.
No matter how long you've been programming, no matter how long you've been working, the day when you get in your car to go to your job and you think "I'm happy to go to my work" will be the day you found your place.

It's not a question of 4-5 years or whatever.

NB : there's a huge difference between "I'm happy to go to my work" and "I'm happy to go to my work because I'll do this and that" / or / "I'm happy to go to my work because I'll learn this and that". If you think "because I'll do this and that" this means "this and that" will end one day and you may not be happy after. This is my 12 years programming experience. And I'm a senior, I'm well paid, but I do realize that 20-25 y.o. people think faster than I do. They just don't have my experience so I've just admitted I can help them to do things, they do it faster, but I help them to make them properly (which is almost never the case when you start developing).

Sorry for my English which isn't perfect, don't hesitate to correct my post to make it proper English.
AnswerKenntnis:
I've been a software developer for twenty-one years -- among other things, that means I've been doing this long enough to drink! ;-) But seriously, though, I can't imagine doing anything else as well or as joyously as programming. I'm apparently one of those rare birds that really love this job.

I was recently freshening up my resume, and I noticed that I've had many, many jobs in the last two decades -- mostly contracts. But what I realized in looking at that long list is that the jobs I loved most (and the ones I stayed at the longest) were with software firms, i.e. companies whose business model involved selling software to customers. The following theories are probably not universally applicable, but they go far to explain my experience.

As I see it, the difference between a software shop and your typical IT shop is surprisingly obvious. It's merely a question of what those in upper management understand. If the company gets most of its revenue from software, then the top brass will really grok software and how it's made. But if revenue comes from selling widgets, they generally don't have the faintest inkling of what it takes to keep their IT infrastructure running, let alone the processes behind software development. Worse, even though they delegate the task to IT staff, they try to map the IT processes to what they know, with results that generally range from disappointing to disastrous.

One reason is that there's a wide range of talent out there, from the merely competent to the software virtuoso. This runs completely counter to the cookie-cutter, one-size-fits-all mentality that works so well for unskilled and semi-skilled positions. The expectation that any one developer can be replaced with any other with a similar "skillset" may seem ludicrous to us, but often seems perfectly reasonable to a management structure that's focused on raw numbers. If this seems incredible, just look at how many shops are still trying to make the Waterfall model work.

You don't necessarily need all virtuosos on your team, but you need at least one or two. And part of their role must be to mentor the junior people so they can grow into the role (or wash out -- it happens). Otherwise, bad-to-mediocre junior code will prevail, with its attendant misfactoring and bloat. Once that cancer sets in, the code rapidly becomes unmaintainable, productivity across the team falls sharply, and people start to burn out.

This creates turnover, until finally somebody says, "We can't keep this thing going anymore! Let's scrap it and start over!" The investment in the legacy codebase goes out the window, and the process starts all over again. "But this time will be different," you promise yourself. And for a while it is. Then somebody hires away your rock star, and you're left with undirected junior people trashing your shiny new codebase all over again.

Lather. Rant. Repent.

But I digress... So, to answer your original question: No, half a decade is not the midlife of a software career. It's more an Age of Reason, perhaps -- the place in your career where the scales fall from your eyes and you start to see the business for what it is. The realizations come in a different order for everyone, and that colors the conclusions you may draw along the way. But hang in there -- if you come out the other side sane, you'll have a much healthier perspective, and you'll ultimately find this crazy business we're in more rewarding than ever before.
AnswerKenntnis:
I think you need five more years experience. Then you will be able to accept the compromises and be happy to fix what you can.

At least that was my experience. At about 5 years (about the time it takes to become an expert) I think I actually "got it". I realized my code was crap. I realized my shiny hacks that got the apps working were just that, hacks. I came to understand that I had been creating sub-par code just to make my bosses happy and keep the paychecks rolling.

At the same time I realized that the desires of the bosses were not going to change. They were always going to want it yesterday without spending any money on good tools. They never were going to care if I used a particular design pattern. Never would they be in awe because I increased the maintainability by applying the Single Responsibility Principle judiciously.

I also realized I would be fighting my younger self embodied by inexperienced programmers. The programmer that hacked the program to get it working fast. The young programmer that resisted all the silly things that took me years to understand. The young programmer that thinks having 5 classes is more confusing than have one huge class that does it all.

But now that I am at my 10 year point I have come to see that those frustrations are just like the frustrations I experienced in my first five years. It is the frustration associated with learning a new skill. But this time the skill are interpersonal. The skills are getting non-programmers to understand the limitations of going down the cowboy path. And these skills are maybe more important in the grand scheme of things than the coder skills I acquired during the first five years.

But what is really amazing about this job is that I can continue, and actually must continue, to developing by technical skills while also working with management.

So I say to you keep at it! Because you have become comfortable with the technology you can start working on your skills dealing with the mushy things called humans.
AnswerKenntnis:
I've found that there are 2 secrets of relative happiness:


do not seek to have everything you want but seek to want everything you have
life it's tough - it doesn't matter how hard you can hit (good you are, personally and professionally); it will always hit back harder. it matters how much you can get hit and still be able to move on.


After all it's all a walk in the park and there are more important things than bits and bytes, and practices and stuff. Just enjoy all experiences as they are and fight back as much you can.
AnswerKenntnis:
Maybe take a look at The Passionate Programmer by Chad Fowler... quoting from the webpage:


  In most cases, remarkable careers donΓÇÖt come by chance. They require thought, intention, action, and a willingness to change course when youΓÇÖve made mistakes. Most of us have been stumbling around letting our careers take us where they may. ItΓÇÖs time to take control.
AnswerKenntnis:
IMO the dissatisfaction you obviously feel is not a crisis, but rather a sign that you reached a certain level. You outgrew "I am just a programmer" slot. You have enough understanding now to see the flaws in the current state of affairs and how it can be fixed. 

The next step is find a connection between what you see as potential improvement and what the company you work for would see as such. Then you have to convince your boss(es) that this is the way to go.

It is not enough to understand things and know how to do things. You also should be able to convince people that you know what you are doing. And if it sounds like sales that is because it is. But this is absolutely necessary to become "great". You are working not with computers, you are working for people and with people, you have to be able to sell your ideas.

Of course some companies would be too conservative to accept your ideas, and your boss can reject them because of the color of the tie you are wearing. Just keep in mind that selling ideas is no easier than generating ones, and unsuccessful sale is not necessarily a result of your boss' ulterior motives it can be bad presentation on your part.
AnswerKenntnis:
I agree with Nikos completely.  I think what you maybe need is to find a company that is technology and product focused.  There are companies where the software developers consider development just their job, and they never think about this stuff in their own time.  The working environment is usually much more geared up for managers doing desk work rather than developers doing mind work.  The average standard of the developers is generally low (although you occasionally get one "guru").  They tend to cut corners on (or even avoid) things like version control, testing, process etc.  The company generally ships software that does do the job, but it's often ugly and buggy, and nobody seems particularly bothered.

On the other hand there are companies where the developers are just there doing what they'd otherwise be doing at home anyway; they go home in the evening and work on their own home dev projects, and and learn new languages/tech in their spare time.  The working environment is usually quite developer-centric, with good monitors, good tools, good atmosphere conducive to thinking.  The developers are often of a high standard, and you find yourself learning from all your peers on a daily basis.  They generally take things like version control, testing and process seriously.  The company generally ships decent quality software, because the developers take it personally if there are bugs or imperfections.

I don't think finding a larger company will necessarily improve matters; in fact in many cases larger companies are worse, and the problems are just deeper ingrained and more institutional.

Obviously I'm polarising things a bit....but if you're a software engineer at heart (and not just on your resume), then you need to try and find one of the latter types of company.  Think of some good interview questions you can ask them, to find out whether a company is that kind of place.
AnswerKenntnis:
Something I've found many programmers don't understand is that not all decisions are technical.  Sad as it is, doing something "right" is not always an option.  This leads many developers to think their bosses are stupid, or make poor decisions... and yes, often that is indeed the case, but equally as often the fact of the matter is that the company might be out of business, or lose a job entirely if they allowed the programmers to set the schedule and make all the technical decisions.

Sadly, sometimes it's our job to work within the political or financial constraints of the job and do the best we can.  

One thing you learn from experience is to be conservative.  The latest techniques may not be proven and you may not have a strong grasp on them even if they are.  Managers don't like spending time on letting you get up to speed on a new technology just because you want to learn something new.

Programmers forget that many companies are not software companies.  They're <insert business industry here> companies that also do software.  Your job as a programmer is to facilitate the mission critifcal processes, not write perfect software.  Sometimes it works out that those are the same thing, but it's actually quite rare.
AnswerKenntnis:
In my opinion it's not midlife crisis - I'd say the honeymoon is over.
AnswerKenntnis:
I can relate in terms of the code written could be better but there has to be a line where one may be too much of a perfectionist as if something works 99.99999% of the time, isn't that close enough to 100% for most people?  Part of my struggles at times with negative thought patterns are perfectionism and intense self-criticism and judgement so maybe I can overly identify with that.

4-5 years is enough time, IMO, to figure out a few things:


What you want - What practices in your current position do you enjoy?
What works for you - This is slightly different in that what works may not always be what you want.
What are your dealbreakers - Could you go work for a company that wouldn't let you have an IDE?  Extreme example but I would hope that illustrates the point somewhat.


There are other questions like "Roadmap to a better programmer," and "How to become a 'faster' programmer," that may have suggestions for you if you do want to improve your skills.

I've also experienced where the senior title is given to someone merely for surviving in the field for so long, rather than achieving a level of skill.  You can either accept this as part of how the world works or you can try to find places that run differently.  How many actually exist I don't know, but I can say that sometimes you can find a good place in terms of how you like to work and what the company and co-workers use to get the job done.

"Sources of Insight" is one of my favorite blogs and I enjoy reading it nearly all the time.  The material is a bit head heavy in terms of there being a lot of intellectual elements to what is written but there are nuggets that one can take and make their life a little better or at least that has been my experience.

I can remember in school when I was a child that there were often transitions after 4-5 years as while my first school I was at for 8 years(Junior Kindergarten to grade 6), then it was 2 years(grades 7 and 8), 4 years(High school which was grades 9-13 where 11/12 was done in one year in my case), and 4 years(university bachelor's degree).

I can accept the idea of crisis coming at various points in life,e.g. finishing university can cause one to wonder who they are or after working for a while wondering if it is all worth it.

Some places can appear as awesome places to work and others can lead to burnout as that has been a question here a couple of times, "What causes developer burnout," and "Developer burnout stories," while other questions are the flip of that, "What is your motivation," and "Programming (de)motivation and further plans..."

Just for the sake of background in my answer here I've been developing web sites/applications for almost 12 years now as I got my first out of university job in February 1998 and aside from an 8 month drought where I wasn't working I have been doing this all the time in a few different environments: A couple dot-coms, an application service provider and now within the Information Systems department's web development team at a local technology company.  I realize this is a long answer, but I think the questions asked don't have short answers to my mind.
AnswerKenntnis:
I have found similar things in my 10 years of programming, and I surmise that these are rather common occurrences.  In the business world (as opposed to academia), money (or lack of money) and time drive the schedule, features, and quality of the programming.  Often those resources are lacking for doing things fully correctly.  This is a prime motivator for finding the most efficient methods to solve problems.  This situation has also guided me to keep in mind that my programming should solve only the problem at hand (with some amount of future consideration) instead of building something that contains many more features than is required.  This is a crucial lesson to learn, in my opinion.

Your comments on "senior programmers" is also, sadly, common in my experience.  I think the reason for this is two-fold - first, many experienced programmers get lazy, using only the tools and methods they have used in their careers.  Technology keeps moving forward, however, and this leads these experienced programmers to become "dinosaurs".  Second, after programming for a while, it can become easy to fall prey to a bit of hubris ("my talents have gotten me this far, so I must be a pretty good programmer").  I try to combat both of these problems by continually trying to learn new methods or technologies to solve my problems.  Sometimes this contradicts the "build only what is required" lesson state above, but the goal is to strive for a healthy balance between the two.

I would suggest using the experiences you have as a motivator to continually improve yourself.  I have quit the programming industry myself after about 5 years because I lost the passion to engineer code.  But I couldn't get rid of the itch to build programs, and I came back to the industry several months later.  I did learn that you have to engage yourself doing what you enjoy doing - if you want to project manage, find a position that allows you to manage projects.  If you want to code all day, find a position to do that.  Finding a job that challenges you and fulfills your desires is a wonderful and necessary part of a happy existence - I wish you luck in finding that.
AnswerKenntnis:
Hey buddy, it was really good reading your question. I am glad you wrote so. You know what, you dont realize what understanding you have right now. The things you wrote make me understand what experience you have had, and believe me this experience is something that not all programmers can have in their life. You are a self driven person, self learning. Right now you are in a very mature state of mind, after working 4 years for small companies. If you would have been into big companies, you would have had nothing to be considered as an experience. Now you have an understanding of how this industry works, how things are done and how they should be done. What level these so called "seniors" have. I have a suggestion for you, if you are so good a self learner and have practiced self learning for over 4 years, why don't you try freelancing as a career. Believe me you would be working for yourself and enjoying much more. 

As an ending note, dont regret what you did in the last 4 years. Its a wonderful experience and only a few have this in their lives :)
AnswerKenntnis:
The difference between senior and junior programmers, when talking about people with experience at all, is generally just a pay-based one. There's a lot of variability in what organizations value in order to change the title, and often it's determined by what you demand when you get hired.

If it's any consolation I'm having a 10 year mid-programming-life crisis, although I did start programming on an Apple ][+ so it might be a 24 year point; I don't know. I just wish people didn't expect magic from programmers.
AnswerKenntnis:
I find the red thread I have seen in my career has been that when I feel I get caught up in a situation where I have no control of my environment that is when I start feeling bored. Just doing what everybody else is telling you. It is important to have an area that is one's own (IMHO) to have complete responsibility of - maybe that is missing in your work as well?

In that case you should speak with your manager, maybe there is some solution in your current job? Asking for more responsibility is always a good move.
AnswerKenntnis:
In my experience, the first few weeks (months at most) are crucial for the general quality of a project. If you happen to start working at a place where other programmers have already created a mess (bad coding standards, no version control etc.) it's very difficult for a manager, and mostly impossible for a new peer, to establish any improvements. Later, deadlines and budged issues will cause some ugliness an almost every project, but if the foundation is well done, the damage will be limited and manageable.

For that reason, if you find yourself in a situation where you have to work with bad peers on a bad project, try to get assigned to a new project (if this is possible in your company) or find a new job. Don't wait too long, since bad habits are contagious.
AnswerKenntnis:
I have had the same questions and possibly looked at the same things you have (great startups, inspirational advice, motivated people creating amazing things, processes, algorithms that make your head heart) only to find none of it in my coworkers, current or former, nor in the people I know who are in the business. So this conundrum of matching this deep interest with a paid job has meant a lot of badly slept nights and a search for a project so ingenious that could be taken on the side and yet grow into its own thing quickly enough to provide a way out. Like a lot of people, I have a family to support, and I personally think the energy it takes to build something in a startup targeting the Brazilian market is not well spent if it's not completely IT-ish and boring. And seriously? I do IT-ish and boring things all day long.

So to me the answer has been keeping the will to learn and improve in one place and work in another. I've taken up processing.org and keep trying to draw, write and cook more. It's been great for me, to be honest: sometimes you're so caught up with matching the people you admire and being taken up to the Pantheon of great achievers that you ignore serious facts about who you are and what makes you feel life is worth living. And while work is something that takes a considerable chunk of your day, the minute you stop worrying about what it all means and where you're going is the moment you realize you don't need it to get there. Keep coding if it's what you love, write software at home, contribute to projects you're passionate about. That type of satisfaction hardly ever comes from a paycheck for most people, and maybe that's the case for you too.
AnswerKenntnis:
As a programmer you're always going to have to balance your desire for perfection and your employer's desire for a working product. At some companies these two desires will be closer to one another, at most companies the two desires will be far apart.

My best suggestion for dealing with the drain of your day job is to start a personal project on the side, one where there is no deadline, where you can work in areas that you don't usually work, one where you make all of the decisions. The most rewarding code that I've written was while working as a Wii developer, I can suggest that you write a game. You'll be able to touch on every different aspect of programming, 3D, networking, AI, etc... and since you're already working with .Net I'd suggest grabbing XNA or Unity

As far as Senior Programmers not knowing much, you're probably right. Most senior programmers at larger firms were promoted at a time that they did know a lot, or when they were able to get the job done. Now that they're senior they have different responsibilities, mainly as managers. It's expected that their coding skills will slip a little. Some are better than others, and some probably did just get promoted because of who they know, but most senior programmers that I've worked with in the past had a solid (although sometimes outdated) skill set.

So to wrap it up, do a personal project to relieve the day to day boredom, and take it easy on your seniors, just do the best work that you can in the time allotted and you'll be fine.
AnswerKenntnis:
I think it is time to be Sr, Developer for you, I m new at this market and learning and improving but employers are not looking for humans, they are looking for monsters like Jr. developer with 6 + years of experience and it is really frustrating.
AnswerKenntnis:
Good question Jeffrey. Do you still enjoy programming? Are you passionate about it, do you do it in your spare time? Or are you just sick of some of the terrible programming jobs you have had. 

If you are feeling like programming is not for you, there are many other disciplines you could branch off and do at this point in your career - project management, sales, pre-sales, analyst. These opportunities wouldn't have existed when you were a junior/grad, so you might not have started thinking beyond cutting code. Perhaps you can approach your employer to sponsor you for certification exams, or if you want to explore a new technology (Azure/Silverlight/WPF?) 

On the other hand, if you feel like your workplace resembles a true-to-life Dilbert comic, maybe it is just time to move on. If you have been working in a large company, how about interviewing for a few startups, or vice versa. You don't have to tell anyone you are applying for jobs, and even when you get an offer, you are in a great negotiation position since already have a job, you can simply decide if the offer get is better than your current situation, and if not then just keep looking. Larger companies can often offer you better career development opportunities and training, whereas with a small company you get more intangibles, like responsibility for the success of the company, flexible working hours, stock options, and seniority/respect later on if/when the company hits it big.
AnswerKenntnis:
If it's any consolation, I've felt exactly the same way about seniors in my work place.  Last week I filed an evidence-based report, and this week I had an hour long meeting with personnel to officially file a complaint about the managers (taking into account I'm a junior).  It was either grow some balls or be unhappy in my job.  It's not malicious complaints, it's constructive complaints.  There's a difference, and it can certainly have a huge impact on your happiness in work.

EDIT

I'd also say don't just "give up" because you have an issue with people.  It seems a lot of people in here just say "quit your job", the truth is that you can make the difference in the work place.  It sounds cheesy I know, but I think you'd make change happen and gain more respect if you're pro-active for things to change.  Don't be scared to take your boss aside and make a complaint.  I have, and already it's made a big difference.  We're adopting new technologies, changing our work procedures and more formally approaching tasks all because of what I said (and I'm a junior developer)
AnswerKenntnis:
Well, you should learn another domain (mathematic, AI, data mining, BI, integration whatever).
Then after mastering this new domain chances are that you have new ideas which respond to real business needs.

At this stage if you are a great programmer, you just need to make a startup. Well it's risky, but way more fun.

Learn different skills and competencies, not just in programming, then combine them to create a new thing nobody has thought. That's easy, you already have the best hammer ever : Programming.
AnswerKenntnis:
My personal experience is the same that you mention. Mostly all the projects i have been working on lately all are made in a rush and all could be better. I've been a program developer for roughly 7 years now and been in the same medium sized company. The management dont have a clue and are running all projects as "non-projects". There is very little order in the chaos. 

The thing i fear the most is, that I fall into the pit you are talking about and start doing "crappy" programming and stupid decision just to get forward in my career. I hope there is someone there to hit me in the head if that happens.
AnswerKenntnis:
Wow!
I like how valuable this conversation is.
I am a web developer with 5.5+ years of experience and I love it. 
I can't vote but I will quote AZ's words.
I agree with that!


  I've found that there are 2 secrets of
  relative happiness: - do not seek to
  have everything you want but seek to
  want everything you have - life it's
  tough - it doesn't matter how hard you
  can hit (good you are, personally and
  professionally); it will always hit
  back harder. it matters how much you
  can get hit and still be able to move
  on.


The book I'd recommend is:
http://www.amazon.com/Software-Measurement-Estimation-Quantitative-Engineering/dp/0471676225
AnswerKenntnis:
My two points:

I hit mine after a mere 2 years in the industry. I overcame it by bettering myself and learning.

Most of the time, it will come because you are doing the same thing over and over. And if you're doing the same thing over and over... well... you're doing it wrong.

Always improve. Always learn. And hell, if your current company isn't giving you that atmosphere, go find one that will. I did and the last 2 years have been the best time of my life (I'm also probably 100x better at my job than I was 2 years ago too)
AnswerKenntnis:
There are a lot of programmers, but only a small fraction of them are software engineers of quality. I suppose that's true in any profession.
AnswerKenntnis:
Welcome to the real world...unfortunately, what you describe happens in most careers with people which just aren't passionated enough about what they're doing. There's only one option (that is, if you love your work): you must find a better shop to work for, though that is not easy in the current days....
AnswerKenntnis:
IΓÇÖve also observed another issue that most so called ΓÇ£seniorΓÇ¥ programmer in ΓÇ£my working environmentΓÇ¥ are really not that senior skill wise. They are ΓÇ£seniorΓÇ¥ only because theyΓÇÖve been a long time programmer, but the code they write or the decisions they make are absolutely rubbish! They don't want to learn, they don't want to be better they just want to get paid


... followed by ...


  IΓÇÖve run into a mental state that I no longer intend to be a programmer for my future career. I started to think maybe there are better things out there to work on.


Then it's time to start looking, and to take action. Because if you remain a programmer with this attitude, you will end up just like those "senior" people that you so clearly despise. The operative word is "trapped": you'll make enough money that you can't justify moving to something else, and your skills will be so narrow that you can't move within the industry.
AnswerKenntnis:
Perhaps consider going to graduate school?  That might open up opportunities for a different, more long-term-oriented kind of work.
QuestionKenntnis:
Why do business analysts and project managers get higher salaries than programmers?
qn_description:
We have to admit that programming is much more difficult than creating documentation or even creating Gantt chart and asking progress to programmers. So for us that are naives, knowing that programming is generally more difficult, why do business analysts and project managers get higher salary than programmers? What is it that makes their job a high paying job when even at most times programmers are the ones that go home late? 

UPDATE

Excuse my ignorance, from some of the response it seems that the reason why BAs and PMs gets higher salary because they are the ones that usually responsible for the mess programmers make. But at the end of the day, it is programmers that get their hands dirty to fix the mess and work harder. So it still does not make sense.
AnswersKenntnis
AnswerKenntnis:
Whether project managers get higher salaries than programmers and business analysts at all exist as a class depends squarely on the software world you live in.

A simple answer to this question would be "because in our societies, we still think the salary is bound to the position in the hierarchy." But this answer whilst reflecting the fact that people are paid based on their perceived value doesn't explain why PM and BA are on top of the hierarchy in many software organisations and why the management goes for hierarchy in the first place as a structure of choice for software project team. These are the two questions that seems to be really worthy asking.  

Broadly speaking there are two categories of software making organisations. I will call them Widget Factories and Film Crews.

Widget Factories are born out of management school of thought revolving around motivation Theory X proposed by McGregor: rank employees are lazy and require constant control and supervision, jobs are held in the name of a pay check, managers are always able to do their subordinates' jobs to the higher or, at least, same standard. This thinking lands to a natural idea that the entire team can easily be replaced with and represented by the manager alone - after all everyone else on the team is either easilly replacable or there just to enhance manager's ability to complete tasks. Hence the hierarchy as a structure and rather horizontal job roles.

Widget Factory management operates on the assumption that software can be manufactured out of a specification prepared by a business analyst through a clearly defined process run under the close supervision of a project manager. The manufacturing is taken care of by staffing the project with enough qualified yet interchangeable programming and testing resources. Work is driven by a prearranged budget based on the initial business case prepared by PM and BA.

Management that runs a Widget Factory is easy to spot just by paying attention to the way these people talk. They are likely to be on about resources (including when referring to team members), processes, operating efficiency, uniformity, repeatability, strict control over use of resources, clear-cut job roles and defined process inputs and outputs. They'd casually mention the actual factory metaphor when trying to convey the image of the ideal software development operation as they see it.

Then there are Film Crews. They are based on the notion that people are intelligent, self-motivated, work really hard and enjoy their jobs as much as kids enjoy playing. Film Crews recognise that due to specialisation individual contributor abilities may by far surpass the abilities of people organising, co-ordinating and directing the work. Since manager can no longer substitute for everyone the hierarchical structure just doesn't work that well - people have to co-operate within a much flatter and complex formation to get things done. Jobs roles themselves tend to be much more vertical - start to finish - and involve a broader variety of skills. This management thinking is underpinned by McGregor's Theory Y.

A director of a Film Crew knows that her vision for a piece of software can only come true should she be able to assemble a great crew, fascinate the imaginations and help the team to gel and work together. Her role is to inspire, guard the vision, provide direction and focus the efforts. Every single person matters because "director" believes that software results from combination of worldviews and abilities of all participants and a unique way the group carries out the work together. Everyone recognises from the onset the importance of getting the stars to join the crew ΓÇô star performers increase every chance for success. Vision drives budget and attracts funding.

When it comes to compensation Widget Factories deem that the most value is derived from the work done by project manager and business analyst who reside on the top of the hierarchy and have to be compensated accordingly, the rest of the team doesnΓÇÖt matter that much as long as theyΓÇÖve got the right qualifications to convert requirements into working code. PM and BA work hard to maintain their position on top of the pack by restricting free access to the sources of project information to the rest of the team. Without formal access to the primary info sources the team struggles to make any value judgements or come up with good solutions, programmers are relegated to taking orders from above and working on the problem as defined by PM and BA. This situation further reinforces the Widget Factory notion that programmers are akin to factory shop floor workers only capable of mechanically carrying out though technically complicated, but nonetheless standard tasks.

In a stark contrast Film Crew acts as a more egalitarian formation; members are given unrestricted access to primary information, encouraged to form value judgements and are free to select a course of actions to fulfil and contribute to the vision. Leadership structure is based on ability rather than a specific role within the team. Compensation reflects how desirable getting a specific person to take part in the project, it often tied to the perception of how much more valuable the end result will become if that person can be convinced to devote their energy to creating that piece of software.  In this environment the role of a project manager becomes less prominent as he is unlikely to be the creative leader; the role comes down mostly to administrative support and external relations. Business analystΓÇÖs duties are partly replaced by the role of visionary (I called her earlier ΓÇ£a directorΓÇ¥) and partly absorbed by other team members. 

Now, it wonΓÇÖt come as a surprise that most in-house software development teams and some consultancies are run as Widget Factories relying on a process to produce consistently boring software; it is these environments where project managers and business analysts are routinely paid more than programmers based on the assumption that they bring the most value with the environment structured accordingly making it difficult for programmers to prove the management wrong.

Successful software companies tend to adopt Film Crew viewpoint, any other philosophy would hinder their ability to attract great people that they rely on so much to produce great software. ItΓÇÖs unlikely youΓÇÖd ever see a business analyst role in that setting and project managers are less prominent and routinely get paid less than great programmers.
AnswerKenntnis:
Because in our societies, we still think the salary is bound to the position in the hierarchy.

The analysts or project managers are higher in the hierarchy, so they should be paid more.

Let me tell you a real story that illustrate why this is a problem.


  A good friend started as a programmer in a big hospital. Thanks to his hard work and dedication, he quickly became Oracle DBA, which was a critical position in a company where data is both sensitive and valuable.
  
  The hospital worked with levels. Levels are bound to your position in the hierarchy, legacy and diplomas.
  
  My friend got a proposal to become DBA in another company that didn't use salary levels. His salary could be increased a lot. Because he liked and respected the hospital he worked for, he decided to talk to the boss, asking for an increase.
  
  The boss refused. It was impossible because of the levels and the unions would not let that happen.
  
  My friend left.
  
  The hospital eventually hired an external consultant (not bound to levels) and posted a job on their website. The consultant did not know anything about the infrastructure in place, so his learning curve was huge. The hospital lost lots of money because of that.
  
  The hospital did lose a lot more. The external consultant was paid as much as 5 times what my friend asked for, and they couldn't find a qualified employee to replace him.
  
  That was almost three years ago. My friend is still at his new place and climbing the hierarchy ladder very fast doing what he loves.
  
  The hospital is still paying 5 times more.


IMHO, salary should be relative to the value you provide to the company.

UPDATE: When you move higher in the hierarchy, there is the leverage effect occurring. So in fact, you are paid for the value you bring. But brilliant programmers that are 10x more productive should be paid 10x more, regardless their position in that hierarchy (usually at the very bottom). That's what I wanted to highlight.
AnswerKenntnis:
They take more risks than programmers do. They have to make decisions based on whatever information we gave them, and then face the stakeholder's harsh criticism when their expectations aren't met. Part of the pay package compensates for this risk.

Another factor may be the years of experience needed to prepare a project manager who can plan, estimate and mitigate properly. In some sense, a nuanced project manager is trained through failures, making it an expensive-to-acquire skill. Once reached the level of seniority, a company may not be willing to let go of such valuable personnel.

Edit:

There are more kinds of risks than financial or physical harm. For example, consider the risk of being reprimanded by the manager or the customer. Although no actual harm is done, it is still undesirable enough that we adapt our behaviors in order to avoid this kind of outcome. However, managers have to make good decisions all the time, and has to balance different kinds of risks in the interest of the company, not according to personal preference.
AnswerKenntnis:
Programming may be more difficult by some measure, but it's also more pleasant. You just sit there and solve the nice programming puzzle while managers deal with all kind of crap between their subordinates, their clients, their own bosses and stakeholders. That's why so few sane people actually want to be managers, so you have to compensate for that by paying more.

Programming is more difficult, but managing sucks more.

One way to think what's someone's value to a company is to imagine what it would be like if that person left the company. Usually managers turn out to be more valuable in that sense than programmers. James Gosling, the creator of Java, recently left Oracle. One could think it's a huge loss, but guess what? Actually it doesn't matter. It hardly has any effect on Java or on Oracle. Dogs bark, but the caravan goes on.

By the way, I (seriously) think that dustmen and cleaners should be paid way more than programmers. Cleaning other peoples' litter is a job that sucks and is indispensable.
AnswerKenntnis:
Reducing management to creating charts and writing documentation is like saying that programming is typing.

To each their own, but for me programming is much easier than managing people.
AnswerKenntnis:
Everyone here is focused on the negatives. I've never met a programmer that likes office politics and good managers shield you from that sort of garbage. Having interacted with a lot of people at our main client, half of them are insane and I'm glad to have my PM there to soak up that insanity for me. If they pay them a lot, that's fine. He or she needs it for the inevitable therapy.
AnswerKenntnis:
It's arguable of course, but a significant reason behind this is that they carry the responsibility of the project if it fails, not the programmers. They might give you a earful for cocking something up, but they face criticism from even higher powers. They're the ones in charge of planning and estimation.

Managing requires a very multi-faceted skill-set: people skills, leadership, ability to estimate costs and time. To do all this they also need to still be in touch with your side of the things (ie have some clue of what you're doing, technically speaking) or be very good judges of character.

If requirements were not defined correctly, it's their fault.

If test plans were not defined correctly, it's their fault.

If you go on vacation or break your leg or get wasted on a Saturday night or leave without giving enough notice and they have to find a replacement or <some reason here> and you cannot get to do your work and the product doesn't get delivered (on time or at all), it's still their fault.

Also note that when I mean they carry the responsibility, it impacts people above and below them. If they screw things up, it might be your team's jobs that are on the line. That's also the kind of pressure you get paid for.

PS: Plus, I don't know if I would say that programming is harder than doing Gantt charts  (to reuse the example you mention). I don't know about you, but I find programming (in general, for 80% of the stuff you need to do in the industry) fairly easy. If you screw something up, you can fix it. If your boss screws up his gantt chart or his cost estimation, now that's going to be a much bigger issue than inverting a != null for a == null. Small mistakes matter on a wider scale for them. Most of the time, of course if you screwed up a test like this in an embedded medical application that went live, that's also a big problem. But they'll get more problems than you!
AnswerKenntnis:
Supply and demand is an economic model of price determination in a market. It concludes that in a competitive market, the unit price for a particular good will vary until it settles at a point where the quantity demanded by consumers (at current price) will equal the quantity supplied by producers (at current price), resulting in an economic equilibrium of price and quantity.
The four basic laws of supply and demand are:


If demand increases and supply remains unchanged then higher equilibrium price and quantity.
If demand decreases and supply remains unchanged then lower equilibrium price and quantity.
If supply increases and demand remains unchanged then lower equilibrium price and higher quantity.
If supply decreases and demand remains unchanged then higher price and lower quantity.


In this case, one reason is that there are too many developers.
AnswerKenntnis:
I've shifted between developer and PM roles throughout my career. I have developers on my project making twice as much as I do and others who are making half. The high wage earners are being paid what they are because:
A) They are "rockstar" developers.
B) They interact with the customers, explain the product in a way that is easy for the customers to understand, and are personable.
C) They direct teams of developers who work on multiple projects.
D) They are always available and eager to please.

They perform the roles of a developer, PM, and BA in varying capacities. Generally, if you're spending 90% of your time heads down, cutting code then you're not incredibly valuable and likely are easily replaceable. If you want to make more money then you need to take on more responsibility... and probably have to find another company that will pay you more.
AnswerKenntnis:
The rationale is that a project manager's area of responsibility (often) is to deliver the whole project on time, with acceptable quality, within a planned budget. There's often a lot of money at stake, so naturally good project managers often have higher compensation than programmers.

However I don't feel that business analysts, on average, earn a significantly higher salary than programmers. And it's my feeling that it is becoming less common that salary level in a company is determined by hierarchy and not by an employee's value.
AnswerKenntnis:
My experience might be different (or I'm living in an different universe with distorted laws of physics), but most business analysts and project managers (not program managers, but project managers or PMPs) positions I've seen are at or slightly below the average salary of programmers.

The salary gap begins to widen more when compared to the average salary of software engineers (on the software engineer's favor). The gap is even more when compared to senior EE or senior software engineers. Almost no senior business analyst or senior PMP will make the same as a senior EE or senior/principal software engineer.

A program manager, however (which is not the same as a PMP), that person will make a lot more than anyone else (and the reasons should be obvious.)



The thing that bugs me the most when I see these complaints about salaries is that as programmers (specially as junior/entry level programmers in the enterprise), we are (or were not) that special. There is nothing really in an entry level programmer right out of school that deserves a rocket scientist salary. No.

All of us that work on software started from zero. We all did. 

And IF we are really honest, we know well that we didn't know crap. Being able to complete our undergrad CS course load is just the starting point. It does not make us that special or ZOMG!!!! uber-Einstenian. Really, NO!

And yet (and thanks to the ill-fated period of the dot-com bubble), we expect to make not just more, but a lot more than another university-educated person just because OH WOW, we are programmers and they are just business analysts and PMPs.

Can you spell arrogance? Newsflash - for most programming tasks in the enterprise, you don't even need a 4-year degree. Really, is that serious.

Put the time on the grind and build the experience to transition from programming to software engineering (or engineering for that matter) at the senior level. Then you can demand to make much, much, pero mucho mucho much more than a business analyst and PMP.

Get it over with - some of us are (or were) overpaid. Period.



Rant aside: reasons for a business analyst and/or PMP to make salaries close or similar to programmers that have not yet accrued the necessary time and expertise to be mid/senior software engineers (or that have still not developed expertise in a highly demanded niche area):

A business analyst is the liaison between software and systems folks and business people/business processes (which are the ones that justify the existence of your paycheck, not the other way around.) They are the ones responsible for breaking down business processes in methodical, analytical manners, as input amenable for forming requirements, the stuff you work on. They make sure that you spend most of your time programming and not dealing with the minutia of business.

Many of you think business is easy shit. If you really think that's true, God help you.

A project manager is the person in charge of juggling multiple projects (whereas you only have to juggle with one or two at the most at any given time.) He's your umbrella, and he's the one that has to do the dirty job most of the remaining unwashed masses don't want to do - to chase people down making sure they do their jobs or removing impediments to your job.

He's the one that will ask you "what are you working on? is what you working on helping moving the project along? do you have problems with your work? what are your obstacles, what do you need? who can give it to you?"...

and then he'll go to others asking the same hard questions, making sure that obstacles are removed, and making sure that you are pulling your weight on the project (if necessary.)

The number one problem I've seen in many failed projects is a lack of PMPs or a disrespect towards PMPs (specially from developers.) It is rare that I see projects fail because of incompetent PMPs, and yet one has to wonder why many programmers are more than eager to say that is the case.
AnswerKenntnis:
I'm in finance, and I think the mentality is similar in most non-tech outfits:

Pay is proportional to career risk

Barring a complete dismissal of a group or team, the low-level programmers always keep their jobs.  It's the nature of the job, and programmers go into it knowing full well that they are taking zero risk.  If there's a bug, its not their heads on the chopping block.

At higher levels, if something screws up, you are the first to go.  I've had many experiences with a subordinate who made a small typographic error which led to us losing money, and I took the heat for it (not the actual programmer who made the error). 

Quite simply, the pay is commensurate with the risk.  Programmers, on the other hand, dont necessarily have any skin in the game, so to speak.
AnswerKenntnis:
If your question had been "why do X and Y get higher salaries than programmers at my company" I might have answered "you may work at the wrong company."

The success of a company in the software business depends more on the abilities of its programmers than anyone else. Companies that don't recognize this are automatically at a disadvantage vs. those that get it. Hiring the best programmers and taking good care of them is your best bet. The difference in the work of the great programmers vs. the rest is enormous; way bigger than the difference in salary they command. But if you insist on underpaying your programmers, you'll get what you pay for.

That said, every other role in the business is important. The great managers have a huge impact. A lot of that is by getting great programmers and keeping them happy. Something similar can be said about business analysis, marketing, sales, testing, and support.

If you are a great programmer and you're not being handsomely rewarded, go somewhere else. Then again, you may not be a great programmer. Unfortunately if you're not great it's hard to see why. If you knew why, you could change and be great, right?

I have been a programmer and I have been a people manager. I worked with a lot of great programmers, but only a few great managers. When I was a manager I was not great, but at least I knew it. My people got more raises than I did, which they deserved.
AnswerKenntnis:
It has little to do with skills and work, I mean little in the economy is tied to how much people deserve to make. 

Deserving to make more money is an ephemeral idea, everyone believes they deserve to make more money.

While it may not be fair, managers make more money simply because the business owners trust them more.  Managers often get higher salaries, simply so they won't take a new job out of the blue at an inconvenient time.
AnswerKenntnis:
I think your whole basis for this question is flawed.

Management must be paid more than their subordinates. Seniority in a company is generally based on salary, and there's no way a junior employee can have the means to command their seniors.

Leading people is a specialist skill. Not everyone can be a project manager (PM). The task is more and more difficult as the number of staff increases. In a technical PM role, the PM needs to have a good grasp of the technology to effectively lead - or they will not have the respect and support of their subordinates.
AnswerKenntnis:
In many professions, a core skill is the ability to sell something. And to sell something will, you need to sell yourself. You need the buyer to trust you and value the product or service you provide as much as you intend it to be. This skill is completely transferable to salary negotiations.
AnswerKenntnis:
I've gone through every post, and I dare to say that most of them are trying to compare apples and bananas. 

First of all, I believe that someone who says that 'manage is piece of cake' never had to manage anything further than his own schedule. On the other hand, say that 'anyone can code anything' is silly (and is in the wrong forum, for God sake!).

I specially liked rwong and luis.espinal asnwers, although I believe there are other facts that needs to be noticed as well.

I don't believe hierarchy as an answer - not nowadays - although it fit perfectly for the last 10.000 years. We lived for centuries in a society where the higher your profits, the higher your power (and vice-versa). I don't believe it applies for our world, in the way it is (specially in our area).

Back to the main question, I believe that managers usually earn more because they're more valuable to a company not because he's higher on hierarchy, but he's higher because of 


all the knowledge he already gathered from previous experiences (usually programmers have less experience than managers in general)
for being able to manage several things at once (programmers have one task - or a tasklist - to accomplish, whilst managers have to manage their own tasks 
they're the head contact for the project they manage, and for this reason they're the first 'target' in case something goes wrong. Is easier to lost your job if you're a manager;
being a developer, you have the 'license to redo something'. That's the 'risk' factor everyone mentioned.
developers are part of a whole project life cycle. I believe when we talk here of  'programmers' we are also thinking of the testers, technical writers and all other people who are highly important for the project's success.
and there's something that I saw only in a couple of posts across this topic: leadership. Being a manager is about to know how to be in touch with people, to negotiate, to keep
everybody motivated, to create synergy when everyone's mood is down. 


In my opinion, the leadership factor is the main reason for the higher salaries, because it generates a huge long-term result for the company and for everyone who's around the leader.

BTW, I had only a few experiences as a team leader (far from being a project leader!) and as much I know what a leader does, as much work I realize I have to do.

Edit: Forgot to highlight: communication skills aren't a strong point for most of us, but is a must for a leader. 
Besides, I'd like to share a very good post at Coding Horror, related to good programmers and communication skills -> http://www.codinghorror.com/blog/2011/02/how-to-write-without-writing.html
AnswerKenntnis:
Think about it this way, the number of skilled managers is less than the number of skilled programmers, therefore managers are more "valuable" to companies.
AnswerKenntnis:
That depends how you define 'difficulty'. Even though, I wonder if you do know what Project Management is about and what Business Analysts should be doing. I read much frustration from your question, so I think you have some bad experiences. Never the less, I want to try to answer your question.

Project Managers and Business Analysts are usually 'older' when they fulfill those positions. Where developers start their career very young (around their 20s), most project managers and analysts are near the age of 30 (which already creates a difference in payment just by age alone). They are also the ones that face customer exposing, which means they have to travel onsite, spend hours of torture to listen to the customer (especially when a project goes dead wrong) and enlist their wishes/needs. They have to be careful what they promise and especially within what scope (time-to-deliver). Even though from your perspective that what they do is only documenting, business analysts are educated to analyze the needs for the business, and project managers are guarding the planning of projects.

They act as a firewall between the customer and the developers. A technical perspective is something different than a sales perspective. Most business analysts and project managers are also facing a large variety of customers - they are exposed and therefor have 'leads'. Their network consists out of decision makers and therefor companies prefer to keep people with such networks within reach; after all a sale is a sale. 

Regarding difficulty? Start a company, have ten developers and try to manage a project. The headache comes with it for free. Do this for a year and then look at your answer again. For BA's? Go for such an opportunity. Sit down with customers who have an AIX machine from 1974 and the designer of that system is dead/retired/dying/alzeheiming and the developer needs to know if a certain value is generated or has some mystical formula. Try to convince 20 people with a powerpoint about your solution within 3 days time. If documenting was that 'easy', Linux would've pwned the world in 1997 already. Really, try writing a technical white paper every month for non-technical people (the ones that think that Facebook is a revolution in computing).

I am a sales engineer. Which means, I develop but my specialism is for prototypes and demonstrations. And I earn more than a business analyst or a project manager. Not because I have a network (I do, though), but because I left the attitude and focussed more on the business perspective, got myself certified and taught myself some soft skills. And the experience to learn that 'no' is also answer, when it comes to overtime.
AnswerKenntnis:
Simple answer: They're more valuable to the company than programmers. 

Why? Because they ensure that projects get completed, even if they're not doing the programming themselves. That means their value (purely in monetary terms to the company) is more than an individual programmer. The company doesn't believe that unmanaged programmers are productive, and therefore valuable... It's only the manager that makes them so.

Sucks, and we may not like it, but that's why the company pays them more. 

Their position (as others have pointed out) comes with drawbacks, though: If they fail to get a project completed by a certain time, it's their fault, not the programmers. They shoulder more responsibility, and are highly likely to get fired for failing (unless there's some BS company nepotism going on).

So, really, they're not allowed to make mistakes, have more pressure on them, and have a much more volatile job... but don't get confused: This isn't why they're paid more -- a company doesn't give a rat's ass how much pressure you're under, how volatile your position is, anything like that. They only care what value you bring to the company. Period.

That's capitalism, folks.
AnswerKenntnis:
I don't know how many times the Gantt Chart knowledge need to be updated in a year.
But doing programming you need to update yourself with new technologies which will not be so easy with your age.

Learning a new technology need hours of sweat, that if you smart enough to absorb.

The skill gained in years doing programming is not valued much in the current company culture.

Comparing newly graduate programmers salary with one with over 10 years of experience is a bit sad story.

Comparing a new PM with a 10 years PM is a great story, the PM might become Director after 10 years of experience.

So why are still so many people want to learn IT in the university?
I don't understand. Are they been properly informed? 

I do not understand how people value the skill nowadays.
AnswerKenntnis:
For the exact same reasons that a CEO can make 263 times as much as their average worker.
AnswerKenntnis:
Management doesn't always make more than the engineering staff.  The senior level engineering staff should be actively involved with business level analysis and decision making and charting the technical roadmap for the company. When this is the case, the senior technical staff can make quite a bit more than the business managers they work with everyday.

One of the popular myths of business is that the manager should be paid more then the people he/she manages.  IMO, you find this notion more deeply entrenched in buracracies than in functional, agile teams.

To put it another way: compensation is supposed to reflect the value of a person's contribution to the company. There are stellar business managers and average managers, and there are stellar engineers and average enginers.  If you have a stellar engineer that cranks out money making technology and has deep knowledge of the company's technologies, isn't it in the company's best interest to compensate this person more aggresively than an average business manager who happens to be managing this stellar engineer?  What is the opportunity cost of losing that engineering expertise and skill set because you neglected this valuable resource?
AnswerKenntnis:
I started one month ago with my first project as a PM. Before I worked as a programmer. (by the way, I get the same money as before.)

I found out that being a good PM means being a good programmer with a wide experience. You should be able to go from one team member to another and discuss the problems they have using your practical experience to help them to understand the problems by providing a different point of view. Your task is, besides other, to manage the interfaces. A PM is like a conductor. You can have the best musicians but if you don't have a good conductor who knows how to play the meta-instrument orchestra well, you get only a mess.

The counterpart is the specialist. This is the programmer who is able to solve difficult problems because he has a deep knowledge of the problem domain. These experienced people often also high paid if they are good enough in negotiation. Unfortunately specialists are often nerds and not so interested in money or good in making a good deal...
AnswerKenntnis:
Programmers don't put salary as the highest priority (Assuming it is at a reasonable rate.). Imagine two job offers where one has a higher salary, same time commitment, but requires technical support, strict business hours, dress code, writing user documentation, dealing with legacy code in an antiquated language you were hoping you never had to use again, how much more salary would you require?
AnswerKenntnis:
If you work for a company that respect programming, math, problem solving, whatever skills, then you may earn more for two things:


Doing more difficult work
Taking more responsibility


Just because a Hospital doesn't pay their skilled DBA much (see the example in the first answer) doesn't mean that this is the same in every company.
AnswerKenntnis:
All right I am slightly surprised with the answers, so here it goes. But before that, I will just like to clarify that I am a Programmer and there is nothing I like more than Programming. That said I have a healthy regard and respect for competent PMs and BAs. I realize that many of us resent PMs and BAs because unlike programming it is possible to excel in them without the required level of competency (office politics, nice suits etc). 

However both Project Management and Business Analysis are critical components of the software development. 

Whenever we think of software development many of us have a tendency to focus only on programming to exclusion of everything else. Yet there is more to it than coding.

First aim of development, which is to create a software which actually addresses and solves customer's issues. This implies first actually figuring out the customer's requirements (as customer may not be really sure what he wants), this is only possible by a detailed analysis of domain in which customer operates and structure of the various artifacts (whether it be people, technical infrastructure or process), and afterwards develop suitable business solution (and its integration with technology) to address those requirements.

Similarly any project of significant size absolutely can not work without effective management. Now I don't know how it is in other places, but so far my experience has been that PMs are usually promoted from ranks of programmers, so they do have some idea about what it takes to organize and execute the project.

To summarize both BAs and PMs are abstraction layer to development.
AnswerKenntnis:
Many people said here, that programming is more difficult and that is why it should earn more. That is a very romantic view. The truth is, that in a normal, healthy company the payment is according to the responsibility, that means to the added value of that person and also the risk.

The risk will often be forgotten. Normally if the programmer fails in his hard-to-do job, there might be some increased costs, but nothing more. Not like 10% of the workers will lose their job or something like that. The risk is quite low.

Also I want to disagree with the idea, that most business people earn more. I bet the normal business guy earns less then most Bachelors of Science/Engineering will earn. For example as a undergrad holiday coder I earned nearly the same as some full time business stuff workers in the same company.

And last but not least, why is the project manager not an engineer? Normally the project manager is a guy with many years at the front in the topic of the project he manages, meaning in programming jobs it will be an experienced programmer who is the project manager.
AnswerKenntnis:
There are corporate environments in which either the command and control pattern or the hub-and-spoke communication pattern dominates. In these organizations, the manager and the chief communicator are often the same person. This makes the manager a single point of failure - any dismal effects of miscommunications or lost-in-translations are amplified. Hence these environments require persons with extensive technical backgrounds as managers to ensure accuracy.

Better organized teams usually appoint a chief communicator to offload this responsibility. Organizations which practice knowledge management do not have any single point of failures in communication. In these organizations, the managers and chief communicators solicit for information and facilitate discussions. These information will be captured and processed for internal sharing. A different set of social skills is required.

Likewise, business analysts are often the single-point-of-contact between the customers and the company's technical staff.
AnswerKenntnis:
This is not always the case. When I worked for Computer Sciences Corporation (CSC), most managers made less than the "people who produced something useful". In the case of CSC, I think this was the case was because the company had been started by a group of programmers.

At the time (1970) there was another software company in LA whose name I forget with an interesting salary schedule. Programmers got paid $25,000/year and support staff got paid $15,000/year. The idea was that if you were the worse programmer there you should not be surprised to get replaced.
QuestionKenntnis:
How Can I Know Whether I Am a Good Programmer?
qn_description:
Like most people, I think of myself as being a bit above average in my field.  I get paid well, I've gotten promotions, and I've never had a real problem getting good references or getting a job.

But I've been around enough to notice that many of the worst programmers I've worked with thought they were some of the best.  Bad programmers who are surrounded by other bad programmers seem to be the most self-deluded.

I'm certainly not perfect.  I do make mistakes.  I do miss deadlines.  But I think I make about the same number of bonehead moves that "other good programmers" do.  The problem is that I define "other good programmers" to mean "people who are like me."

So, I wonder, is there any way a programmer can make some sort of reasonable self-evaluation?  How do we know whether we are good or bad at our jobs?

Or, if terms like good and bad are too ill-defined, how can programmers honestly identify their own strengths and weaknesses, so that they can take advantage of the former and work to improve the latter?
AnswersKenntnis
AnswerKenntnis:
A good programmer understands that that they have to continue to learn and grow.  They strive to do their best at every effort, admit to failures and learn from them.

They are extraordinarily communicative.  Not only are they able to explain complex technical terms to a layperson, but they go out of their way to act as devil's advocate to their own idea to make sure they're giving the best options to their client.

The best programmers know and accept that there is more than one way to do things, that not every problem is a nail, and that because there is always a better way to do something than how they were planning on they constantly seek to learn new techniques, technologies, and understanding.

A good programmer loves to program, and would do so in their spare time even if they already spend 80+ hours a week programming.

A good programmer knows that she/he is not a great programmer.  Truly great programmers do not exist, there are only those who claim to be great, and those who know they are not great.
AnswerKenntnis:
As Paul Graham points out eloquently in this pod cast, you can't. Only your coworkers can tell you.
AnswerKenntnis:
I've always found that it's easiest to judge your performance by doing two things.


Surround yourself with other good programmers
See how much they complain about the code you write.


The issue of course is finding good programmers, and then being a good programmer also isn't just about coding. You need to be able to work well in groups, yet also work well by yourself. 

Now for the sake of going off topic, I will quote Robert A. Heinlein and his view on the subject:


  "[A kick-ass programmer] should be
  able to change a diaper, plan an
  invasion, butcher a hog, conn a ship,
  design a building, write a sonnet,
  balance accounts, build a wall, set a
  bone, comfort the dying, take orders,
  give orders, cooperate, act alone,
  solve equations, analyze a new
  problem, pitch manure, program a
  computer, cook a tasty meal, fight
  efficiently, and die gallantly.
  Specialization is for insects."
   - from The Notebook of Lazarus Long.
AnswerKenntnis:
This is the Programmer Competency Matrix just for you: http://www.indiangeek.net/wp-content/uploads/Programmer%20competency%20matrix.htm
AnswerKenntnis:
Jeff has one of my favorite blog posts on this topic...Why I'm The Best Programmer In The World

"...it's not our job to be better than anyone else; we just need to be better than we were a year ago."
AnswerKenntnis:
I think the fact that you are asking the question proves you are not a bad programmer, so, in my opinion, you are half way there.  :)

Bad programmers always think they are great programmers, in my experience.
AnswerKenntnis:
@Nick's statement "Bad programmers always think they are great programmers..." is explained by the Dunning Kruger Effect, which generalises that people who know a little about a subject often over estimate how much they actually know.

Being a bit facetious... the less you think you know the more you probably do.... unless of course you are a really self aware idiot.

Answering the original question, though i tend to think that the more influence (not control) you have generally is a good indicator. If you see other's following your lead, or picking up your practises then you are on the right path.
AnswerKenntnis:
The answer that got the most plus votes is really distressing. Basically it says that you have no life outside of programming. What about family? Community? Hobbies? What kind of a profession are we in where you have to be preoccupied to the point of obsession just to be considered "good"? I really think we need to get some perspective here.
AnswerKenntnis:
You could try competing in the TopCoder algorithm contests.
AnswerKenntnis:
I'm not perfect. I make mistakes. I miss deadlines. But I think I make about the same number of bonehead moves that "other good programmers" do.


That realization alone makes you a better programmer than most of the bad programmers out there.  

A lot of the worst programmers tend to think that they already know everything there is to know and are not aware of their limitations.  As a result, they never improve their skills.
AnswerKenntnis:
If you look at your code from let's say a year ago, and think, jeez, I could have done that a lot better, you're probably good :).
AnswerKenntnis:
Here are some real life examples of bad programming. Of course, similar code was all over the place copy/pasted in 100 places. Guy got fired, but I've heard that he got a nice job again. Enjoy:

a)  

if (! TableObject.loadList("sql condition").isEmpty()) {  
    List<TableObject> myList = TableObject.loadList("sql condition");  
    ...  
}


b)

public static Type getInstance() {  
    if (instance == null) {  
        return new Type();  
    }  
    return instance;  
}


c)

getForeignKeyObjectProperty1() {  
    return ForeignKeyObject.loadByPrimaryKey(foreignId).getProperty1();  
}  

getForeignKeyObjectProperty2() {  
    return ForeignKeyObject.loadByPrimaryKey(foreignId).getProperty2();  
}  

...

getForeignKeyObjectPropertyN() {
    return ForeignKeyObject.loadByPrimaryKey(foreignId).getPropertyN();
}


d)

public boolean isHasImage() throws SQLException {
    StringBuilder query = new StringBuilder();
    query.append("select user_name");
    query.append(" from user");
    query.append(" where has_image = 1");
    query.append(" and user_name ='"+getUserName()+"' and user_image is not null");
    Connection c = Database.getInstance().getConnection();
    Statement st = c.createStatement();

    try {
        ResultSet rs = st.executeQuery(query.toString());
        if (rs.hasNext()) {
            return true;
        } else {
            return false;
        }
    } finally {
        st.close();
    }
}


If you make this kind of code, stop programming. If you don't see anything strange in this code, stop programming. Otherwise you aren't bad, so you even might be good :)

EDIT: To answer comments: I got job before graduating, and this guy already had few years of programming experience. He got fired few months after I got employed so I wasn't in position to tutor anyone. Examples above were just from top of my head - every peace of code he touched was flawed in various and imaginative ways. Most of the stuff started to creep out after he went from the company, because only then other people saw some parts of the code. He is generally a nice guy, pleasant to talk with etc. but he will NEVER be a good programmer, just as I will never be a good painter or a writer or whatever.

To contrast this with another example, guy who came to replace him was also undergrad at the time. He studied college more famous for management then programming. He isn't too geeky in a sense that he programmed anything for fun or would sit home and read about java or programming, yet he is doing just fine. He adjusted quickly and started producing useful and maintainable code. Some people can do that, other can't - just ask dailywtf.
AnswerKenntnis:
There are a few things you could try, to get a better gauge of how you stack up.


Compare code reviews. See whose review revealed more problems.
Ask when was the last time they read books that were peripheral to their normal course of study. Then ask yourself the same thing.
Ask yourself who brings in new ideas to the company (and how well they work).
Last (and least), is there some form of company recognition?


(I put that one last because at my last company, one programmer received "developer of the year" twice in three years. After he left, we found at least 20 TDWTF-worthy code snippets. He developed code fast, but not necessarily well. Management just didn't know the difference.)
AnswerKenntnis:
Allow other developers who you respect to work with or see your code.

Have people actually use what you like and see what they think.

-- Kevin Fairchild
AnswerKenntnis:
Can you understand this?

if(rp->p_flag&SSWAP) {
	rp->p_flag =& ~SSWAP;
	aretu(u.u_ssav);
}


:-)
AnswerKenntnis:
Just the mere thought that you need to self-evaluate makes you a cut above the rest.

One way I always judge myself is listening to what my co-workers have to say about me. Trick is in finding the right people.
AnswerKenntnis:
It's always subjective who is a good programmer. I agree with Nick that simply asking the question is a step in the right direction. I think the constant desire to learn more and improve is what makes a good programmer.
AnswerKenntnis:
To me the very best programmers are never looking for work.  They have standing offers for new positions just based on their reputation.  So a good programmer may have offers from previous employers to return, if they desired.  Or a good programmer would have had inquiries from former coworkers about coming to work for them at a new company.  

In terms of strengths/weaknesses, you probably already know them.  If not, ask a more senior person on your team.  It doesn't even have to be a developer.  A good project manager knows the strengths/weaknesses of programmers.  Besides, the traits that define a good programmer are not just limited to code.  Understanding the business, communication skills, judgement, etc. are all performed outside of your IDE.
AnswerKenntnis:
I think this is about like wondering how you can know if you're a nice person.

I think, unfortunately, the answer is that only other credible people can tell you that.  I don't think it's something you can accurately determine for yourself (at least it's very difficult - but I think it might be impossible).
AnswerKenntnis:
i would simply say: if you're passionate (i mean REALLY passionate) about what you do, if you're flexible (another language, new technology, complete remake of an old project - let's do it!), if you keep learning and improving your skills and never, never think you're good enough to stop - then you're a good programmer!
AnswerKenntnis:
95% of all programmers think they are among the top 5% programmers, the other 5% are managers. So if you are a manager, you are probably not a good programmer, otherwise you probably are.
AnswerKenntnis:
I think it's more a matter of what you do with your programming skills. Being a great programmer is fine, but what does it matter if you're coding bank software all day (no offense). It just doesn't add up.

To really prove to yourself that you're a good programmer, take on an interesting a difficult side project. This shows a few things: you're interested in programming on your own free time and genuinely enjoy the subject -- this is essential to being a good programmer. It shows versatility in that you can extend your skills beyond what you do in your work environment. It shows motivation and creativity as well: you've defined a problem on your own and are taking steps to solving it.

All of these aspects define a good programmer to me.
AnswerKenntnis:
My answer will be politically incorrect. However, I am actually a founder of a start up and my job is to hire the best programmers.

How do I know which people will be good programmers or can be good programmers?

1 word: IQ.

IQ is for programming as height is for basket ball players.

I would put Math skills as another more politically correct indicators. My computer science teachers told me that there is one criteria that correctly predict who will succeed at programming courses. High School Math scores. Those are effectively IQ anyway.

This may come as a surprise. We are told that programming is a college degree job. Guess what, I don't care about degree. I care only about 3 things.


Programming skills (that can be tested).
Aptitude.
Common sense (not too low) people's skills.


Unfortunately IQ only measures one thing. So I do some tests.

I ask people questions. Simple question I expect elementary school kid should be able to answer.

I ask questions like:


What's the distance between short and long hand of the clock at 7:35
If I go from Jakarta to Semarang and want to maintain 60km/hour speed. Then on first 2 hours I drive 100km/hour. How fast I should go for the rest of the trip so that my AVERAGE speed is 60km/hour?


If you look carefully those questions do not require creativity at all. Only common sense. Any elementary school kids with IQ can answer that.

MOST college graduate cannot. Now imagine if someone cannot answer that? Can he program? Think about it. How often you will have to specify what he should do?

You can also ask questions like Microsoft that do require creativity. You can ask questions on how to weigh 8 balls in a balance scale to know which one is the heaviest. The problem with such questions is that it involves luck. However, some are quite easy it should be a minimum requirement.

Getting an awesome programmer can improve morales of other programmers. Getting one lawsy programmers can lower everyone else' morale.

After people can answer questions like that, which is very easy, I would make them write some google jam code.

Good programmers can solve google jam code and can predict which problems are easier.
AnswerKenntnis:
imho you are a good programmer if 

-you have a sound theoric background. reinventing the wheel as well as an algorithm or a framework is a waste of time, most of the times.

-you can see things and problems from a skewed perspective sometimes. this may bring you to find innovative solutions.

-you spend time money and efforts to have the best tools and the best skills updated.

-your code is easy to modify. if you design clean, elegant and understandable code, modyfing it will not be painful.

-if your code/bugs rate is reasonably high. I know this may seem trivial, but I know many creative and skilled developers that nonetheless are very prone to do trivial bugs. this impairs greately their effectiveness and usefulness.

-people around you know that you can help deciding what choices to do

-you can solve problems using different tools. bad programmers keep using the same tool (be it a language or a technology or an architecture and so on) for whatever problem they get

-you keep learning, you are curious.

-you have fun programming, after all these years
AnswerKenntnis:
"Sir, I have been through it from
  Alpha to Omaha, and I tell you that
  the less a man knows the bigger the
  noise he makes and the higher the
  salary he commands."
   Mark Twain


... my conclusion is that good programmers command a low salary and make little noise... :)
AnswerKenntnis:
Best Programmer: Complete the task with lowest number of code statements on time.
AnswerKenntnis:
How many bugs does your code have per some metric? (i.e. bugs per line)
How much of your code has to be recoded when new features need to be added?
Do your coworkers have issues modifying your code?

Ultimately, the question is next to impossible to answer given that the question of "what is quality code" is still a hotly debated topic after all these years.
AnswerKenntnis:
I really like what Adam V said above. 

In addition to what others have said, look at the maintainability and support history of your work. If your code has heavy maintenance requirements and is constantly being fixed, or is difficult to change, those can be indicators of its quality. But these can also be indicators of poor requirements.
AnswerKenntnis:
It's very hard to self-assess.  Incompetent people tend to have wildly inflated assessments of their own abilities.  

A good metric is whether other people whom you respect as programmers themselves want to work on/with code that you've written.  Given the chance to work directly with you, or to inherit code that you've written, will then turn it down? Agree grudgingly?  Or jump at the chance to learn from your work?  This technique works even better if it's a peer, not an underling or someone who thinks there may be any benefit to them from making you feel good.
AnswerKenntnis:
If you really want to find out, submit some code to your peers, and ask for a (constructive) code review.  In the worst case, you learn something.  The mere fact that you're here on this site seeking enlightenment already sets you apart from the unwashed masses.  (unless you're just here for ego stroking. :-)
QuestionKenntnis:
Why can't the IT industry deliver large, faultless projects quickly as in other industries?
qn_description:
After watching National Geographic's MegaStructures series, I was surprised how fast large projects are completed. Once the preliminary work (design, specifications, etc.) is done on paper, the realization itself of huge projects take just a few years or sometimes a few months.

For example, Airbus A380 "formally launched on Dec. 19, 2000", and "in the Early March, 2005", the aircraft was already tested. The same goes for huge oil tankers, skyscrapers, etc.

Comparing this to the delays in software industry, I can't help wondering why most IT projects are so slow, or more precisely, why they cannot be as fast and faultless, at the same scale, given enough people?



Projects such as the Airbus A380 present both:


Major unforeseen risks: while this is not the first aircraft built, it still pushes the limits if the technology and things which worked well for smaller airliners may not work for the larger one due to physical constraints; in the same way, new technologies are used which were not used yet, because for example they were not available in 1969 when Boeing 747 was done.
Risks related to human resources and management in general: people quitting in the middle of the project, inability to reach a person because she's on vacation, ordinary human errors, etc.


With those risks, people still achieve projects like those large airliners in a very short period of time, and despite the delivery delays, those projects are still hugely successful and of a high quality.

When it comes to software development, the projects are hardly as large and complicated as an airliner (both technically and in terms of management), and have slightly less unforeseen risks from the real world.

Still, most IT projects are slow and late, and adding more developers to the project is not a solution (going from a team of ten developer to two thousand will sometimes allow to deliver the project faster, sometimes not, and sometimes will only harm the project and increase the risk of not finishing it at all).

Those which are still delivered may often contain a lot of bugs, requiring consecutive service packs and regular updates (imagine "installing updates" on every Airbus A380 twice per week to patch the bugs in the original product and prevent the aircraft from crashing).

How can such differences be explained? Is it due exclusively to the fact that software development industry is too young to be able to manage thousands of people on a single project in order to deliver large scale, nearly faultless products very fast?
AnswersKenntnis
AnswerKenntnis:
Ed Yourdon's Death March touches upon a number of these meta type questions.

In general, the software industry lacks a lot of the following, which gets in the way of large projects.


Standardization and work item breakdown.  


This has certainly gotten better, but the design constructs still aren't there to break out a big system. In some ways, software can't even agree on what's needed for a given project, much less being able to break things down into components.
Aerospace, building construction, auto, etc.. all have very component-driven architectures with reasonably tight interfaces to allow fully parallel development. Software still allows too much bleed through in the corresponding areas.

A large body of successful, similar projects. The A380 wasn't the first big airplane that Airbus built. There are a lot of large software applications out there, but many of them have suffered dramatically in some aspect or the other and wouldn't come close to being called "successful."
A large body of designers and builders who have worked on a number of similar and successful projects. Related to the successful project issue, not having the human talent who has been there, done that makes things very difficult from a repeatability point of view.
"Never" building the same thing twice. In many ways, an airplane is like any other airplane. It's got wings, engines, seats, etc.. Large software projects rarely repeat themselves. Each OS kernel is significantly different. Look at the disparity in file systems. And for that matter, how many truly unique OSs are there? The big ones become clones of a base item at some point. AIX, Solaris, HP-UX, ... herald back to AT&T System V. Windows has had an incredible amount of drag forward through each iteration. Linux variants generally all go back to the same core that Linus started. I bring it up, because the variants tend to propagate faster than the truly unique, proprietary OSs.
Really bad project estimation. Since the repeatability factor is so low, it's difficult to project how large it will end up being and how long something will take to build. Given that project managers and Management can't put their hands on the code and actually see what is being done, unrealistic expectations regarding timelines get generated.
QA / QC is not emphasized as heavily as it could or should be for larger projects. This goes back to having looser interfaces between components, and not having rigid specifications for how components should work. That looseness allows for unintended consequences and for bugs to creep in.
Consistently measurable qualifications. Generally, people speak of the number of years they've worked in X language or in programming. Time in is being used as a substitute for caliber or quality of skill. As has been mentioned many times before, interviewing and finding good programming talent is hard. Part of the problem is that the definition of "good" remains very subjective.


I don't mean to be all negative, and I think the software industry has made significant strides from where we've been. Forums like this and others have really helped promote conversation and discussion of design principles. There are organizations working to standardize on "baseline" knowledge for software engineers. There is certainly room for improvement, but I think the industry has come a long way in a reasonably short period of time.
AnswerKenntnis:
The answer is surprisingly simple: those 'other industries' do have a high failure rate.  We're just comparing the wrong things.  Writing software is often called 'build', and so we compare it to the manufacturing or construction phases in other industries.  But if you look at it, it's not construction at all: it's design.  Software designs are written in code, and building is done by computers, whether by compiling software or directly interpreting it on the fly.

Many designs in other industries either take way longer than originally estimated, cost way more, or simply never see completion.  Sound familiar?

So, what are we doing when we're planning software?  Well, we're still designing, but at an earlier stage.

In software, there's no manufacturing line of note.  Building the final component is (comparatively) cheap, and replication of that final product is both perfect and effectively free--you copy the build artifacts.
AnswerKenntnis:
To point out some figures:


Change of requirements after implementation started; for example when the first Airbus A380 started to be created in the factory I cannot believe that if someone wanted 200 more seats, those would be put there; but in a large software project even after the programmers started development 5 more types of users can be added.
Complexity - large software projects are extremely complex; probably the most complex projects human kind designed and developed;
Not enough resources are spent in architecture and design phase;
Field immaturity - software engineering is relatively a young discipline compared 
with other engineering sisters; this has two implications:
a) Not so many heuristics and good practices;
b) Not so many very-experienced specialists;
Lack of mathematical proof - in most of the cases mathematical formal methods are not used to prove that a piece of software works as required; instead testing is used. This does hold true in other engineering fields which rely more heavily on mathematics;  the reason of this is as complexity;
Rush - many managers have unachievable deadlines; so quality of code is placed second, because of the deadline.


Answering strictly to the question - I tend to believe that others have very high expectations (especially in delivery time) from programmers and do not understand exactly how difficult programming large software is.
AnswerKenntnis:
The premise of the question is a bit flawed.  Both the A380 and the Boeing 787 were delivered years late.

In the case of the A380 much of the delay was caused by the French and German units of Airbus using different and slightly incompatible versions of CATIA design software. This incompatibly manifested itself as wiring harnesses that didn't quite fit the airplane.  

There wasn't anything wrong with CATIA, which is the most widely used aircraft design software, but someone somewhere dropped the software configuration ball.

The Boeing 787 also suffered from software related delays, but most of its problems were more traditional new airplane problems like weight control and delivery of substandard parts by suppliers.

Both the A380 and the B787 had to modify their wing designs after the initial aircraft had structural issues.  

Large complex projects are difficult for humans in all fields.
AnswerKenntnis:
Skyscraper guy here. Not sure if I can answer your question but I can surely shed some light into various items in the thread. Buildings do indeed occur very fast. A major constraint is locale (regulations). But in general it takes 3 to 10 years for a tall building from start to finish. 

I think comparing a new building with a new software project is not very accurate. A new building is closer to a new version of a kernel or OS. In this respect software development is much faster. We never build from zero as this will be a high risk task the client would never sign up for. Most design and development in buildings is derivative of proven and completed projects.

From personal experience only one in ten projects ever get built. The process is development-driven rather than design-driven, so the moment something like the price of steel goes up the whole project is out the window, or designed, as design is the cheap component of the process.

Design takes a month for concept to schematic, two to six months to design development, another six months to details and construction documents by a team of architects, planning consultants, structural engineers, wind engineers, services engineers, quantity and cost consultants, surveyors, accessibility engineers and the list goes on...

The argument of virtual versus physical is not very accurate. We also work mainly with virtual tools, and moreover we are both time- and scale-remote from our final product. In most cases we can not even test aspects of buildings in mockup scale and we use simulation to try predict what may come about.

Complexity-wise there are differences, but overall it is maybe about the same. We not only have interrelated units and multiple levels of tiered abstractions and interfaces but also people are very much specialized in small parts of the process that make communication very difficult.

As for the argument of design versus development, I think both processes are design-built. It sounds academically nice to keep these separated but it is not possible to design if you don't know how things work. You just increase the risk of failure.

Overall my (potentially wrong) estimation as per OP's question is that programming is more of an art than engineering. Why would people take pleasure and even do it for free, find expression and elegance in it? Computer science is also (as on the tin) more of a science than engineering. Why would you try to prove algorithms instead of just patching existing parts together and work to make things just work? Not sure if this makes any sense; I'm not a software guy.

One aspect that strikes me with software design and development is about the medium itself. Computers make human-centric work very unnatural. Everything is so very explicit and there are few tolerances. It's hard to mentally work your way around this, and some get away with it by dumping complexity within. If nothing else this may have something to do with it?
AnswerKenntnis:
Then how long did the design of those took? Year? Two? Ten years? The design is the most complex part of building something, the construction itself is easy.

Based on this article, it is slowly being understood, that software development is mostly design process where design document is the source code itself. And the design process is totally different from the production process. It requires experienced people and is impossible to plan, because even small requirement changes can result in huge changes in the overall architecture of the project. This understanding is the basis for agile methodologies that focus on improving code quality as the final design document and taking testing and debugging as parts of the design process, just like they test airplane models in wind tunnels.

The construction itself is handled automatically by compilers. And thanks to that, we are able to build whole products in a matter of minutes.

The reason why software projects are finished with huge delays and inflated costs is because managers still think they can estimate, predict and plan such a design process. This backfires more often than it is actually valid. They still think that by tying people into a rigid construction process they can somehow increase quality even though end result is mostly opposite with increased costs and missed deadlines.
AnswerKenntnis:
As someone with a mechanical engineering background working in IT, I've often wondered about the reasons of the low success rate in IT.

As others in this thread, I've also often attributed the failures to the immaturity of IT, the lack of detailed standards (yes I'm serious, have you ever checked the standard sheet of a simple bolt?) and the lack of standardized components and modules.

Other industries, like building construction or ship building also have much more "beaten paths": knowledge and experience of a particular solution prototype, which - in customized form - is re-used again and again. Ever wondered about why buildings, ships or airplanes of different size and purpose somehow look so similar? (there are exceptions to the rule of course...)

That is because those prototypes are well researched, well understood, generally used and have a proven track record. Not because it couldn't be done any other way. In IT standardization is rarely the case: (large) projects tend to re-invent components, doing research and delivery at the same time and with the same people!

These inevitably lead to one-off products, which are expensive to develop and service, are error-prone and fail in unpredictable ways under uncertain conditions. And because of this, of course, these products are much quicker obsolete, written down and replaced at equally great costs with only slightly better ones. What IT needs is the equivalent of the industrial revolution, which turned middle-age artisans into efficient factories.

That said, there are factors that make IT truly unique however. As opposed to those other mentioned industries, IT is truly ubiquitous: it is used in every aspect of our modern life. So it's a small miracle IT achieved this much progress and is capable of delivering the results it does.
AnswerKenntnis:
I'm afraid that I disagree with your statement.

Airbus and Boeing are two examples of companies that build planes. How many companies that build planes are there? Very few, if you would compare it to how many companies build software.

It is equally easy to screw an airplane project as to screw a software project. If only the entry barrier was so low in the aircraft-building industry as it is in the software industry, you will certainly see many failed aircraft projects.

Look at cars; There are high-quality manufacturers that build very durable and highly advanced automobiles (think Land Rover, Mercedes) and there are ones that build cars that won't last a year without having to repair them (think Kia or Cherry). This is a perfect example of an industry with slightly lower entry barrier, were you start to have weaker players.

Software is no different. You have lots of buggy products, on the other hand, Windows, Office, Linux, Chrome, or Google Search are very high-quality projects that were delivered on time and had similar quality level as an aircraft. 

The other point that many people miss is how much maintenance goes into maintaining a car, a tanker or an aircraft that we just take as a fact of life. Every plane has to undergo a technical check-up before every take off. You have to check-up your car every several k miles and do so regular stuff like change oil, change tires.

Software needs that too. If only people spent as much time on diagnostics, prevention or auditing software's state and quality as they do with mechanical/physical products, I would expect way less statements like these. Do you read your application's logs each time before you launch it? Well.. if it was an aircraft you would have to ;-)
AnswerKenntnis:
Imagine, in the middle of the design of the Airbus A380, someone piped up in a meeting and said, "Heh, could build it as a triplane?" Others joined in saying, "Yeah, yeah. A triplane. More wings are better." The next thee years is spent turning the A380 design into a triplane. At another meeting, someone says, "A triplane? That's old. We want a biplane. Just remove one of the wings."

Or imagine, in the middle of a bridge construction project, someone says, "Heh, we just made a deal with a shipping company. They need the bridge to be another 40 feet higher, because their ships are much taller. Fix it. Thanks."

These are but some of the reasons why software projects, big and small, end in failure at an alarming rate.
AnswerKenntnis:
Engineering standards and practices are very different in IT (as an independent industry) than in aerospace. This is perhaps most easily understood by considering how IT professionals react when encountering standards documents for IT in aerospace. For example, the Joint Strike Fighter C++ Standards that have made their way around the Internet in recent times.

Many express bemusement or wistful resignation (wish we could do that way); and many respond with ridicule, claiming there is no practical way to deliver consumer products in this way. This may even be right, given the expectations, not of consumers, but of management. There is a great deal of distrust for coders who just code and code for a few weeks, not demoing anything. God help the coder who merely designs something for two weeks. Not so with airplanes.

In software, people really expect to have something right now. Sure, the reasoning goes, it will take a little while to have it really solid; but can't we have some complex thing described in simple terms in a week? One learns, also, that complex things described in honest, complex terms rarely excite the imagination; and thus many engineers end up being complicit in an imagined world of really simple things being put together in creative ways (as opposed to a world of hard things being done really well).
AnswerKenntnis:
I have often wondered the same thing. It certainly feels (occassionally) like we're a bunch of amateurs that don't have any idea what we're doing. I dislike explanations that put the blame on managers or other external factors -- we the developers should be responsible for what we create.

I think we are in a business where errors are cheap. Patching software is cheap, compared to rebuilding a skyscraper, or recalling every sold cellphone.

This has created a culture where bugs are a part of everyday life. They are accepted with a shrug. While some bugs are probably unavoidable, should they dominate our day to day work? I completely understand managers who don't feel that QA is worth the trouble, precisely because they expect bugs anyway. I don't understand programmers who don't make every effort to produce error-free code, because correcting bugs is boring as hell.

In essence I believe it is a culture problem, and I hope it will change.
AnswerKenntnis:
Digital building blocks can be 1 or 0. There is no inbetween.

A mechanical design has a level of tollerance. I can put one less than perfect rivet into a bridge and it will most likely will not fall down, however, in code even just once instance of putting a 0 where a 1 should be can fail the entire program.

Due to the logical and interative nature of computing, any, even very small changes, can lead to drastic failure.
AnswerKenntnis:
Some stuff from me:

1- Standards and parts: They are plane manufacturers, not developers. I am not entirely sure, but my guess is that a lot of parts are outsourced. They don't build their own electronic/instruments, they get seats from some company, the engines are probably developed elsewhere, etc.

Software projects, on the other hand, almost always start from scratch with just some small frameworks/helpers in place. 

2- Time to hit the market: Time is not a critical issue for planes. I bet the design of the Airbus was finalized years before it was finished, and they did chose to neglect any major breakthroughs that might happen in that time. (Same for car manufacturers, for example, the cutting-edge technology they develop at the moment will hit the streets in 5-10 years.)

For software you need to be very agile, if I start a huge project now and take three years to develop it without any change the chances are pretty high that I am relying on technology that is not available anymore or my product is completely outdated. This in turn offers a lot of problems.

3- Release-cycle and versions. - A plane needs to be completely finished when it is "released". There are no stable beta versions, nightly builds or similar. Additionally, once it's done, it can only be modified in a small way. You can't add an additional level with 100 seats to an existing boeing, it's just not possible.

Software on the other hand has incremental changes that are often just "builds that work", but not necessarily finished products. Also, in IT it's not unusual to say "hey, let's add another luggage compartment to our plane which holds additional 50 tons".
AnswerKenntnis:
Software engineering and management is fundamentally different than a lot of other engineering areas. The deliverables aren't physical, and the production process is the design and development process. Creating another copy of a piece of software has essentially zero marginal cost; all the cost is found in developing the first copy. 

Because of the relative youth of software engineering and management as a discipline, there is some misinformation and falsehoods out that are still taken as fact (see this reference) which hinders software development and engineering as a whole.
AnswerKenntnis:
Not all developers are created equally.  Some are good, others are, well, not. 

Try reading other people's code all the time to get a feel of what I'm saying.  Too many extra logic statements can add risk.  These risks can lead to ill behavior or bugs.  Not enough logic statements and now you have null references.  The good programmer understands this and knows when to do what and where.  But no one is perfect.  Things are complex.  Add the poorly thought out work of others and it is easy to see how projects run away.
AnswerKenntnis:
Cathedrals used to take up to 100 years to build.

Some Airbus airplane needs 1 million lines of code to work.

The more time you have been improving something, the more improvement you get, so give the software industry a couple of centuries of trial-error to get better, and we'll see how much it takes a to develop a solid huge project without bugs or flaws.
AnswerKenntnis:
I will try answering using a verbatim copy of an article from Jack Ganssle's embedded muse. While this says firmware everywhere, just mentally replace it by software.


  Compared to What?
  
  Firmware is the most expensive thing in the universe. In his wonderful
  book "Augustine's Laws," Norman Augustine, former Lockheed Martin CEO,
  tells a revealing story about a problem encountered by the defense
  community. A high performance fighter aircraft is a delicate balance
  of conflicting needs: fuel range vs. performance. Speed vs. weight. It
  seems that by the late 70s fighters were at about as heavy as they'd
  ever be. Contractors, always pursuing larger profits, looked in vain
  for something they could add that cost a lot, but which weighed
  nothing.
  
  The answer: firmware. Infinite cost, zero mass. Avionics now accounts
  for more than half of a fighter's cost. That's a chunk of change when
  you consider the latest American fighter, the F-22, costs a cool third
  of a billion a pop. Augustine practically chortles with glee when he
  relates this story.
  
  But why is software so expensive? Tom DeMarco once answered this
  question with these three words: compared to what? He went on to
  discuss relatively boring business cases, but that answer has
  resonated in my mind for years. Compared to what? With software we
  routinely create product behaviors of unprecedented complexity. Sure,
  the code's expensive. But never in the history of civilization has
  anyone built anything so intricate.
  
  Consider the following bubble sort, lifted shamelessly from Wikipedia
  and not checked for accuracy:

void bubblesort(int * A, int n){

    for(int i(0); i < n; ++i)

        for(int j(0); j < n - i - 1; ++j)

            if(A[j] > A[j + 1])

                std::swap(A[j], A[j + 1]);

}

  
  It's a mere 110 non-space characters, perhaps tossed off in an hour or
  two. Suppose we didn't have software and had to implement a sort using
  some other strategy. What would it cost?
  
  A mechanical engineer might boast that his profession built sorters
  long before computers. Consider IBM's 1949-era model 82 card sorter
  (http://www.columbia.edu/acis/history/sorter.html) with a throughput
  of 650 cards per minute, rather less than our code snippet might
  manage even on a 4 MHz Z80. The model 82, of course, only sorted one
  column of a card at a time; to completely sort a deck could take
  dozens of passes.
  
  How long did it take to design and build this beast? Years, no doubt.
  And its functionality pales compared to our code which is so much
  faster and which can handle gigantic datasets. But that was 1949. How
  long would it take to build a bubble sort from electronic components -
  without FPGAs and VHDL, or a CPU?
  
  In an hour I managed a rough block diagram, one above the chip level
  (blocks have names like "adder," "16 bit latch" and the like). But the
  sequencing logic is clearly pretty messy so I've just tossed in a PLD,
  assuming at some point it wouldn't be too hard to write the
  appropriate equations. And, yes, perhaps that breaks the
  no-programmable-logic rule, but to design and debug all that logic
  using gates in any reasonable amount of time is as unlikely as
  buck-a-gallon gas.
  
  Assuming 16 bit words and addresses, the circuit will need around a
  dozen 16 bit latches, adders, and the like. Plus memory. And I have no
  idea how the unsorted data arrives into the RAM or how the results get
  exported. Those are unspecified design requirements.  The
  software-only solution naturally resolves these requirements just by
  the act of writing the function prototype.
  
  Translating the rough block diagram to a schematic might take a day.
  Then there's the time to design and produce a PCB, order and load
  parts (and change the design to deal with the unexpected but
  inevitable end-of-life issues), and then of course make the circuit
  work. We could be talking weeks of effort and a lot of money for the
  board, parts and appropriate test equipment.
  
  All this to replace 7 little lines of code. Few real embedded programs
  are less than 10,000; many exceed a million. How much hardware and how
  much engineering would be needed to replace a real, super-sized
  computer program?
  
  Consider a real program like a spreadsheet. How much circuitry would
  it take to make one without a processor? It could be the size of a
  city.
  
  Firmware is the most expensive thing in the universe, but only because
  of the unimaginable complexity of the problems it solves. But it's
  vastly cheaper than any alternative. So when your boss irritably asks
  why the software takes so long, you know what to say.
  Compared to what?


So there! Software/firmware has unparalleled complexity.
AnswerKenntnis:
I have a shorter version for you: 

Whatever is easy to do, or streamlined, we write a program to do it instead of us.

And then fight with the meta-process instead.

It's not that much true, per se, but every day thousands of blogs are set up, instead of writing blog engines. Every workday, thousands of Excel macros are written, instead of writing specially-designed database applications for these.

There are a lot of other factors - some of them mentioned here - but I wanted to add this point to the discussion.
AnswerKenntnis:
I think the answer is quite simple:

1) Physical construction and implementation have been around for as long as people have - we've had thousands of years to develop our methods and techniques for implementing physical projects. Software 'construction', which requires an entirely new and different skill-set, is no more than 50 years old - we haven't had enough time figure it all out yet.

2) Virtual construction is harder - you have to 'see' things in you mind that have no physical reality whatsoever. It requires you to analyze and abstract a lot of information before you even know what your product is supposed to look like and the steps it will take to create it. Not so when building a bridge or a building.

3) It's often much more difficult to find the source of a software failure or bug than it is when doing physical engineering. If a girder buckles, you see where it's buckling and you see the supports that are holding it and failing, etc. Finding a software defect can entail examining a great deal of code and interacting processes - difficult, time consuming, and not bound to the laws of physics and math in the way that physical structures are.
AnswerKenntnis:
Most large projects have a high degree of separability of sub-projects, where you can define a small number of design constraints; the whole project will work when those sub-projects each are completed.  If something goes wrong in a sub-project, the whole effort is not thrown into question; you just look for alternate ways to complete the sub-project (e.g. use a different engine).

This is possible but difficult, both practically and as a matter of human nature, in software projects.

In part, other industries have learned the hard way that this sort of separability is a good thing.  For example, if you're going to use Rolls Royce aircraft engines, you do not need to use special Rolls Royce bolts and attachment points that only work with wings with a particular design, and then if you try to switch to Pratt and Whitney, you have to redesign your entire wing from the ground up (which, in turn, requires a complete redesign of the fuselage, which in turn requires you to buy different seats, which in turn requires you to buy a different in-flight entertainment system, which...).  There may be a few linkages--you can't just swap engines without a care--but big projects generally work better when such things are minimized.

I postulate that big software projects designed as a cluster of small software projects with clean interfaces between each other will not fail particularly often, as long as the big project is actually solved by the cluster of small projects.  (It is possible to make a mistake in this regard.)
AnswerKenntnis:
Large projects often occur in large organizations. If you've never worked in a large organization, there is one thing that is guaranteed to kill performance and productivity: bureaucracy.  

Surprisingly, many people do not know what bureaucracy is (it is often confused with politics), or even if/when they have a bureaucracy problem.  

We recently concluded a project to implement smart card authentication. It was originally estimated at three months. It took 15 months. There were not any cost, budget, scope, or technical reasons for the delay. The scope was actually quite narrow - only for accounts with elevated privileges (administrator accounts), about 1,200 total accounts.  

Another significant factor is your business partners. This would include vendors. If your partners have a problem that introduces a delay in your project, there aren't many options that will actually fix the delay problem.  They don't work directly for you, and you may not be able to fire that one person at a partner that may be the cause. The partner can be fired, or can be subject to financial penalties or disincentives, but that does not change the fact that the project has incurred a delay. This is precisely what occurred with Boeing when they undertook a mammoth sourcing strategy with the Boeing 787 Dreamliner.
AnswerKenntnis:
Building software systems is very different from building physical structures. That is, the implementation is very much different. While for example building a huge tanker, you do lots of relatively simple (not easy though!) tasks, such as welding parts together by a robot or by hand, tightening all the nuts and bolts, painting, do the decoration by carrying in all the equipment and furniture and such. All of this is very simple stuff to do, really.

However, when it comes to software, it gets much more complex. For example, how exactly do you implement the secure login and user credential storing part of the product? What libraries and tools can you use? With what libraries and tools are you familiar with? How exactly do you go about writing the hashing + salting function and how do you ensure it is secure? You can do this in so many ways that it's impossible to set any actual practical design patterns for these kind of things. Yes, the said design patterns do exist on a smaller scale as "best practices", but every single software system is very different from the others, and the field advances and changes at so rapid pace that it's essentially impossible to keep up.

When building a house, you don't really run into such problems where you realize that the main supporting walls seem to be inadequate and need to be replaced, requiring you to demolish the progress so far and start from the base by redoing the support walls. You tackle such issues at the design phase, because it's relatively simple to predict what kind of support walls your building needs. 

That is not the case with software though. You can't really design the whole product as a single entity and then implement it. The software design process is usually iterative, and the goals and requirements change as the product is being implemented and tested. Software development as a whole is an iterative process in which things usually change when least expected, and many times such changes impose challenges which require more work, more complexity and unfortunately and ultimately more money, time and hard work to get right.

So, in essence, the reason why it is hard to deliver big projects and estimate project timelines and roadmaps is that software development and especially working design are very complex fields. Complexity is the root problem.
AnswerKenntnis:
Lack of accountability... People get sued when an aircraft crashes. The software industry declines any responsibility in any software defect, therefore creating a lack of incentive to create a robust and safe product.
AnswerKenntnis:
The definition of "large project" is skewed.

A large project, technically, can be delivered on time, and flawlessly, granted it is something that's been built many, many times over the years.


A Pac-Man clone.
A calculator
A text editor


I'm sure you're thinking..."but those are small projects! A text editor is simple." I would disagree with you. Computers are outrageously complicated. Just installing and setting up users on an operating system can be difficult at times, and you didn't even write the OS, or build the hardware.

The projects you're talking about are huge projects, akin to space exploration. How do you know how long it takes to develop inter-galactic travel? What model do we base it on? You have the known knowns, the known unknowns, the unknown knowns, and finally, the unknown unknowns, which happen to come up a lot in software development.

I think the problem is one of expectation. Just because the technology is there doesn't mean using it is going to be successful (or wise to use) for a while. If other industries behaved like the software industries did, we'd have black hole powered vacuum cleaners for sale by the end of the decade. Or some "visionary" would have the resources to build a moon base, and decide that a Starbucks would really "round out" the experience for visitors. I don't think the problem is the software industry, but the expectations placed on it.
AnswerKenntnis:
Though it's hardly the only thing that could be mentioned, I think one basic thing is worth pointing out. Most products are intended to fit with existing behavior. Even a product that's a radical breakthrough (for example, the car) is generally built to fit with existing behavior, and simply make it a bit simpler/easier/cheaper/whatever to do that. Yes, there's often some side effect on existing behavior as well (for example, getting fuel for the car instead of food for the horses), but most of the latter tends to be a fairly minor side effect.

By contrast, almost any software that doesn't change the behavior of the users (for example, let them do their job considerably more easily) is basically guaranteed to be a complete failure from day 1. Worse, large software projects don't just involve the behavior of users on an individual level, but the behavior of large groups -- often the entirety of the organization.

In short, designing the software itself is often the easiest part of the job. The hard part is redesigning peoples' jobs for them. That's difficult to start with; doing it in a way that will not only work, but also be accepted is much more difficult still.
AnswerKenntnis:
Airbus A380 was not a successful project as you have mentioned. I happen to work in a CAD/CAM company, and I was told that it (we had the Airbus prioject too) was delayed by few years, because they were using different version of software in different company. That is, different parts were being designed in different part of the world. And while integrating they came to know that all the design ca'nt be integrated, so they have to redesign it in one version. So looking at it I don't think it was successful. Had it came 2-3 years before, it would have been game changer for Airbus.

Also regarding robust software, you look at any airplane, car (ABS, EPS, climate control, etc.) or space shuttle they have more than 50% software which are running them and belive me they are very robust. It's just that we are more close to software, and there are many more software programs, so we see more errors in them.

Visit: http://www.globalprojectstrategy.com/lessons/case.php?id=23
and see how much successful Airbus A380 was.
AnswerKenntnis:
Software engineer here, with an engineering background, and a wife who works in construction. We've had long discussions (and arguments) on the differences of our jobs.

Software engineering is about designing new things. Almost everything basic has been done in an open source library somewhere. In almost any job where a software engineer is hired, she has to design something that doesn't exist.

In something like construction and most forms of engineering, things that would otherwise be in a 'library' in software are already fully designed. Want to build a tower? Just copy and paste the plans from an existing structure, with a few modifications.

In fact, one of the main reasons I decided not to become an engineer is that you spend most of your time designing a 10% improvement to an existing invention, when that same time could be used to program something more visible, like a social network. 

There are not many new designs in engineering; an extremely skilled engineer is someone who can manipulate an existing design into something new or improve on it. But almost every programmer is expected to modify designs, hack them, or create something new.

Look back at how far the standards have changed completely, from assembly to C to C++ to Java, JavaScript, C#, PHP, and so on. There's not a lot of code that can be recycled from 10 or 20 years ago. This is very different to say... the automotive or aeronautics industry when you can keep improving on designs from decades back.

Project management is notoriously difficult in software. Time estimates are best done by people doing the work, but when they're busy making estimates, they're not writing code. This tempts people to avoid any project management at all.

Often, a lot of code depends on specific people, and if these people are late or unable to perform, the code does not move ahead. In contrast, if you wanted to build a car, you can simply hire different people to assemble the tires, the chassis, the battery, the engine, and so on. Object oriented and existing frameworks makes this possible, but it may not be practical when you're designing everything from scratch.

Failures may be allowed in software. Testing can be costly. In software, it's very tempting to skip all that testing, when you can just fix a crash. In most forms of engineering, a 'crash' can be fatal. 

You do have programming methods that use extensive testing, like extreme programming (which was actually used on software megaprojects). But with tight deadlines (that can be made tighter on purpose), it's tempting to skip all that programming and launch with bugs. The Joel Spolsky style of "always fixing all bugs" will save more time in the long run, but the undisciplined will skip this and fail.

Small projects are better. My wife once asked me to get a job in a big company. It ended up in an argument that big companies are bad companies... to her, a big company had a lot of resources, experience, functional project management, and the right procedures. To me, a big company is a dinosaur, where most of your time is spent on fixing code, testing it, and documentation.

I've seen million-dollar IT projects worked on by less than 10 people. More people would have slowed down the project and added unnecessary bureaucracy. WhatsApp is an example of a 'small' project that's worth billions of dollars. It's not that big projects aren't possible, but you simply don't need thousands of people to produce billions of dollars worth in software.
AnswerKenntnis:
For me the main problem that software engineering face is use cases, user and cross platforms.

Use cases

How many use cases does an airplane have? Most of it is just to fly from one place to other. Maybe there are more such as radar, traffic control, etc., but the use case is clear and not much. In software engineering, we are faced with unclear requirements and user who do not know what they want. Different user need different configuration / flow, can an airplane be customized as user want (I want to have tv and refrigerator!)?

User

Who operates an airplane? A pilot, a copilot, some stewards (if counted) and tower operators. They are all pros and has their job descriptions clear. Software are used by noobs and dummies, not to mention evil hackers and crackers, while still need to be integrateable with other modules (such as OpenID or Google AdSense). If an airplane can be operated by dummies while still surviving from missiles or ninja robbers, then you can say that the airplane has the same quality with software.

Cross platforms

An airplane fly only in the earth's sky. I'm unsure about how they handle the foggy or windy or hot, cold and humid climate, but an airplane is not designed to fly at different gravitation level (I will be amazed if it can fly to Mars). Sometimes, an application must survive different platforms, such as Internet Explorer, Google Chrome, Firefox and Safari for browser (sorry Opera), or Windows XP / 7 / 8, or Linux for OS level. Not to mention mobile devices and different resolution and orientations.
AnswerKenntnis:
There is active pushback and resistance from software developers to improving the process of software development. Requirements gathering is dismissed as unrealistic and always resulting in change which means that documentation is always out of date.

On the other hand, agile methods are resisted as well. Daily releases and constant refactoring are presumed to only be allowable for teams that have more time and budget.

There's no guarantee of a culture of professional. The culture and the processes of software engineers varies from company to company and that's one of the problems. You cannot rely on a minimum accepted standard other than "can you code something in language X using libraries A, B, C and get it done by Tuesday?"

This lack of professionalism and active resistance to professionalism is what causes bugs. We can hardly get people to do regular code reviews though there are numerous studies that prove how effective they are at reducing the number of bugs in a product. We can hardly get them to document their own code which can lead to new bugs and leads to wasted days months down the road when everyone's forgotten exactly what the code is supposed to do and why it was built that way.
QuestionKenntnis:
Why don't all companies buy developers the best hardware?
qn_description:
I must be missing something. 

The cost of employing a programmer in my area is $50 to $100 an hour. A top end machine is only $3,000, so the cost of buying a truly great computer every three years comes to $0.50/hour.  ($3000/(150 wks * 40 hours))

Do you need a top-end machine? No, the $3000 here is to represent the most that could possibly be spent not the amount that I would expect. That's roughly the cost of a top-end iMac or MacBook (17 inch).   

So suppose you can save $2000 every three years by buying cheaper computers, and your average developer is making $60. (These are the most charitable numbers that I can offer the bean-counters. If you only save $1000, or $750, it only strengthens my case.) If those cheaper computers only cost you 10 minutes of productivity a day. (Not at all a stretch, I'm sure that my machine costs me more than that.) then over 3 years the 125 lost hours would add up to a loss of $7500. A loss of 1 minute a day ($750) would give a net gain of $1250, which would hardly offset the cost of poor morale.

Is this a case of "penny-wise and pound-foolish" or have I oversimplified the question? Why isn't there universal agreement (even in the 'enterprise') that software developers should have great hardware?

Edit: I should clarify that I'm not talking about a desire for screaming fast performance that would make my friends envious, and/or a SSD. I'm talking about machines with too little RAM to handle their regular workload, which leads to freezing, rebooting, and (no exaggeration) approximately 20 minutes to boot and open the typical applications on a normal Monday. (I don't shut down except for weekends.)

I'm actually slated to get a new machine soon, and it will improve things somewhat. (I'll be going from 2GB to 3GB RAM, here in 2011.)  But since the new machine is mediocre by current standards, it is reasonable to expect that it will also be unacceptable before its retirement date.

Wait! before you answer or comment:


$3000 doesn't matter. If the machine you want costs less than that, that's all the more reason that it should have been purchased.
I'm not asking for more frequent upgrades. Just better hardware on the same schedule. So there is no hidden cost of installation, etc.
Please don't discuss the difference between bleeding edge hardware and very good hardware. I'm lobbying for very good hardware, as in a machine that is, at worst, one of the best machines made three years ago.
$50 - $100 / hour is an estimate of employment cost - not salary. If you work as a contractor it would be the billing rate the contracting agency uses which includes their expenses and profit, the employers Social Sec. contribution, employers health care contribution etc. Please don't comment on this number unless you know it to be unrealistic.
Make sure you are providing new content. Read all answers before providing another one.
AnswersKenntnis
AnswerKenntnis:
Many companies are certifiably insane around this.

Seriously. If you asked 10,000 tech mangers, "Let's say you paid Danica Patrick $100,000,000. Do you think she could win the Indianapolis 500 by riding a bicycle?", I'm sure not one of them would say, "Yes."

And yet a good percentage of these same managers seem to think that highly-paid software developers ought to be just as productive with crappy tools and working conditions as they are with good ones - because, of course, those lazy, feckless programmers are getting paid lots of money and ought to be able to pedal that bicycle faster.

Now, what exactly good tools and working conditions consist of depends on the job to be done. People who code the Linux kernel need different kinds of hardware than web site designers. But if the company can afford it, it's crazy not to get people what they need to be as productive as possible.

One company I worked for had a 9 GB source code base, primarily in C, and the thing we most needed were fast builds. Unfortunately, we were mostly working with hardware that had been mediocre five years before, so people were understandably reluctant to build much other than what they were working on at the moment, and that took its toll via low productivity, quality problems, and broken builds. The company had money to upgrade the hardware, but was strangely stingy about it. They went out of business last summer after blowing through over $100 million because their two biggest clients dropped them after repeatedly missed deadlines. We were asked one time to suggest ways to improve productivity; I presented the same kind of cost-benefit analysis the OP did. It was rejected because management said, "This must be wrong - we can't possibly be that stupid", but the numbers didn't lie.

Another company I worked for had fine computers for the programmers, but insisted everybody work at little tiny desks in a big crowded bullpen with no partitions. That was a problem because a lot of us were working with delicate prototype hardware. There was little room to put it on our desks, and people would walk by, brush it, and knock it on the floor. They also blew through $47 million in VC money and had nothing to show for it.

I'm not saying bad tools and working conditions alone killed those companies. But I am saying paying somebody a lot of money and then expecting them to be productive with bad tools and working conditions is a "canary in the coal mine" for a basically irrational approach to business that's likely to end in tears.



In my experience, the single biggest productivity killer for programmers is getting distracted. For people like me who work mainly with compiled languages, a huge temptation for that is slow builds.

When I hit the "build and run" button, if I know I'll be testing in five seconds, I can zone out. If I know it will be five minutes, I can set myself a timer and do something else, and when the timer goes off I can start testing.

But somewhere in the middle is the evil ditch of boredom-leading-to-time-wasting-activities, like reading blogs and P.SE. At the rates I charge as a consultant, it's worth it for me to throw money at hardware with prodigious specs to keep me out of that ditch. And I daresay it would be worth it for a lot of companies, too. It's just human nature, and I find it much more useful to accept and adapt to normal weaknesses common to all primates than to expect superhuman self-control.
AnswerKenntnis:
I would suggest that, in reality, one cost is visible and quantifiable, while the other cost is neither.

If failing to upgrade the hardware bleeds even as much as $1000 per developer per week from the budget, no one outside (read: above) the tech department ever sees that. Work still gets done, just at a slower rate. Even in the tech department, calculating that figure is based on numerous unprovable assumptions.

But if a development manager asks for $3000 per developer, particularly in a company with 50+ developers, then this takes a lot of justification. How does he do that?
AnswerKenntnis:
I will put my 2 cents in here from the employer's side ... who is also a developer. 

I agree that low end machines are useless but top end machines are overkill.

There are a number of reasons why you don't get the top end machines:


Cashflow is a real issue, not just a theory. You might be getting paid $60K-$80K per year, but this month we have a total amount in the bank which has to be split amongst every competing thing in that month.
There is a sliding scale of price and benefit. Low end machines are on the whole pretty useless ... if you're getting a celeron or low power chip then whinge away ... mid range machines have good overall performance, once you get into the top you are starting to tune for a given purpose (CAD, Gaming, Video encoding etc) ... and the tuning costs extra. 
General parts are generally cheaper, replacements, warranties and insurance all play a part in the overall running costs and the down time while you source a replacement.
Top end machines depreciate just faster than ones 1/3 the price. 
If you're doing high end graphics programming or CAD work then the extra grunt is valid; if you're just writing standard business software, running visual studio or eclipse and surfing Stackoverflow for answers then the extra power is cool bragging rights, but realistically a mid range machine will not max out the CPU or memory in a standard box today.
Mid range machines built today hammer and in 2 years time they will be twice as fast (well kind of). Seriously, they are lighting quick.
At the end of the day most of what you do is type raw text into text files and send it to the compiler ... that bit really hasn't changed since VI in the 1970s and the low end machines today are a millions times faster than the ones back then ... your pace of coding really isn't that different.


SO to summarize, you should have good gear and good tooling, it makes a big difference but top end machines are not really justifiable for the "general developer".

... ah, and now I read you edit and that is what you are talking about, I will leave the above cos I have written it now ... Yeah, your machine is underspecced for the tooling.

To clarify a mid range machine should have 


2 cores min, 4 cores good anymore at this stage is overkill. 
4GB is a min, 8GB is good and anymore is nice to have. 
SSD should be standard but really a 10KRPM WD or seagate 80-100GB drive should do fine. 
2 x 19" monitors is a minimum with a reasonable video card.
AnswerKenntnis:
The difference of productivity between the "top-end" machines and "almost top-end" machines is negligible. The difference in price is significant.

Not to mention the IT support for different machines instead of having all the developers using the same HW and SW images (which you can't do if you're buying a top-end machine for every new hire, the top-end will be different every time). Also, people who got the last year's top-end will want to upgrade because that newbie next cube has a "better" machine than them, and they're oh so much more important, aren't they?

Unless you really need the top-end machine for your work, I see no reason why to throw away the money.
AnswerKenntnis:
Because most employers do not understand how developers think, act or work.  Or, how top tools can save the company money while increasing productivity.  This leads to the loss of a point on the Joel Test, failure to provide "the best tools money can buy".  This also leads to loss in productivity and job satisfaction.  Thats just the way it is.  Maybe one day you can start your own company and score 13/13.  Until then, ask questions up front with your employer so you know what to expect before ever taking the job.  

As far as your current situation, if you feel they listen and trust you then bring up the discussion.  See if they will give you an upgrade.  I know I'd work a little bit longer if I had a top of the line rig with dual 50" monitors to work with.  Stick me in the matrix.

Same reason people want a Mercedes CLS when a Toyota Camry gets you there just the same.  Sure, you may only squeeze a few more seconds of compile time out with a new machine, but appearances do matter.
AnswerKenntnis:
Your math doesn't seem to include the time required to manage the constant flow of hardware into and out of the company -- it would take an extra IT guy or two depending on the size of your company, so tack another $50-$100k/year on top of your numbers. Plus, you lose productivity on the day they swap your computer out. If they skimp on dedicated IT staff you'll have to do the backups and restores yourself, possibly losing a day or two in the process. In other words, I think it's a bit more complicated than you think it is.
AnswerKenntnis:
One problem with your argument is cashflow. If they don't have the money, the point is moot. The other is return on investment.

This may not apply to the companies where you've worked. Some companies are highly leveraged and/or cash poor. They would rather spend the savings you describe on something that will sell more widgets or software. You have to show that your gain in production outweighs an equal investment in other areas. 

If a software company is in maintenance mode and needs more sales, there may be a better return on spending the money on sales and marketing.

I think you need to address the fact that in your case, the money is better spent on a programmer, than another area of the company.

Be careful with this argument if you're on salary. They'll just want you to work harder to make up the difference ;)
AnswerKenntnis:
I made this argument at my work for switching from laptops to desktops.  I said everyone should be on a desktop and if they need a computer at home - get them one there too.

The speed advantages of a good computer are not negligible, especially if you remove crashes from really old hardware.  

Concerning "top of the line" and "near top of the line" - I would argue near top of the line is always where you should be.  At "near top of the line" you can upgrade every 2 years instead of 3 and end up with better hardware on average.

I recommended cyberpowerpc.com and my company let me purchase a PC from them (marketing guy), but they bought all the programmers pcs from Dell because the support was worth the extra cost.  Think about that... its 1.5-2x to buy a PC from Dell, but you all appreciate if the PC goes down and you can't fix it fast you lose money.

A slow PC is like a broken PC you aren't repairing.
AnswerKenntnis:
There's also a question of budgets - usually developers are paid out of a different budget than hardware for said developers, and their might simply not be enough money available in the hardware budget.
AnswerKenntnis:
First, to answer the question asked: 

They can't do the Math or if they do, they somehow believe that it doesn't apply to them. 
Budget and accounting for hardware and personnel are separate.
People in decision-making position never heard of the issue and are totally unaware that a problem exists at all.

Now, to the real question: "How do I handle this situation?"

It's essentially a communication problem. You explain the problem and the interlocutor hears "bla bla bla we want shinny new toys". They just don't get it. 

If I were in your position, I would make a quick video titled "Can we afford old computers?":
Stills of a typical workstation. On the right side, a blank area titled "cost". 

Still of the power button. Below: "Starting the computer. 20 minutes". In the blank area, "Starting the computer = $40". 
"Opening IDE = $5", "Computer freeze = $80", "building the product = $600"

Run through at a quick pace and keep adding the numbers then compare with the cost of a new computer and don't forget to end with "This video was produced at home on a $500 store-bought laptop that outperforms all the "professional" development machines currently available.  

If you are concerned that raising the issue will cause problems for you, you could also just bring in your own laptop to work. 

If there is no way to get that issue across, then perhaps you should consider finding another job.
AnswerKenntnis:
Discounts play a big part in the buying process as well.

Spit ball (not real numbers):
100 machines @ 1000 w/ 15% discount = 85,000

90 machines @ 1000 w/ 10% discount = 81,000
+ 10 machines @ 2000 w/ 5% discount = 19,000 => 100,000

As has been already mentioned, the extra cost in supporting the "special" machines needs to be added in the mix.
AnswerKenntnis:
Personally I have always had at least an OK development computer when I worked for a 'small' company but when it comes to Big companies, programmers are a dime a dozen compared to a project manager having a budget.

Specially if he/she is one of those having great ideas, read: budget approved.

Whatever the 'good' idea, that person will need really good programmers to actually implement the " New 'better' product" so they will pay the programmer the price needed.

Getting the new development computer, as far as I have been concerned, does not goes through the same 'department' as the other budget though so do expect working under bad conditions if you are paid well :-)
My last work: Dell E5xxx + One LCD 1280x1024 ...
AnswerKenntnis:
Buying new hardware involves money, money involves decision makers and usually they're not developers if your company is big enough. Of course we have exceptions...

As @Rob explained, there is a lot of reason why you'll not get the best hardware. Your company may have a policy defining what kind of hardware is bought, as always with bureaucracy it's hard to have a bleeding-edge policy. Many managers won't bother adapting it to your personal needs, etc.

Poor communication, risk aversion and other flaws:

Let's consider you have reaaally crappy hardware, it's no longer possible to work in these conditions and you want to do something about this.

Now you have to go convince your manager. Well, usually you'll have to convince your project manager who tells your manager who reports to his boss and you'll need to make sure that that guy really understands your issues.
 Involves communication skills and the technical understanding of the management.

Second step, if you're lucky enough, the management will think about it. What do they get ?


You'll work faster with some uncertainties (they don't directly get money as you'll try to explain).
It'll cost money, now.


That means they'll have to trade money, and their actual planning of your work, for an eventual opportunity to let you do something else in the future and that, that's an investment but also a risk.
Sadly, many managers are risk-averse. Not to mention that the poorer their understanding of your issue, the riskier it appears. Some may also have a hard time recognizing that someone did not buy the suited hardware in the first place. 

Moreover, management usually has a shorter definition of what long term means. If they're asked to do some sort of monthly budget optimization, they may even have direct financial incentives not to buy you new hardware! And they won't care about the two weeks you may save six months later..

Of course you don't always have to wait so long when you can do wonderful stuff in one day!

That works better if you have smart and open-minded managers who listen, understand your issues, are ready to take reasonable risks and trust you enough to let you explore creative ways to use the freed time.

That's not always the case: I waited 3 months to get a graphic card to connect my second screen while being forbidden to buy it myself (30Γé¼), lost 3 days for not having an extra 500GB HDD, regularly had to wait several hours when preparing data for the client because of the slow 100Mbps network. After asking several times for 2GB of ram, I was told to buy it myself and stop bothering the management with those technical issues. And we where doing scientific computing for a big industrial client who was ready to pay the price..
AnswerKenntnis:
Math aside, all of your users are not likely to have top end machines.  Developing on a machine that is spec'ed more closely to something that is average in price will acquaint the developer more closely with the experience (and pains!) of their users.

Your QA department may have a min-spec machine, but how often is it used?  Developing on a machine that is a realistic target environment exposes issues early on (unresponsiveness, poor performance, race conditions because of that slow performance, etc), which drives teams to fixing them sooner.
AnswerKenntnis:
I was asked to spec out the machine I wanted to use here, within a fairly tight budget.  I managed to come up with a halfway decent system that works despite not being perk heavy.

I was originally thinking along the same direction as the OP here, the time I sit here waiting for compiles or loads is money out the window.  As I've been moving along I also recognize that the time I spend going to get a coffee, or walking to the printer are also money out the window.

Rather than worry about the small amounts of time that I do have to wait, because we went with a less expensive development system, I've looked at my own habits and improved the larger amounts of time I spend doing nothing particularly useful (ahem... stackexchange is useful, and productive to boot, and I'm sticking to it!! :-) )  Of course we need breaks, but this is time other than "breaks".

So in a way, in a general sense, this question could be the "premature optimization" of work efficiency.  Many great points about migration costs, loosing out on volume purchasing, etc.

In your particular situation, where you are losing time on the order of a break in order to reboot/open programs, yes, it makes a lot of sense to upgrade to decent equipment as your productivity is seriously impaired, a halfway decent i3 system with 4 GB RAM is on the order of $500 ... I'm sure it won't take long to recoup that cost.
AnswerKenntnis:
One big factor is the kind of bloatware that the IT in a typical big company tends to put on the laptop. If you have a Windows 7 machine at home and just some antivirus, a standard SSD-3GB-Quad-core system will boot up in less than 10 seconds. Compare that to the bloatware my company puts in, and it takes forever to boot. I have seen some folks using zapping the OS completely and installing their own to speed things up. I think that solves a problem to an extent, although it is a huge InfoSec violation. But seriously - 10 minutes?!
AnswerKenntnis:
In large corporate organisations the choice of hardware is pre-defined and locked down due to the fact that such organisations have fixed, centrally managed desktop and laptop specifications and configurations. The specifications for these will have been dictated overwhelmingly by a combination of "procurement" and "support" considerations. The company I am currently working at, for example, has over a 100,000 employees and they work on the basis that "one size" fits all, and that size will have been primarily driven by commercials. Once such policies are in place, they are locked down because support services usually invest a considerable amount of time in testing and deploying the software to that "standard" machine specification. Arguments around "developer" productivity, in such environments, simply fall on deaf ears; production services are not going to make an exception for a small group on the basis that they may be more productive; if they did so, they would quickly be swamped with requests for deviations, and in any event they (production support) are incentivised to keep the support cost as low as possible. > 1 desktop/laptop configuration increases the support cost. In an organisation where the primary "product" is the result of software engineering, such arguments are invalid, but the reality is that most organisations are NOT, and the key driver is keeping support costs low.
AnswerKenntnis:
Simply because, best hardware does not make 'best' developers! That being said, the company is to blame if it is hindering the work of the programmer.

However, if the hardware is sufficient for the developer to work, then he has nothing to complain about.

Also, no point in having the 'best' hardware and using only an IDE to code - waste of resources that way.
AnswerKenntnis:
"We have met the enemy and he is us." - Pogo

Either way you slice this question - the collective group "programmers" bears direct responsibility for any failure to buy the best tools in the workplace.


Business finance is incredibly complicated with numerous conflicting motivations and levers. Without concrete knowledge of what your finance department is currently tracking (tax avoidance, managing quarterly expenses, driving up future capital expenses, maximizing EBITDA or whatever else is on their radar), any discussion of true costs is irrelevant. How would you react to a marketing person bugging you about compiler optimizations for code you know is about to be transitioned to an interpreted language? If programmers can not demonstrate in specific terms how the tools they have don't contribute directly to the bottom line, the business is correct to spend as little as possible. We also have to learn to listen to business finance so we can understand the realities facing resource allocation.
We as a group vote with our presence in the workplace far louder than asking for better tools, submitting the most awesome white paper to our managers, or even posting on the internet. There are organizations that have created a culture of ensuring its employees either have the tools they justifiably need or understand the case as to why not at the moment. Until competitive pressure requires this from the majority of employers, we can only vote by seeking out employers we believe in.


Each of us has to either make this something that matters to the core, or let it go.
AnswerKenntnis:
I used to be a developer at a large company and then a startup. Here are my two cents:


8GB DDR3 DIMM (2x$4GB) costs $50-$55 today (Circa july 2011)
21" LCD Monitor costs $200 (circa july 2011)


If your company allows you to bring your own equipment, just use your own $ and upgrade the RAM and LCD monitor. Why you ask?


isn't your own productivity something you value?
aren't your eyes worth $200?


You can always take the monitor with you when you quit the job (remember to clearly label it as your personal property). I've done the above recipe (upgrading RAM and using my own LCD monitor) in both my previous jobs - and my current job.
AnswerKenntnis:
I don't see how you can group all employers together in one basket. I've worked for a few employers as an employee and as a consultant and always got hardware that was more than sufficient for my needs - for current job I was handed a bright shiny new HP quad core with 4 gb ram and Win64 on the first day - not top of the line, but very sufficient - (I use Delphi XE and XMLSpy as my main development tools) - in fact so nice I went and bought the same machine for myself at home. (Maybe I'm not all that productive! LOL.) 

If you don't get good hardware, try asking for it - and if you feel you can't ask for it, you're probably not working at the right place because they don't view developers as a resource, but as a liability.

So I guess the answer to your question is: those companies that don't and/or refuse to provide sufficient hardware for a developer are companies that consider their developers a liability - jobs they'd rather outsource and not deal with at all.
AnswerKenntnis:
CFO side.

The company has a lot of expenses. Every department needs more $ in order to do better and in every department the expense is a must.

when you come to choose the best way to use available $ you take into account:



how much do they need? smaller sums are easier to approve.
 
will it increase sales? better pc's usually don't contribute directly to increase of sales

does the department like to spend $ or do they understand cash flow. Most r&d departments I have seen have an arrogant "we deserve the best" approach. This is understandable as they earn a lot of $ and when you do you think you deserve the better things in life. $ needs of r&d teams usually give a feeling of a spoiled child requesting more toys while his parents are struggling. "A delicate genius".


The 10min a day waste is not a reasoning that would work with most finance departments. Most r&d teams waste a lot more on all the none programming activities they enjoy during the day. Lets chart all the waste  in your department and see what can be done to improve productivity.
AnswerKenntnis:
Simply put, purchasing decisions are often made by bean counters (Accountants, and middle managers) rather than by project managers.  

Lots of people have given potential reasons, and all of them are a factor in one situation or another, so there isn't any single overriding situation.  Buying large scale equipment may mean they lose some money on productivity for programmers, but gain money in other areas.

Still, it often just comes down to a budget.  You have to fit in the budget, and that's all there is to it.
AnswerKenntnis:
I used to work for a networking company where they upgraded ram from 512 MB to 1 GB last year. We were working with f**king CRT monitors in 2010. The funniest part was the managers' hardware was upgraded to 2 GB ram. Why on earth would anyone want 2 GB to create damn PPTs and how someone would develop applications with 1 GB ram, I would never know.
AnswerKenntnis:
It comes down to who handles the money.  In larger organization IT is given a budget of say $1M for the year.  That includes support salaries, servers, etc.  They have to spread it around between all their resources.  They cut deals with vendors like Dell or IBM to get x number of the same kind of computer.  This they give to everyone from customer support to the programmers.  They also get deals on support etc., when they only have to maintain a limited set of models.  They are not programmers either, I have had numerous arguments with non-programmers about computers.  When I went over my IT managers head for some new HD one time, the CEO said buy them and boom, everybody finally had enough disk space to run virtual machines.  

I actually blew up and cussed out my boss because IT was going to take away my 19" second monitor because I had a laptop.  They stiffed me on that too, giving me a 13" model when others were getting 15".  That goes back to politics in IT which is another problem.  It's kind of an us vs. them thinking sometimes.
AnswerKenntnis:
From the perspective described by the asker, the question makes complete sense. However there are more costs involved with keeping hardware current.

Here are some of the costs that also need to be considered:


requisition cost (research and details that goes into purchasing)
installation & configuration cost
support & maintenance cost
software licensing cost
disposal / upgrade cost


In some cases, these can be 2-5x greater than the cost of the hardware itself. Even more if there is sophisticated software licensing involved.

In general the scale of these costs depends on the size of the company or the complexity of the organizational structure. Smaller teams with direct access to purchasing power can keep these costs low, whereas in a larger organization these costs can get very high.
AnswerKenntnis:
Because a lot of companies outside of typical tech start-ups are not interested in hiring rock-stars. They're investing in someone who can just do work. So if they don't care how you do you work as long as you do it why should they care what equipment you use? I've worked at places that still use 15-inch CRTs and everyone does just fine. Sometimes when i read questions like this I wonder if people realize that not everybody in the world works for a cool start-up.
AnswerKenntnis:
I've worked for companies that skimped on hardware in the past. It sucks, and if they need convincing the battle is likely to be a never-ending one.

Turns out that companies committed to using the best available tools are rare, but they do exist; I work for one. I've got a quad-core 17" 2011 MBP, 8GB RAM, Vertex 3 SSD, 2 x 24" external monitors, plus a quad-core desktop and a 4GB Xen slice; as well as quiet offices.

Could I get by with lesser hardware? Sure. But I think we'd all rather be bragging than bitching.
AnswerKenntnis:
In my opinion, there are only two defensible objections a company could raise to keeping developers set up with solid workstations. The first is that they are undergoing a cash crisis. That better be short lived, or the company will not be a going concern for long. If you work for a company like that, you should keep your resume up to date.

The other is that their organization is simply not bottle-necked on software development capacity. That is, an increase in the quality or speed of software development output would not improve the bottom line. If the company's main business is selling software, that will be practically impossible. If software isn't their main business, and they aren't bottle-necked on it, they should be trying to reduce their software workforce by transferring or letting go of their weakest team members. Supplying poor equipment will reduce the size of their team from the opposite end, I'm afraid.
AnswerKenntnis:
New machines, newer technologies mean newer problems. Not everyone at every company is a techwiz and not every company has the IT resources to train people and handle problems 24/7.

Yes, perhaps if you're a freelance programmer working on your own personal desktop it would be worth blowing $1000 on a rig to squeeze out 10 min of extra productivity everyday. However when you're deploying hundreds of these machines to people who may lose productivity because of new equipment, the prospect seems a little more grim.
QuestionKenntnis:
What is negative code?
qn_description:
I was reading the Wikipedia article on Douglas McIlroy and found a quote that mentions 


  "The real hero of programming is the one who writes negative code."


What does that mean?
AnswersKenntnis
AnswerKenntnis:
It means reducing lines of code, by removing redundancies or using more concise constructs.

See for example this famous anecdote from the original Apple Lisa developer team:


  When the Lisa team was pushing to finalize their software in 1982, project managers started requiring programmers to submit weekly forms reporting on the number of lines of code they had written. Bill Atkinson thought that was silly. For the week in which he had rewritten QuickDrawΓÇÖs region calculation routines to be six times faster and 2000 lines shorter, he put "-2000" on the form. After a few more weeks the managers stopped asking him to fill out the form, and he gladly complied.
AnswerKenntnis:
There's a Bill Gates quote along the lines of measuring programmer productivity by lines of code is like measuring aircraft building progress by weight.

I'd like to add that the LOC metric has encouraged the use of overly long-winded languages and deliberate reinventing the wheel to meet quota.
AnswerKenntnis:
When I was in high school -- and yes, we had computers back in the 70s, though we had to make them out of animal skins using stone knives -- one of the math teachers ran a programming contest. The rules were that the winning program would be the one that produced the correct output, and that had the smallest product of lines of code times run time. That is, if your program took, say 100 lines of code and ran for 5 seconds, your score was 500. If someone else wrote 90 lines of code and ran for 6 seconds, his score was 540. Low score wins, like golf.

It struck me as a brilliant scoring system, rewarding both conciseness and performance. 

But the entry that technically met the winning criteria was disqualified. The problem was to print a list of all prime numbers less than 100. The disqualified entry went something like this (most of the students were using BASIC back then):

100 print "2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61,"
110 print "67, 71, 73, 79, 83, 87, 89, 91, 97"


The student who wrote that entry pointed out that not only was it short and very efficient, but the algorithm should be obvious to anyone with even a minimal knowledge of programming, making the program highly maintainable.
AnswerKenntnis:
It's tongue-in-cheek. If it costs you $N per average coded line, then coding "negative lines" is surely a winner.

This means, as practical advice, that small code that accomplishes the job, is much better than big code that do the same thing, all other things being equal.
AnswerKenntnis:
Writing the same program in less code is a goal for everyone.

If a program took 200 LOC to code, and I write it in 150, I wrote -50 LOC. So I wrote negative code.
AnswerKenntnis:
Thilo's answer is probably most accurate historically, but the "negative code" metaphor can also include performance and memory use - rewarding efforts to defer execution or allocation of something until it is actually needed.  

This "procrastination pays" mentality produced such tongue-in-cheek axioms such as "Doing nothing is always faster than doing something", "The fastest code is the code that never executes", and "If you can put it off long enough, you might not ever have to do it" (referring to deferral of expensive operations until actually required)

One technique to realizing negative code is to challenge initial assumptions and definitions of the problem.  If you can redefine the problem / input domain such that "sticky issue #3" is categorically impossible, then you don't have to spend time or code dealing with sticky issue #3. You've eliminated code by optimizing the design.
QuestionKenntnis:
How should I behave as a developer in a project that's headed for failure?
qn_description:
I am a developer in a 5-member team and I believe our project is headed for disaster. I'll describe why in a moment, but my question is: how should I behave? 

The deadline is in 1.5 months, and I feel no matter what we do, this project will fail. I'm of the opinion that we should just terminate the project and stop wasting our time, but politically I think it's impossible for our manager to do that.

What should I do in this case? Should I put in extra effort, or should I just take it easy? And what should I say to the manager?

Reasons this project is headed for failure: 


With deadline approaching many of the must-have features are not finished 
Application is unstable and very difficult to use 
System is very convoluted, code very hard to understand, very difficult to change - Data model is too driven by a complex relational database (100+ tables)
Unclear leadership; the manager responds to new information with major changes 
Almost no automated tests or unit test 
Heavily depends on other systems, but no integration tests yet 


In fact, we actually just inherited this project (along with the mess) around 1-2 months ago from another dev team under the same manager, who has worked on it for a few months.
AnswersKenntnis
AnswerKenntnis:
Communicate your concerns in the most concise and non-confrontational way possible up the management ladder. Summarize the risks, but do not impose your conclusion on them. 

Management must always have the choice of what to do, but it is your job to assess and communicate the situation. Use email, so as to leave a paper trail when things go south.

Having done that, keep working on the project in good faith.

Keep in mind, you may not know everything there is to know about the project, its backers, and financial decisions behind it. Management decisions that seem stupid to you might actually be based on smart reasoning that isn't visible to you.
AnswerKenntnis:
Keep a paper trail (e.g. diary, saved emails, etc).  Only include facts and objective observations.  Leave all conclusions up to whomever (if anyone) reads what you've written.

As a developer, if you're not viewed as an obstacle to the project you're likely to come out fine from the finger-pointing that will no doubt happen.  Your manager may not be so lucky, but that's not relevant here.

Just on general principles, update your resume and make sure you occasionally meet with other devs outside your company.  If you're not part of any local developer groups, go join 2 or 3.  It takes years to build up a network of friends and acquaintances but in the long run it's worth it.  Be sure to look on it as a 2-way street - if you can help fill an opening in your company with someone capable, that's just as good as someone helping you find a job.
AnswerKenntnis:
I'm going to recommend you take a little time to read 2 books.  

Death March is the canonical book that describes a pathological project management style that is widespread in software development. Due to schedule compression, feature bloat, or mismanagement, many projects end up in a bad state; it helps to understand that you are not alone and your project is not the only death march. Author Edward Yourdon categorizes dead-end projects into 4 quadrants, each of which has different coping strategies. Sometimes the only coping strategy is to walk away. I think this book will help you figure out what your range of options is and narrow down what you can and cannot do.  



Catastrophe Disentanglement is written more for project managers. It is aimed at figuring out how to triage a broken project: What needs to be cut, what can be cut back, and how to pitch that to customers? "Conventional" software project management gets us into messy projects, and we won't escape our problems using the same thinking that got us into them in the first place. This book is a little harder to read than Death March, but a good one to have on your bookshelf.
AnswerKenntnis:
3 simple and cynical strategies to maintain career/sanity.


See a train wreck in the making - get off the train: Failing projects are terrible for morale and unless you have ninja upward management skills will have some negative impact on your career. Jump now if you can see any soft landing.
If that doesn't work keep your head down: People are going to start looking for people to blame - don't make it easy for them! While escalating concerns up the management chain might be the right thing to do it is both ineffective and risky. Your own manager is unlikely to pass your concerns on up and bypassing him/her will not only make you both look bad but could brand you a 'troublemaker' i.e. the kind of person who can be blamed for undermining the project in the first place etc. This of course also means be professional, put in your hours but no heroics.
Plan for an emergency exit on T+1: give yourself some options and do the groundwork for a potential internal or external transfer now. Talk to people; there's no reason to wait for others to decide what to do with you when the project funding gets cut or to get swamped in the inevitable 'stampede' of people migrating away in a couple of months. 


Apologies if it sounds overly cynical but if you've called it correctly then there is no upside to suffering the unpleasantness coming your way. Be professional, be optimistic but always be realistic.
AnswerKenntnis:
What impact will this soon-to-be failed project have on your career at the firm, and beyond? In my experience, merely being associated with successful projects is not an indicator of your own personal excellence.

The qualities that you exhibit in the face of adversity and sometimes what looks to be certain failure, often gets noticed by the higher-ups, more so than you think. And I'm talking about beyond your immediate manager.

I personally have experienced seeing my immediate manager get fired for incompetence, and then found myself promoted into his position the very same day. Not pleasant, but it showed that people were watching me, and liked what I did.

Oftentimes, the same chaos and disorganization that comes with a failing project, affords you the opportunity to shine.

So look at the project this way: what opportunity does this failing project afford, for the light to shine on all of my strengths and best qualities? What lessons am I learning from this experience, that will make me a better professional and a better person?

Essentially, the sum of experiences drawn from failures is what fuels true success.

Note: Thomas Owens asked, what specific things a person can do in a project like this. I have a few general suggestions, which I've used as personal guidelines in these situations. Will it help a distress project to miraculously succeed? No - but I've found it has helped me to keep a proper perspective on things, and find personal success in spite of the bad situation.

1) Focus on personal excellence - strive to write ever better code, meet ever higher standards of quality and functionality.

2) Focus on personal metrics - how much code do you write, that spawns subsequent bugs? Reduce that ratio to as low as you can. When asked to provide an estimate for a task assigned to you, are you generally accurate, or do you find that you've over/under buffered the timeline too much? When actually assigned a task, do you provide a good level of feedback on the progression of work, including giving well-and-ahead advance notice of delivery timeline problems?

3) Focus on team metrics - These are just some things off the top of my head: Are other team members lagging because of a dependency on a task you are working on? Are you good at delegating or dividing your task/subtasks to others in the team? Do you find it difficult to communicate with one or more members of the team? All areas that I work on to regularly improve in.
AnswerKenntnis:
In a situation like this, as the lowest rung of the ladder, there is only so much you can do to help the project.


Make sure your work is spotless
help identify the biggest problem areas
Try to provide answers, not just problems. Look like you are trying to fix them.


Aside from that, you really do have to look after number 1.


Document everything
keep all emails, IM conversations
Try and find a way out of the project if it all possible
AnswerKenntnis:
Failing projects can be toxic to the soul, cause depression, over work and low self-esteem.

It's all relative to perspective.

I've worked on horrible projects while sitting across from another guy who had a smile on his face every single day. Oh how I wanted to slap that smile off his face.

Some people aren't bothered by the current state of affairs on a project. They enjoy their contribution, their tasks and are working with in the domain of their interestes. While others, have a strong negative reaction to the current state of things. It's all about our perceived expectations for our daily jobs.

While you might be doing some of the work you enjoy. There is clearly elements in the current project you dislike. 

You need to identify what those problem elements are, and address them.


deadline pressure
quality control
professionalism
guidance by management


There are many teams and companies that don't find the above aspects of development important. What I've found is they often think the following.


Deadline pressure is perceived as way to motivate people.
Quality costs more and returns limit.
Professionalism applies to other areas of the business.
A manager is a timekeeper and not a person who contributes to development.


These problems aren't yours. It's theirs, and you shouldn't waste any energy over their behavior. If you aren't one of the guys who smiles and enjoys his job even as it heads for a cliff, then you should think about finding a place of like minded developers.

You'll be much happier.
AnswerKenntnis:
Try to be proactive about finding a new way to achieve success for the project.  Think about how you can propose some alternatives.  Right now your boss is probably getting beat up about the project being a failure, wouldn't (s)he appreciate someone coming in with solutions instead of problems?  

Maybe there is a way to split the features into staggered deliverables?  There are often degrees of "must have", so see if you can get those prioritized and group them into milestones.   Better to have some product at the end of the timeline than nothing.  Or consider splitting the team between people working on new functionality and others working on the stability, this way you can show some progress on both fronts.

If these efforts succeed, you will have shown that you are a team member who can find a way to succeed, if not, you will still have demonstrated that you don't give up and will work to find a solution.
AnswerKenntnis:
Sounds like most projects I've been on. It probably won't end as badly as you think, however:

1) Do your job. Don't worry so much about the overall project as long as you complete your responsibilities.

2) CYA. If the project does fail and you suspect the manager will start blaming everyone but himself, make sure you have enough proof that you did everything required of you (see item 1).

3) Make a few polite suggestions for improvement. Don't sound the warning bells, don't be doom and gloom, just be polite and subtle. 

For example, if the team isn't writing effective unit tests (or any tests), write a few unit tests closer to what you would like to see and causally mention how doing so helped you solve a particular problem, or saved you time. 

Whatever it ends up being if you want to affect change focus on the positive steps that have concrete results. This project may never turn out to be a winner, but maybe the team can learn for the next one.

Also:

4) Opportunistic refactoring is your friend.
AnswerKenntnis:
Work hard; but not at the expense of your family or your health.
Keep a record of all critical design decisions; especially as they pertain to your work.
Keep networking, and keep your options open if the situation becomes too difficult or you become a victim of a mass layoff.
Try not to think of your project as a "failed project". Everyone likes people who stay positive and fight hard in the face of adversity. So for as long as possible, try to be that person. A positive outlook, grit, and determination are always good for the workplace.
If you're anticipating a failed project, then you're anticipating a post-mortem meeting. At the post-mortem meeting, everyone will be held to account. Be prepared to defend all of your code. [Note: As a general rule, you should always write clean code so that it's easy to defend it later.] If you have email or design document that inspired your decisions, then that's even better.
At that post-mortem meeting, try to stay positive; and only present your email and design document evidence if your judgement, effort, or workmanship is called into question.
AnswerKenntnis:
What I've found most effective is Robert L. Read's recommendation on how to fight schedule pressure. Here's what Read writes:


  The key to fighting schedule pressure is simply to turn it into time-to-market pressure. The way to do this to give visibility into the relationship between the available labor and the product. Producing an honest, detailed, and most of all, understandable estimate of all the labor involved is the best way to do this. It has the added advantage of allowing good management decisions to be made about possible functionality tradeoffs.
  
  The key insight that the estimate must make plain is that labor is an almost incompressible fluid. You can't pack more into a span of time anymore than you can pack more water into a container over and above that container's volume. In a sense, a programmer should never say ΓÇÿnoΓÇÖ, but rather to say ΓÇÿWhat will you give up to get that thing you want?ΓÇÖ The effect of producing clear estimates will be to increase the respect for programmers. This is how other professionals behave. Programmers' hard work will be visible. Setting an unrealistic schedule will also be painfully obvious to everyone. Programmers cannot be hoodwinked. It is disrespectful and demoralizing to ask them to do something unrealistic.


When you say that the project is "headed for failure," that's based on your assessment of what tasks need to be done and how much effort each one of them will require. Make that assessment explicit, understandable, and detailed. Separate tasks into their component parts; explain in as much detail as you can how development time will be spent.

Once you do that, you have a solid foundation for discussing concerns with management. In all probability, once you've broken down your assessment into all the specific tasks that need to be completed, you will be able to demonstrate that the job takes more time than you have - possibly much, much more.

Once you're discussing this detailed schedule with your manager, be prepared to be flexible. Your manager might say "Task X won't take a month; it'll take a week at most," or "Task Y is entirely unnecessary, cut that from the schedule." You can certainly discuss these points, but what's far more important is to reach an understanding between you and your manager on some realistic version of a schedule. That way, if you're not being allocated time for, say, testing, then you've got an explicit directive not to test, rather than just running out of time "unexpectedly". And it's definitely possible that your manager is legitimately willing to explicitly cut certain corners in order to ship on time - there's plenty of cases where that's a perfectly reasonable call! 

The estimate gives you concrete substance to debate and discuss. It puts you on the same page; it helps you feel confident that your manager has taken your concerns into account. And it brings you to the point where if your manager is plainly asking the impossible, it will be perfectly evident to both of you. If the project is well and truly doomed, you will have made that clear, and it will be up to your manager to decide precisely how he wants you to spend your time.
AnswerKenntnis:
I participated in three projects that were clear failure. These were quite painful but looking back, two of three did not have negative consequences on my career, and even third one wasn't the end of the world.

Here are some observations I recall.

Developers at junior positions ("code per spec", "fix the bug", stuff like that) don't get affected much, unless they slack off due to lowered morale in the team. At positions like these, a sensible and even sometimes successful survival strategy could be just doing the best you could.


For example, one of the failures I experienced has been overcome by plain, methodical fixing of more than hundred known bugs which (coupled with particularly smart approach of promoting this progress by tech lead) eventually led upper management to decision to recover the project and give it yet another chance with a new release, which in turn made a reasonable success.




Programmers at more senior, influential positions would better be prepared to share the negative consequences of project failure. An architect, tech lead, senior developer are typically expected to make an impact big enough to be considered responsible for project success or failure.

At senior position, one would better be prepared to gain from failure "indirectly", by analyzing what went wrong and what could have been done better.

These bits of knowledge, post-mortem lessons may be invaluable if learned right, the very successful career at senior positions may depend on how well these are learned, as explained in this brilliant answer at WP:


  Judgement comes not from success, but from failures. Most companies want to hire people that have had their failures paid for by previous companies...




On a more practical note, one can consider "next / update release" approach as a possible way out of the failure. Coincidentally or not (I think not), but both of the failures that didn't damage my career went by very similar scenarios: release N was a total disaster,  release N+1 was tolerable, releases N+2 and later were plain success.

Walking in your shoes, I would most likely put some effort in preparing / promoting the idea of "next release". Make (and communicate!) something like a tentative list of known issues that you would want to fix after planned release. Draft an informal, rough road map for next release(s).

Think of how you could communicate these ideas to people around you, how you could influence management to consider this plan. If the project has someone with good marketing skills, try to involve them into amortizing the failure damage by wrapping up coming release into smoother terms, like "early access", "beta", "customer preview", "introductory release", stuff like that.

Think of a backup plan in case if higher ups will appear deaf to this idea. Remember above story about "fixing of more than hundred known bugs"? there is a chance for things to change, really.

Management may appear deaf to next-release ideas now, but there is a good chance for them to reconsider in the face of strong convincing evidence of project quality progress.


It is quite likely that there will be rather long time between freezing code for planned release and management decision to drop it completely. That time is your chance: if you focus effort on fixing known issues and properly "evangelizing" the progress, this could make a difference (as it once made to me).
AnswerKenntnis:
There are reams of practical (and otherwise) advice here already, but to me the keys to this mystery are the following two items (emphasis mine):


  The deadline is in 1.5 months


and


  ...we actually just inherited this project (along with the mess)
  around 1-2 months ago from another dev team under the same manager...


So....

Welcome to Team Patsy The Avengers

The prior team had better things to do...than fail. And somehow the manager went along with that. Changing teams this close to deadline is not the best project-management move to make, so...what's up with that?

Correction: The prior team was replaced, ~3 months before the deadline.

--> Find out how the prior dev team managed to escape, and do that. <--

3 months is barely enough time to come up to speed on a system of this apparent size, much less correct its architecture, add tests, and finish it. You need more time

And if you can't do that, unless you are absolutely certain that the project will be extended indefinitely, or aided by magic elves or something, you do not want to be the last man standing left holding the bag.

Harsh? Yes. But while poor planning (at least!) by prior teams/managers is not your fault, 'failure to deliver' will be.

The prior team bailed. The prior team was kicked to the curb. What does that tell you? It tells me that you are the turnaround team...and your manager is counting on your heroics to save the day.

A 5-person team, and 1.5 months to go. The new team has only had the project for 1-2 months, so the new team is not even over the learning curve. Taking a rough guess at the size of this project, it looks to me like there is no way the new team would have been over the learning curve even if the project had no problems at all.

So, I (still) think you've been shafted.

If that is the case - and only you can decide what is true, or what you want to believe - you do not owe anyone an explanation or an apology for getting out of the way of this train wreck. You do need to be discreet about it though.

I doubt seriously that the manager is unaware that the project is in jeopardy, which means that gentle explanations will get you nothing but negative attention. Unless your manager is Mr. Rogers, your future is in jeopardy.

But always remember: do not take career advice from strangers on the Internet.
AnswerKenntnis:
Since your manager knows it will probably fail, you're better off than most. I would consider working with the manager and see if there are any parts/features of the app that can be excluded. 

Too often we think every client request is a 'deal killer' and go out of our way to promise the delivery. Until someone works with the client and probes deeper, you may not be able to make these decisions. If you're not able to do this, still try to deliver what you think is most important. Sometimes it's easier to ask for forgiveness than permission.
AnswerKenntnis:
This is an incredible opportunity for you! Let's take an entrepreneurial viewpoint on this.  

Assuming that management wants this project to succeed you are in a great position to help them do so. The reason this realization is so important is because you have to develop the conviction and confidence that the warning signs you're seeing are in fact going to lead to this project's failure[1].

This is your chance to practice important skills in systematic thinking and interpersonal communication. Understand and visualize the issues and potential opportunities that are being missed so you can develop a strategy to communicate these as clearly and simply as possible. 

Recognize your opportunity here to improve you skills. 

[1] Canceling the project would actually be success. Failure would be spending good money after bad.
AnswerKenntnis:
What can you do


Treat this in your own self excellence terms, it's "their" project, but also yours, take ownership even though you know it will fail. Why? because a) you might help it fail a little less, b) in time of challange, this is where you learn the most c) you should measure yourself via your own metrics of excellence, doing your best might not save the project, but you might end up still proud of yourself
Talk to other fellow developers and see what they think, you will probably find out that many share the same frustration, if done carefully (don't make people think you want to start a mutiny or something) then this can help not just you but others as well
Ignoring the problem is not going to make it go away, talking about ways how to, against all odds still pull it through, e.g. at least get some decent must haves, or the main "wow factor" use case done right, might make this project fail less miserably. How to do it? well, few ideas of "when going gets tough tough gets going" or "desperate times justify desperate measures" and other cliches, for example: massive use of OSS, extreme programming / agile methodologies, weekend hackathons (volunteers only, not forcing people to work weekends). This is time where you can show leadership (carefully if you are not in a team lead / senior position) but you can take this advantage and become their mockingjay, just get people to feel that they might get this project just a little less fail than everyone knows it will. You can show some leadership skills here that might help you later along the way, treat the problem as a challenge.
Make sure your management knows, but very, very carefully, if they know, they'll appreciate you telling them this in a non confrontational, calm way, if they don't they will appreciate it even more, and wonder why no one told them about it before. You should tell them this as a service, without any emotional side, just plain facts, and without an agenda. If they'll ask you what you think they should do (which is a great sign, but rare) then see the next section


What you can't do, but your management probably should


They should not add more people to the project "to help" see this
Call the customer immediately and tell them the bad news. Why is this a good idea? well, I'll leave quoting the manifesto for agile software development, but even without it, even water fall lovers hate bad surprises. If the customer knows in advance this project is doomed to fail, they will be unhappy, but will be much happier that you tell them there are issues before it blows in their face, and that you are on it, and doing your best to accommodate it. The customer can do many things but most of them are not worse than finding out in the last minute they don't have a deliverable (or they do but it's of non usable quality). The customer will appreciate it as they will be able to delay hiring testing staff, offshore IT people, change their internal training plans, and all just because you were honest with them. 
with the customer, think of ways to still make something out of the project, the most common one is descoping things. for example there might be some features that are just too hard to develop the way they were designed, and if the customer will agree for some small modifications and simplifications, then some features will become simpler.
Update their own higher management, it will help them keep their job as well...


What you should NOT do


Ask to add more people to the project (see this)
Quit / look for another job (at least not yet), this is something you can learn from, and what will make you one day a better developer or better manager. You'll learn to appreciate many things better, better time manage, design better, write code better, and work with peers and management better. Look for a job after these 2 months are over if you don't like working there, not because of any other reason.
Whine, complain, or be negative about the project, management, the bad design you inherited, the newbie developers you got to mentor that it takes you 3 hours to explain them to do something that takes you 1 hour, be positive as much as you can. 
Criticize management and become a target as a trouble maker, they are in the same boat, and they might know things that you don't, all you can do is update them (always update your own direct manager, never bypass her / him)
Blame people, (or yourself), it doesn't help, never
Take it too seriously, unless it's medical equipment, no one is likely to die if you miss the deadlines, it's software, we miss deadlines for a living, relax. 


That's just my two cents, YMMV
AnswerKenntnis:
You should go to work and do what you would do on any project, whether it was succeeding or not. You are being paid to perform your job. Do it to the best of your ability. Assuming you aren't the project manager/lead, it is not your responsibility to decide that a program should be cancelled or not. You deciding to slack is the same as you making the decision to cancel the project. That's not part of your job description.

However, in the bigger picture, that doesn't mean that you should be working 50, 60 or more hours per week to try and meet the schedule. Once again, that's management's job to figure out how to achieve the project objectives. 50+ hour work weeks is not a reasonable option unless they are willing to pay overtime and you are willing to put your family life on hold. Once again, it was management's job to put together an achievable schedule. They can't assume that they can force employees to ignore their family life in order to meet unrealistic schedules.

Also, while the schedule may say 1 1/2 months remaining. That is probably not the "drop dead" date. Some customers may take what you have by that date, even if it isn't all. Some customers may come up with extra funding, since they've already sunk a lot of money into the project. Sometimes the company itself may take on the extra costs to ensure a satisfied customer.  All of that is out of your control. Do your job to the best of your ability. That will give management options to work with that could turn a disaster project into a great success story. I've seen it happen many times.
AnswerKenntnis:
I can only reiterate what others have said, but I emphatically emphasize: Always strive for your best work and keep a paper trail. for both CYA and technical reasons.

Any fall out coming your way depends on managements' personal character; but let me tell you it's hell being on the receiving end of incompetent, lying management. But the very worst you'll leave with your self (and peer) respect intact.

Keep good notes of technical aspects of your Death March. When you get the inevitable interview question have you been on a failed project; why did it fail and what did you do to try to prevent it? Bonehead management is not an answer - even if it is the reason.
AnswerKenntnis:
It may be an easy way out to assume that it is an inevitable and utter failure and just give up on the project.
As a consultant I am often invited to help with the projects that are in various stages of degeneration, failed or failing and part of what I am being paid for is reviving and completing such projects.

What you view as a complete failure, may not be perceived as such by everyone, depending on the perspective.
In an ideal world, every feature would complete, coded elegantly and independently of other systems and every line of code tested.
I haven't seen a single, project that looked like that in rev 1.
In real world, shipping a project means taking some compromises, maybe even missing some key features but the product can ship and even though it will be beta quality at best, at least you'll get the feedback on which things to fix first.
The upside of this is that it may inform the management that software is never done and rather than trying to develop a certain v 1.0 in vacuum it is better to iterate and respond to changes in an agile way.
Finally for you as a developer in this position, I think the key thing is to develop as good code as possible in these conditions - document it, write tests yourself, if you have to (especially for external dependencies) and use your knowledge of the system to communicate which features are truly crucial and must-have and which ones can wait a week or a month.
Make yourself vocal about your concerns and justify them as you did to us.
Rather than prepping for plan B, prepare for the fact that you and other poor souls will probably be fixing all that buggy and not done code yet AFTER the product has shipped - as stated above this means writing your code in a modular decoupled fashion - if you think that the persistence layer code is bad, hopefully you should be able to replace it completely in the next revision.
Finally, don't despair - dealing with such situation is part of what you're being paid for and it's a learning process. Regardless of the outcome in this particular project, this will count as a valuable experience in the future.
AnswerKenntnis:
Find the problems your project is running into and try to clearly quantify as objectively possible. With every "metric" you quantify make sure you define why you believe this metric is important. You want to give your manager insight into what the consequences would be if a certain metric would not be within "acceptable range". You will need to give some guidelines for each metric to indicate which values are "Good", "Acceptable", "Problematic" or "Bad". Define every criterion up front. If possible you could describe what would be minimally needed for a project to success and contrast this with the current project.


Static Code Quality can be quantified by lots of static analysis tools. You can keep this as simple or detailed as you see fit. The metrics I suggest you start with:

Cyclomatic Complexity
Size of your code (eg, Nr of lines per function/class, number of files, number of tables...)
Identify which modules are too large
Duplication. 
Adherance to code style

Defect rate

per KLOC by module and or subsystem (identify troublesome parts of your system)
you could calculate how much per team member but I think you should keep that for yourself
solved vs found
time needed to solve a bug (perhaps if this is inclining make a graph of this)
perhaps make an estimate of how much time is needed if this keeps going at the current rate

Planning

Extrapolate time used for the features built. Take into account complexity of the feature. This doesn't have to be very precise. What you want to convey with this would be along the lines of "Feature A, B and C are around the same complexity of D, E and F. With features ABC used 170% if the planned time. If nothing changes we expect the same time is needed for DEF" or along the line of "The average feature is taking X% time longer than calculated. We have no reason to assume that the rest of the functionality is easier to build so we should compensate for this in the future planning. It is of no use to have a planning if it is not realistic.
Try to have some monthly or preferably even shorter release schedule. If only internal. This could help you further with extrapolating the time you need for the project. This could also save you from making unrealistic commitments (if you don't commit to new work before a release is done). Make a planning for each cylce and sure new features only enter into the next cycle (ie. they can never be added to the current cycle).

Test coverage: explain the "normal" test coverage values and show how what your current coverage is
Documentation: How much % is actually documented? How good?
Modularity:

Class based (coupling and cohesion)
Package based
Subsystem based (how many communication paths are there?)
QuestionKenntnis:
How do you tell if advice from a senior developer is bad?
qn_description:
Recently, I started my first job as a junior developer and I have a more senior developer in charge of mentoring me in this small company. However, there are several times when he would give me advice on things that I just couldn't agree with (it goes against what I learned in several good books on the topic written by the experts, questions I asked on some Q&A sites also agree with me) and given our busy schedule, we probably have no time for long debates.

So far, I have been trying to avoid the issue by listening to him, raising a counterpoint based on what I've learned as current good practices. He raises his original point again (most of the time he will say best practice, more maintainable but just didn't go further), I take a note (since he didn't raise a new point to counter my counterpoint), think about it and research at home, but don't make any changes (I'm still not convinced). But recently, he approached me yet again, saw my code and asked me why haven't I changed it to his suggestion. This is the 3rd time in 2--3 weeks.

As a junior developer, I know that I should respect him, but at the same time I just can't agree with some of his advice. Yet I'm being pressured to make changes that I think will make the project worse. Of course as an inexperienced developer, I could be wrong and his way might be better, it may be 1 of those exception cases.

My question is: what can I do to better judge if a senior developer's advice is good, bad or maybe it's good, but outdated in today context? And if it is bad/outdated, what tactics can I use to not implement it his way despite his 'pressures' while maintaining the fact that I respect him as a senior?
AnswersKenntnis
AnswerKenntnis:
First, as a senior developer, I expect the juniors I lead on projects to bring their concerns to me in a straight forward and direct manner. If they disagree, that's perfectly alright with me. In some cases, I will take action on their concerns. In most cases, their concerns are tossed aside put aside with a short explanation of the reasoning, not out of disrespect for the developer him/herself but because of some other reason such as:


The junior doesn't have all of the information at hand to understand the decision as it's been made. In some cases, a little explanation can help the developer move on past the concern and deal with an unideal situation.
The junior has BAD information. Don't forget that you are, in fact, a junior. That's the equivalent of being a teenager in software terms. I'm sure you have a lot of great ideas, but it's just possible that you don't know everything. I find the most resistant junior developers are the ones who firmly believe they know what's best for the code, the company, the world. These developers are better served by acquiring humility.
The decision to do something a particular way was made above the senior's head. The senior still works for someone else in the end. There may actually be a better way to do something, a more efficient way to write something, or better software/hardware to help do the job. The business still drives the decisions though. Business managers, directors, VPs, etc. often make decisions that impact the development process. These are beyond the control of the senior, and when the juniors complain about it, all they're doing is adding to the senior's stress.
The senior just flat out doesn't have time to take it into account. There are deadlines, and sometimes changing patterns, practices, and behaviors midstream is costly to that deadline. Since it's his neck on the chopping block it's often more important to get the product out functional and on time than "perfectly written". 


These are just the things I can think of off the top of my head. There are a ton of reasons why an idea, practice, concept might be dismissed or discarded by someone higher up than you. Many of them are unpleasant, but they all boil down to the fact that we're all human and we all have opinions. His opinion just happens to be numerically superior to yours at the moment.

Bearing those concepts in mind you should continue to bring your concerns to the senior developer. Find another senior developer who may be able to fill in the blanks. Many senior developers are where they're at because they are better with software than with people. Some are where they're at because they knew whose butt to kiss when they were interns. Find one who actually understands what it means to mentor someone and get their honest opinion. They may disagree with you and fill in the blanks you don't have. They may agree with you and help to rally your cause or make your situation better. 

At no time should you mount any kind of insurrection. Even if you believe in your heart that your way is right, you have been given an instruction to follow and you should follow it (unless it's illegal obviously). If you have trouble following these instructions, you may want to try to reason out why because you're going to discover this pattern of behavior to be very prevalent in very many companies that produce any kind of software.

Your best option is to continue to do your job ethically and professionally. Get the software you're asked to do complete in an exemplary fashion and escape the situation by being promoted out of it. If promotions don't come, you'll have plenty of references and experience to pursue opportunities in other departments or companies.
AnswerKenntnis:
There is a trick, one that a junior successfully pulled on me (the extremely bad tempered know-it-all senior developer): 


Do whatever you are asked to do, and exactly how you are asked to do it - be fantastically professional or go home,
If you have (any) concerns, write them down - never assume you'll remember (true developers log everything),
If possible, try to validate your concerns before raising them - it would be nice, but not necessary since you're still learning and it's the senior's job to validate or invalidate them,
Find an appropriate time each week to raise these concerns, but only after you've completed the related tasks - appropriate time is when the senior looks relaxed and chatty - never ever the day before the big deadline,
Try to repeat at regular intervals - and prepare to be amazed when the senior developer comes to you when you skip one - train us well and we'll be friends for life,


For extra points present your concerns as asking for help (apply to our vanity). An example: 


Bad: "Hey, senior, how can you be senior when you've never written a unit test?"
Good: "Hey, senior, I'm reading up on unit tests and can't quite grasp the concept. Can you give me a few examples on our code?"


For extra extra points, always remember that from my point of view my time is extremely more valuable than yours, try to be extremely concise and precise. 

That's all, young padawan.
AnswerKenntnis:
Respect the senior developer.  He is more on the line for the success of the project than you are.  Since he has the responsibility he also gets the authority.  If he says change something then do it.

That being said, don't hesitate to present your concerns, when you encounter a problem like you already have.  

Lastly, sit down with him and explain to him the same question you posted here.  Maybe you are missing something big, maybe he will open up more to your suggestions, either way, don't keep him in the dark if you think his advice is poor.
AnswerKenntnis:
When this happens, what you need to do is have a conversation with the senior developer. Perhaps he knows something that you don't about the code or the technical/business requirements. If so, you should learn it.

Do so in private. It might be seen as challenging authority, and such things are best done one-on-one. Show a willingness to compromise and collaborate by acknowledging and respecting his seniority, but be persistent in getting your questions answered. Approach the situation from a collaborative frame, not a combative one. You might consider asking him to mentor you.

At the end of the day, you have to balance your own ideas (which, to be fair, are relatively new and untested) with his. Perhaps you are indeed right, but you should do your best to learn from more experienced people so you can make a more informed decision. A good senior developer welcomes the opportunity to collaborate, mentor and learn, even with junior developers, and welcomes having their ideas challenged in a constructive way because they too know that they are sometimes wrong.
AnswerKenntnis:
The way you often tell is through a common sense approach.  Keep in mind that the senior developer may know more about the project, but might not know more about the right way to do things.  You have to gauge what he tells you to do - he's giving you bad advice if what he says flies in the face of what his betters claim (i.e. people more senior than him; not necessarily in your company... I'm talking about well-known "celebrity" developers who often post or write books about the correct way to do software development, or at the very least industry-accepted best practices).

Here's an example of "bad advice" from a senior developer (or any developer): If they are totally ignorant of what loose coupling is and why it's a good thing, and you are told to write all code in, let's say, the code-behind of an ASPX file, it's obvious the senior developer is clueless and his advice should not be listened to.  

If, on the other hand, he is telling you how a specific module in the system works, it's often best to listen as long as again, what he's telling you doesn't spit in the face of proper development principles.

Here's my rule of thumb: A senior developer at a company might simply be the developer with the longest tenure; he might not have any real skill.  Is he saying things that go against what some of the most respected developers in your field say (people with much more experience than him, and who are a lot more competent and respected)?  If he is, chances are his advice is bad unless there are very extreme circumstances.

Fully expecting downvotes/disagreements for extremely biased viewpoint.
AnswerKenntnis:
It may be hard to understand the vantage point of the senior developer, and yes he may be leading things down the wrong path, however when it comes to large projects, consistency is more important.

Having 50 some developers all following their own coding styles, standards, methodologies, and design patterns would be utter chaos.  If things are being done wrong it is always better to be consistently wrong than many different types of wrong and some things that were done right.

When it comes time to perform maintenance, add features, or fix what is "wrong" then it is much easier if the problems with the existing design are known upfront.

It is good to respectfully disagree but in the end you are best to fall in line.  Anybody on a team who goes rogue is not seen as a team player.
AnswerKenntnis:
As a senior developer this type of passive aggressive nit picking would drive me crazy and after confronting you would drive me to give you a bad review.  The perfect solution is the one the team can live with together.  

As for style this should be dictated by your style guide and best practices determined by your origination.  If you are passionate argue for it, but once a decision is made live with it and work with in the bounds of the team you are on.
AnswerKenntnis:
If the senior can't give you good reasons why they are ignoring industry best practice, then don't waste your time there.  You will never advance because you are too threatening, and anyway, do you want to spend time maintaining a pile of bad code?

Most developers stop reading when they leave college.  You haven't, so you are already in the  top 10%.  There is plenty of opportunity these days.  If there's no job market in your town, then look for a better town.
AnswerKenntnis:
Wow that's a wonderful opportunity for you to shine and show off your skills. Early in my career, I had someone who was my supervisor who was unable to make a decision, any decision, so I used that opportunity to learn how to do the work of the next higher level up. It got me promoted. You should do the same.
AnswerKenntnis:
I would strongly suggest not trying to "not implement it his way". 

So far, it sound like you have done the right thing. You were humble and brought up a counterpoint. I could not tell from your question whether you simply disagreed with his method, or you disagreed and presented an alternative. Bottom line though, always offer a clear and thought out alternative when you try to shoot down someone else's approach.  As he might see it, you have a good idea that might work, and he has an idea that does work.  

In any position, we are forced to do sub-optimal things all the time.  If you really don't like it, then you can do as you did and bring it up. After that, its the boss' way or the highway.  On the brighter side, you are insulated from much of the risk of poor decisions as a junior.
AnswerKenntnis:
You're in a not-so-good situation but, as HLGEM pointed out, you can turn these positions into blessings-in-disguise. Your question is multi-faceted, so I'll approach it in parts.


  I suspect, he does not know. At the same time, he tries to show me
  arrogance may be so that i don't turn up to him much for questions.


This could very well be true. There are a rash of developers who have been in the industry for decades and not be capable software leads - from a development or a mentoring standpoint (there is a difference). Experience comes from tackling fresh challenges and trying new ideas and learning new skills, but the majority of programmers spend their lives in a wing of The Corporate Office, working on The Payroll Application with their faithful Visual Basic and Java tools, never seeing the world racing by their cold, grey office.

There's nothing wrong with this. For many developers this all they ever wanted and are perfectly content with the situation - it is not, however, an ideal situation for fostering a future generation of programmers, let alone lead developers.

Bravado and arrogance can be a defense mechanism, trying to cover for inadequacies. How do you handle it? Don't confront it head on - if your lead is incompetent, and the boss is unwilling to rectify the situation then you'll have to live with it. That doesn't mean roll over and die, but you can't force somebody to be a good lead.


  However, he just only reviews my code, and most of his comments are
  related to coding guidelines


This is what makes me think you're right in that he may not be a good programmer. That's not to say that he isn't a better programmer than you at the moment (at the very least he'll have more experience and exposure from being in the industry for so long) but again, that doesn't translate into being an effective lead. Guidelines are all well and good, but they are second to the function, effectiveness, and efficiency of the code.


  What should i do in such situation?
  
  I have informed my manager of this situation, and i have requested him
  to change my lead, although he tells "yes" everytime, but he does not
  take any serious action.


Have you enumerated all of these specific points to the manager, and with specific examples to back it up? If you've simply gone to him and said, "I need a new lead", he won't take you seriously and perceive it as an "interpersonal problem" rather than a technical problem. The reaction of a lot of bosses in this situation is to ignore it in the hopes that it will "work itself out".

Here's some suggestions.


Don't chafe at your situation - trial by fire, while not fun, teaches you some critical researching skills for later in your career.
Start pushing your lead to be more involved. Do this carefully and respectfully. Continue to push and be persistent until he gives in (this actually works for a lot of situations in life).
If your lead won't get involved, see if there are others who could help you. Unless you're working in a sweat-shop that is only concerned with milking hours then your company won't mind another developer with more experience showing you some ropes and reviewing your code.
Start keeping an eye out for a new job. I wouldn't say that you're in a "must leave" situation but it wouldn't hurt your career to be in a place that takes your development as a programmer more seriously.
AnswerKenntnis:
Of course as an inexperienced
  developer, I could be wrong and his
  way might be better so my question is
  what ways can I do to better judge if
  a senior developer advice is a good or
  bad/outdated one.


You can become an experienced developer.  Until you do that, you'll be ill-equipped to judge whether or not your intuition as a junior was right, and by then it won't matter.

In the mean time, read Joel Etherton's answer.
AnswerKenntnis:
Pick you battles.  If your talking about an hour of work your going to have to change your code at times until you work your way up.  Next time you get a large project ask ahead of time for a chance to present your ideas before starting.  Put in some extra time and make a amazing demo or prototype.
AnswerKenntnis:
If you have a better way to do solve a particular problem, just DO IT. 

Let your code/solution be your best argument. Otherwise stick to what you have been told.

Case in point


  when I was a junior I had a
  situation where a particular section
  of code was not the best it could be.
  Rather than just debate about it, I
  simply improved it THEN showed the
  results to my senior. He accepted it,
  because the code is always king.
AnswerKenntnis:
Shockingly enough what I've done is to just stop asking. When given another option I just digress and do it his way but add my own flair to it. Use it as a learning experience to build your abilities and still pacify his or her need to keep it old school. 

In the end not every senior developer pays attention to new ways of doing things. They at times have old school ways that were great for the language they started on 20 years ago but are considered to be hacky, or "have a bad smell" in todays world. 

This may sound like an awful way to keep going, and it is. But I've also learned a lot from my seniors way of doing things. But this is really just my opinion. In the end you have to be happy at your job and keep distractions and tension at a low level. And by not objecting to things you will see stress go way down.
AnswerKenntnis:
good answers above, would add that a response like "best practices" or "more maintainable" is an opportunity to learn something. You say, "Can you give me an example of why that way is best so I can understand the difference?" or "In what situations would this way be more maintainable that some other way, so I can learn to plan ahead like that?"

If the senior is right, giving you an example will be easy. If he is a parrot... give him a cracker to stop the squawkin', and do what you think is best. Until ordered to otherwise.

If the senior's role is to mentor, he will explain why when asked.
AnswerKenntnis:
Don't trust senior people because of there seniority. Challenge authority as often as possible. A competent authority should be able to convincingly answer any question. That's what makes him an authority in the first place, isn't it? 

Because someone holds superstitious beliefs for all of his life doesn't mean he is right. Remember, in the middle ages people believed that the earth is flat and some of those self-righteous asses even felt justified killing the doubters. It turned out that the doubters were right. So much for bad reviews. 

Never fear a bad review. Would you trust a blind man judging color?
AnswerKenntnis:
I've noticed something over the last few years, nearly every company I'm at, with nearly every project I work on, with nearly every new hire (regardless of skill/experience level) wants to do something 'different'.

It might be the coding standards or the overall architecture or the language or the methodology.  But it's always something.  A lot of times, it's just stating the obvious, 'Shouldn't everything be documented better, for our end users?'

My advice to you is don't be that guy.

Some day, you'll be a kick-butt senior level guy who is hired and paid to make those decisions.  When that day comes, go to it!  Until that day, realize what your position is.  I have a boss, as far as I'm concerned my entire job is to make my boss happy.  It's not to second guess decisions made outside of my pay-grade.  If you really aren't sure, talk to your boss/supervisor and find out.  

Generally speaking though, it's far better to have everyone on-board with an outdated approach than half the team following the outdated approach, 1/4th of the team following some new approach and 1/4th of the team trying to develop a cutting-edge, brand-new approach.
AnswerKenntnis:
There's an undercurrent to all of these that I think should be brought out in clear: don't be combative.  Preserve  your relationship with him.  It's great to take his advice with a grain of salt and validate it in books and on sites like these, but don't attack him.  If he's a senior developer and has been through lots of projects, he's not an idiot, and there's lots to learn from him.  As it seems like you've already done, express your desire to understand his viewpoint.  Even if you're sure he's wrong and you're right, accept the possibility of the opposite (seems like you already understand this).  Try to make it clear that you're arguing because you want to understand his viewpoint better, not because you're trying to prove him wrong.

If he doesn't get back to you right away when you ask him a question, or if his answer is vague and/or unhelpful, don't assume he's blowing you off.  As has been mentioned here already, he may well be busy and/or stressed.

It's also great to be patient.  Keep a list of things in your head that you think should be done differently, and present them at the proper time.  Make sure you have justification for the suggestion besides "it's best practice."  And be careful to do things right and to not make mistakes, so that you have credibility when you make your arguments later on.
AnswerKenntnis:
As HLGEM and Jarrod said before this is really a blessing in disguise. Both of their answers are great and I want to add some points. 

As your lead is not in your domain you get to take some of the important decisions for your part of the application as your lead doesn't know much about middleware. You also have closure with your manager about how the application should go, what the product users want and how your manager would want to tackle a situation presented to him by the product team. Tell me how many people will get that kind of knowledge on an application. 

When you are in a large team you will sure have help from your teammates and/or your lead but you will not have the knowledge of how your manager thinks or the product team thinks because that kind of thing generally goes through your lead and may be some senior devs in a team. 
I agree one person projects are kinda tough to carry but if the other side of the coin is so great why miss a chance. Learn what you can learn, enjoy while you can and if it gets too hard convince your manager as Jarrod said or find a new job/project depending upon the situation.
AnswerKenntnis:
You're going to be dealing with people like that your entire career.  And there will be many with whom you will disagree about the best approach to any coding problem.  The best thing I think that has worked for me over the years is that if they keep pressing on an issue, be up front with them and tell them that you have considered their solution amongst the various possible solutions and that you felt that the solution you settled on was the one you felt was the best approach in this situation.  

Now, if you choose against their advice every time, then occasionally you may need to give in and use their "advice" from time to time just to smooth a few ruffled feathers.  As long as they don't feel you're rejecting their input every single time, then that will go a long way to keeping the peace between you.

Consider also that they are a senior developer and have been working in that environment for longer than you have.  Real life coding often doesn't mesh with best practices or community accepted standards.  There may be a reason that they recommended you do something a certain way that they are not able to articulate fully.  So even if you disagree with them, make sure you don't dismiss their advice out of hand based solely on the fact that you believe your solution is better.

It's all about balance.  And not just coding balance, but team balance.  Many a project has failed not because the developers weren't capable of doing it, but because they couldn't find ways to work together amicably.
AnswerKenntnis:
I would like to provide some stuff from my personal experience, which should be considered as a supplementary answer to one posted here by jzd...


Ask another senior developer,
Do research on your own.


In one professional appointment, I was supposed to be mentored by someone senior. He knew some stuff but honestly not that much, unfortunately he did not know it, so he was ultra confident on his answers. I somehow felt that what he said was wrong. I had some evidence when stuff he did were against the best practices mentioned in the MS certification I took :-). After that I started asking other people working in other companies (stackexchange was not up and running back then) and started reading blogs to compare answers. 

It would be great if I was wrong, cause I'd just change my behaviour, but I wasn't.
AnswerKenntnis:
So far I have been trying to avoid the
  issue by listening to him, raising a
  counter point, he raises his original
  point again (most of the time he will
  say best practice, more maintainable
  but just didn't go further), I...
  think about it at home, but... I'm still not
  convinced. But recently he... saw my code and asked me
  why haven't I changed it to his
  suggestion. This is the 3rd time in
  2--3 weeks.


(Slightly edited for the actions.)

This part concerns me.  One way you can see if you are correct or not is understanding what he is saying.  What I read (with my own history, others may be different) is a junior developer not understanding what the mentor is saying, and not asking for clarification.  One way you can figure it out is to ask him to clarify: how is this a better practice than that?  Or Why is this more maintainable than that for our code?  If you don't know his answers to this, you really don't know if he's giving good advice or not.

The part that really worries me is that he's asked you to change it multiple times, and you haven't.  Here is one way it might look from his side: he requests you make the change, you ask why, he gives you a (valid, to his mind) reason, and you refuse to make the change.  You don't ask for clarification, so he assumes you understand the reason, and are either too lazy or too stubborn to change it -- neither one a good thing for a senior developer to think about you.  Trust me, it's much better to ask questions than to get a reputation this way.
AnswerKenntnis:
A few thoughts:

1/ Does it work? 
Is his way working or not? Is the any objective reason why his way would be inferior? 

By objective reason, I mean something that can be measured without ambiguity (performance, bugs, length of code...) If his solutions work and there is no objective metric that indicate it's a poor solution, do it his way. His way is better... because it is probably more consistent with the rest of the codebase and because it will be easier for him to use/reuse your work. You might not like it, but that's not what it is about, is it?

If it does not work, or underperform on important metrics, implement it, compare his solution with your solution, then tell him that you have tried his way, but you can't get it to perform well (give the metrics) and ask him if you made a mistake in your implementation or if there is a requirement that you were not aware of

2/ Star programmers say...
Why give a damn? You will find famous programmers deeply at odds with each other on many fundamental subjects such as planning, design, OOP vs procedural, unit testing, exceptions handling, source control, on and on and on. 

If the only difference in workability between 2 solutions is who favors it, skip it. You might benefit from the mental workout required by working in a paradigm that you do not like
AnswerKenntnis:
Based on the idea that you should only take advice from people you want to become like, the answer is that the senior advice is good if you want to become like him/her.
AnswerKenntnis:
Most of my problems I have sorted by posting to forums and getting replies from others, and
  I have been surviving for around 1 year like this.
  
  What should I do in such situation?


To be honest, this is what a lot of technical jobs are like. You need to be a self-starter who can solve difficult problems by yourself (with the help if the internet and it's denizens) if you have to.

From the point of view of getting code reviews and getting help with architectural designs, even when I've had good managers I've never had much of a code review beyond "static variables should be prefixed a s_".

Take the opportunity to learn and learn how to learn; those will be skill that you can use into the future.
AnswerKenntnis:
If you think for a second that management doesn't understand how capable you REALLY are to get the job done, then you are probably wrong.  Management probably also understands that your lead right now would be completely useless if he replaced you and took over your work.

The REAL reasons they haven't relocated you are because despite all of your challenges that you present to them, you are still the best person for the job.  They obviously value your work too much to risk handing it off to anybody else.

Don't underestimate management's intelligence...

They are a lot smarter than most developers give them credit for.  I didn't understand this until I started managing.  They probably also are aware of how useless your lead really is but they are probably powerless to address that problem.

Let me paint a picture for you, Lead A with 5 yrs experience at the company is incompetent.  Manager knows this, recommends to his upper manager to lay off Lead A.  Upper manager asks why he wasn't dealt with years ago if he is that incompetent.  Manager looks bad now especially since Lead A has a bloated salary that was being paid for no benefit...

Here is another potential scenario, Lead A is close friends with somebody important.  He is too hot politically to take on,

Either way, with a long running mistake like that it is easier for large organizations to sweep imcompetent people under the rug, give them busy work and fake power that is appropriate to their years of "experience" where they can't do TOO MUCH damage.  Unfortunately many organizations would rather do this then actually deal with the problems.

Of course the reason for this is that the manager in this kind of organization is always better off short-term to deal with bad talent this way than to address the long term problems that these kinds of people can bring to the organization.

So while it is short-sighted and potentially unethical, you have to admit it is a little smarter than many developers actually give credit for.
AnswerKenntnis:
ughh ahhh. This question reminds me of many things. First all i'll say i couldn't get along with one of the managers at my last workplace. It wasn't a personality issue. It was a communication issue. I said XYZ and that specific manager would interrupt what i was saying as ABC. I would not be able to communicate well unless i worked with that manager for a more then a year.

This past weekend. A guy argued/disagreed with me about singletons. I said they are not good and should NEVER be used and absolutely no reason to use them. I linked him to http://www.gmannickg.com/?p=24 and the more in depth article it links to a few days after the argument. The day of another programmer (DudeB) mention he only uses singletons when its appropriate (which i added to with 'never'). DudeB didn't say anything about that but DudeB did say that DudeB was in a project which had memory contention because all the threads were accessing the singleton. After mentioning this, showing the article and mentioning memory contention the guy i argued with said we'll have to agree to disagree which i agreed to because i don't like talking about singletons (yet i am writing this d'oh)

Point is. Sometimes you could be dead wrong like this guy is (maybe someone will disagree about singletons with me in a comment). In my situation with my manager who was my senior programmer I just did what i was explicitly asked to do and i did not take quality seriously at that workplace ever again. I did do what i prefer to do when allowed but always did what i was asked to do but if i disagreed i would bring it up at least once.
AnswerKenntnis:
I have a completely different/controversial opinion on this.

Often times people loose track of the end goal which for most industries its about maximizing profits and minimizing loss. I know this sounds heartless (hence the negative points) but experience and sagacity matter very little if you are not going to be producing results.

People can get caught up with extremely indirect minute things to quibble about that have very little impact on the direct results of the company. 

If you think you are right about something your best bet is to show how that will produce better measurable results.
AnswerKenntnis:
I would initially try and stick it out, at the end of the day resistance to the senior is likely to be futile at least until you gain some more experience and respect. Use this as a learning experience and in 2+ years time if you still feel the same way - move on to another company. That is when you can use a combination of your and your seniors good ideas to impress your new employer. Of course you may start to realise some of the reasons for what you perceived to be bad decisions earlier on in your career and at some point you may have a junior developer working for you.
QuestionKenntnis:
Perks for new programmers
qn_description:
I intend on hiring 2-3 junior programmers right out of college.  Aside from cash, what is the most important perk for a young programmer?  Is it games at work? I want to be creative...  I want some good ideas
AnswersKenntnis
AnswerKenntnis:
In my experience, good programmers want to program with as few distractions as possible.  Some of these are more relevant to big companies, and I'm not sure where you work, but here are some examples:


Casual dress code: Young programmers in particular will have a tough time avoiding resentment of a strict dress code.  "I'm just going to sit at my desk all day--why do I need to wear slacks/polos/other uncomfortable business clothes?"  In my opinion, this is half rebellion and half honest productivity-seeking: It really is much easier to program in jeans and a t-shirt than slacks and a formal button-down.  The question you probably need to ask yourself is if the potential productivity gain and morale boost is worth the potential loss of "professional" atmosphere.  It all depends on your situation... there are startups and Fortune 500 companies out there which allow jeans & t-shirts.
Few meetings: Almost nothing is more distracting than a constant stream of meetings.  Try to avoid team-wide "status meetings" that could be carried out via individual e-mails or conversations.  Programmers like it when their employer lets them program.
Experienced coworkers: Good programmers want to improve.  If any of your other employees have contributed to big open source projects, or have worked individually on some particularly successful internal projects, let your prospectives know!
Private offices: This is rarely practical anywhere but venture-capitalized startups, but if you can offer candidates their own offices, they'll leave the interview with hearts in their eyes.  Programming is so much easier when you aren't distracted by foot traffic and people singing happy-birthday one cube over.
Cool stuff: If you can afford it, subsidize games for lunch breaks and post-work hang out sessions.
Best practices: This will ensnare good programmers and intimidate less experienced ones: Show that your candidates will be working with reliable, sane version control, and that there are coding standards about unit tests or inheritance or anything.  Organization is important.
Don't nickel-and-dime: If you can be flexible with hours, do it!  No one likes having to clock out every time they go to the restroom; it feels like you're not being valued as an employee.
Dual monitors: Instant win for almost any programmer who's worked with dual monitors before.
AnswerKenntnis:
A quality chair
AnswerKenntnis:
Admin rights to their PCs 
An internet connection that's not gimped by bizzaro proxy rules 
Dual Monitors
Work from home privileges
A soda fountain (not a drinking fountain that dispenses soda instead of water ala Brawndo, but like you'd use at the Taco Bell to refill your drink)
AnswerKenntnis:
The opportunity to work alongside experienced programmers.
AnswerKenntnis:
I always love going to conferences and training and consider that a perk. Not all companies pay to have their devs continue to learn. There's always more to learn. You benefit because they are learning more. They benefit from that too, but also have fun and get away from things for a couple of days and get to mingle with other devs.
AnswerKenntnis:
Give them each a budget and let them configure their own computer setup.  Make them submit a plan for what they intend to purchase.  Talk over the plan with them.  It will be a great way to kick things off.
Give them a budget for a cell phone and unlimited plan that the company will pay for.
Pay for their home Internet service.


Little things like these they will show their friends to the response of, "Cool - I wish my company did that!"
AnswerKenntnis:
The type of people you'd like to hire tends to be a first-order concern when deciding what sort of perks to offer.  For the programmer who's thinking about or in the process of raising a family, paternity leave, company matching of adoption funds up to $X/year, flexible vacation and working hours, and a sense of job security may be much more attractive than a soda machine and free Segways for all.  You mention that you're looking for "junior" or "young" programmers, but many young folks do still fall into this category.

I sense, however, that by "young", you might mean "too young to be into that whole 'work-life balance' thing".  Let's call this 'The Google Strategy'.  The idea here is to make it so it just doesn't make sense to their analytical minds to ever leave work.  Have on-site services like free food, drink, and laundry, provide gathering places for informal conversations.  Make them feel like they're the rock stars of the company, and they'll repay you with long hours and hard work.  The good news for you is that these types of perks don't cost you much at all relative to the increased hours they'll be willing to put in.  The bad news is that this model tends not to be sustainable, and this dot-com era "irrational exuberance" no longer satisfies your programmers when they start to want to take vacations, get married and go on a long honeymoon, have kids, and so forth.  At that point, they want flexibility, more vacation time, a 401k, etc.  Besides the first one, these all cost significant coin.

Here's the most important point though: if you'd like to hire the absolute brightest people you can find, don't try to outsmart them.  Odds are, the really sharp ones will be a little less interested in the size of the Free Red Bull Fridge and the number of air hockey tables at their disposal, than whether you'll value them as an asset to the company and as an individual (both in terms of compensation and employer/employee relations in general), whether you have a sustainable business model/plan, whether your work really excites them, and whether your work really excites you.  I'd suggest reading a couple essays on Joel On Software, he treats the subject of hiring good programmers in a fair amount of detail ("Smart, and Gets Things Done", I think, is the name of one of the essays).

While your question certainly isn't without merit, and providing a work environment with some of the same perks as your competitors will make your sales pitch somewhat easier, the only people that will be truly swayed by these kinds of things are not the people you want the success of your small company to depend on.  Good developers want to feel like they're making a contribution to something that matters, like their skills are valued and put to good use, like they are responsible to their peers and to themselves.  Focus on having a truly great, dynamic company that does great work, and that treats its technical people with respect (things like private offices help here, too), and you'll really attract the type of people you're looking for.

(Thanks to Thomas Kammeyer for a tip on the last paragraph!)
AnswerKenntnis:
Two flat-screen monitors, an optical mouse -- two things I don't currently have -- and each their own whiteboard with a few markers.
AnswerKenntnis:
Being able to work remotely + flexible hours, Tech books give-a-way, and lots of love!
AnswerKenntnis:
A boss who would ask this question.
AnswerKenntnis:
Philip Greenspun wrote about this once. He suggested making the office a better place to be than home, which is easier for young programmers. For example, domestic hardware that someone living alone cannot justify: expensive coffee machine, pool table, huge TV with DVDs to watch.

Make the office more sociable: put beer in the fridge and have a drink together at the end of the day. Provide better food (easy for people who can't cook): get deli deliveries or a caterer.
AnswerKenntnis:
Casual dress (for voting)
AnswerKenntnis:
give them responsibilities and some degree of freedom.

make them feel like they are developing something for themselves, with passion
AnswerKenntnis:
Work from home. (for voting)
AnswerKenntnis:
Private offices (for voting)
AnswerKenntnis:
be flexible about the starting hour.
AnswerKenntnis:
I'm currently slightly experienced but I still call myself junior.
Here is what I appreciate of my employer:


Buys me books. I have a diverse taste from C# to perl to C to Asm to database design to tsql etc. Book prices vary from $20 to $50. This usually requires a PO and approval and such.
Allows me to critique current projects. I've re-written a few project to be MUCH cleaner through the experience I gain. Each time I document why I made those changes. Every now and then I re-write my re-writes. It's amazing to see how much you change. I do this one on my own. I initiated it.
A fast computer and a 24" monitor. This actually helps a lot, but for any developer. Less frustration and more code on the screen. Monitor also rotates for those kinds of days.
AnswerKenntnis:
This is a sort of negative answer.

Don't give the office more entertainment than home.  No TV, video games or beer.  The office is for work and that is why I go to the office.  I go home for video games and TV.

Don't bother with team outings.  It's not relaxing.  It's just more work.  If I wanted to go somewhere to have fun, I'd go there with my own family or friends.  Or I would stay home and sleep late.  No doubt some people believe everyone else in the office wants to be friends and spend all their time hanging out.  It isn't true.  Sorry.

The same is true about company meals.  I like to go out and away from the office for lunch and dinner.  If there is a lunch meeting at the office, I will be making plans to leave work an hour early (with exceptions for crunch time, which had better not last more than a month or two out of each year.)
AnswerKenntnis:
I'm surprised the cynics amongst us haven't said 'non brain-dead leadership'!

Attracting young people with toys is a bit patronising, better to say:

"Yeah so we could offer you lots of new shiny toys, but how about we guarantee you no PHBs instead?"

;-)
AnswerKenntnis:
Invite your whole team to the restaurant of their choice every Friday for lunch. A former boss of mine used to do just that and it really helped team bonding. 

If budget doesn't allow it, you can do it once every two weeks or once a month. But think of the value of having closer team members.
AnswerKenntnis:
Programmers need vacation.  Lots of it.  Four weeks a year to start.  Minimum.
AnswerKenntnis:
Matthew 7:12 


  Therefore all things whatsoever ye
  would that men should do to you, do ye
  even so to them: for this is the law
  and the prophets.


Mohammed


  The most righteous of men is the one
  who is glad that men should have what
  is pleasing to himself, and who
  dislikes for them what is for him
  disagreeable


Confucius - Analects XV.24


  Never impose on others what you would
  not choose for yourself.
AnswerKenntnis:
A career path.  Not that they necessarily have to follow it, but give them the thought that they don't have to be a junior forever, and show them that there are opportunities in the company.  Give them an idea of what it takes to advance.
AnswerKenntnis:
Good hardware: I'd be very interested if I was told that I would get a desktop system (WinXP is still my system of choice) and a Linux server box. Something I have root on and can run services on (local at a minimum, world visible would be nice.) A Virtual private server in the company data center instead of dedicated hardware would also work.

Another thing that would be nice would be access to good references: "We will buy you any books that are apropos to your job!" same with software to some point, "if it's under $60, we will just get it."

Edit: large screenS on pivot stands, good chairs, white boards, etc.
AnswerKenntnis:
Lets them, on company time, do some private projects (things that could be useful for the company, but things they get to pick)
AnswerKenntnis:
Actually, Joel Spolsky has a really good article on this subject that I refer to from time to time:

Joel on Attracting Developers

EDIT: I read Joel's book on hiring devs, Smart and Gets Things Done. In the book, he says that this article is an embarassing bubble-era relic and he has learned a lot since then. I don't think the blog post is all bad, but it's true that the book is a lot more sophisticated.
AnswerKenntnis:
Treat them as peers
AnswerKenntnis:
The access to training and mentors.  The things that Junior developers want is pretty much what every programmer that I know wants.  They want to work in a relaxed and flexible environment with people who are at least as smart as them if not smarter.  They want to feel like they are a part of something.  They want to constantly be learning.

Make sure that you have a training/book budget.  Make sure that they are always learning and always have something interesting to work on.  Make sure that you do team building or some kind of thing like that on a fairly regular bases.  Lunch and learns are an increasingly popular tool these days.

One thing that Junior Developers might like more than more Senior developers is the use of cutting edge or even bleeding edge technology.  Be careful about this one, cause it can byte you in the butt, but it always helps.
AnswerKenntnis:
Casual dress code and office environment
Flexible hours
Allow listening to music while working (earphones allowed)
Multi-monitor/powerful workstations
Skilled/experienced co-workers/bosses
Code reviews done by those co-workers/bosses
Being able to work on creative projects that they come up with, and having them reviewed by those skilled co-workers/bosses (Most valuable perk!)
AnswerKenntnis:
My company has purchased an O'Reilly Safari Online account for each of our developers. I have access to thousands of books online at any time.

We also have training videos available at online from CBT Nuggets but I find their content limited.

Also, some productivity tools, for Visual Studio, such as CodeRush/Refactor Pro or Resharper

Quality Coffee in-house.
QuestionKenntnis:
Why isn't Java used for modern web application development?
qn_description:
As a professional Java programmer, I've been trying to understand - why the hate toward Java for modern web applications?

I've noticed a trend that out of modern day web startups, a relatively small percentage of them appears to be using Java (compared to Java's overall popularity).  When I've asked a few about this, I've typically received a response like, "I hate Java with a passion."  But no one really seems to be able to give a definitive answer.

I've also heard this same web startup community refer negatively to Java developers - more or less implying that they are slow, not creative, old.

As a result, I've spent time working to pick up Ruby/Rails, basically to find out what I'm missing.  But I can't help thinking to myself, "I could do this much faster if I were using Java," primarily due to my relative experience levels.  

But also because I haven't seen anything critical "missing" from Java, preventing me from building the same application.

Which brings me to my question(s):

Why is Java not being used in modern web applications?  


Is it a weakness of the language?  
Is it an unfair stereotype of Java because it's been around so long (it's been unfairly associated with its older technologies, and doesn't receive recognition for its "modern" capabilities)? 
Is the negative stereotype of Java developers too strong? (Java is just no longer "cool")
Are applications written in other languages really faster to build, easier to maintain, and do they perform better?
Is Java only used by big companies who are too slow to adapt to a new language?
AnswersKenntnis
AnswerKenntnis:
Modern day startups need to hit the market as soon as possible. They don't need to spend about six months in order to release their Java web application.  

Twitter for example was built using Rails/Ruby but once it became unscalable, they migrated to the JVM.

Not to mention that the development process isn't productive: code -> compile -> deploy while it is in frameworks like (Rails/Django/Grails): run testing server -> code -> change things and see what happens.

The good news is that JRebel  lets you see code changes instantly.
AnswerKenntnis:
In my experience, Java for web applications is overkill for small applications. A simple blog with one database table hold blog entries, for example, could be done in something much simpler. 

I have usually seen Java do much better in much larger web applications (think banks and insurance companies) that communicate with a number of other systems (such as mainframe back-ends and databases and peer web-services background batch-processing systems... all in the same application).

From what I've seen, the architecture of a JavaEE web application is just usually more than is needed for small/simple web applications.
AnswerKenntnis:
Start Ups want the shiny. Whatever the shiny is: RoR, Groovy, Grails, OOP w/ PHP, Foobar, Wibble, Narf, etc.

Enterprise wants stable, reliable and scalable: Java and .NET fit that bill (when done correctly).

Current gig: Financial Services. Platform: ColdFusion (essentially a Java Tag Library) and Java.

Previous gigs: 


Education Testing Services - ColdFusion
High Risk Insurance - ColdFusion and Java
401k - ColdFusion and Java
Travel - Java w/ internal ColdFusion apps
Securities - ColdFusion (pre-Java version)


These are all high-volume, high-security sites. No one at any of these companies ever considered PHP, some looked at RoR and saw too many issues. The 401k company had a sister company running a .NET application with competent developers, the app just kept crashing every week. They finally converted it to Java and gained stability.

The only people that look down on Java are those who have no or little actual experience with it or have been involved with poor implementations and are now gun shy. They see the shiny and figure if all the cool kids are using it, why not me?
AnswerKenntnis:
An addition to the FrustratedWithFormsDesigner's answer: Since I guess that your question more targets towards smaller sites, there is an important aspect that you need to consider for a lot of people: Hosting is ubiquitous for PHP but its harder for Java or ASP sites. This however is not a defect of those languages.
AnswerKenntnis:
Java absolutely is used for modern web application development. Particularly once you get to the slighly larger / more complex / scalable end of the web application spectrum.

If you are interested in modern, productive tools and frameworks take a look at:


The Play framework
Google Web Toolkit
Vaadin
Tapestry 5


But I think most truly modern web development on the JVM platform is likely to be done in one of the new JVM languages rather than using Java directly, with Java simply providing the backbone in terms of underlying libraries and back-end infrastructure. There is a lot of web development happening in Groovy (Grails), Scala (Lift), JRuby (JRuby on Rails) and Clojure (Noir, Ring/Enlive+lots of custom frameworks) to name but a few.

With all the innovation happening the new JVM language space, I personally suspect that Java will ultimately become the "assembler of server-side programming".
AnswerKenntnis:
I programmed java web apps for 10 years before I switched to python, 4+ years ago. I feel that I'm much more productive using python and can get much more done in a shorter period of time, and to be honest, I'm much happier when I develop in python. Here are some of the reasons why I think python is better then Java based on my personal experience, your milage may very.

Web Frameworks:

When I first start programming web apps in Java, Struts just came out, and it wasn't great, but it was the best thing available. I created a bunch of struts apps, and a few in other frameworks along the way. Whenever a new framework came out (Tapestry, Wicket, GWT, stripe, grails, AppFuse, Play, RichFaces, Spring, etc), I would try it out and see if it was any better, and most times it was only a little better, and sometimes not better at all. I do have to say the play framework is a step in the right direction.

Batteries not included:

One of the most annoying parts of Java was the fact that most of the libraries that you use were not included in java itself, you had to include a ton of 3rd party libs from places like apache commons. If you use something like hibernate with any other large library, you end up in Jar dependency hell, where hibernate needs one version of a jar, and something else needs another version. If you load the jar files in the wrong order, you are out of luck. You need to depend on tools like maven, and ivy to manage your dependencies, and this just brings in more dependencies into your project which results in projects being huge. I had some war files 100MB+ war files for the simplest web apps.

Too many options:

For some reason there seems to be way too many different ways to do the same thing in Java. There are over 38 different web frameworks for java according to wikipedia ( http://en.wikipedia.org/wiki/Comparison_of_web_application_frameworks#Java ) and 23 different ORM's ( http://en.wikipedia.org/wiki/List_of_object-relational_mapping_software#Java ) just to name a couple of examples. If you look at other languages they have a more reasonable number. Some people think that having lots of options is a good thing, but it isn't it leads to a lot of wasted effort in the developer community, everyone is reinventing the same wheel, and if you are a new person to the language you have too many option to pick from.

App servers:

Java web applications are really heavy, and require a lot of resources to run. They are especially memory hungry. Like any piece of software they can be tuned to reduce their resource footprint, but compared to other languages their out of the box setup is horrible. In my past I have used weblogic, websphere, Jboss, tomcat, and jetty. I only used the first three when I was forced to use EJB's, but even if you aren't using EJB's they were large app servers and sometimes hard to configure and get running correctly. Tomcat and Jetty are much better and easier to setup, but are still resource hogs. 

App Hosting:

If you aren't running your own server it is real hard to find shared hosting for your java apps at a reasonable price. The main reason is because java apps require much more memory compared to other languages, so it doesn't make sense for a shared hosting provider to spend their valuable RAM running a java site, when they could run 5 php sites in the same place. That means there are less providers offering java hosting, which in turn means higher costs to run your website.

Development Time:

When I developing in java, I found myself much slower then what I can do in python. I would need to make a change, compile, redeploy and then test, and this slows down the iterative process. I know there are ways to make this faster, but even at it's best, I felt much slower then what I can do in python. 

There is also a lot less boilerplate code to do the same thing in python, so I spend less time developing the code as well.

Java just feels over engineered in a lot of parts, A lot of the API's and interfaces are just way to complicated for what you want to do. And everyone and their brother thinks they are a java architect and this results in big complicated systems that are hard to use and develop with. 

IDE:

When I was developing in Java, I felt stuck to the IDE, I was lost without it. IntelliJ is the best IDE's on the market, and it was hard switching to python because there wasn't anything like it for python. So instead of an IDE, I just used textmate, which is just a normal text editor. It was hard at first, but because it was just a text editor, it was a really fast and responsive application. I could open my whole project in a few seconds, whereas when I want to open a project in an IDE it could take a minute or more, with a machine with a ton of RAM. The makers of IntelliJ came out with a python editor called pycharm, I bought it when it first came out, and it is great. But what I realized is that I don't need an IDE for python, I'm fine with a text editor. When I go back to working on Java web apps which I have to do from time to time, I try to use the text editor, but I haven't quite mastered that yet. I personally need the IDE for Java more because If I mess up something it takes longer to recompile and redeploy, which slows me down. 

ORM:

When I first started using Hibernate as an ORM, I thought it was great, it had it's problems, and it wasn't perfect, but it was better then what I was doing before. I was happy with it, until I did an application with Django's ORM on a python project, and that opened up my eyes, that is how an ORM is suppose to work. After that project I went back to hibernate, and I just felt disappointed, and longed for going back to Django's ORM. Another great python ORM is sqlalchemy, which is similar to Django's ORM, but a little different. I have limited experience with ROR's ORM, but from what I remember, it was pretty good as well.

Templates:

The web templating systems in Java aren't that good, and I think I have tried them all (tiles, freemarker, velocity, etc). Most of them offer only basic functionality and are a pain to work with. On the Python side, my two favorites are Django templates and Jinja2, they have everything that I could need in a templating engine, and are really easy to use.
AnswerKenntnis:
Well, I recently met with a Java guy that was really excited by the new Spring Data project, because of how little code it takes to get basic CRUD access to your DB going.

I can build a CRUD app using Rails (not just db access, but views and controllers) with a few commands.

(Off the top of my head: new project, 1 scaffold command per entity, 1 command to migrate the database, 1 command to start the server.)

It has nothing to do with the language, it's all about the tools. And it seems that dynamic languages tend to have the tools and frameworks that remove a lot of boilerplate code. (To make up for our lack of powerful IDEs that generate boilerplate for us.)

Also I feel that dynamic languages tend to make writing such tools and frameworks a lot easier. I can grok the code for say, Padrino or Rails (ruby web frameworks) a lot more easily than I can grok the code for say Spring Roo.
This could be due to the fact that I know Ruby a lot better than I know Java, though.
AnswerKenntnis:
Java has been positioned in the recent years to be "enterprise". Which is on the other side of the spectrum of what a startup needs. In web application development you need 4 things - painless database access, great string manipulation, syntax sugar and rapid iterative process to make the numerous little changes your app requires.

Performance,scalability and stability are a bit lower on the priority list.

Also Java is very unfun language to code in. It got the revolutionary ability to use string in a switch statement just yesterday. And javascript is very hackerish language so after developing your frontend you feel very constraint when you return to java.

So I suppose these are the reasons webstartups avoid java.
AnswerKenntnis:
Does Google, Amazon, or LinkedIn count as modern?

Java is used for modern web applications.  If you look across the enterprise it is the most heavily used language for web applications (internal).  

That said, Java went through a period were it's web development standards tried to be everything to everyone (arguably still do).  "Don't repeat yourself" was a response to the xml hell and long build cycles of Java web development.  As a result, Java (EJB, Struts, JSF, etc) became seen as the thing all of the new paradigms were trying to overcome.

Java, the language is verbose.  That is a pro and a con (great for maintenance, sucks for dev). There are a number of modern language features that have not yet made it into Java that can cut coding time down substantially (properties, events, closures, generators, list comprehension, etc).  So, it can be frustrating when coming from a more modern language. That said, they are difficult to add in to a mature language without becoming the rats nest that C# is becoming. 

Many languages used in modern web development are dynamically typed. This enables tooling that can dynamically reload code as it is written (this is harder to accomplish in a static language - jrebel). Since web development lends itself to fast iterations, dynamic reloading is a huge win. It significantly reduces the development cycle on greenfield projects and makes it easier to get the UI and the UX right (trial and error by nature). 

Static languages have their place, too. For backend logic that is complex, must run for years, must scale without issue, must be very fast, and must be completely error free, statically typed languages (like Java or even C) are preferred. 

Additionally, as developer count/turnover grows and products mature the likelihood of well intentioned folks introducing bugs skyrockets. The rigor and discipline that a well designed Java project (interfaces, patterns, and holy water for those php vampires :)) enforces helps to reduce long term risk. While, this can also be achieved via unit testing, the safety net derived from static checking (and static analyzers like findbugs and clang) delivers a built in level of code coverage that is hard to replicate with handwritten tests. Don't get me wrong, there should be unit tests and functional tests, but real organizations never achieve 100% coverage.  For what they check, static analyzers do. 

So, in large projects (as defined more by team size than code size), where there is complex interoperation between independently developed chunks of code, languages like Java are still preferred. Examples include large/complex web applications like those at financial brokers (ameritrade), financial exchanges (nasdaq, nyse, maybe london after the .net failure), online banking (almost all of them), email (google), auction (ebay), etc. 

From a performance and scale perspective, nothing tops the Java platform for it's combination of scalability and performance for web applications (depending on how you count facebook's application partitioning). Twitter, for example, had to rewrite large portions of it's Ruby infrastructure in Scala on the Java VM in order to put the fail whale back out to sea. I've heard of other large examples, but they elude me right now. 

ItΓÇÖs also worth considering security.  While Java browser plugins have suffered their fair share of security vulnerabilities, the java platform itself is one of the more secure platforms created.  Java web applications have a reputation of being very secure. ItΓÇÖs coding practices, libraries, and architecture have long discouraged mistakes that make attacks like sql injection or buffer overflows possible.  While other web platforms(rails) have a good security reputation, none surpass Java.

Bottom line, most web applications are simple technically.  For simple, Java is often overkill (just like in the old days when we wrote them in C :)).  If, though, the webapp is complex (backend or otherwise) or is expected to have 100+ developers, Java is hard to beat.

--

On a personal note, I use Grails a lot because it gives me the best of both worlds (the same can be said of JRuby which I hear is becoming more and more popular in the Ruby world).

BTW - I find the rise of PHP to be truly baffling. PHP as a language is the rough equivalent to perl in readability and VB in the quality of the results.  It encourages horrible practices, is next to impossible to maintain, 3rd party libraries rarely work as expected, and it has a syntax that would drive Larry Wall up...well...a wall.  The only explanation I can conjure is that it lends itself to incremental learning (like VB). In other words, you can accomplish something useful knowing very little about programming/administration and you can extend your knowledge a small chunk at a time.  There is a lot to be said for that from an adoption perspective.  However, for anyone that has every had to support or replace one of the billions of VB apps that were written by "programmers" in the corporate/mfg world, you are probably shaking your head and planning your retirement. :)
AnswerKenntnis:
I currently work in a company which has quite a few "I hate Java" Developers. It used to stun me too. I certainly hate all the hoards of technologies that are available with Java. This makes taking decisions too difficult. Its like When you have too much choice you have no choice. You have to spend time with 100's of frameworks to really come up with the framework that works for you. The standard Servelt architecture is waayy to complicated for most applications. This is not the case with Ruby, Django and stuff. They are are more of a single framework rather than language.

The biggest complaints I hear from developers


The Syntax is too long. Just to print something we have to write System.out.print. You cant really use a simple VI like editor and write out a working piece of code in a few hours.
Weak test frameworks. Even though the testing frameworks are very similar in Java and Ruby, Ruby takes a step forward by making thing easily available for testing. This is especially true if you are using DB extensively in your application. Even many of the Web frameworks dont think about testing.
Templates are a pain. Makes the relatively simple language into a Noodle Soup.
Not Cool. Most Java applications are written in huge companies, which is associated with Bureaucracy that does not go so well with developers. People don't think Google when they think Java. Google == Python. It has also to do a lot with no books coming out indicating do X in Y days.
Dont like to compile. To most developers compilation is a decade old phenomenon. It made sense in 80's with C but mordern computers can do a lot more. They dont write code in compiled languages. Java is one of the very few languages that is compiled and used to write web applications.
Too many Oops Concepts. Even though developers have quietly adopted the Oops domain. They dont like it in full. They dont like when you write an application with 10 classes with each class doing just one thing. Makes you open 100's of files and imagine interaction across 100's of classes, sometimes with frameworks. Makes the whole programming activity a chore. This could be true with most languages but I have seen that Java Developers pay a lot of attention to what a class does. Its the Java Developers who often come up with a code with 100's of classes. This is good from many perspectives but non java developers hate it.


So all in all Java imposes a steep curve at the beginning of the project, which means too much money to be committed. Add to this a huge community attached to java, each thinking in different ways and no one to really spear head the whole community. They also dont see talks and conferences conducted by the community showing off all the cool new things. No new cool books. Java it looks like will go down because it was used to solve too many different problems a few years back.
AnswerKenntnis:
In a recent interview with, Joseph Snarr, a technical lead for google plus explained how the application uses Java Servlets for the back end and JavaScript on the front end. 

So to answer your question Java is still used for very modern web development. Just not for the start-ups that have been getting so much press recently.

I think the reason that a lot of the start-ups are using other technologies is because theyre sexier and have a more publicized an open source push behind them.
AnswerKenntnis:
My team and I are currently developing a greenfield web application in Java 6 + Stripes.  Within the last year I also worked on another greenfield web application using Java 6 + Stapler (a somewhat unknown web framework developed by Kohsuke Kawaguchi of Hudson/Jenkins fame).

Java is absolutely used for modern web development.  Certainly it doesn't have the "sexy" appeal of Ruby or other dynamic languages, but I'm far from convinced that dynamic languages are a good thing once a project starts to scale.

Modern Java app servers are very competitive with ASP.NET in terms of performance, and both are orders of magnitude faster than any dynamic language VM I know of.

Don't get me wrong... I'm not saying Java is always the best choice (not remotely!) -- but neither is it always a wrong or "outdated" choice.
AnswerKenntnis:
The frameworks for doing Java web development have quite a bit of learning curve, they're often overkill for what you need, and much of the indirection required to make things work is just...painful...to work with. 

I used to work for a company that did Spring/Java development, and I found the framework cumbersome at best. I don't have a lot of pleasant things to say about Spring's framework, except I had a friend who used to do Struts development and he thought Struts was even worse. The web-framework is nothing like doing desktop applications or mobile (eg: android) applications, and has a lot of very abstract ideas that take some time to really grasp (though, certainly, that gives you a lot of power and capability if you're a pro and doing something really complex like an enterprise grade app). I love programming java for mobile or desktop devices, but java for web-apps? Not so much.

I haven't done any programming personally in Ruby/Rails, but my friend who used to do Struts is now doing Ruby web programming and testifies that things that are difficult to do in Java web programming require a lot less code and complexity to achieve in Ruby. There's certainly a learning curve to the different syntax and language rules, but for prototyping apps, it has an advantages in terms of how much code is required to achieve a desired result. As others have mentioned, scalability is an issue to consider as well, and one of the reasons more mature apps are not seen as frequently in more hip languages.
AnswerKenntnis:
It comes down to costs and trends. The Web 2.0 Startup is created by an under 30 visionary who has more talent than money (I'm generalizing of course but this is what you'll see "on average"). He is going to use a language he is familiar with because he's doing the programming (along with maybe a few friends). He's most likely a self-taught programmer.

Java has been targeted as an enterprise environment (by Java, I mean the language, the framework, and the standards). There are a bunch of expensive tools that the IBM's, Oracles, and BEA's of the world want to sell enterprises. 

The steps to become proficient with Java are complex and/or expensive. I know the landscape is changing there but is it too little too late?

After the startup gains traction comes the growth. Recruiting talented developers is difficult. Most "become a programmer in six weeks" programs teach Java (or .NET) and the market is saturated with "six week programmers" (oddly enough I've seen developers with resumes saying 7 years experience that still show the knowledge of a six week programmer). Using a non-mainstream non-"enterprisey" environment can be a natural filter for six week programmers. It takes dedication and personal investment to learn a Ruby or Scala outside of a job requirement. This is the biggest indicator to me of potential for a candidate. 

Knowledge comes with experience but a dedicated / passionate programmer will gain knowledge more rapidly (on average) than someone without that dedication / passion. Just like a kid who loves playing guitar will become better more quickly than a kid taking lessons because his dad made him.
AnswerKenntnis:
Java is too complicated. I do a ton of PHP work and it's just easier and faster for most situations. The ability to just SSH into a server open a php file make changes save and be done is great. The few Java apps I've worked on have always required a restart for the simplest change. (not saying it's always the case just what I've delt with). Additionally PHP hosting is cheap and readily available. 

I also think what you have at least with PHP is a lot of developers who like me started off 14/15 years ago with static HTML. As things progressed we started adding PHP to our sites because it was easy, simple, and affordable. Over the years the language has grown and has expanded it's abilities way beyond it's humble beginnings and now tries hard to be what I think is a lot of things it's really not.

On the flip side most PHP devs I know see Java as this giant overly complex 800lb gorilla, almost like getting out the 18 wheeler semi truck to drive down to the grocery store and get a loaf of bread.

I've tried to learn Java, my first impressions where that it was very long winded and carpal tunnel inducing. Additionally starting off it left me with a lot of questions that probably seem easy to a Java veteran. OpenJDK, or Sun? Tomcat, or Glassfish, or? Plus it seems every intro to Java book starts you out writing code for the command line. I think most people these days find that a snooze fest.
AnswerKenntnis:
Java is more complex to learn than PHP/Python/Ruby
Java ecosystem is very complex, very large and quite confusing to beginners 
There are many historically bad frameworks with negative reputations associated with java, you have to know which frameworks to avoid wasting time on
Java build tools are way to complex (maven & ant) 
Java does not have a module systems that is easy to use (OSGI is way too complex) 
Java IDE such as Eclipse while very powerful with amazing features are difficult to configure for effective web development without lots of experience. 
If you are using anything other than Tomcat or Jetty as a server then you will be frustrated by long startup times of WebSphere/WebLogic/JBOSS
Java EE solves problems that many people don't have such as distributed transactions 


A new developer getting into professional development will find Java an Order of Magnitude harder than rails, python, or php to get going with so they go with what is easy to learn.

Having said all the above, I made the decision to use Java for my Startup because a properly configured Java Development environment is very productive to work in. By properly configured I mean.


Less than 10 seconds start-up time 
Properly configured eclipse workspace, with a all frameworks wrangled up and configured 
Good selection of libraries (Spring, Spring MVC, Spring Social, Spring Security, JPA, Hibernate, Velocity, .... etc) 
Fast developer machines with SSDs
Orielly Safari subscription
AnswerKenntnis:
Since you mentioned web development and Java, many people tend to forget that at the beginning using Java Applets in a web browser did not preform well, not only that, but the "sandbox" for the applets were not fully developed and there were security issues with Java Applets being able to run in the browser and access local machine data (aka client side security issue).  Sure Java was solid in the backend and stand-alone applications but I think associating Java the language with Java applets (run on the browser) together kinda screwed up some perceptions about Java as a web development component.  I don't think they ever recovered from that.
AnswerKenntnis:
About 5 yrs back, me and a colleague were given a programming task for some internal project. A simple enough task which required command parsing.

I came up with the entire thing in about 80 lines of java code and my colleague took a week, about 20 java classes and much more lines of java code to do the same. Needless to say, his code was picked. 

This made me wonder. Everywhere, complexity was appreciated. ( I was working in one of the biggest software products company. ) Java was the tool of choice and  design patterns were THE way to code. 

Now, is it the mindset or just arrogance that rejects simplicity. Well, I always thought common sense should prevail. Whether it is an enterprise or a simple web application, the basic use cases are the same. It should be correct and verifiable.

I don't use java anymore for several reasons. But one of the factors -  complexity, is the prevailing mindset in a ton of java devs when it comes to developing software. 

As for scaling dynamic languages, JVM is the result of decades of research. There is a lot of the same happening for Ruby etc.

Scala is one language which I find is extremely clever and practical. Play! with Scala is just as excellent for web/enterprise application development as any out there. 

As for Ruby and Rails being the shiny new thing for start-ups, it is extremely difficult to hire a solid Rails developer. It is actually an impediment to any start-up whereas the plethora of java devs should make more business sense.
AnswerKenntnis:
Traditional web applications on Java, though well structured, are very far from "rapidly developed".  Though I've only ever written one full web application (Java/Tomcat/Struts), it was extremely picky, took longer than expected to debug, and was generally painful when implementing the business logic layer.  In Java's potential defense, it was the only web application I had written in Java (though I'm used to programming systems-level applications in Java), and I believe I could write another web application slightly faster the second time around.

Having said that, I've also written applications in PHP and C#, and they just work better, and are far more forgiving than Java.  More than that, Ruby on Rails was written specifically for rapid application development, which, like Robbie said, allow easy CRUD access to databases.  The problem is that most web sites you will be developing on your own do not need the level of customization that Java offers (and requires you to perform).  Additionally, every DB connection object must be written by hand and isn't that easy to templateize.  There may be a better framework around, especially one that takes advantages of Java 7's new dynamic language support features, but I haven't done the research yet.
AnswerKenntnis:
The question should be "Why isn't Java used by startups or for small projects?". Java certainly used for "modern Web apps". At Google, Java is used on the backend for many services, and closure compiled JS or GWT is used for the frontend. The issue is one of speed vs scale. Startups need to get to minimum viable product. They are usually small teams of 1-3 engineers, and value iteration speed over performance or maintainability. Running up against scalability issues or team code code maintenance issues is a problem "you'd like to have", that is, by the time you reach that stage, it's a sign your initial implementation helped you over the initial hump of getting customers or investment. You can afford to rewrite the app at that point.

A company like Google can afford the luxury of building things for scale up front, even though they may be wasting their time implementing scaling for something that might get no users, because they can absorb the loss.

At least, that's my opinion, that many many "cool", "hip", "modern" companies build small apps with small teams where iteration speed and simplicity are the greatest requirements.
AnswerKenntnis:
Depends how you define "modern web application development".  If you are talking startup, fast turnaround websites, you will need to consider languages and frameworks designed for that purpose.  If you are looking for stable, scalable, enterprise level web development, you look for languages and frameworks that support those ideals.  In my book, those are two very different goals.  RoR, Groovy, etc., are good for the first and Java is more appropriate, in general, for the latter.
AnswerKenntnis:
Google App Engine supports Java, so that you can write your entire web app in Java, using Eclipse as the IDE and deployment interface, with a reasonably documented Google API --  so I wouldn't say it isn't used or isn't usable.
AnswerKenntnis:
This is true, but not because of Java and its ecosystem. It's because of people that, when using Java tend to create big messes and heavy abominations.

There are enough frameworks (spring-mvc, grails, play, etc.) that allow you to build things fast. The fact that people overengineer their systems is a problem that comes with the increased knowledge people get when they work with the Java ecosystem - you know many more thing, and you have them available (there are tools for everything), and "everything looks like a nail". 

If you are "hacky", you can do pretty much the same with Java as with other languages, and here's a study indicating that: 

Study of 49 programmers: static type system had no effect on development time... http://www.cs.washington.edu/education/courses/cse590n/10au/hanenberg-oopsla2010.pdf
AnswerKenntnis:
Simple answer: learning curve to base productivity.

Framework based systems like RoR tend to put the "magic" in the language/syntax. It is very easy to ramp up on your basic RoR syntax and get an app up and going.

Java was a language first, and tools and frameworks showed up later. So you have to learn Java first, and then you have to learn Spring, or Grails, or your super IDE, or whatever. Favorite example of Ruby, it doesn't require setters and getters. Fact is, Java IDEs got rid of the manual coding too...but it is still in your source. The benefit of this approach, is that below the framework, there is a language that consistent that all Java developers can work with. 

This benefit is dubious to small startups where time is of the essence. Usually, they are doing very little that they couldn't do with an out of the box framework. So they can grab their RAD system of choice and have an app live the next day.

But if you look at Facebook and Twitter, as they expanded, they found things that couldn't be handled by out of box frameworks and so they had to use lower level technologies.

This holy war that framework developers have that they can do anything faster is bogus, they can do a lot of what they need simpler and with less of a learning curve. And for a lot of things, that is "good enough." Use what is right for the problem.
AnswerKenntnis:
To add just a bit to what's already been said, I think a lot of it has to do with how fast you can go from nothing (literally) to a functional web application.

If all you have today is an idea, going from where you are now to writing your web application is almost as easy as falling down, whether you choose a hosting provider or your own infrastructure (like an EC2 image).  Choosing Java, in my experience, is usually more work, and often costs more too.

Additionally, if you go with Linux and PHP/Python/Ruby, the tools and the platform are complimentary and designed to support each other.  With Java, sometimes it seems the two worlds (OS and Java) sometimes don't seem to be working in harmony with each other.
AnswerKenntnis:
In the startup I work for we chose to use both Java and JRuby to implement our API because they complement each other.

For infrastructure, process distribution and communications we leverage Java's robustness, whereas for the actual implementation of the API endpoints we chose JRuby since all the calls involve JSON and it makes a lot more sense to manipulate a loosely-typed representation (JSON) using a loosely-typed language (Ruby).

If we see one of our JRuby classes is becoming a bottleneck, we just re-implement it directly in Java (basically a line-by-line translation). This can happen quite often with classes which must do a lot of computation, and in this context JRuby behaves much like a prototyping language.

We implemented our own dynamic class loader which means we can change Java classes on the fly without restarting the server, and we've been very happy with the choice. So the "you need to compile and restart each time" argument does not hold much weight.

The key is to avoid all the Java EE stuff - it's huge and cumbersome and anti-agile.
AnswerKenntnis:
I still have the feeling that Java is being used in a lot of web development. But it's usually on the more business-oriented-no-mainly-tech-big-company kind of developments, which typically are less open then new startups that have to get some traction and promote their own work, as well as more interested in technology. So, even if it is used in a lot corporate web sites, you'll probably never know, as they won't really care to tell publicly about their technology stack.

That said, commenting all the original questions... 

Is it a weakness of the language?
Compared with other languages like Python or Ruby, Java is verbose and tends to need more code to do similar stuff. But it's not just the capabilities of the language, also the community surrounding it and the kind of developers that uses those tools. 
So, most of the modules and tools on Python, Ruby, PHP, etc are open source and are easier to find than in Java world, just because this one is more focused on giving (and charging) services.
For example, the Ruby community is really really oriented to web development, so every developer that is able to use Ruby will know about the problems and available tools for a web project. That is not necessarily true for Java developers, that could have been working on other kind of systems, like reporting systems. Of course, any good developer will catch up, but the perception is that the average Java developer is less worried about learning new technologies and new languages.

Is it an unfair stereotype of Java because it's been around so long (it's been unfairly associated with its older technologies, and doesn't receive recognition for its "modern" capabilities)?
Java is not really that old, and, being fair, it has greatly improved. It was the cool, relevant platform about 10 years ago. But since then, there have been new platforms with newer problems in mind, like Ruby on Rails. The core sector of Java has been mainly the corporate world, with different problems, so the people searching for new projects outside that has been looking for different tools. 
Also, the main advantage of Java design, being multiplatform, is not as relevant today as it was before.

Is the negative stereotype of Java developers too strong? (Java is just no longer "cool")
That has also some truth in it. Java still is the language to learn "to get a job". So, if you don't care, but just want to learn something to earn money, you'll end learning a little Java and not caring ever again to improve.
Again, is a lot about perception and visibility. There are tons of great Java developers that are coding without sharing their knowledge, while there are lots of PHP developers, maybe not as good, that are writing blogs and collaborating into open source. That leads to think that the PHP developers are better than Java ones, as you have certain feedback about them.

Are applications written in other languages really faster to build, easier to maintain, and do they perform better?
I'd say that they are faster to build. The principles of languages like PHP, Python or Ruby makes them quite good for generate software that can change constantly. E.g. Dynamic typing makes easier to change an interface. In Java having a well defined interface is important, which leads to more stable (and difficult to change) interfaces.
This is very important in a new startup, which main problem is to get a product before you run out of money.
About performance, it is very very easy to misunderstood the needs and try to use magic tricks to achieve the required performance, like "Java is faster than Ruby. Period" or "MongoDB is web scale". Reality is more difficult than than.

Is Java only used by big companies who are too slow to adapt to a new language?
Definitively, having already an existing team of Java developers in the company, makes easier to keep using the same language for new projects. This is perceived as "the safe bet", specially if the core of the company is not technology. 
But, anyway, Java is not ONLY used on big companies, there are still a lot of startups that uses Java for cool stuff (For example, FightMyMonster or Swrve uses Java extensively), but I'd say that the general tendency in the startup scene is to use other languages. 
That is also a way of getting people, as most people will be more exciting to work with Ruby, Python or PHP, perceived as more "friendly" and "fun" languages than to work with Java.
AnswerKenntnis:
Many might associate Java and web application development with the horrors of J2EE which, bundled with monstrous J2EE application servers from big blue and red corporations, equaled weeks of work before the basic "Hello World" was online.

True, recent JEE specifications and implementations are lighter weight, but I'd still think thrice before suggesting anything like this for a short cycle rapid development project.

This is still the standards-based way of doing web application development in Java.  The alternatives, many of which are mentioned in other answers, convey a more mixed and confusing picture with too many choices to make.

Other languages portray a single turnkey solution instead of this multitude.  This makes this choice seem more fit-for-purpose when you have more important fish to fry.
AnswerKenntnis:
I think it is used alot more than you'd think -- the use is just below the waterline. There are many, many ruby on rails wrappers around thick, fancy java services. Especially when you start dealing with anything approaching big data . . .
AnswerKenntnis:
Who says it isn't?

Spring MVC + Spring Data JPA or Mongo + Thymeleaf for templating + coffee-maven-plugin for Coffee to JS transpiling and you're good to go.
QuestionKenntnis:
What software programming languages were used by the Soviet Union's space program?
qn_description:
I got interested in the Soviet space program and was interested to discover that the software on the Buran spacecraft circa 1988 was written in Prolog.  

Does anyone know what languages might have been used in earlier missions, especially the Mars PrOP-M rover missions of the early 1970s which were somewhat autonomous and could navigate obstacles?

Edit 

My source for the Buran Prolog is this declassified document from the CIA site from May 1990.  I couldn't find an OCR version, so here's the relevant quote from p. 0449:


  According to open-source literature, the Soviets used the
  French-developed programming language known as Prolog to develop
  on-board system software for the Buran vehicle...
AnswersKenntnis
AnswerKenntnis:
There's a book in Russian, German Noskin, First computers for space applications (╨ô╨╡╤Ç╨╝╨░╨╜ ╨¥╨╛╤ü╨║╨╕╨╜, ╨ƒ╨╡╤Ç╨▓╤ï╨╡ ╨æ╨ª╨Æ╨£ ╨║╨╛╤ü╨╝╨╕╤ç╨╡╤ü╨║╨╛╨│╨╛ ╨┐╤Ç╨╕╨╝╨╡╨╜╨╡╨╜╨╕╤Å), ISBN 978-5-91918-093-7. 

The author himself participated in many early projects (mostly in hardware) and according to him analog hardware was in favor for a long time, he mentions that space rendezvous tasks didn't use digital computers until the late 70's. Due to this policy many digital computers were really proofs of concept although used in other areas of soviet economics. The first computer according to him used on-board was the Argon-11S (╨É╤Ç╨│╨╛╨╜-11╨í) on the unmanned missions to the Moon closer to Apollo-8 in time. Also Noskin briefly says that the on-board computer Salut-4 was compatible with general-purpose computers ES used in Soviet economics so it was possible to develop software in PL-1 and Fortran. 

There are several mentions of Buran program languages on Russian websites. According to Vladimir Parondjanov, an engineer from the program (Russian Post) three languages using Russian as a base were developed: PROL2 (╨ƒ╨á╨₧╨¢2) for onboard programs, Dipol (╨ö╨╕╨┐╨╛╨╗╤î) for earth tests, and Laks (╨¢╨░╨║╤ü) for modelling. All of them were intended for use not only by professional programmers but also engineers from other areas. 

When the Buran program was closed they were merged into a new language Drakon (╨ö╤Ç╨░╨║╨╛╨╜, Russian word for "Dragon") that is claimed to be be a "graphical" language having 2-dimensional descriptions of the programs and using arbitrary well-known languages for code generation. This language was also intended for use by non-programmers. The language probably does not have and international community and isn't even well-known within Russia although heavily promoted by its author, Vladimir Parondjanov (the Russian Wikipedia article article is very long and was even deleted once for not following Wikipedia rules). Drakon was first used for programming for the Sea Launch missions and has been used in other Russian space programs since.
AnswerKenntnis:
In the 80s, the third generation of Argon airborne computers used Pascal, Fortran and Si. (Si?). I have not found any other language beside assembler used before that.

From Argon airborne computer history


  Argon family was created in three stages. During the first stage (1964 - mid 70s) 11 types of computers for space, airborne and ground automated control systems were produced.


and


  In mid80s the third stage of development of Argon computers began. In 1986, the state program was adopted whose goal was to create unified mobile computer families, so-called SB EVM, based on ES EVM, POISK and SM EVM architectures.


One of these, the SB-3580 airborne computer, had:


  Programming facility: OS RAFOS-11 cross-system. Programming languages include: Assembler, Si, Pascal, Fortran.╨░


source: Introducing Argon
AnswerKenntnis:
My father worked in ╨ª╨¥╨ÿ╨ÿ ╨á╨ó╨Ü and participated in the development of software for Buran manipulator ( http://www.buran-energia.com/bourane-buran/bourane-consti-bras.php )
Software was developed on ES EVM (Soviet clone of IBM S/360-370) using Fortran due to heavy computations.
AnswerKenntnis:
I came across this article in ACM quote quad a few years ago,  "A History of APL in the USSR".  It was written by Andrei Kondrashev (Computing Center of the Russian Academy of Sciences) and Oleg Luksha (The Obninsk Educational Center).  

An interesting quote from the article:


  It may be of interest that APL was directly related to the design of
  the Soviet space shuttle "Buran." Heat protection is one of the major
  elements in the construction of apparatus of that type. It is made out
  of composition materials. Mathematical models of the process of drying
  and roasting of plates covering the hull were made with the help of
  APL. As a result, the percentage of defects during the production of
  the cover plates was reduced, saving a lot of money and effort.
QuestionKenntnis:
Why is 80 characters the 'standard' limit for code width?
qn_description:
Why is 80 characters the "standard" limit for code width? Why 80 and not 79, 81 or 100? What is the origin of this particular value?
AnswersKenntnis
AnswerKenntnis:
You can thank the IBM punch card for this limit - it had 80 columns:
AnswerKenntnis:
As oded mentioned, this common coding standard is a result of the IBM's 1928 80 column punched card format, since many coding standards date back to a time when programs were written on punch cards, one card/line at a time, and even the transition to wider screens didn't alter the fact that code gets harder to read the wider it becomes.

From the wikipedia page on punched cards:


  Cultural Impact
  
  
  A legacy of the 80 column punched card format is that a display of 80 characters per row was a common choice in the design of character-based terminals. As of November 2011 some character interface defaults, such as the command prompt window's width in Microsoft Windows, remain set at 80 columns and some file formats, such as FITS, still use 80-character card images.
  


Now the question is, why did IBM chose 80 column cards in 1928, when Herman Hollerith had previously used 24 and 45 column cards?

Although I can't find a definitive answer, I suspect that the choice was based on the typical number of characters per line of typewriters of the time.

Most of the historical typewriters I've seen had a platen width of around 9 inches, which corresponds with the standardisation of paper sizes to around 8"-8.5" wide (see Why is the standard paper size in the U.S. 8 ┬╜" x 11"? and the History of ISO216 A series paper standard).

Add a typical typewriter pitch of 10-12 characters per inch and that would lead to documents with widths of between 72 and 90 characters, depending on the size of the margins.

As such, 80 characters per line would have represented a good compromise between hole pitch (small rectangular vs. larger round holes) and line length, while maintaining the same card size.



Incidentally, not everywhere specifies an 80 character line width in their coding standards. Where I work has a 132 character limit, which corresponds to the width of typical wide line printers of yore, a 12pt landscape A4 printout and the typical line width remaining in an editor window of Eclipse (maximised on a 1920x1200 screen) after Package Explorer and Outline views are taken into account.

Even so, I still prefer 80 character wide code as it it makes it easier to compare three revisions of a file side-by-side without either scrolling sideways (always bad) or wrapping lines (which destroys code formatting). With 80 character wide code, you only need a 240 character wide screen (1920 pixels at 8 pixels per character) to see a full three-way-merge (common ancestor, local branch and remote branch) comfortably on one screen.
AnswerKenntnis:
I'd say that's also because old terminals were (mostly) 80x24 characters in size: Back in the days of 80x24 terminals...

EDIT:

To answer more precisely and more thoroughly to the question, 80 characters is the current "universally accepted" limit to code width inside editors because 80x24 and 80x25 formats were the most common screen modes in early I/O terminals and personal computers (VT52 - thanks to Sandman4).

This limit is still valid and somehow important IMHO for two main reasons: the default geometry that many Linux distros assign to newly spawned terminal windows is still 80x24 and many people use them as-is, without resizing. Moreover, kernel, real-time and embedded programmers often work in a "headless" environment without any window manager. Again, the default screen resolution is often 80x24 (or 80x25), and, in these situations, it may even be difficult to change this default setting.

So if you are a kernel, real-time or embedded programmer you should force yourself to respect this limit, just to be a little more "friendly" towards any programmer that should read your code.
AnswerKenntnis:
A related question is "why has 80 column persisted". Even the responses on this page are approximately that width. I agree with the historical reasons for 80 columns, but the question is why the standard has persisted. I would claim readability - for prose and code. Our minds can only absorb so much information in one piece. I still use the 80 column marker in my code editor to remind me when a statement is getting too long and obscure. It also leaves me plenty of screen real-estate for the browser, and the supporting IDE windows. Long live 80 column - as a guide not a rule.
AnswerKenntnis:
While probably not the original reason for the 80 character limit, a reason that it was accepted widely is simply reading ergonomics:


If lines are too short, text becomes hard to read because you must constantly jump from one line to the next while reading.
If lines are too long, the line jumping becomes too hard because you "lose the line" while going back to the start of the next line (this can be mitigated by having a bigger inter-line spacing, but this also wastes space).


This is widely known and accepted in typography. The standard recommendation (for text in books etc.) is to use something in the region of 40-90 characters per line, and ideally about 60 (see e.g. Wikipedia, Markus Itkonen: Typography and readability ).

If you aim for 60 characters per line, your upper limit must obviously be a bit higher to accommodate the occasional long expression (and things like margins markers and line numbers), so having an upper limit of 70-80 makes sense.

This probably explains why the 80 character limit was taken over by many other systems.
AnswerKenntnis:
Another common line length limit in the days of fixed pitch fonts was 72 characters. Examples: Fortran code, mail, news.

One reason was that columns 73-80 of a punch card were often reserved for a serial number. Why a serial number? If you dropped a card deck, you could pick the cards up in any order, line up the upper left corners (which always had a diagonal cut) and use a card sorting machine to get them back in order. 

Another reason for the 72-character limit was that common fonts were 10 points high and 6 points (1/12") wide. An A4 or 8.5" wide page could hold 72 characters in a 6" wide column and still have room for margins of over an inch.
AnswerKenntnis:
I personally stick to "about column 80" for my end of line because further than that causes wrapping or lost code when you print it.

There's punch card legacy as well, but I don't think laser printers or 8.5x11 inch paper was set to conform to punch card limitations.
AnswerKenntnis:
Scroll in printers papers were Letters size or 15" wide.

It were the 80 cps line printers for hardcopying of codes or reports, and later on Epson supports 132 cps condensed printing (escape code \015 for condensed print).
AnswerKenntnis:
One of the reasons for the 80 column cards may be associated with the 'hand punch' which was probably in use before the electronic card punch machines. It is one which I used in the early 70s on an ICL System 4-50 main frame computer site. One had to punch a section of three? punch knives in the carriage at the same time.
QuestionKenntnis:
I'm a Subversion geek, why should I consider or not consider Mercurial or Git or any other DVCS?
qn_description:
I try to understand the benefits of distributed version control system (DVCS).

I found Subversion Re-education and this article by Martin Fowler very useful.


  Mercurial and others DVCS promote a new way of working on code with changesets and local commits. It prevents from merging hell and other collaboration issues


We are not affected by this as I practice continuous integration and working alone in a private branch is not an option, unless we are experimenting. We use a branch for every major version, in which we fix bugs merged from the trunk.


  Mercurial allows you to have lieutenants


I understand this can be useful for very large projects like Linux, but I don't see the value in small and highly collaborative teams (5 to 7 people).


  Mercurial is faster, takes less disk space and full local copy allows faster logs & diffs operations.


I'm not concerned by this either, as I didn't notice speed or space problems with SVN even with very large projects I'm working on.

I'm seeking for your personal experiences and/or opinions from former SVN geeks. Especially regarding the changesets concept and overall performance boost you measured.

UPDATE (12th Jan): I'm now convinced that it worth a try.

UPDATE (12th Jun): I kissed Mercurial and I liked it. The taste of his cherry local commits. I kissed Mercurial just to try it. I hope my SVN Server don't mind it. It felt so wrong. It felt so right. Don't mean I'm in love tonight. 

FINAL UPDATE (29th Jul): I had the privilege to review Eric Sink's next book called Version Control by Example. He finished to convince me. I'll go for Mercurial.
AnswersKenntnis
AnswerKenntnis:
Note: See "EDIT" for the answer to the current question 



First of all, read Subversion Re-education by Joel Spolsky. I think most of your questions will be answered there. 

Another recommendation, Linus Torvalds' talk on Git: http://www.youtube.com/watch?v=4XpnKHJAok8. This other one might also answer most of your questions, and it is quite an entertaining one.

BTW, something I find quite funny: even Brian Fitzpatrick & Ben Collins-Sussman, two of the original creators of subversion said in one google talk "sorry about that" referring to subversion being inferior to mercurial (and DVCSs in general).

Now, IMO and in general, team dynamics develop more naturally with any DVCS, and an outstanding benefit is that you can commit offline because it implies the following things:


You don't depend on a server and a connection, meaning faster times.
Not being slave to places where you can get internet access (or a VPN) just to be able to commit.
Everyone has a backup of everything (files, history), not just the server. Meaning anyone can become the server.
You can commit compulsively if you need to without messing others' code. Commits are local. You don't step on each other's toes while committing. You don't break other's builds or environments just by committing.
People without "commit access" can commit (because commiting in a DVCS does not imply uploading code), lowering barrier for contributions, you can decide to pull their changes or not as an integrator.
It can reinforce natural communication since a DVCS makes this essential... in subversion what you have instead are commit races, which force communication, but by obstructing your work.
Contributors can team up and handle their own merging, meaning less work for integrators in the end.
Contributors can have their own branches without affecting others' (but being able to share them if necessary).


About your points:


Merging hell doesn't exist in DVCSland; doesn't need to be handled. See next point.
In DVCSs, everyone represents a "branch", meaning there are merges everytime changes are pulled. Named branches are another thing.
You can keep using continuous integration if you want. Not necessary IMHO though, why add complexity?, just keep your testing as part of your culture/policy.
Mercurial is faster in some things, git is faster in other things. Not really up to DVCSs in general, but to their particular implementations AFAIK.
Everyone will always have the full project, not only you. The distributed thing has to do with that you can commit/update locally, sharing/taking-from outside your computer is called pushing/pulling.
Again, read Subversion Re-education. DVCSs are easier and more natural, but they are different, don't try to think that cvs/svn === base of all versioning.


I was contributing some documentation to the Joomla project to help preaching a migration to DVCSs, and here I made some diagrams to illustrate centralized vs distributed.

Centralized



Distributed in general practice



Distributed to the fullest



You see in the diagram there is still a "centralized repository", and this is one of centralized versioning fans favourite arguments: "you are still being centralized", and nope, you are not, since the "centralized" repository is just repository you all agree on (e.g. an official github repo), but this can change at any time you need.

Now, this is the typical workflow for open-source projects (e.g. a project with massive collaboration) using DVCSs:



Bitbucket.org is somewhat of a github equivalent for mercurial, know that they have unlimited private repositories with unlimited space, if your team is smaller than five you can use it for free.

The best way you can convince yourself of using a DVCS is trying out a DVCS, every experienced DVCS developer that has used svn/cvs will tell you that is worth it and that they don't know how they survived all their time without it.



EDIT: To answer your second edit I can just reiterate that with a DVCS you have a different workflow, I'd advise you not to look for reasons not to try it because of best practices, it feels like when people argue that OOP is not necessary because they can get around complex design patterns with what they always do with paradigm XYZ; you can benefit anyways.

Try it, you'll see how working in "a private branch" is actually a better option. One reason I can tell about why the last is true is because you lose the fear to commit, allowing you to commit at any time you see fit and work a more natural way. 

Regarding "merging hell", you say "unless we are experimenting", I say "even if you are experimenting + maintaing + working in revamped v2.0 at the same time". As I was saying earlier, merging hell doesn't exist, because:


Everytime you commit you generate an unnamed branch, and everytime your changes meet other persons' changes, a natural merge occurs.
Because DVCSs gather more metadata for each commit, less conflicts occur during merging... so you could even call it an "intelligent merge".
When you do bump into merge conflicts, this is what you can use:




Also, project size doesn't matter, when I switched from subversion I actually was already seeing the benefits while working alone, everything just felt right. The changesets (not exactly a revision, but a specific set of changes for specific files you include a commit, isolated from the state of the codebase) let you visualize exactly what you meant by doing what you were doing to a specific group of files, not the whole codebase.

Regarding how changesets work and the performance boost. I'll try to illustrate it with an example I like to give: the mootools project switch from svn illustrated in their github network graph.

Before



After



What you are seeing is developers being able to focus on their own work while commiting, without the fear of breaking others' code, they worry about breaking others' code after pushing/pulling (DVCSs: first commit, then push/pull, then update) but since merging is smarter here, they often never do... even when there is a merge conflict (which is rare), you only spend 5 minutes or less fixing it.

My recommendation to you is to look for someone that knows how to use mercurial/git and to tell him/her to explain it to you hands-on. By spending about half an hour with some friends in the command line while using mercurial with our desktops and bitbucket accounts showing them how to merge, even fabricating conflicts for them to see how to fix in a ridiculous ammount of time, I was able to show them the true power of a DVCS.

Finally, I'd recommend you to use mercurial+bitbucket instead of git+github if you work with windows folks. Mercurial is also a tad more simple, but git is more powerfull for more complex repository management (e.g. git rebase).

Some additional recommended readings:


Distributed Revision Control Systems: Git vs. Mercurial vs. SVN
Git vs. Mercurial: Please Relax
Distributed Version Control & Git [Part 1] & Distributed Version Control & Git [Part 2]
Git/Mercurial vs. Subversion: Fight!
DVCS vs Subversion smackdown, round 3
Why I Like Mercurial More Than Git (part 1, part 2)
Contributing with Git: Reducing the frictions of Open Source collaboration with the Git VCS
"How have the DVCSs worked for you so far?" - post on the Joomla! Framework Development mailing list asking for feedback after the experimental adoption of mercurial and git for the Joomla! Platform Project
DVCS branching (with Mercurial) - A detailed and example-oriented introduction to what branching looks like in a DVCS, which is possibly the most important feature in these systems
AnswerKenntnis:
What you are saying is among other things that if you essentially stay on single branch, then you don't need distributed version control.

That is true, but isn't it a needlessly strong restriction to your way of working, and one that doesn't scale well to multiple locations in multiple timezones?  Where should the central subversion server be located, and should everybody go home if that server for some reason is down?

DVCSes are to Subversion, what Bittorrent is to ftp

(technically, not legally).  Perhaps if you think that over, you might understand why it is such a large leap forward?

For me, our switch to git, immediately resulted in


Our backups being easier to do (just "git remote update" and you're done)
Easier to commit small steps when working without access to the central repository.  You just work, and synchronize when you come back to the network hosting the central repository.
Faster Hudson builds.  Much, much faster to use git pull than updating.


So, consider why bittorrent is better than ftp, and reconsider your position :)



Note:  It has been mentioned that there are use-cases where ftp is quicker and than bittorrent.  This is true in the same way that the backup file your favorite editor maintains is quicker to use than a version control system.
AnswerKenntnis:
The killer feature of distributed version control systems is the distributed part.  You don't checkout a "working copy" from the repository, you clone an entire copy of the repository.  This is huge, as it provides powerful benefits:


You can enjoy the benefits of version control, even when you don't have internet access such as...  This one is unfortunately overused and overhyped as a reason DVCS is awesome---it is just not a strong selling point as many of us find ourselves coding without internet access about as often as it starts raining frogs.
The real reason having a local repository is killer is that you have total control over your commit history before it gets pushed to the master repository.


Ever fixed a bug and ended up with something like:

r321 Fixed annoying bug.
r322 Argh, unexpected corner case to annoying bug in r321!
r323 Ok, really fixed corner case in r322
r324 Oops, forgot to remove some debugging code related to r321
...


And so on.  History like that is messy---there was really only one fix, but now the implementation is spread between many commits that contain unwanted artifacts such as the addition and removal of debugging statements.  With a system like SVN, the alternative is to not commit (!!!) until everything is working in order to keep the history clean. Even then, mistakes slip by and Murphy's Law is waiting to brutalize you when significant amounts of work are not protected by version control.

Having a local clone of the repository, that you own, fixes this as you can re-write history by continuously rolling the "fix it" and "oops" commits into the "bug fix" commit.  At the end of the day, one clean commit gets sent to the master repository that looks like:

r321 Fixed annoying bug.


Which is the way it should be.

The ability to re-write history is even more powerful when combined with the branching model.  A developer can do work that is entirely isolated within a branch and then when it's time to bring that branch into the trunk you have all sorts of interesting options:


Do a plain vanilla merge.  Brings everything in warts and all.
Do a rebase.  Allows you to sort through the branch history, re-arrange the order of commits, throw commits out, join commits together, re-write commit messages---even edit commits or add new ones! A distributed version control system has deep support for code review.


Once I learned how local repositories allowed me to edit my history for the sake of the sanity of my fellow programmers and my future self, I hung up SVN for good.  My Subversion client is now git svn.

Allowing developers and managers to exercise editorial control over commit history results in better project history and having clean history to work with really helps my productivity as a programmer.  If all this talk about "rewriting history" scares you, don't worry as that is what central, public or master repositories are for.  History may (and should!) be re-written up to the point where someone brings it into a branch in a repository that other people are pulling from.  At that point the history should be treated as though carved upon a stone tablet.
AnswerKenntnis:
dukofgamings answer is probably about as good as it can get, but I want to approach this from a different direction.

Let us assume that what you say is absolutely true, and that by applying good practices you can avoid the problems that DVCS was designed to fix. Does this mean that a DVCS would offer you no advantage? People stink at following best practices. People are going to mess up. So why would you avoid the software that is designed to fix a set of problems, choosing instead to rely on people to do something that you can predict in advance they are not going to do?
AnswerKenntnis:
Because you should continuously challenge your own knowledge. You are fond of subversion, and I can understand because I used it for many years, and was very happy about it, but that doesn't mean that it is still the tool that would suit you best.

I believe that when I started using it, it was the best choice at the time. But other tools come up over time, and now I prefer git, even for my own spare time projects.

And subversion does have some shortcomings. E.g. if you rename a directory on disc, then it is not renamed in the repository. File move is not supported, making a file move a copy/delete operation, making merging changes when files have been moved/renamed difficult. And merge tracking is not really build into the system, rather implemented in the form of a workaround. 

Git does solve these problems (including automatically detecting if a file has been moved, you don't even need to tell it that is a fact).

On the other hand git does not allow you to branch on individual directory levels like subversion does.

So my answer is, you should investigate alternatives, see if it fits your needs better than what you are familiar with, and then decide.
AnswerKenntnis:
With respect to performance, Git or any other DVCS has a big advantage over SVN when you have to switch from one branch to another, or to jump from one revision to another.  As everything is stored locally, things are much quicker than for SVN.

This alone could make me switch!
AnswerKenntnis:
Yes, it hurts when you have to merge large commits in subversion. But this is also a great learning experience, making you do everything possible to avoid merging conflicts. In other words, you learn to check in often. Early integration is a very good thing for any co-located project. As long as everyone is doing that, using subversion shouldn't be much of a problem.

Git, for instance, was designed for distributed work and encourages people to work on their own projects and create own forks for an (eventual) merge later. It was not specifically designed for continuous integration in a "small and highly collaborative teams" which is what the OP is asking for. It is rather the opposite, come to think of it. You wont have any use for its fancy distributed features if all you're doing is sitting in the same room working together on the same code.

So for a co-located, CI-using team, I really don't think it matters much if you use a distributed system or not. It boils down to a matter of taste and experience.
AnswerKenntnis:
Instead hanging onto the idea that "by applying best practices you don't need a DVCS", why not consider that the SVN workflow is one workflow, with one set of best-practices, and the GIT/Hg workflow is a different workflow, with a different set of best practices.

git bisect (and all of its implications on your main repository)

In Git, a very important principle is that you can find bugs using git bisect. To do this, you take the last version you ran that was known to work, and the first version you ran that was known to fail, and you perform (with Git's help) a binary search to figure out which commit caused the bug. To do this, your entire revision history has to be relatively free of other bugs that can interfere with your bug search (believe it or not, this actually works pretty well in practice, and Linux kernel developers do this all the time).

To achieve git bisect capability, you develop a new feature on its own feature branch, rebase it and clean up the history (so you don't have any known non-working revisions in your history -- just a bunch of changes that each get you partway to fixing the problem), and then when the feature is done, you merge it into the main branch with working history.

Also, to make this work, you have to have discipline about which version of the main branch you start your feature branch from. You can't just start from the current state of the master branch because that may have unrelated bugs -- so the advice in the kernel community is to start work from the latest stable version of the kernel (for big features), or to start work from the latest tagged release candidate.

You can also back up your intermediate progress by pushing the feature branch to a server in the meantime, and you can push the feature branch to a server to share it with someone else and elicit feedback, before the feature is complete, before you have to turn code into a permenant feature of the codebase that everybody* in your project has to deal with.

The gitworkflows man page is a good introduction to the workflows that Git is designed for. Also Why Git is Better than X discusses git workflows.

Large, distributed projects


  Why do we need lieutenants in a highly collaborating team with good practices and good design habits?


Because in projects like Linux, there are so many people involved who are so geographically distributed that it's hard to be as highly collaborating as a small team that shares a conference room. (I suspect that for developing a large product like Microsoft Windows that even if people are all located in the same building, the team is just too large to maintain the level of collaboration that makes a centralized VCS work without lieutanants.)
AnswerKenntnis:
Why not use them in tandem? On my current project we are forced to use CVS. However, we also keep local git repositories in order to do feature development. This is the best of both worlds imo because you can try various solutions and keep versions of what you're working on, on your own machine. This allows you to rollback to previous versions of your feature or try several approaches without getting into issues when you mess up your code. Having a central repository then gives you the benefits of having a centralized repository.
AnswerKenntnis:
I have no personal experience with DVCS, but from what I gather from the answers here and some linked documents, the most fundamental difference between DVCS and CVCS is the used working model

DVCS

The working model of DVCS is that you are doing isolated development. You are developing your new feature/bugfix in isolation from all other changes until the moment you decide to release it to the rest of the team. Until that time, you can do whatever check-ins you like, because nobody else is going to be bothered with it.

CVCS

The working model of CVCS (in particular Subversion) is that you are doing collaborative development. You are developing your new feature/bugfix in direct collaboration with all the other team members and all changes are immediately available to all.

Other differences

Other differences between svn and git/hg, such as revisions vs changesets are incidental. It is very well possible to create a DVCS based on revisions (as Subversion has them) or a CVCS based on changesets (as Git/Mercurial have them).

I am not going to recommend any particular tool, because it mostly depends on the working model that you (and your team) are most comfortable with.
Personally, I have no problems with working with a CVCS.


I have no fear of checking in stuff, as I have no problems getting it into a incomplete, but compilable state.
When I experienced merge-hell, it was in situations where it would have occurred in both svn and git/hg. For example, V2 of some software was being maintained by a different team, using a different VCS, while we were developing V3. Occasionally, bugfixes would have to be imported from the V2 VCS to the V3 VCS, which basically meant doing a very large check-in on the V3 VCS (with all bugfixes in a single changeset). I know it was not ideal, but it was a management decision to use different VCS systems.
AnswerKenntnis:
I know how to use SVN, and I'll go on using it for a long time.

I know I'll have to learn a DVCS because everyone seems to use it, but I'll do it only once it's clear which one of the two contenders is the winner, either GIT or MERCURIAL. In this way at least I'll have to learn only one DVCS.
QuestionKenntnis:
What should every programmer know?
qn_description:
Regardless of programming language(s) or operating system(s) used or the environment they develop for, what should every programmer know?

Some background:

I'm interested in becoming the best programmer I can.  As part of this process I'm trying to understand what I don't know and would benefit me a lot if I did.  While there are loads of lists around along the lines of "n things every [insert programming language] developer should know", I have yet to find anything similar which isn't limited to a specific language.

I also expect this information to be of interest and benefit to others.
AnswersKenntnis
AnswerKenntnis:
How to swallow pride and admit mistakes without taking them personally.
AnswerKenntnis:
How to think like a user, and not like a techie geek programmer.
AnswerKenntnis:
When to ask for help, and when NOT to ask for help.
AnswerKenntnis:
How to read other people's code.
AnswerKenntnis:
Familiarity with version control systems. It doesn't have to be every one, but the basic concepts that can be applied to all of them should be known.
AnswerKenntnis:
Here's my 10 bits:


How to be humble. We are all here to learn. You may be smarter than others, but there are a lot of people smarter than you.
How to study/consume information. I don't know about you, but I am forever studying! Books, the Internet, whatever!
What a dictionary is and how to use one, and how to find out acronyms quickly.
What the basic tools of the trade are and what they do (IDE, CVS et al).
Know common terminolgy and what they mean: design patterns, usability, testing (ha!), stack,  etc.
Have an understanding of OOP.
Be "capable" in at least one language, nothing amazing, just know how to identify variables and methods, etc. From here you can learn FAST.
Understand that people ultimately use software and want to make those people happy.
AnswerKenntnis:
Maybe it's too subtle, but I think of it as "knowing which problem to solve." A lot of programmers (and normal people) waste tremendous effort solving things that simply aren't very important; or they create a solution, with a great deal of extra work, that isn't quite what is needed.
AnswerKenntnis:
How to relax. It's the secret to productivity. 

Eventually, willpower and caffeine are not enough. This constant contraction we do is very damaging.

This is a big deal.
AnswerKenntnis:
Basic data type & algorithm theory.   Things like Big O notation, arrays, queues, etc.
AnswerKenntnis:
How to choose the right tool for the right task, and not taking part in silly flaming wars about his favourite programming tools.
AnswerKenntnis:
Well, here's my .02$ :



The learning never stops. No matter how good you think you are, there is always someone better than you, and there is always something you can improve about yourself. If you stop learning, you'll inevitably degrade as a programmer. Read books. Read blogs. Talk to other programmers.
Try to learn several languages. At least one of them object-oriented. Also, you should know something about various technologies related to the language you learn (e.g. If you learn Java, it would be nice if you knew something about Spring, and so on..). 
Refactoring. Sooner or later you're going to need that knowledge.
Learn how to deal with legacy code.
Write unit tests. Learn about TDD.
Learn to work in a team.
Write elegant and readable code. As the old saying goes:"Write your code as if the person that's going to maintain it is a psychotic serial killer who knows where you live."
Learn how to be lazy and disciplined at the same time. Good programmers posess both of these qualities. Weird as it seems, they're not contradictory to one another, but complementary.
AnswerKenntnis:
You can't test quality into a product.
AnswerKenntnis:
Every programmer should understand design patterns.
AnswerKenntnis:
I'm a little late to this one, but I'll go with the knowledge laid down by Edsger Dijkstra:


  Besides a mathematical inclination, an
  exceptionally good mastery of one's
  native tongue is the most vital asset
  of a competent programmer.


If you can't write a good paragraph, chances are you can't write good code either.
AnswerKenntnis:
Don't get too emotionally vested in, attached to, or religious about any given technology, OS, or language - none are perfect - in the long run you're likely to end up wishing you could create your own ala carte from what you like about each different one. 

Think of it like cars - you may not have driven a particular car before, but they all have keys, steering wheels, accelerators, and brakes - you should be able to get in one and quickly drive off once you sort out what's where. Treat OSes and languages similarly and focus on learning the essential concepts underlying them even if you're in the throes of the specifics of any given instance.

And over time try to understand and appreciate the ancestry, heritage, and commonalities of the various technologies which will help you keep perspective. Realize for example that, while the evolutionary tree is actively branching and full of dead-ends, over time technology tends to repeatedly converge around 'best practices' and 'economies of scale' (e.g. notice a Mac isn't all that different than a PC under the hood these days...).

Last, remember no matter how much fun you're having with it all, technology essentially represents an imperfect lens between what your mind can envision and what you actually produce. Do your best, learn to learn, and remain adaptable...
AnswerKenntnis:
How to program in C.
AnswerKenntnis:
That the day you stop learning should be the day you're no longer a programmer.
AnswerKenntnis:
Unit testing and debugging.
AnswerKenntnis:
Regular Expressions

What they're good for
When not to use them
How to write readable regexes
How to debug them



It was mentioned before but I think it deserves it's own answer.
AnswerKenntnis:
No-one wants to use software. They want problems solved.
AnswerKenntnis:
Coffee and IntelliSense are your best friends ever.
AnswerKenntnis:
How to observe a big complicated object and decompose it in small simple objects that still accomplish the same task when put together again.
AnswerKenntnis:
Never trust a user (especially if the app is public!), they will often do everything in their power to break your app one way or another.

Make it future proof & expandable ΓÇô you never know when you want to expand it in a few years time and realise how much effort it would take to re-code badly created code.
AnswerKenntnis:
That the programmer doesn't know everything and should always try to learn new languages/technologies, etc.
AnswerKenntnis:
The basics of good UI design and communication (aka graphic) design.

I see so many apps and projects ruined by bad design or poor usability. Just learning the basics can make a world of difference. Plus the visual problem solving techniques (i.e., how to communicate a concept visually) are a stimulating challenge that should open your eyes to new ways of seeing, which should in turn have an impact on your code.

A recommended book is The Non-Designer's Design Book by Robin Williams 

Here's what Joel Spolsky says of it:


  Wow! Everybody has to do some graphic design, and not every software team has the luxury of professional designers. This excellent, thin book will give you a grasp of the principles behind page layout, fonts, etc. The good news is, you can read it in the bath before the water gets cold, and the next day, your dialog boxes and powerpoints and web pages will start looking better.
AnswerKenntnis:
Every programmer should know how to learn quickly.  A lot of times you come into a job and will be asked to develop in a technology you've never used.  They might give you a week or so to get on your feet (if you're lucky) before you're asked to write production-quality code.
AnswerKenntnis:
Version control. And to quote my girl friend: "I don't just want you to do the dishes, I want you to like it!"
AnswerKenntnis:
Requirements change, your code will have to adapt, and it may or may not be you who has to adapt it.  

There have been several questions here related to topics that are affected by this.
AnswerKenntnis:
Off the top of my head:


Very few programming problems require math beyond addition, subtraction, multiplication, and division. If you're thinking of using calculus to solve a problem, research the alternatives exhaustively before doing so.
Any time you find yourself guessing about how something should work, you're doing it wrong. It's not your job to be telepathic.
The person giving you the spec rarely knows everything he wants until you've hashed it out.
More than half of being a great programmer comes from dealing with human beings. Interacting with your team, managing your manager, and finessing the end user are half of the job.
Good code is written to be read by people as much as it is to be read by your compiler.
Best practice and practical reality will be in conflict more than the programmer thinks, but less than the manager does. When they appear to be in conflict, it's up to you to delineate and understand the conflict and then give in to the practical. The subtle and clever solution is only better than the ugly, brutish one if it's more cost effective in the long run.
Great tools can't make great programmers, but bad tools make us equally awful.
Never look down on a technology, but always look for the best alternative.
The more languages you know, the better you'll be in the one you're using.
Don't be disturbed by the slow creep of programming-oriented thoughts into your daily life. Even when we're not at a computer, we all suffer from bandwidth limitations, have performance penalties from task switching, and need to load things from backup storage. Computers are supposed to mimic human thought and the analogues are everywhere.
AnswerKenntnis:
Reading other peoples' code is not going to spoil your brain, but rather figure out why you would not have done it that way (if better or not is another question).

This gives you programming gedankenexperiment, and occasionally you do find someone implementing something way better! Like in way better.

This answer naturally expands to reading your own code, thus it expands to use version control and DIFF, and thus to 42.
QuestionKenntnis:
How can I deal with the cargo-cult programming attitude?
qn_description:
I have some computer science students in a compulsory introductory programming course who see a programming language as a set of magic spells, which must be cast in order to achieve some effect (instead of seeing it as a flexible medium for expressing their idea of solution).

They tend to copy-paste code from previous, similar-looking assignments without considering the essence of the problem.

Are there some exercises or analogies to make these students more confident that they can, and should, understand the structure and meaning of each piece of code they write?
AnswersKenntnis
AnswerKenntnis:
You could present them a series of exercises, each one building on the previous while adding some extra element or twist to the problem, or investigating the issue from a different perspective, which reveals a weakness of the previous solution, requiring a new, different approach. This forces them to think about, analyse, modify and experiment with each solution, instead of just copy-pasting a ready-made piece of code.

Another possibility - although not strictly a programming task - is to ask them to estimate various things. E.g. how much water flows through the Mississippi delta per second? Such questions don't have a set answer, especially because one needs to make certain assumptions to get to a convincing (range of) value(s). And - although answers to many of these "classic" ones can be googled indeed - you can easily make up new ones which aren't (yet) found anywhere on the net.

Examples to both of these kinds of exercises can be found in e.g. Programming Pearls by Jon Bentley. Also The Pragmatic Programmer has some good challenges.

A third kind of task would be to present them some piece of code with (one or more) bugs in it, which they must find and fix. This again forces them to use their analytical skills and reason about how the program is actually working.

Update

Feedback from a comment by Billy ONeal:


  The problem with the "series of exercises" is that students who have a problem with an earlier exercise are completely screwed for remaining exercises.


You are right, although I feel this is more about the general problem of setting course difficulty to the right level / grouping students of similar skill level together. Moreover, one can arrange the students into smaller groups where they are required to discuss and debate about the problems and solutions, and solve the problems together. If someone doesn't get it, the others can help (this setup would improve teamwork skills too). And if someone tries to be lazy and let the others do all the work, it is surely noticed by the teacher (who is supposed to be walking around, supervising and mentoring the students, not playing WoW on his laptop in the corner ;-)

And one can also adjust the exercises to accommodate students with different skill levels. Beginners can go slower, experienced ones faster.
AnswerKenntnis:
You are battling the students' balancing act of their need to care about your subject and their need to get passing grades.  Many students feel that:

(Get it Wrong || Experiment) == (Failing Grade && Waste Time)

As soon as a student feels that their time or grade is at risk, even for an interesting subject, they stop learning and jump straight to "I don't care, just give the teacher the right answer".  Your students are trying to cut corners (or so they think) by thinking as little as possible about the problem and just hacking away at it by copying and pasting.  

Here are my suggestions on how to deal with this:


Use the Bob Ross method: Prove to them that it is both possible and faster to start anew vs. copying and pasting.  Create new programs before their eyes during class - really show them that programming can be like painting a picture.
Provide assignments that require creativity.  For example, have each student create their own data structures (what are the objects required to create a Zoo, Pet Store, Town, College etc.) on paper to use throughout the course.  Assignment #2 can be converting those structures to classes or objects, etc.  Basically, entice them into thinking abstractly - reward them for being creative, and then reward them for turning their creativity into a computer program.
Use the least amount of syntax possible.  Boilerplate stuff like creating classes and language syntax is so prevalent in the introduction of programming that it often misleads students into thinking that all of programming is just knowing where to put curly braces - they don't realize that what is in the middle of the curly braces is where creativity flows.  Pick a simple language, and provide sample files (such as an empty class file) for the students who still want to copy and paste something.  You can gradually become more strict about syntax and compilable assignments as the course progresses.
AnswerKenntnis:
Several things that come to my mind:


Give them assignments where they actually have to explain the code someone else (you) wrote. Understanding of the previous code or more specifically lack of it is both the largest cause and danger of cargo cult programming. Ask them to use comments, line by line if necessary, to explain your program in plain English (or whichever human language you use).
Only after they've explained the code, ask them to modify it in order to make a certain change. For example, if you gave them a sort function that sorts descending, ask  them to make it sort ascending. Or something more demanding. But make sure it's something that requires understanding of the given code.
You can, if you like, put some easter eggs in code. A line or two that does nothing useful or even related to the problem at all. Give them a hint that such lines exist and grant extra points to those who remove them.
Then and only then you can give them an assignment to write a piece of code from scratch on their own. At this point they should have a much better understanding of what code really is. They might even find it a little easier to do it themselves.


The basic idea is that programming is not just writing code, it's also reading it. Reading code should also be taught.
AnswerKenntnis:
Look at it in another way. This cargo-cult phenomenon is the novice stage of the Dreyfus Model of skill acquisition. This is how we learn. When I first learned to program, all I was doing was typing in pages of code from the back of Compute! magazine. Repetition is key. Babies learn to talk by copying the sounds they hear their parents make. Everything we learn is through imitation. We just have to be taught how to go from imitation to mastery.

The problem you have is your students aren't repeating anything, they're copying it from the Internet. There is some benefit to that but gains are minimal. The act of actually typing out the code is what got me to a place of understanding. I started to see patterns in what I was typing and gained an understanding of what I was doing.

One option is to structure your lab as a code dojo. Have students take turns pairing with each other on the same problem. Pick a problem that takes about 10 to 15 minutes to solve. Repeat that problem over a few labs and introduce a new twist to the problem as the class' proficiency grows. Perhaps start the lab by having the students watch you program the solution, and have them repeat it. Switching pairs with each iteration.

For your tests have a code kata where each student works through the problems from the semester in front of the rest of the class. Focus not only on correctness but form and creativity. I think this would provide a deeper understanding of how to program than giving take home assignments.
AnswerKenntnis:
I have taught introductory classes in the past and as I recall looking back now:

Some students think programming is like that for different reasons. I remember once a good kid that cargo-culted a lot
what I did:

Believing that it was not an isolated issue, but other students in the same class might have similar behaviour or approaching to the problem and not express it, I was always addressing the class.


Some time was spent to explain some stuff such as determinism, which meant for them that in the same environment with the same data and code, they will have the same results (dispell the "randomness"),
Since problem solving is up to what the student's actions and not anything else, the attention should be in solving the problem and not finding the right spell, 
They are in an education environment, so the problems are crafted in order to offer a learning experience, the outcome is to learn how to program (or in some cases like classes for System Administrators, how programs work, which is different) and not to give me a solution. ("World does not need another calculator, it is an exercise"), so their problems could be solved with the materials available (example: notes provided),
I think it is in Code Complete: "Even if you copy and paste, the code is yours". If someone did it, it should not be cargo-style. Every line had to be explained to me (individually) or to another student (same) or to the class.
AnswerKenntnis:
Did your students start at the correct 'abstraction level' at the beginning of the course? E.g. a homework that introduces them to the main programming structures like loops and conditionals without writing a single line of code? 

When I took intro to programming, our first assignment was called 'Rick the Robot'. We had a piece of paper with an aerial map of a city with interesting points, like banks, grocery stores, etc... We had a dude called 'Rick' and had actions like 'take one step',  'look left', 'look right', 'cross the road' and we could use things like 'repeat' and 'if something, then do something'. (This is not 100%, as I could not find this assignment) The idea was that Rick could only use what he was given and he had to get to different places on the map. 

This was a fun exercise and something that introduced you to the basics (which are sometimes the hardest to grasp to newcomers). There is no one good answer to this problem (its a game) and there are no solutions to copy and paste from. Something like this might also allow you to play with their creativity a bit more without intimidating them with code. 

Finally, the idea is that you start with the abstract and move towards the concrete. They cannot copy paste abstract. They have to understand it to solve a problem.
AnswerKenntnis:
What you're asking them to do is demonstrate analysis and synthesis in the cognitive domain of Bloom's Taxonomy, where they're currently only demonstrating application.

Unfortunately, it's sort of a "lead the horse the water" type of situation.  Analysis and synthesis are also very difficult to do when you are still struggling with comprehension.  Without the comprehension in place, analysis and synthesis activities are going to act more as weed outs than learning activities.

My personal opinion is that it's okay not to expect anything more than application in intro to programming classes.  This is the first time students have been exposed to these concepts, so it's like teaching kids to read before asking them to write an essay.  Those higher order skills will follow in their later classes.
AnswerKenntnis:
Have you considered supplying them with some code to start with?  Whatever simple scaffolding the assignment needs, like an empty main function (I don't know what language you're using).  Something that compiles and runs and does nothing.  Then they can start adding their code with some degree of confidence that at least part of it works.

This is actually pretty common in the "real world"; lots of IDEs and other tools create empty projects with typical libraries/templates/configuration files already in place.
AnswerKenntnis:
Change your idea of projects!

In the programming world, rarely do we actually create new projects for every solution that comes around.  Most of the time we modify old ones.

Change your idea of a project from one solution for each assignment to one solution for the entire semester.  Each assignment builds on the previous assignment.

Example

Project: Build an elevator system


Assignment 1: Print out the current floor
Assignment 2: Create the data structures for an elevator class and print out the floor based on the elevator
Assignment 3: Create code that "moves the elevator", printing out the floor.  Accept keyboard input (>enter floor: )
Assignment 4: Handle multiple elevators


The point is that you build on the previous assignment instead of recycling old assignments for a new task.
AnswerKenntnis:
Any sort of cargo-cult mentality (including cargo cults themselves) comes from a lack of fundamental understanding of the technology involved.

Cargo-cult programming shouldn't be thought of as a problematic habit, but rather a symptom of the underlying confusion that the programmer is facing. 

More importantly, the assumption that the student's lack of understanding of is simply the outgrowth of his lack of confidence is fundamentally misguided and does not address the underlying problem.

Instead, the student's copy-paste programming style should be a red flag telling you that this student is overwhelmed by the complexity of what he's expected to do. 

He's instinctively using past work as a scaffold upon which to build his current project, attempting to compose a solution using previously solved problems as building blocks. We all do this to a certain extent, but most of us do this by using the knowledge gained from past work as our building blocks. This student is instead using the work itself, which means he doesn't truly understand the blocks he's working with. He's decomposed the work as far as his understanding permits, and treating large blocks of code as atomic units because he does not understand how they work. He only knows what they do.
AnswerKenntnis:
Consider using a very high-level language that requires a minimum of boilerplate code. 

To me, it's often the boilerplate code in large frameworks or verbose languages that feel like magic spells and hinders comprehension.

I was personally taught ML at my CS introductory programming course. For many years, Lisp was taught as introduction to programming at MIT. Both are excellent choices. Some of the benefits they have are


Interactive interpreter. Very important, as this allows exploration.
Very succinct. No boilerplate at all. It allows the students to concentrate on the ideas they are trying to express.
Relatively obscure and alien (at least compared to Java, C or other mainstream languages that the students may already have some experience with). Yes, I list that as a pro. It levels the playing field for the students, since likely noone will have prior experience. And it makes it less likely that they will be able to just copy-paste solutions to homework form the web.
AnswerKenntnis:
Similar to JoelFans idea is have them do the initial assignments on paper using a pseudo code (language) that you create. You can add the structures as you see fit and they don't worry about the magic box.
AnswerKenntnis:
You could ask them questions about pieces of code that require written responses? Like "What is this code doing?" "Why did the programmer solve it like this?" "Is there a better way?", etc? 

That will actually get them thinking about the problem, which is something they can do without even touching code.
AnswerKenntnis:
Challenge them to create the shortest possible solutions to the
problem.  
Reward the more succinct solutions with an incentive. 
Create exercises that revolve entirely around refactoring code
Have the students trade assignments, and mock grade them for
efficiency and code cleanliness, and use some of the least
efficient ones as examples, on an overhead projector.
AnswerKenntnis:
I did some research into the problems of novice programmers in the 80s. Based on my experience with novice programmers today, not much has changed. Novices don't have a useful mental model of what computers actually do. They resort to magic incantations because the machine itself is magical. 

Programming requires breaking naturally simple tasks into unnaturally small steps. Since novices don't deal with such fine granularity in daily life, it's hard for them to figure out what the small steps should be, especially when it's not clear what small steps the machine makes available. But even if they do manage to figure that out, they are then confronted with the stilted syntax and limited semantics of a programming language (an unnatural language masquerading as a quasi-natural one) that controls the mystery machine by remote control.

Since they can't make the connection between a logical solution to the problem and the functionality of the machine, they focus on satisfying the demands of the language. The first goal is to write something - anything - that compiles. The second is to tweak that program - whatever it actually does - to keep it from crashing. Then, if they have the time, the energy, and the interest, they try to get the program to produce results that resemble what the problem requires. Along the way, they may accidentally produce well-written code.

In all likelihood, novices who do learn to program succeed because they have inferred a useful mental model of the computer, not because they've been intentionally given one and internalized it.
AnswerKenntnis:
I'm going to assume that by 'cargo cult', you mean that they're inserting stuff that think is necessary, but actually does absolutely nothing towards solving the problem.

If that's the case, you can always add some factor into the grading that's based on conciseness -- leaving unnecessary or redundant code in your program is asking for problems in the future, as it may break, or just make it more difficult to maintain.

They'd be judged similarly in an writing exercise in an English class -- no one wants stuff that goes off in a random tangent or is just generally rambling without getting to the point.

When I took an assembly class, the teacher would tell us for each exercise if he wanted us to write the code for speed, size, or memory usage, and he'd mark off if you didn't come close to optimizing what he asked for.

...

If they're copy & pasting code from previous similar assignments, and it's actually something to solve the new problem ... well, that's just code re-use, and unless they made bad assumptions about the suitability for the code for re-use, I think it's perfectly reasonable for them to do. (eg, reading from a file, propting for input ... all modular parts that can be re-used.  If you didn't want them doing it, you need to make the exercises dissimilar.
AnswerKenntnis:
Unfortunately, thats the way a lot of people's brains work.  So go into this understanding that there are people who you cant 'cure' of this.  There are a lot of people who are unable to work at the level of mental precision necessary for programming.  Some people are simply not designed for it.  I'm not saying give up on the students - I'm saying dont assume you are failing if not everyone picks it up.

Without knowing more about the context of the class, I'd say focus more with these problem students on the very basic structures - simple if/thens, loops, etc.  Simple routines to print out odd numbers, every tenth number, etc.  Nothing more than 10 lines of code each.  If they are 'magic thinking' they obviously havent mastered those basics yet.  Have them do a lot of different simple routines until they grasp whats going on.  Someone else mentioned writing the code out on paper - I think that'd also be a great way to do these simple routines.

You also might consider having them learn flow charting.  For some people, being able to see the flow of an algorithm, and then how that connects to code, may be helpful to them connecting the code to the flow.
AnswerKenntnis:
Teach the class using a programming language that is technically good but is so obscure they won't be able to find any existing code to copy.
AnswerKenntnis:
Teach subroutines.  Have them take the code they are grabbing from previous assignments and turn it into a subroutine.  Teach them about function documentation to help them understand what the subroutine is actually doing.
AnswerKenntnis:
Ideally in the first lecture, start them out with something completely abstract: have them (as a group, with you as the leader) write up the instructions on how to go grocery shopping from a list, and break down the high-level instructions progressively until they reach enlightenment.

It helps to tell them that these instructions are going to be followed literally by a robot, who doesn't know how to infer things.  It has to be a very hands-on activity where you are in charge of guiding them, though.
AnswerKenntnis:
Alistair Cockburn talks about the concept of Shu-Ha-Ri and how it applies to programming, http://alistair.cockburn.us/Shu+Ha+Ri. I think it may be important to bear in mind where your students are in this continuum.  First that will help ease some of your frustration. Copy/imitate is a very natural response and accepted mode when you are first starting to learn something.  Second, it may help you get some ideas of how to move forward. For example, you may want to consider choosing a problem that can be solved in multiple ways (loops vs. recursion, console vs. web/gui) then explicitly have them first solve it one way, then another way -- bonus they can learn about legitimate code reuse, componentization, creating reusable libraries, etc.

Another successful way that I've seen used is to have a series of projects that build on one another, making a default, working version available at each step after assignments have been handed in to prevent people from falling behind.  Each step of the process should introduce something new.  I'll grant you that this may be easier to do in a design class than a programming class, but it should still be doable.  One nice thing about this is that you explicitly give them a good (hopefully) implementation to compare against theirs at each step.  Expose this comparison as an assignment, i.e., do a mini-code review of their own code against their effort.  You might want to make this one of several extra credit options.

While I'm typically not big on "comments," you might want to make documentation of the code one of the grade items.  Have them produce a "Theory of Operation" document for each project that describes their approach, how each component fits in, and how they together solve the problem.  Normally, I'd want the code to do much of this on its own, but it would drive them to put their thinking caps on and put it to paper.

Lastly, you might want to get creative and have your students review each other's code and give it an evaluation.  Put peer pressure to work for you. Allow this to become part of the grade or extra credit for the highest rated code (and docs).
AnswerKenntnis:
Just a quick suggestion. Whenever I have a programming problem that needs to be solved or debug I like to look at my code and 'play computer' where In my head I keep track of the variables and their values and what I expect them to be when each line is run. So if I copied some code from somewhere, unless it is complete on it's own and I just need to reference it, I like to go line by line to understand exactly what is going on. Essentially playing computer. VBA Debugger essentially makes this task easier but making your students do it on paper might give them fundamentals like. What does this line actually do?
AnswerKenntnis:
I taught introductory programming at the college level.
It was a bread-and-butter course, all the faculty did it, and I think we did it quite well.
We followed a common text and had common exams, but we each had our own classroom method that worked.
It's been a long time since then, but occasionally I get to tutor some kid in programming, and the whole picture is about the same.

The way I do it is to start at the bottom, as concrete as possible.
What students know is a structure.
They already have a lot of concepts.
I am building further concepts on top of those, and I am pruning off concepts they may form which are counter-productive.
At the same time, I make them learn by doing.

I had built a little computer with an Intel 8008 chip, some EPROM, and a few circuits.
I had programmed it to play a little duet when the I/O chip was connected to a couple speakers.
I would explain how the little program worked, with an inner loop to count down a counter.
That would act as a delay.
Then it would toggle the output bit and do it again.
It would do that for a while, and then switch to another delay, giving another pitch, and so on.
The memory chip had a little timer, and if I tucked a capacitor lead under one of the timer inputs, the program would run veeeeery slowly.
The class could hear the speakers going click, click, click...
I wanted the class to understand that the computer was doing very simple things one step at a time.
Then I would un-hook the capacitor lead, and the "music" would burst forth. (applause)

Then I had built a simulator for a very simple decimal computer, having 1000 memory locations, each holding a signed 4-digit decimal number.
It had very simple opcodes like "add to accumulator", "jump if negative", and so on.
I would have them write little programs in this "machine language", like adding two numbers, or adding up a list of numbers.
Then they could watch it work by single-stepping, or holding down the Enter key to watch it run "fast".

The point of this was to put in place the concept that computers can only do a very small number of different basic operations, and they do them one-at-a-time.
This is to counter the impression they have that computers are complicated, and that they do everything all at the same time, and read your mind in the bargain.

From there we went on to programming in a "real" language (BASIC :), starting with very simple but interesting programs, working up through conditionals, loops, arrays, files, merging, and so on.
The object was to put in place a sufficient skill set so they could take on a project of their own choosing, because that's the only thing that makes programming interesting - the use to which you can put it.
I would throw out some ideas for projects, and then they would take it from there.
I would ask for written ideas, and then progress reports, to keep them from postponing it to the last minute and then panicking.
I think the projects were the best part, because they were learning under their own power.

That initial grounding in a very concrete understanding of what computers do made it much easier to teach concepts later on that would otherwise be real speed-bumps, like arrays or (in a later course) pointers.
We tend to glorify the concept of "abstraction" as this wonderful thing, but it needs to be built on a concrete foundation, not on air.
AnswerKenntnis:
A s  a  s e l f - t a u g h t   programmer I believe animation to be the most challenging in terms of knowing what the code is doing. When a program contains algorithms and mathematical transformations performing abstract manipulations, the only way to understand what the math is doing at any given point (unless you're a genius) requires understanding the execution of code itself. 

Correct me if my naive idea is incorrect. What you want to do is not prevent your students from using "design patterns", but to find a way to ensure they understand what they are CnP's? Then challenge your students to manipulate an animation. In order to tweak the output in an animation its necessary understand what is happening at each step. For your stated concern, I imagine a well conceived animation project will manifest in obvious ways when a student "gets it"--when they have realized a transformation you didn't expect or tweaked certain related, interdependant variables.

Without knowing the pedagogical limits and goals you are working under, I can't say animation is the complete answer. A whole curriculum of animations outside of animation profession is, I should hazard to guess, out of the question. A few projects may none the less result in something artful and wonderful, which isn't bad. 

On another note, I read a newspaper article (yes, paper!) about a high school-level Coding Olympics--wot-wot--competition for pre-college programmers. The description of their challenges was the clearest articulation of pure coding I can recall having read. The competitors are judged against each other and by good practice standards. For these competitions students must both plan their solution and hand code the elemental "design pattern" which the problem requires to finish within time limits. Thus, the solution to your concern regarding CnP programming is to test if the students can write the same "code chunks" they are CnP'n!

I'm sure it was in the NY Times. A quick search didn't find it. A similar example is ACM's International Collegiate Programming Contest. This contest emphasizes speedy programming: "Lightning-quick programming in team competition is a decidedly quirky skill, not exactly one many job seekers would place at the top of a r├⌐sum├⌐." Thus I would recommend abstraction from real-world problems is the answer.

Also,

HP Code Wars
AnswerKenntnis:
I doubt that this behavior is because of a belief that programs are magic spells - more likely it's laziness and lack of motivation.

So I think your job as a teacher is to motivate your students - no student who's genuinely motivated will cut and paste a solution (that's only for working programmers with deadlines and bottom-lines to meet...)
AnswerKenntnis:
You could also treat them the hard way. 

Find a way to make a copy-paste harmful to them. I don't have precise example but if you craft a first exercise whose solution, if pasted in a similar looking second exercise, bring the cargo-cult students in a very long and painful "unstable instability" or "silent data corruption" bug. Meanwhile a "non cargo-cult" 2 mn thinking would have brought an obvious solution to even the worst student (hadn't he seen the first exercise solution). Then, maybe there is some possibility they could learn the lesson, and think twice before copy pasting code into the third exercise.
AnswerKenntnis:
Never give them similarly sounding assignments.

Or, more crazy, learn them TDD from the beginning. It pushes for writing (not copying, writing) a lot of code (namely tests) that actually helps to formulate the problem that is being solved.
AnswerKenntnis:
Make them do the assignment in front of you in the classroom with no Internet access provided (have the school cut it off, allow no phone use as well during class). At least do this for tests. There is no reason at all that they should be using the Internet for basic programming expercises. The book should be a sufficient resource for intro exercises. Allow Internet usage once you are in an advanced class and they already have learned how to think.
AnswerKenntnis:
Something I have found to be very helpful for the people in my class is writing a small project on their, on a subject they can choose themselves.

When I started programming, it was hard for me as well, and I did copy a lot in class. Then at home, I started making little games, as I want to become a games programmer, and I found them to be much easier to make. Even though they were much harder then the stuff we saw in class. Just because it interested me.

A few other people in my class went from 40-50% on their exams to 90-100%, because they did the exact same thing.
AnswerKenntnis:
1) Teach them the history of computers, programming.
2) Teach how different programming languages work.
3) Teach what a good programming and programming practices are.
4) Teach them how to analyze a problem, find a solution and then start coding.
QuestionKenntnis:
What does SVN do better than Git?
qn_description:
No question that the majority of debates over programmer tools distill to either personal choice (by the user) or design emphasis, that is, optimizing design according to particular uses cases (by the tool builder). Text editors are probably the most prominent example--a coder who works on Windows at work and codes in Haskell on the Mac at home, values cross-platform and compiler integration and so chooses Emacs over TextMate, etc.

It's less common that a newly introduced technology is genuinely, demonstrably superior to the extant options.

Is this in fact the case with version-control systems (VCS), in particular, centralized VCS (CVS and SVN) versus distributed VCS (Git and Mercurial)?

I used SVN for about five years, and SVN is currently used where I work. A little less than three years ago, I switched to Git (and GitHub) for all of my personal projects.

I can think of a number of advantages of Git over Subversion (and which for the most part abstract to advantages of distributed over centralized VCS), but I cannot think of one contra example--some task (that's relevant and arises in a programmers usual workflow) that Subversion does better than Git.

The only conclusion I have drawn from this is that I don't have any data--not that Git is better, etc.

My guess is that such counter-examples exist, hence this question.
AnswersKenntnis
AnswerKenntnis:
Subversion is a central repository

While many people will want to have distributed repositories for the obvious benefits of speed and multiple copies, there are situations where a central repository is more desirable. For example, if you've got some critical piece of code that you don't want anyone to access, you'd probably not want to put it under Git. Many corporations want to keep their code centralized, and (I guess) all (serious) government projects are under central repositories.

Subversion is conventional wisdom

This is to say that many people (especially managers and bosses) have the usual way to number the versions and seeing the development as a "single line" along time hardcoded into their brain. No offense, but Git's liberality is not easy to swallow. The first chapter of any Git book tells you to blank out all the conventional ideals from your mind and start anew.

Subversion does it one way, and nothing else

SVN is a version control system. It has one way to do its job and everybody does it the same way. Period. This makes it easy to transition to/from SVN from/to other centralized VCS. Git is NOT even a pure VCS -- it's a file-system, has many topologies for how to set up repositories in different situations -- and there isn't any standard. That makes it harder to choose one.

Other advantages are:


SVN supports empty directories
SVN has better Windows support
SVN can check out/clone a sub-tree
SVN supports exclusive access control svn lock which is useful for hard-to-merge files
SVN supports binary files and large files more easily (and doesn't require copying old versions everywhere).
Adding a commit involves considerably fewer steps since there isn't any pull/push and your local changes are always implicitly rebased on svn update.
AnswerKenntnis:
One benefit of Subversion over Git can be that Subversion allows checking out sub-trees only. With Git the whole repository is a unit, you can get only all or nothing. With modular code, this can be quite nice compared to Git submodules. (While Git submodules have their place, too, obviously.)

And a small tiny benefit: Subversion can track empty directories. Git tracks file contents, so a directory without any file won't show up.
AnswerKenntnis:
I can think of three. First, it is quite a bit easier to grok, especially for non developers. Conceptually it is much simpler than DCVS options. Second is maturity, especially TortoiseSVN on Windows. TortoiseHg is catching up fast though. Third, the nature of the beast--just an image of a filesystem where you can check things out from any level of the tree--can come in pretty handy in some scenarios -- like versioning a variety of widely different configuration files across dozens of servers without dozens of repositories. 

This isn't to say we aren't moving all development over to Mercurial.
AnswerKenntnis:
SVN repositories are more manageable from a manager's and administrator's point of view (ACL + authentication methods + hotcopy + mirrors + dumps)
SVN *-hooks are easier to implement and support
svn:external has more power and flexibility than submodules
Filesystem-based repository trees are easier to use than monolithic repositories with logical-only separation
SVN has more different client-tools
SVN has third-party usable web-frontends, much better than Git's
SVN doesn't break your brain
AnswerKenntnis:
I wrote this as a comment on somebody else's answer, but I guess it deserves to be an answer in and of itself.

My company's carefully-chosen configuration for SVN requires file-level locking to edit content, and never ever ever uses merge. As a result, multiple developers contend for locks, and it can be a major bottleneck, and there's never ever any chance of a merge conflict.

SVN definitely does top-down managerial control better than Git. In my skunkworks migration to Git (asking forgiveness rather than permission) I've terrified my manager many times with the notion that there isn't any software-enforced central master controlling server to which we are all slaves.
AnswerKenntnis:
This is all relative, and like maple_shaft said, if one works for you, don't change!

But if you really want something, I'd say it is maybe better for designers, web programmers and such - it handles images and binary files a bit better.
AnswerKenntnis:
My reasons:


maturity - both the server, and the tools (e.g. TortoiseSVN)
simplicity - less steps involved, a commit is a commit. DVCS such as Git and Mercurial involve commit, then push.
binary handling - SVN handles binary executables and images better than Git/Hg. Especially useful with .NET projects since I like to check build-related tools into source control.
numbering - SVN has a readable commit numbering scheme, using just digits - easier to track revision numbers. Git and Hg do this quite differently.
AnswerKenntnis:
I appreciate that you are looking for good information, but this kind of question just invites, "I think Git is so much win, and svn teh suxorz!" answers.

I tried and personally found Git a little too much for my small team. It seemed like it would be great for a large distributed team or group of teams that are geographically dispersed. I understand greatly the benefits of Git, but source control in my opinion isn't something that keeps me up at night.

I am a deer hunter, and when me and my friends go out they are armed to the teeth, bristled with ammo and high tech gear. I show up with a single rifle, seven bullets and a buck knife.  If I need more than seven rounds then I am doing something wrong, and I do as well as anybody else.

The point I am trying to make is that if you are a small team working on a medium to small project and you are already familiar with SVN then use it. It is certainly better than CVS or (shudder) SourceSafe, and it doesn't take me more than five minutes to set up a repository. Git can sometimes be overkill.
AnswerKenntnis:
When you donΓÇÖt need to branch and merge, SVN (with TortoiseSVN on Windows) is very easy to understand and use. Git is overly complex for the simple cases, as it tries to making branching/merging easy.

(A lot of small projects never have to branch if they are managed well. Git is aimed at the complex cases; therefore most Git documentation assumes you need to do complex operations like branching and merging.)
AnswerKenntnis:
Usability. It's not really Subversion's merit but rather TortoiseSVN's.. TortoiseGit exists, but it still has a long way to go to match TortoiseSVN.

If you asked what Git does better than SVN I would reply GitHub. It's funny how third-party tools make such a huge difference.
AnswerKenntnis:
Well, my grandpa could use SVN on his Windows XP computer, but he'd have trouble using Git. Maybe this is more an accomplishment of TortoiseSVN over TortoiseGit, but I think it's rather tied to the fact, that Git is inherently more powerful and thus more complex, and it would be pointless to dumb it down to the same level.

Now it doesn't really matter for my grandpa, because he hasn't done any programming as of late. However, I was once on a team, where our graphic artists were using our source control as well.  

And those are people who simply expect their tools to work (so do I, but I have a realistic chance to get them to work in case of failure) and be intuitive. Apart from the fact, that it would had been quite an effort to get them to get along with a DVCS, there was little to be gained. And with only two programmers in the team, it made sense for us to also just stick with SVN.
AnswerKenntnis:
At work I couldn't switch the developers from SVN to any DVCS for two reasons:


Partial checkouts (like only three folders from different depths in the project tree)
File locking (binary format reports)
AnswerKenntnis:
Other people have posted some pretty good answers, but what about Permissions?  Using Subversion over SSH, you can make a separate account on your server for SVN.  Different developers can be given different access to different parts of the repository.  Can GIT do that?  I guess there is gitolite, but it just doesn't seem as flexible or robust to me, nor do I know anyone who is actually using it.
AnswerKenntnis:
I post this as a response rather than a comment.

I admit than DVCS are quite in the trend right now, but I'll try to tell why.

DVCS are better because just like lots of people have been saying, "this is the way we should have been working from the beginning". It's true, you can do with a DVCS what you used to with SVN, so in a way that makes SVN obsolete.

However, not every software project is as good as it gets: 


Long term project have a lot benefited from DVCS, because it uses less overhead, allows much better management (branching etc), and is greatly supported by hosts like google code and github.
Those projects are not the only ones, there are other kind of projects that are being developed in companies without any assistance from the outside world or internet: everything is done internally, and often on a short term. A good example: a video game. Code evolves rapidly.


For the latter case, developers don't really need branching or sophisticated features DVCS can offer, they just want to share source code and assets. The code they make is not likely to be reused, because there is a deadline. They rely on SVN rather than a DVCS because of several reasons:


Developers have machines that belong to the company, and this might change rapidly. Configuring a repo is a loss of time.
If those guys don't have their own machine, they are less likely to be working on the same source code/part of the project. Plus one for the centralized data.
network speeds and big money allows the use of a centralized server that deal with everything SVN, it's bad practice but backups are made etc.
SVN is just simpler to use, even if it's the wrong way: synchronizing files on peers without redundancy is not a simple problem, and "do it the right way" just cannot make it in time if you always want it to be perfect.


Think about the game industry machine and how SVN is a time saver; people communicates much more on those projects because games are about repetitive programming and adaptive code: there is nothing hard to code, but it has to be done the right way, ASAP. Programmers are hired, they code their part, compile it, test it a little, commit, done, game testers will deal with the rest.

DVCS are made for the internet and for very complex project. SVN are for small, short term, tight team projects. You don't need to learn a lot from SVN, it's almost a FTP with a dumb diff.
AnswerKenntnis:
Speed. Sometimes takes 10 or 15 minutes to clone a big git repository, while a similar sized subversion repository takes a couple of minute to check out.
AnswerKenntnis:
Sense of security when doing an 'svn commit'. Since commits go to the server, there's no possible screwup that I can do that will erase my changes after they are committed. In git, commits are local, so until I push, a stray 'rm -rf' and it's all over.
Commits are authenticated. By default in git I can tell it I'm committing as anyone.
Fewer commands to learn for everyday use. 'svn checkout', 'svn up', and 'svn commit' will get you pretty far.
Shared checkouts work a lot nicer. When you go to commit, it asks you for your username/password, you don't have to remember to pass certain command line arguments. Sometimes at work we have a shared machine with a shared svn checkout of some project. Someone might say "Bob can you look at this on this machine" and he can, and he can commit his changes as his user without any screw ups.
Repository layout. You can have an umbrella project and subprojects and choose what to check out when.
Revision numbers are incrementing integers. 
Designed for the central repository workflow.
AnswerKenntnis:
Subversion and Git both encourage particular (and very different) approaches to development and collaboration. Some organizations will get more out of Git, and others will get more out of Subversion, depending on their organization and culture.

Git and Mercurial are both excellent for distributed and loosely organized teams of highly competent professional programmers. Both of these popular DVCS tools encourage small repositories, with reuse between developers taking place via published libraries with (relatively) stable interfaces.

Subversion, on the other hand, encourages a more centralized and tightly coupled management structure, with more communication between developers and a greater degree of organizational control over day-to-day development activities. Within these more compactly organized teams, reuse between developers tends to take place via unpublished libraries with (relatively) unstable interfaces. TortoiseSVN also permits Subversion to support multidisciplinary teams with members who are not professional programmers (e.g. systems engineers, algorithms engineers or other subject-area specialists).

If your team is distributed, with members either working from home or from many different international sites, or if they prefer to work alone and in silence, with little face-to-face communication, then a DVCS like Git or Mercurial will be a good cultural fit.

If, on the other hand, your team is located on a single site, with an active "team" approach to development, and lots of face-to-face communication, with a "buzz" in the air, then SVN may be a better cultural fit, particularly if you have lots of cross-disciplinary teams.

Of course, it is possible to configure Git and Hg (powerful and flexible as they are) to do pretty much whatever you want, but it is definitely more work, and they are definitely harder to use, particularly for those members of the team who would not naturally be inclined to use any form of version control whatsoever.

Finally, I also find that sharing functionality using "hot" library development under Svn (with CI & a test-driven approach) permits a pace of coordinated development that is difficult to achieve with a more distributed and loosely-coupled team.
AnswerKenntnis:
There are two major trends in version control right now; Distribution, and Integration.

Git is great at decentralization.

SVN has lots of tools built that integrate with it. It's common to set up your SVN server so that when you check in a fix, you mention the bug number in the checkin comment, and it automatically sets that but to an "in testing" state, and alerts the assigned tester that they need to look at it. Then you can tag a release and get a list of all the bugs fixed in this release.

The tools that do this with SVN are mature, and used every day.
AnswerKenntnis:
Below are the two reasons that I still stay with SVN.


The learning curve. SVN is easier than Git, even the setup (server or client too), usability, and commands.
Better server packages like Subversion Edge, VisualSVN, uberSVN etc.
AnswerKenntnis:
In Subversion, you can edit log messages (also called "commit messages") after the commit is done and pushed.  For most practical uses, this is impossible by design in decentralized systems, including git.  Of course, in git you can do "git commit --amend", but that doesn't help you after your commit has been pushed elsewhere.  In SVN, when you edit the log message, you're doing it on the central repository that everyone shares, so everyone will get the edit, and without the revision's identifier changing.

Editing log messages after the fact is often very useful.  Aside from just fixing a mistake or an omission in a log message, it enables you to do things like update a log message to refer to a future commit (such as a revision that fixes a bug or completes the work introduced in the current revision).

There need be no danger of losing information, by the way.  The pre-revprop-change and post-revprop-change hooks can save a history of the old messages if you're really concerned, or send out emails with the log message diff (this is more common, actually), so that there's an audit trail.
QuestionKenntnis:
Torvalds' quote about good programmer
qn_description:
Accidentally I've stumbled upon the following quote by Linus Torvalds:


  "Bad programmers worry about the code. Good programmers worry about
  data structures and their relationships."


I've thought about it for the last few days and I'm still confused (which is probably not a good sign), hence I wanted to discuss the following:


What interpretation of this possible/makes sense?
What can be applied/learned from it?
AnswersKenntnis
AnswerKenntnis:
It might help to consider what Torvalds said right before that:


  git actually has a simple design, with stable and reasonably well-documented data structures. In fact, I'm a huge proponent of designing your code around the data, rather than the other way around, and I think it's one of the reasons git has been fairly successful [ΓÇª] I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important.


What he is saying is that good data structures make the code very easy to design and maintain, whereas the best code can't make up for poor data structures.

If you're wondering about the git example, a lot of version control systems change their data format relatively regularly in order to support new features.  When you upgrade to get the new feature, you often have to run some sort of tool to convert the database as well.

For example, when DVCS first became popular, a lot of people couldn't figure out what about the distributed model made merges so much cleaner than centralized version control.  The answer is absolutely nothing, except distributed data structures had to be much better in order to have a hope of working at all.  I believe centralized merge algorithms have since caught up, but it took quite a long time because their old data structures limited the kinds of algorithms they could use, and the new data structures broke a lot of existing code.

In contrast, despite an explosion of features in git, its underlying data structures have barely changed at all.  Worry about the data structures first, and your code will naturally be cleaner.
AnswerKenntnis:
Algorithms + Data Structures = Programs

Code is just the way to express the algorithms and the data structures.
AnswerKenntnis:
This quote is very familiar to one of the rules in "The Art of Unix Programming" which is Torvalds' forte being the creator of Linux.  The book is located online here

From the book is the following quote that expounds on what Torvalds is saying.


  Rule of Representation: Fold knowledge into data so program logic can be stupid and robust.
  
  Even the simplest procedural logic is hard for humans to verify, but quite complex data structures are fairly easy to model and reason about. To see this, compare the expressiveness and explanatory power of a diagram of (say) a fifty-node pointer tree with a flowchart of a fifty-line program. Or, compare an array initializer expressing a conversion table with an equivalent switch statement. The difference in transparency and clarity is dramatic. See Rob Pike's Rule 5.
  
  Data is more tractable than program logic. It follows that where you see a choice between complexity in data structures and complexity in code, choose the former. More: in evolving a design, you should actively seek ways to shift complexity from code to data.
  
  The Unix community did not originate this insight, but a lot of Unix code displays its influence. The C language's facility at manipulating pointers, in particular, has encouraged the use of dynamically-modified reference structures at all levels of coding from the kernel upward. Simple pointer chases in such structures frequently do duties that implementations in other languages would instead have to embody in more elaborate procedures.
AnswerKenntnis:
Code is easy, it's the logic behind the code that is complex. 

If you are worrying about code that means you don't yet get that basics and are likely lost on the complex (ie data structures and their relationships).
AnswerKenntnis:
To expand on Morons' answer a bit, the idea is that understanding the particulars of the code (syntax, and to a lesser extent, structure/layout) is easy enough that we build tools that can do it.  Compilers can understand all that needs to be known about code in order to turn it into a functioning program/library.  But a compiler can't actually solve the problems that programmers do.

You could take the argument one step further and say "but we do have programs that generate code", but the code it generates is based on some sort of input that is almost always hand-constructed.

So, whatever route you take to get to code:  be it via some sort of configuration or other input that then produces code via a tool or if you're writing it from scratch, it's not the code that matters.  It's the critical thinking of all the pieces that are required to get to that code which matter.  In Linus' world that's largely data structures and relationships, though in other domains it may be other pieces.  But in this context, Linus is just saying "I don't care if you can write code, I care that you can understand the things that will solve the problems I'm dealing with".
AnswerKenntnis:
Linus means this:


  Show me your flowcharts [code], and conceal your tables [schema], and
  I shall continue to be mystified; show me your tables [schema] and I
  won't usually need your flowcharts [code]: they'll be obvious.


-- Fred Brooks, "The Mythical Man Month", ch 9.
AnswerKenntnis:
I think he's saying that the overall high-level design (data-structures and their relationships) is much more important than the implementation details (code).  I think he values programmers who can design a system over those who can only focus on details of a system.

Both are important, but I would agree that it's generally much better to get the big picture and have issues with the details than the other way around.  This is closely related to what I was trying to express about breaking up big functions into little ones.
AnswerKenntnis:
Well, I can't entirely agree, because you have to worry about all of it. And for that matter, one of the things I love about programming is the switches through different levels of abstraction and size that jump quickly from thinking about nanoseconds to thinking about months, and back again.

However, the higher things are more important.

If I've a flaw in a couple of lines of problems that causes incorrect behaviour, it probably isn't too hard to fix. If it's causing it to under-perform, it probably doesn't even matter.

If I've a flaw in the choice of data structure in a sub-system, that causes incorrect behaviour, it's a much bigger problem and harder to fix. If it's causing it to under-perform, it could be quite serious or if bearable, still appreciably less good than a rival approach.

If I've a flaw in the relationship between the most important data structures in an application, that causes incorrect behaviour, I've a massive re-design in front of me. If it's causing it to under-perform, it might be so bad that it would almost be better if it it was behaving wrong.

And it'll be what makes finding those lower-level problems difficult (fixing low-level bugs is normally easy, it's finding them that can be hard).

The low-level stuff is important, and its remaining importance is often seriously understated, but it does pale compared to the big stuff.
AnswerKenntnis:
Someone who knows code sees the "trees." But someone who understands data structures sees the "forest." Therefore a good programmer will focus more on data structures than on code.
AnswerKenntnis:
Knowing how the data will flow is all important. Knowing flow requires that you design good data structures. 

If you go back twenty years, this was one of the big selling points for the object oriented approach using either SmallTalk, C++, or Java. The big pitch -- at least with C++ because that's what I learned first -- was design the class and the methods, and then everything else would fall into place.  

Linus undoubtedly was talking in broader terms, but poorly designed data structures often require extra rework of code, which can also lead to other problems.
AnswerKenntnis:
What can be applied/learned from it?


If I may, my experience in the last few weeks. The preceding discussions clarified the answer to my question: "what did I learn?"

I rewrote some code and reflecting upon the results I kept seeing & saying "structure, structure..." is why there was such dramatic difference. Now I see that it was Data structure that made all the difference. And I do mean all.


Upon testing my original delivery, the business analyst told me it was not working. We said "add 30 days" but what we meant was "add a month" (the day in the resulting date doesn't change). Add discrete years, months, days; not 540 days for 18 months for example.
The fix: in the data structure replace a single integer with a class containing multiple integers, change to it's construction was limited to one method. Change the actual date arithmetic statements - all 2 of them.


The Payoff


The new implementation had more functionality but the algorithm code was shorter and clearly simpler.


In Fixing the code behavior/results:


I changed data structure, not algorithm.
NO control logic was touched anywhere in code. 
No API was changed. 
The data structure factory class did not change at all.
AnswerKenntnis:
I like to imagine a very clever team of librarians in a beautifully made library with a million random and brilliant books, it would be quite a folly.
AnswerKenntnis:
Can't agree more with Linus. Focusing on the data helps greatly distill a simple and flexible solution to a given problem. Git itself is a proving example -- giving so many features supported in the years of development, the core data structure largely remain unchanged. That's magic!  --2c
AnswerKenntnis:
I've seen this is numerous areas.

Think about business analysis...   Let's say you're analyzing the best way to support Marketing at a consumer products company like Colgate.  If you start with fancy windows, or the latest technology, you won't help the business nearly as much as if you think through the data needs of the business first, and then worry about presentation later.  The data model outlasts the presentation software.

Consider doing a webpage.  It's much better to think about what you want to show (the HTML) first, and worry about style (CSS) and scripting (pick your tool) after.

This isn't to say coding isn't important too.  You need programming skills to get what you need in the end.  It's that data is the foundation.  A poor data model reflects either an overly complex or unthought business model.
AnswerKenntnis:
I find myself writing new functions and updating existing ones a lot more often than having to add new columns or tables to my database schema.
This is probably true for all well designed systems. If you need to change your schema every time you need to change your code, its a clear sign you are a very bad developer.

quality of code indicator = [code changes] / [database schema changes]

"Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I wonΓÇÖt usually need your flowcharts; theyΓÇÖll be obvious." (Fred Brooks)
AnswerKenntnis:
It seems like this idea has various interpretations in the various types of programming. It holds true for systems development and also holds true for enterprise development. For example, one could argue that the sharp shift in focus toward the domain in domain-driven design is much like the focus on data structures and relationships.
AnswerKenntnis:
Here's my interpretation of it:  You use code to create data structures, so the focus should be on the latter.  It's like building a bridge - you should set out to design a solid structure rather than one that looks appealing.  It just so happens that well written data structures and bridges alike look good as a result of their efficient designs.
QuestionKenntnis:
Will high reputation in Stack Overflow help to get a good job?
qn_description:
In a post, Joel Spolsky mentioned that 5 digit Stack Overflow reputation can help you to earn a job paying $100k+. How much of that is real?

Would anyone like to share their success in getting a high paid job by virtue of their reputations on Stack Exchange sites?

I read somewhere that, a person got an interview offer from Google because a recruiter found his Stack Overflow reputation to be impressive. Anyone else with similar stories?
AnswersKenntnis
AnswerKenntnis:
No


  the real answer: spend a few months earning a five digit Stack Overflow reputation, and you'll be getting job offers in the $100K+ range without an interview.


There is no reason why a high reputation (or "score") on any site will get you a job at all. I have pointed this out before, you are more likely to get a job by maintaining open source projects, writing proficiently, leaving good impressions, and making personal connections within the community.

Are these people good programers? Undoubtedly yes! Does that mean they are a good fit for your team? Absolutely not. Calling these people "superstars" may be completely correct, but that doesn't make them perfect. 1

What determines if you are a good fit? Interviews and connections. You can't replace meeting people face to face with a number.



Having a high reputation can't hurt, but it isn't a magic bullet.



1: In no way do I mean to imply these people are bad programmers, I mean to emphasize the inability to instantly and wholly judge someone based on a number.
AnswerKenntnis:
Yes

Come on, seriously: look at the first page or two of Stack Overflow users. Pick anyone at random. Look at three or four of the highly voted answers they wrote. If you've ever hired a programmer in your life, it's obvious those people are all some of the best programmers you could ever hire.

Then keep going deeper and deeper. Scroll to page 5. Edit the URL and go right to page 100 where they have reputations in the 3000 range. Look at everyone. With the very rare exception of someone who got a lot of points for a silly answer, these are all obvious superstar programmers... the kind that most teams would kill for.

Will this actually get them jobs? Ultimately, we're betting it will. You'll have to find a company where there are actual technical people making hiring decisions, not resume-reading-monkey-recruiters. And you may have to find a city to live in where there are a lot of good programming jobs: if you really, really don't want to leave Roswell, New Mexico, you'll be limited to a vanishingly small number of telecommuting jobs. But if you're in the top, say, 5000 Stack Overflow users, and you can work in Silicon Valley, New York, Seattle, Boston, Austin, Research Triangle, London, Hyderabad, Bangalore, Sydney, Beijing, or Tel Aviv, I would be extremely surprised if you aren't in high demand and earning in the top 10 percentile.
AnswerKenntnis:
It's not a case of direct causation. That is, having a high reputation does not lead to a high-paying job. Being intelligent, articulate, passionate and knowledgable lead to a high-paying job. It's just that those qualities also lead to a (generally) higher reputation on Stack Overflow as well. So you shouldn't be aiming for a high reputation score. You should be aiming to be intelligent, articulate, passionate and knowledgeable.

A higher reputation will just be a bonus.

Edit: I just found the post where Joel says that. He's saying he would hire someone with a 5 digit reputation on Stack Overflow, not that you'd be able to get any $100k+ job with a 5 digit reputation. Also, he's not saying that he wouldn't look at the actual content of your profile, so if you've just posted hundreds of junk questions in the hopes of artificially raising your score, it's not going to help you. Basically, he's using your reputation score as an indication of those qualities I mentioned above.
AnswerKenntnis:
Gaming the System?

Someone should do a study of whether having high rep on SO begets higher rep on SO. That is, once someone reaches critical mass of, say, over 20K rep, do their answers tend to attract more upvotes than other equally valid answers? Anecdotally speaking, I have noticed that low-ranked members will garner 1 or 2 points for answers that will garner higher-rep people (often answering later) 5 or 10 points. And I'm talking about answers that are qualitatively no different. Success begets success.
AnswerKenntnis:
I have a 5 digit SO reputation, and my salary is barely half of what you mention.

Obviously a good SO reputation doesn't guarantee anything, but even though most recruiters don't even know what SO is, surely it's worth mentioning it in one's CV. It's substantial, after all, unlike all kinds of official-looking certifications from two-day courses etc.

On the other hand: while Joel has a point, your activity on Q/A sites isn't really supposed to define your salary in any way. It may be a proof of some proficiency and enthusiasm, but the recruiting company is interested in what you're going to do for them if they recruit you, not in your abilities to answer programming questions. So by all means, consider SO reputation as a part of your professional credibility, but don't expect it to be something huge.
AnswerKenntnis:
There are a couple of factors:


The type of personality that does well on social web sites (let's be bluntly rude and generalise that as the "pontificating, likes to hear self talk, theoretically-obsessed, anorak type") is by absolutely no means necessarily the type of personality who does well leading and/or working with a group of human beings (let's be bluntly rude and generalise that as the "smooching, glad-handling, compromising, deal-maker type").


As a broad rule, businesses are looking for the latter, not the former.

This is both very sad (I mean sad for us pontificators) and very true.

An interesting point: think of your actual BOSS, the person who has to handle all your problems all day, make you feel at least OK about yourself, balance everyone's renumeration, try to keep enough money coming in to your department so that all of you can feed your children while you fondle memory algorithms, deal with the chumps upstairs in management so you don't have to, and so on.  Would you really give a flying crap if that poor person has any connection, whatsoever, to some chat website? Of course the answer is no, you just want your boss to be good at to pulling off all that juggling.


Talk about "superstars" (let's be bluntly rude again) is a little star-struck. You're a superstar if you're so clever you've made millions (or at least the odd hundred thousand) because you're so clever. Oh, by the way, to do, that as well as being so clever, you have to as an incidental matter be utterly A+ rated at memory management and tricky algorithms.  If you're looking at only the latter incidental part, i.e., being utterly A+ rated at memory management and tricky algorithms, then sadly --- horribly ---- unfaily ---- bitterly ---- how can the world be so miserable? ---- you're "just" someone who's utterly A+ rated at memory management and tricky algorithms.


(OK, for all this bitterness and sadness, there is the exception of the extremely small number of purely extremely technical, really research, type ways to make money - in those cases sure, the only thing that matters is your various research breakthroughs, perhaps as a humorous side issue your amazing score on grouptheoryoverflow, and so on.  But there are so few such jobs it's probably not really in the spirit of the question "getting a job in the real world." And - bitterly - they don't pay that much.)


The question further is emphasising "high paying" jobs.  (We'll ignore the issue of whether $100,000 p/a is high paying or not, since Bernanke's hyperinflation makes it hard to keep up with such numbers.) It's probably sadly even more true that the kind of realistic bitterness outlined in this answer, is more and more and more true if you are talking about not merely "a job" (where you are expected to do something specific that you will be told to do, provide a cog in the process), but A Real Job (where you are expected to positively generate massive amounts of money for the company somehow or other as a matter of course, day in and day out) 


It's hard to keep up with current salaries everywhere; it could be that "$100,000 !" fits in to the "rare purely proufoundly technical" job I mention in the aside just above.

However, the spirit of the question in some sense seems to be - to paraphrase and simplify - do top jobs have anything much to do with specifically purely technical, mathematical, expertise?

Sadly the answer there is pretty much definitively no.

It's more the "smooching, glad-handling, compromising, deal-maker type" - or, if you prefer to be bitter in another direction - the "big picture, ideas driven and generating and follow-through type" - that can - simply - generate enough money that some business can afford to give the person that much money.

Don't forget - to say you want to "earn" some fabulous salary, what you're essentially saying is you can generate jobs.  How so? If you're making 200k somewhere, you're a horse that is carrying a number (10? 25?) normal "do a job" employees on your back. You have to be a producer.

It is, sadly, difficult to see how some essentially test of excellence of technical knowledge (SO chat web site) can help with that.

Steve Jobs said it all - good engineers (he said) are not merely worth 10 or 20 ordinary engineers, they are worth some 100s of ordinary engineers ... they can generate that much product, stuff, free monthly cashflow, for the company.  I think that's more what Jobs would think of as one of his superstars. It's difficult to see how purely the technical aspect (a necessity) can bring that.

If that's the sense of the question in terms of "big jobs," then that's probably the answer.
AnswerKenntnis:
I personally turned a sizable Stack Overflow rep and a higher (relative to the site) Hacker News reputation into a job, though I do not make $100K/year (which as a number matters more depending on the location of the job). SO and HN gave me a 'body of work' that showed off that I was passionate about my work, able to relate to others, demonstrate some level of technical competence, understand people's problems and clearly communicate solutions. And since the job is as a Developer Evangelist, these are all highly-desirable skills for that position.

I tweeted the other day that the perfect resume would be your Stack Overflow profile and your GitHub profile plus some other social interaction history like Hacker News or Twitter.

P.S. If this sounds like you, we're hiring.
AnswerKenntnis:
Having something to point at is certainly a good way to demonstrate a general competency in an area - that it is something you actually know about (and can articulate helpfully), rather than just some blank labels copied onto your CV - "XML, Ajax, JSON" isn't nearly as convincing as something someone can look at. It also opens up various potential discussion topics, etc., so having a findable (typically via google against your name) public persona is certainly never a bad thing. Unless the only thing on your public persona is garbage...

Speaking more personally, my presence on Stack Overflow very certainly helped me get my current job - I now work for... Stack Overflow! However, I have been approached a number of times by people seeing my posts; in relation to employment, consultancy, and tech-article opportunities. I was also contacted by a Google recruiter who (when introducing themselves) directly cited my Stack Overflow posts (by this time I was already working for Stack Overflow, so I thanked them for their consideration, but I didn't choose to interview).

Of course, getting a public persona out there can also be done in a range of other ways; blogging, hobby-projects, public-speaking (user-groups), etc. So how about another reason to use Stack Overflow:

I learn something every single day...

...either by spending a minute-or-two researching an answer, or just by reading the other replies to an interesting post. The questions can be equally interesting, especially when introducing unexpected behaviour*. And learning is never a bad thing.

Still not enough?

Then consider this. We have unicorns! That should be enough for anyone.

*=for example, just this morning I found out that Visual Studio with the debugging host-process enabled processes attributes on the entry-point (AKA Main()) differently to the core CLR execution.
AnswerKenntnis:
I hate to break it to you but...

High SO rep != great programmer

Actually, many of them are. However, there are some users that are on the site so much, answering so many questions but their average score per answer is very low yet that still have a 5 digit rep due to their high activity.

If there's enough data there, an interviewer could possibly read answers to gleen insights into their approaches to solving problems but to go on straight score would make a company look downright silly.
AnswerKenntnis:
Consider what it takes to get a good Stack Overflow rating...


intelligence
knowledge
experience
good language skills
immense amounts of "free" time


Now take the last point there, people who have very high reputations here take a great deal of time to answer the questions of the masses.  Now whilst hugely thankful for such benevolent people, this implies one of the following.


They are unemployed
They are self-employed and have a
lot of "free" time
They are employed and spend all
their personal time sitting at a
screen (for example, "in want of a life")
They are employed and waste working
hours answering non-business
questions.


All but number three are reasons for a prospective employer not to take you on, and the third makes you wonder if they are ideal for spending all day with.

Anyhow, my boss is watching my screen a bit so I had better stop this post, before I have to use my Stack Overflow reputation to get me a new job...
AnswerKenntnis:
My current job pays me about $54k a year for 25h a week, most of which I work from home. Plus working as a game developer was kind of a childhood dream, that has now come true (although I sometimes think of moving on to something more "meaningful"). So I suppose my job is arguably equivalent to or even better than the average $100k+ jobs you get as a developer.

On my application I added a section "participation in online communities", wherein I listed SO (and a few smaller german forums). I don't have a superbly high score, but among ActionScript 3 developers I am very well placed.

The references I had were not really outstanding. Mostly the stupid kind of stuff you do as a Flash Developer to get money for sites you'd never visit yourself. But upon arrival to the interview I immediatly sensed, that I had a very high standing from the start. I have personally accounted this to my SO activity.

Getting a high rep on SO means, that you are eager to exchange thoughts, that you are knowledgeable enough to solve the problems your peers pose you (real problems from they actually encounter in every day life), and that you are able to communicate your ideas. These are key abilities for good developers. Thus one can say, people who have a high rep on SO have those abilities. The reputation is given to you by independent developers, not by some comittees or someone vaguely in touch with the subject. I think a high rep on SO is a much better measure than good grades (which appearently are still consider a criterium by some employers).

That being said, I'd never hire someone just because they have a high SO rep. But let's say a person with a high rep will have a head start while trying to convince me they're good. Appearently my current employer reasons similarly.
AnswerKenntnis:
YesΓÇªish

IΓÇÖve got (for want of a better term) job ΓÇ£invitationsΓÇ¥ via email which explicitly mention my Stack Overflow profile, from companies that boast a 12/12 score on the Joel Test and which honestly seem like extremely cool work environments.

And I know that the same has happened to several other people.

But ΓÇª

I donΓÇÖt believe for a second that I (or any of the other people) would score a job without an interview. Like most others here, I think that Stack Overflow may help you to get exposure on the web. But thatΓÇÖs all. It merely gets your name out. It shows that youΓÇÖre active and motivated but not whether youΓÇÖre qualified.
AnswerKenntnis:
If you're looking for a good paying job, my recommendations are as follows:


Love what you do and be passionate about it.
Educate yourself, don't fools others - Most people neglect the fundamentals and jump into the advance stuff thinking it'll make them look smart. All tall buildings are built upon a stable foundation.
Know your worth - Don't accept a job only because the employeer said they want to hire you. It's a two way street. What are they offering you long term? Is there growth? Does the work appeal to you?
Good pay is subjective, so know what you're willing to accept and settle for nothing less.
Build a reputation! Speak at conferences, Write articles, commit to Open Source projects


Don't be lazy. Just because someone will Google your name up and find out you answered a million questions on stackexchange.com that it makes you any way qualified for more money. Any code monkey can use Google to answer a question. Problem solving, strong technical knowledge, good personal skills, etc. make the world of difference when it comes to money.

I'm not perfect. Just perfect enough to know I'm good, but I'm not sure how good.
AnswerKenntnis:
If you spend time getting 5-digit reputation on Stack Overflow then you probably know what you're doing. That, more than any numerical value, is what get you the jobs.
AnswerKenntnis:
Beyond landing a job, Stack Overflow can be of great benefit for independent developers and consultants.  By demonstrating knowledge in a specific area, along with the ability to communicate what you know, you can become very attractive to someone looking for a consultant in that area.

I've been contacted on several occasions by individuals and companies who were looking to hire me as a contractor, based solely on an answer or two that I had left on Stack Overflow.  I don't have the time to consult, so I can't say what those opportunities would have been worth.  A couple of them sounded interesting, though.

However, I've received a greater number of these requests from the non-Stack-Overflow content that I've put out there like my course, the open source projects I've worked on, and the guides that I've published on my blog.  All of this public information can help to identify you as an expert in an area.  Think of how many people use the phrase "he wrote the book on that" when speaking about an authority on a subject.

Overall, I look at Stack Overflow as a way of keeping my skills sharp, while at the same time giving back for all of the help I've received from others over the years.  I take time that I would have wasted on blogs, etc. and channel it here.  If it benefits my career, that's a great side effect.
AnswerKenntnis:
Maybe Joel's implicitly offering a job to anyone with a 5-digit reputation? On a more serious note, some employers do follow sites like this, and also open source projects and mailing lists. I was contacted by a firm about a job opportunity a few years ago based on a recommendation from someone I worked on an open source project with. Having a reputation for being knowledgeable and helpful surely has to count for something.
AnswerKenntnis:
It may be of benefit as a decider I'd say. Like MS certifications

It'd never be the sole reason to employ someone.

It might be a reason to not employ someone if they spend many hours a day here...
AnswerKenntnis:
Yes. Maybe less so now, but more so in the future. I'm gainfully employed, but I have had 4 google interviews, and all the front line HR people asked me questions that they could only have learned if they googled me and my contributions in other sites (perhaps the thing that hurts stackexchange is the alias - rather than a direct name response). It's only a matter of time before they make the leap to asking for aliases on sites like stack exchange.
AnswerKenntnis:
I've gotten job offers before solely based on participation in online forums. The way I see it, it doesn't automatically get you hired, but it can be a way to get noticed.

However, I've been at my first and only job for exactly 6 years today, and am not planning to jump ship. Therefore, I don't participate in online forums to pad my resume. I do it to help people and to check whether I really am as clever as I think I am ;)

I don't think participation in stackoverflow is a particularly efficient way to get an "in" with prospective employers. I can think of lots of better ways to make myself more "marketable", if I wanted to do that. If marketability is your motivation to be here, you might as well not bother.
AnswerKenntnis:
I Sure Wish

Yeah... I really do. Having almost a five figure SO-rating, in my geographical area, nobody really cares and finding a new job is quite a daunting task still.

It all boils down to your contact network and experience.
AnswerKenntnis:
The head of computer science at my university insisted that my high grades would open doors.  That only seemed to be the case at the university itself.  In fact, some employers I interviewed with considered my grades to be somewhat low, because my school had lower 'grade targets' than the university local to that company.

Stack Overflow reputation demonstrably helps your chances in getting a career with Stack Overflow.  Beyond that, it really depends on the company.
AnswerKenntnis:
A nice opportunity, your skills and (hopefully) a good recruiter will get you the job. Reputation in SO (or others) is as likely to help you as to fire back at you. There's many ego wars going on in here...
AnswerKenntnis:
When I start my own company in the next 1-2 years, high reputation score in stackoverflow will be the 1st criteria for me to hire developers. I hope when that time comes, stackoverflow still serving the humanity.
AnswerKenntnis:
A high SO score can get your foot in the door at plenty of jobs.  But it's always the interview that makes or breaks you.  If you're bad at interviewing (like me), then a high SO score will get you the interview but not the job.
AnswerKenntnis:
Common sense, team play and people skills are as much important as technical skills. It's better to have a decent score overall. 

Dealing with 'Hi Tech' bosses with low score in other skills is a nightmare.
AnswerKenntnis:
If I was to hire, I would rather see how the person performs in the interview.
AnswerKenntnis:
Probably not. Look at it from a different perspective:


How many 5-digit SO users are looking/willing to accept another job for "only" 100K?
What percentage of people looking to fill 100K jobs have ever heard of SO and if so would use it as a criteria for hiring?


Some day they may be able to make the connection between SO scores and their job & CV posting site.

Maybe another question would be, would you consider SO scores for hiring?
AnswerKenntnis:
This reminds me of Drake's equation in astronomy. It is an equation that is suppose to give a statistical hint of the number of civilizations in the universe. It is a waste of time actually because it assumes that involved parameters are independent while they are clearly not.

I have four children and a good job. Maybe that is the key to the good job?
AnswerKenntnis:
Speaking as someone who spends way to much time at SO:

If I'm looking to hire someone, their reputation on SO is not going to be a consideration.  At all.  Their rep on SO is going to tell me nothing about their work habits or their suitability to the job.  For that, I'm going to look at their documented work experience and any products they have for review (web sites, open source projects, etc.) as well as their performance in an interview or series of interviews.  

SO is a great resource, but not for screening potential hires.
AnswerKenntnis:
A good SO flow is just one of the signals that will get you noticed (amongst others such as a high gpa, open-source contributions, certifications, etc). Most probably it will just get you the interview. After that, its all upto your performance during the 5-6 hour onsite interview and coding solutions on the whiteboard to complex algorithm problems (atleast at the top tech companies). While SO does discuss algorithms and data structures, there is no guarantee that every SO user with a 5 digit score knows or would do well in a high-pressure whiteboard coding interview.
QuestionKenntnis:
Why do ads for s/w engineers always say they "offer a fast-paced environment"?
qn_description:
Who wants to work in a fast-paced environment? Not me! I want a civilized environment where people have a sense of balance. Higher quality work gets done that way and work life isn't full of stress and anguish.
AnswersKenntnis
AnswerKenntnis:
It's code for "We change our minds a lot about what we want from the software, and if we hire you we don't want you to complain about it.  In fact, we expect you to put in a lot of overtime to implement our latest whim decision because we're fast paced. You've been warned."

In programmer-speak, it means "we have no specs, unit tests, or for that matter, anyone still around who remembers why our software is the way it is."
AnswerKenntnis:
Because most job ads aren't written by software engineers.

It's the exact same reason so many require the "ability to multi-task". Every respectable software developer knows that the human brain cannot multi-task. Or have 10 years experience in Windows Server 2008.
AnswerKenntnis:
Maybe it's the old fart in me, but when I see that I think they're looking for the youngsters who will work with great passion and excitement and dedication... and for less money.

But that's me.
AnswerKenntnis:
Many people (think HR since they are the ones who write these things up) equate a "fast paced" with excitement, and if it is not exciting, then it is boring.  

Who wants a boring job?  If it is boring, certainly "top talent" does not want to do it and it is not really worth doing.

That line of thinking, though prevalent, is wrong.
AnswerKenntnis:
Don't let them know not to put that in, we need to be able to easily weed out the companies we don't want to work for! To me "fast-paced environment" = lots of unpaid overtime.
AnswerKenntnis:
I disagree with the negative takes on this.  When I hear, "Fast-paced environment", I think "lots of interesting responsibilities, because work gets done effectively so you can move on to something else".  I would describe my current work environment as "fast-paced", but I would also rave about the work-life balance I get.   

I think the issue is that recruiters confuse "fast-paced" with "we have lots of emergencies and make people put in lots of hours" - which is almost the opposite of fast-paced.  Emergencies derail and randomize people, slowing down the pace of business.  Working long hours is a symptom of not being fast-paced during normal business hours, and trying to make up for it by working more.  "Working hard, long hours while being randomized when things go wrong" and "producing great business value rapidly" are two different, often contradictory things.

"Fast-paced", in the sense of "our team delivers lots of business value in a short period of time," is desirable because it leads to an improved work-life balance.  You get great resume content and develop your talent without investing lots of time into independent study in your own time, since you are learning so much on the job - meaning your free time can be spent on non-programming hobbies without your skills becoming obsolete.  "Fast-paced" is also not boring, since you complete one project quickly and then can move on to something fresh.  IMO, constantly dealing with emergencies and "fires", covering for other people's mistakes, coping with crummy tools and poor documentation, and so on, is very frustrating and boring.

Edit:  I thought of a few other things "fast-paced" implies that are positive and can be attractive:  First, it suggests that the team uses Agile practices; waterfall isn't fast-paced.  Second, it implies that the company or team is small and light-weight, and not bound up in miscellaneous process; people who sit in lots of meetings and need to fill out three forms for each bug fix aren't going to be moving quickly.  Third, it hints at a growing company (maybe a start-up?) or team that is making strides and delivering lots of value to their customers, vs. a company or team that has already done its interesting work and is now just sitting there doing maintenance and getting money for the work they already did.

One comment below also points out that "fast-paced" is a contrast to "in-depth".  A fast-paced environment is one where you pick up a lot of skills in a very short time, but may not become an expert in any of them.  A slower environment where lots of experts are needed and things need to be done well the first time is more likely to grow in-depth skills.  Having lots of breadth in one's skills and having a few skills in depth are very different career tracks, so "fast-paced" is also a signal that people who want to become experts at a relatively narrow set of skills probably shouldn't apply.
AnswerKenntnis:
You probably only read ads for software developers, but the truth is that all job postings say they "offer a fast-paced environment."
AnswerKenntnis:
I'm a recruiter in the tech space. Most job "descriptions" get washed by HR before they get out in front of the public. So an engineering manager might be sitting there, and write up a reasonable req, talking about how "TBD" is never allowed when setting requirements, describe their team, the types of projects they focus on that set them apart, etc etc, yadda yadda. Then, HR gets it, and says "but, everyone likes to 'work hard, play hard, right'?? I mean, that's what every other job description out there says. We'd better tell people we're 'fast paced', or we'll look too old school and boring. Google's fast-paced, right? Right?? We have to be like Google!! Oh, and get rid of anything we could vaguely, potentially, 1-in-a-million chance get sued over."

Uggh. I hate most job descriptions. Full of junk like what Chuck described. Plus, they're just a bullet point list of requirements, with no meat to them. Most HR departments go to their competitors web site, find a similar job to the one they need to fill, copy and paste it, change the name of the company, maybe tweak the requirements a bit, and then post it.

They're ads: they should do a good job of communicating what the company's really like (ie, so when you take the job you aren't sitting there in 3 months feeling like you got suckered, and then they have to replace you when you quit); be funny (because, c'mon, who doesn't like funny - again, it's an ad); and not freak people out with "fast-paced" and "hard-charging" types of phrases if they aren't true.

End rant...
AnswerKenntnis:
The cliche term here is "young and dynamic team". In other words, a group of people who're too inexperienced to know that putting in 20 hours overtime a week without pay is not normal and a not a sign of a healthy project, that requirements that change several times a day up until 5 minutes before delivery is abnormal and unhealthy, etc.

It's also a way to tell you that you're too old without actually stating so openly (which would be illegal under age discrimination laws).
AnswerKenntnis:
If everything seems under control, you're just not going fast enough. 
Mario Andretti (race car driver)

I take "fast-paced" as code for "somewhat out of control".  Some people like that.  Others don't.
AnswerKenntnis:
So far my favourite statement along with the "fast paced environment" was the HR manager trying to make it sound like some sort of fun slumber party type thing to stay at the office til midnight pushing out code.

Personally I want a job that's relaxed and has cookies... 

mmmmm..... cookies..... (>^.^)>(#) mmmmmm..... coookies....

But it does seem that most HR folk are wanton to believe that us programming folk want fast, high speed, stress filled jobs, not sure who gave them that idea but it needs to be squashed.
AnswerKenntnis:
Maybe its an attempt to scare away the lazy programmers...
AnswerKenntnis:
Many people have focused on what fast-paced can mean but I think there's one other reason: many shops are slow-paced to the point of stagnation - picture the kind of place where finding a problem results in either requesting off-site training or filing a ticket and going home for the day. If you're hiring, there's a desire to avoid submissions from the kinds of developers who got into the industry thinking "indoor job, no heavy lifting" but this particular wording is too cliched to actually do that.
AnswerKenntnis:
Do you mean this?
"You must be able to thrive in a Agile / Scrum environment that is fast paced and has minimal structure and process and is growing rapidly."

I think it means exciting. Then again, It could also mean extreme overtime and lost wages.
AnswerKenntnis:
Well... us rockstar ninjas prefer action!
AnswerKenntnis:
Who the hells wants a fast-paced
  environment?


** raises hand **

A 'fast paced environment' can either be an environment from hell, or one where technological challenges abound. I stay away from the former, but I purposely seek the later. Obviouly one should seek a balance (specially if you are like me, with family and kids). However, if your job doesn't challenge your skills and passion, you are not learning. And that is the worst you can do to your professional career development.

To assume a fast-paced environment is always bad reveals a particular outlook to life and the type of technology-oriented career we have chosen for ourselves. Every job has its warts. What you make of out them, even the worst environments, it is solely and squarely up to you.

There are some 'fast-paced' jobs (on the bad sense of the word) that were just horrid, and I would never set foot on those companies again. But the experiences themselves taught me how to handle pressure in a professional manner, and how to get things done as much as humanly possible. Those jobs were horrid not because of the technical and requirement challenges, but because of horrid personal dynamics and management style.

On the other side of the coin, the best jobs I've ever had were also 'fast-paced', in terms of changing requirements and technological challenges. That's where you really learn how to rise to the ocassion and deliver, which is ultimately what every programmer (or any professional) ought to seek.

Difficulty of something is not an excuse for avoiding its accomplishment.

Just people change their minds when it comes to software is not a bad thing. It is a reflection of world dynamics, and we in software, we are the business of creating realistic executable models of the world. I'm amazed at how many programmers actually fail to understand this.

The challenge is in knowing how to manage the continous (and usually chaotic) rate of change. And there are two sides of the coin in this: there is non-technical management and there is technical management (your part as a programmer and software engineer). And the later is as important, and perhaps more so, than the former.

Ultimately, you want to stay away from bad working environments, but for the purpose of cultivating your professional career, you should always look for legitimately fast-paced enviroments. Otherwise, we might just look for a 9-to-5 job maintaining COBOL/RPG reports.
AnswerKenntnis:
Wow, your all looking deep in to the exact meaning of that phrase :-)

http://en.wikipedia.org/wiki/Hanlon%27s_razor says "Never attribute to malice that which is adequately explained by stupidity."


  Why do ads for s/w engineers always
  say they ΓÇ£offer a fast-paced
  environmentΓÇ¥?


Because the job ad was written by a someone who can't actually write, has no imagination and so has to fall back to tired old phrases like this?

I mean come on, we all know how good programmers are at writing documentation :-P
AnswerKenntnis:
It could just be that HR wanted something that sounds like "cool", but since the company does salary calculations rather than robots on Mars they go for that vague "it means whatever you want it to mean" style. Proof of that is in several of the answers here.
AnswerKenntnis:
The answer is because good developers get bored easily and when they get bored they leave. Saying that you offer a fast paced environment is an attempt to find good developers. Actually offering a fast paced environment is a good way to reduce turn over.

Also, I saw someone mentioned 'lazy programmers' I agree with that too. Fast Paced means that you'll have no time to learn how to be a dev so you should already be one. 

Also, most of the time the ads that people write are from HR or someone who doesn't know anything about the position. "We require expert levels in [insert alphabet soup here] and BA and 10yr Experience and [insert other insane requirements here]". I hate those ads.
AnswerKenntnis:
Is just syntactic sugar for stress and chaos.

This is what it means. It will mean you have to adapt fast, that will invent deadlines for yesterday and you'll have to deal with it because is a fast paced environment. Just an nice syntax and an excuse for an environment that will be stressful.
AnswerKenntnis:
We might be overthinking this. 

I think often it's used because "gee I've only got three sentences and a ridiculously large set of bullet points ... I'd better think of something else to say".
AnswerKenntnis:
Keeping up with technology or at least the illusion of it would be another reason I'd note.  Wouldn't you want to work where new things are always coming at you when people, "Hey, could we do this?  How about we get started on it now?"
AnswerKenntnis:
When I hear it, I think of this blogpost from Coding Horror. Fast-paced can certainly be a huge advantage when it comes to iterations and development. Unfortunately I doubt the "fast-paced" in job ads refers to the speed of iterations.
AnswerKenntnis:
for me fast-paced-environment means that the applicant have to be willing to learn new things. 

Example


Last months project was in java 
Next project will be in c#
AnswerKenntnis:
As someone who recently wrote a job description, I was tempted to use that phrase but not really for most of the reasons that everyone has mentioned above. It's not because things are out of control and it's not because everything is a mess or because we need you to put in a boat load of unpaid overtime. That's not fast-paced. That's just mismanagement.

Fast-paced to me is: We have a lot to do and you'll need to keep up. If you want work at a relaxed, government-job pace, it's not the right place for you. 

This is a positive because it means there is a lot of opportunity to create value for the company, which will translate to more growth for the company and higher compensation for you.
AnswerKenntnis:
Isn't the English language brilliant. A simple two word statement can stir up a great debate based on interpretation, ambiguity and personal experience. 

I don't think there's anything wrong with "face-paced" provided it's qualified. Having said that given a lack of a qualified statement, I'd favour an alternative synonym or more detail. If your trying to say "frequent, fortnightly iterations" or "lots of work enthusiastic programmers who want to spend 80% of their programming" say it, don't say "fast paced" as it's ambiguous and open to interpretation. 

Far too often jobs add are kept concise with buzz words that recruiter think you want to hear. 

In my experience, fast paced has often meant fire fighting, or working on projects against deadlines with not enough time to deliver everything to the best of your teams ability. This has at times been to poor management, and unforeseen circumstances, but I'd choose a more relaxed balanced (yet efficient and streamlined delivery) approach over "fast-paced" one.

I once read a job ad where the recruiter put something like "the best job you'll see this year". Immediate reaction, nonsense, it's a sales pitch, why? because I put recruiters in the same group as estate agents (there in it for the commission and they can't afford to be honest).
AnswerKenntnis:
I have a friend, which is a self-taught developer. You know, the only language he is somewhat proficient in is C#, which is not a bad thing, it's OK to know just one language, but to know it good. He had no professional schooling, aside from Microsoft certificates (MCSE or something), no college degree or anything, he barely made it out of high school.

Now, he keeps posting on Facebook about his latest idea for an Android app, for a new twitter client, URL shortening service, image hoster...

He wanted to develop a new browser in C#, but stopped mid way.

He keeps making videos on youtube about TFS, and how cool project management is with all those nice Microsoft products.

Now, he has another startup, a social networking integration client for use with MS Office products.

*sigh *

The kind of "Start-Ups" and Software he develops, is exactly the kind I picture behind those ads. Brain farts that sound exciting, but lose are pretty quick. They plan to make the software freeware and charge for support. It all seems kinda vapor ware. It seems like working for them is putting in some work, of which most stuff won't be used eventually, and then the whole thing shuts down and everyone is looking for the culprit, the lazy ass, that didn't get the work done.
AnswerKenntnis:
It's their way of saying "the sh*t will flow downstream faster than you can paddle against it.". Requirements will change faster than you can finish coding the previous version, and features will be added whenever the RedBull-fueled project manager thinks them up. 

I'm so glad I no longer have to deal with implementing other folks' "shower visions".
AnswerKenntnis:
It probably means in part that the company values business-based decisions driven by customer requirements, over the needs of their engineering staff. 

Not trollin', just trying to be realistic...
AnswerKenntnis:
Usually because they think that using Boost and C++ make them cutting edge.
QuestionKenntnis:
So Singletons are bad, then what?
qn_description:
There has been a lot of discussion lately about the problems with using (and overusing) Singletons. I've been one of those people earlier in my career too. I can see what the problem is now, and yet, there are still many cases where I can't see a nice alternative - and not many of the anti-Singleton discussions really provide one.

Here is a real example from a major recent project I was involved in:

The application was a thick client with many separate screens and components which uses huge amounts of data from a server state which isn't updated too often. This data was basically cached in a Singleton "manager" object - the dreaded "global state". The idea was to have this one place in the app which keeps the data stored and synced, and then any new screens that are opened can just query most of what they need from there, without making repetitive requests for various supporting data from the server. Constantly requesting to the server would take too much bandwidth - and I'm talking thousands of dollars extra Internet bills per week, so that was unacceptable.

Is there any other approach that could be appropriate here than basically having this kind of global data manager cache object? This object doesn't officially have to be a "Singleton" of course, but it does conceptually make sense to be one. What is a nice clean alternative here?
AnswersKenntnis
AnswerKenntnis:
It's important to distinguish here between single instances and the Singleton design pattern.

Single instances are simply a reality.  Most apps are only designed to work with one configuration at a time, one UI at a time, one file system at a time, and so on.  If there's a lot of state or data to be maintained, then certainly you would want to have just one instance and keep it alive as long as possible.

The Singleton design pattern is a very specific type of single instance, specifically one that is:


Accessible via a global, static instance field;
Created either on program initialization or upon first access;
No public constructor (cannot instantiate directly);
Never explicitly freed (implicitly freed on program termination).


It is because of this specific design choice that the pattern introduces several potential long-term problems:


Inability to use abstract or interface classes;
Inability to subclass;
High coupling across the application (difficult to modify);
Difficult to test (can't fake/mock in unit tests);
Difficult to parallelize in the case of mutable state (requires extensive locking);
and so on.


None of these symptoms are actually endemic to single instances, just the Singleton pattern.

What can you do instead?  Simply don't use the Singleton pattern.

Quoting from the question:


  The idea was to have this one place in the app which keeps the data stored and synced, and then any new screens that are opened can just query most of what they need from there, without making repetitive requests for various supporting data from the server. Constantly requesting to the server would take too much bandwidth - and I'm talking thousands of dollars extra Internet bills per week, so that was unacceptable.


This concept has a name, as you sort of hint at but sound uncertain of.  It's called a cache.  If you want to get fancy you can call it an "offline cache" or just an offline copy of remote data.

A cache does not need to be a singleton.  It may need to be a single instance if you want to avoid fetching the same data for multiple cache instances; but that does not mean you actually have to expose everything to everyone.

The first thing I'd do is separate out the different functional areas of the cache into separate interfaces.  For example, let's say you were making the world's worst YouTube clone based on Microsoft Access:


                          MSAccessCache
                                Γû▓
                                |
              +-----------------+-----------------+
              |                 |                 |
         IMediaCache      IProfileCache      IPageCache
              |                 |                 |
              |                 |                 |
          VideoPage       MyAccountPage     MostPopularPage


Here you have several interfaces describing the specific types of data a particular class might need access to - media, user profiles, and static pages (like the front page).  All of that is implemented by one mega-cache, but you design your individual classes to accept the interfaces instead, so they don't care what kind of an instance they have.  You initialize the physical instance once, when your program starts, and then just start passing around the instances (cast to a particular interface type) via constructors and public properties.

This is called Dependency Injection, by the way; you don't need to use Spring or any special IoC container, just so long as your general class design accepts its dependencies from the caller instead of instantiating them on its own or referencing global state.

Why should you use the interface-based design?  Three reasons:


It makes the code easier to read; you can clearly understand from the interfaces exactly what data the dependent classes depend on.
If and when you realize that Microsoft Access wasn't the best choice for a data back-end, you can replace it with something better - let's say SQL Server.
If and when you realize that SQL Server isn't the best choice for media specifically, you can break up your implementation without affecting any other part of the system.  That is where the real power of abstraction comes in.


If you want to take it one step further then you can use an IoC container (DI framework) like Spring (Java) or Unity (.NET).  Almost every DI framework will do its own lifetime management and specifically allow you to define a particular service as a single instance (often calling it "singleton", but that's only for familiarity).  Basically these frameworks save you most of the monkey work of manually passing around instances, but they are not strictly necessary.  You do not need any special tools in order to implement this design.

For the sake of completeness, I should point out that the design above is really not ideal either.  When you are dealing with a cache (as you are), you should actually have an entirely separate layer.  In other words, a design like this one:


                                                        +--IMediaRepository
                                                        |
                          Cache (Generic)---------------+--IProfileRepository
                                Γû▓                       |
                                |                       +--IPageRepository
              +-----------------+-----------------+
              |                 |                 |
         IMediaCache      IProfileCache      IPageCache
              |                 |                 |
              |                 |                 |
          VideoPage       MyAccountPage     MostPopularPage


The benefit of this is that you never even need to break up your Cache instance if you decide to refactor; you can change how Media is stored simply by feeding it an alternate implementation of IMediaRepository.  If you think about how this fits together, you will see that it still only ever creates one physical instance of a cache, so you never need to be fetching the same data twice.

None of this is to say that every single piece of software in the world needs to be architected to these exacting standards of high cohesion and loose coupling; it depends on the size and scope of the project, your team, your budget, deadlines, etc.  But if you're asking what the best design is (to use in place of a singleton), then this is it.

P.S. As others have stated, it's probably not the best idea for the dependent classes to be aware that they are using a cache - that is an implementation detail they simply should never care about.  That being said, the overall architecture would still look very similar to what's pictured above, you just wouldn't refer to the individual interfaces as Caches.  Instead you'd name them Services or something similar.
AnswerKenntnis:
In the case you give, it sounds like the use of a Singleton is not the problem, but the symptom of a problem - a larger, architectural problem.

Why are the screens querying the cache object for data?  Caching should be transparent to the client.  There should be an appropriate abstraction for providing the data, and the implementation of that abstraction might utilize caching.

The issue is likely that dependencies between parts of the system are not set up correctly, and this is probably systemic.

Why do the screens need to have knowledge of where they get their data?  Why are the screens not provided with an object that can fulfill their requests for data (behind which a cache is hidden)?  Oftentimes the responsibility for creating screens is not centralized, and so there is no clear point of injecting the dependencies.  

Again, we are looking at large-scale architectural and design issues.

Also, it is very important to understand that the lifetime of an object can be completely divorced from how the object is found for use.

A cache will have to live throughout the application's lifetime (to be useful), so that object's lifetime is that of a Singleton.

But the problem with Singleton (at least the common implementation of Singleton as a static class/property), is how other classes that use it go about finding it.

With a static Singleton implementation, the convention is to simply use it wherever needed.  But that completely hides the dependency and tightly couples the two classes.

If we provide the dependency to the class, that dependency is explicit and all the consuming class needs to have knowledge of is the contract available for it to use.
AnswerKenntnis:
I wrote a whole chapter on just this question. Mostly in the context of games, but most of it should apply outside of games.

tl;dr:

The Gang of Four Singleton pattern does two things: give you convenience access to an object from anywhere, and ensure that only one instance of it can be created. 99% of the time, all you care about is the first half of that, and carting along the second half to get it adds unnecessary limitation.

Not only that, but there are better solutions for giving convenient access. Making an object global is the nuclear option for solving that, and makes it way to easy to destroy your encapsulation. Everything that's bad about globals applies completely to singletons.

If you're using it just because you have a lot of places in code that need to touch the same object, try to find a better way to give it to just those objects without exposing it to the entire codebase. Other solutions:


Ditch it entirely. I've seen lots of singleton classes that don't have any state and are just bags of helper functions. Those don't need an instance at all. Just make them static functions, or move them into one of the classes that the function takes as an argument. You wouldn't need a special Math class if you could just do 123.Abs().
Pass it around. The simple solution if a method needs some other object is to just pass it in. There's nothing wrong with passing some objects around.
Put it in the base class. If you have a lot of classes that all need access to some special object, and they share a base class, you can make that object a member on the base. When you construct it, pass in the object. Now the derived objects can all get it when they need it. If you make it protected, you ensure the object still stays encapsulated.
AnswerKenntnis:
Its not global state per se that is the problem.

Really you only need to be worried about global mutable state. Constant state is not affected by side affects and thus is less of a problem.

The major concern with singleton is that it adds coupling and thus makes things like testing hard(er). You can reduce the coupling by getting the singleton from another source (eg a factory). This will allow you to decouple the code from a particular instance (though you become more coupled to the factory (but at least the factory can have alternative implementations for different phases)).

In your situation I think you can get away with it as long as your singleton actually implements an interface (so that an alternative can be used in other situations).

But another major draw back with singletons is that once they are in-place removing them from code and replacing them with something else becomes a real hard task (there is that coupling again).

// Example from 5 minutes (con't be too critical)
class ServerFactory
{
    public:
        // By default return a RealServer
        ServerInterface& getServer();

        // Set a non default server:
        void setServer(ServerInterface& server);
};

class ServerInterface { /* define Interface */ };

class RealServer: public ServerInterface {}; // This is a singleton (potentially)

class TestServer: public ServerInterface {}; // This need not be.
AnswerKenntnis:
A singleton is not in a fundamental way bad, in the sense that anything design computing can be good or bad.  It can only ever be correct (gives the expected results) or not.  It can also be useful or not, if it makes the code clearer or more efficient.

One case in which singletons are useful is when they represent an entity that really is unique.  In most environments, databases are unique, there really only is one database.  Connecting to that database may be complicated because it requires special permissions, or traversing through several connection types.  Organizing that connection into a singleton probably makes a lot of sense for this reason alone.  

But you also need to be sure that the singleton really is a singleton, and not a global variable.  This matters when the single, unique database is really 4 databases, one each for production, staging, development and test fixtures.  A Database Singleton will figure out which of those it should be connecting to, grab the single instance for that database, connect it if needed, and return it to the caller.

When a singleton is not really a singleton (this is when most programmers get upset), it's a lazily instantiated global, there's no opportunity to inject a correct instance.  

Another useful feature of a well designed singleton pattern is that it is often not observable.  The caller asks for a connection.  The service that provides it can return a pooled object, or if it's performing a test, it can create a new one for every caller, or provide a mock object instead.
AnswerKenntnis:
Use of the singleton pattern that represents actual objects is perfectly acceptable. I write for the iPhone, and there are lots of singletons in the Cocoa Touch framework. The application itself is represented by a singleton of the class UIApplication. There's only one application that you are, so it's appropriate to represent that with a singleton.

Using a singleton as a data manager class is okay as long as it's designed right. If it's a bucket of data properties, that's no better than global scope. If it's a set of getters and setters, that's better, but still not great. If it's a class that really manages all interface to data, including perhaps fetching remote data, caching, setup and teardown... That could be very useful.
AnswerKenntnis:
My main problem with the singleton design pattern is that it is very difficult to write good unit tests for your application.

Every component that has a dependency to this "manager" does so by querying its singleton instance. And if you want to write a unit test for such a component you have to inject data into this singleton instance, which may not be easy.

If on the other hand your "manager" is injected into the dependent components through a constructor parameter, and the component doesn't know the concrete type of the manager, only an interface, or abstract base class that the manager implements, then a unit test could supply alternate implementations of the manager when testing dependencies.

If you use IOC containers to configure and instantiate the components that make up your application, then you can easily configure your IOC container to create only one instance of the "manager", allowing you to achieve the same, only one instance controlling global application cache.

But if you don't care about unit tests, then a singleton design pattern can be perfectly fine. (but I wouldn't do it anyway)
AnswerKenntnis:
First question, do you find a lot of bugs in the application? perhaps forgetting to update cache, or bad cache or find it hard to change? (i remember an app wouldnt change sizes unless you changed the color too... you can however change the color back and keep the size).

What you would do is have that class but REMOVE ALL STATIC MEMBERS. Ok this isnt nessacary but i recommend it. Really you just initialize the class like a normal class and PASS the pointer in. Don't frigen say ClassIWant.APtr().LetMeChange.ANYTHINGATALL().andhave_no_structure()

Its more work but really, its less confusing. Some places where you shouldnt change things you now cannot since its no longer global. All my manager classes are regular classes, just treat it as that.
AnswerKenntnis:
IMO, your example sounds okay. I'd suggest factoring out as follows: cache object for each (and behind each) data object; cache objects and the db accessor objects have the same interface. This gives the ability to swap caches in and out of the code; plus it gives an easy expansion route.

Graphic:

DB
|
DB Accessor for OBJ A
| 
Cache for OBJ A
|
OBJ A Client requesting


DB accessor and  cache can inherit from the same object or duck type into looking like the same object, whatever. So long as you can plug/compile/test and it still works.

This decouples things so you can add new caches without having to go in and modify some Uber-Cache object. YMMV. IANAL. ETC.
AnswerKenntnis:
A bit late to the party, but anyways.

Singleton is a tool in a toolbox, just as anything else. Hopefully you have more in your toolbox than a just a single hammer.

Consider this:

public void DoSomething()
{
    MySingleton.Instance.Work();
}


vs

public void DoSomething(MySingleton singleton)
{
    singleton.Work();
}
DoSomething(MySingleton.instance);


1st case leads to high coupling etc; 2nd way has no problems @Aaronaught is describing, as far as I can tell. Its all about how you use it.
AnswerKenntnis:
Then what? Since nobody said it: Toolbox. That is if you want global variables.


  Singleton abuse can be avoided by looking at the problem from a different angle. Suppose an application needs only one instance of a class and the application configures that class at startup: Why should the class itself be responsible for being a singleton? It seems quite logical for the application to take on this responsibility, since the application requires this kind of behavior. The application, not the component, should be the singleton. The application then makes an instance of the component available for any application-specific code to use. When an application uses several such components, it can aggregate them into what we have called a toolbox.
  
  Put simply, the application's toolbox is a singleton that is responsible either for configuring itself or for allowing the application's startup mechanism to configure it...


public class Toolbox {
     private static Toolbox _instance; 

     public static Toolbox Instance {
         get {
             if (_instance == null) {
                 _instance = new Toolbox(); 
             }
             return _instance; 
         }
     }

     protected Toolbox() {
         Initialize(); 
     }

     protected void Initialize() {
         // Your code here
     }

     private MyComponent _myComponent; 

     public MyComponent MyComponent() {
         get {
             return _myComponent(); 
         }
     }
     ... 

     // Optional: standard extension allowing
     // runtime registration of global objects. 
     private Map components; 

     public Object GetComponent (String componentName) {
         return components.Get(componentName); 
     }

     public void RegisterComponent(String componentName, Object component) 
     {
         components.Put(componentName, component); 
     }

     public void DeregisterComponent(String componentName) {
         components.Remove(componentName); 
     }

}


But guess what? It is a singleton!

And what is a singleton?

Maybe that's where the confusion begins.

To me, the singleton is an object enforced to have a single instance only and always. You can access it anywhere, any time, without needing to instantiate it. That's why it's so closely related to static. For comparison, static is basically the same thing, except it's not an instance. We don't need to instantiate it, we can't even, because it's automagically allocated. And that can and does bring problems.

From my experience, simply replacing static for Singleton solved many issues in a medium size patchwork bag project I'm on. That only means it does have some usage for bad designed projects. I think there's too much discussion if the singleton pattern is useful or not and I can't really argue if it's indeed bad. But still there are good arguments in favor for singleton over static methods, in general.

The only thing I'm sure is bad about singletons, is when we use them while ignoring good practices. That's indeed something not that easy to deal with. But bad practices can be applied to any pattern. And, I know, it is too generic to say that... I mean there's just too much to it.

Don't get me wrong!

Simply put, just like global vars, singletons should still be avoided at all times. Specially because they are overly abused. But global vars can not be avoided always and we should use them in that last case.

Anyway, there are many other suggestions other than the Toolbox, and just like the toolbox, each one has its application...

Other alternatives


The best article I've just read about singletons suggests Service Locator as an alternative. To me that is basically a "Static Toolbox", if you will. In other words, make the Service Locator a Singleton and you have a Toolbox. That does go against its initial suggestion of avoiding the singleton, of course, but that's only to enforce singleton's issue is how it's used, not the pattern in itself.
Others suggest Factory Pattern as an alternative. It was the first alternative I heard from a colleague and we quickly eliminated it for our usage as global var. It sure has its usage, but so does singletons.


Both alternatives above are good alternatives. But it all depends on your usage.

Now, implying singletons should be avoided at all costs is just wrong...


Aaronaught's answer is suggesting to never use singletons, for a series of reasons. But they're all reasons against how it's ill used and abused, not directly against the pattern itself. I do agree with all the worrying about those points, how can't I? I just think it's misleading.


The inabilities (to abstract or subclass) are indeed there, but so what? It's not meant for that. There is no inability to interface, as far as I can tell. High coupling can also be there, but that's just because how it's commonly used. It doesn't have to. In fact, the coupling in itself has nothing to do with the singleton pattern. That being clarified, it also already eliminates the difficulty to test. As for the difficulty to parallelize, that depends on the language and platform so, again, not a problem on the pattern.

Practical examples

I often see 2 being used, both in favor and against singletons. Web cache (my case) and log service.

The logging, some will argue, is a perfect singleton example, because, and I quote:


  
  The requesters need a well-known object to which to send requests to log. This means a global point of access.
  Since the logging service is a single event source to which multiple listeners can register, there only needs to be one instance.
  Although different applications may log to different output devices, the way they register their listeners is always the same. All customization is done through the listeners. Clients can request logging without knowing how or where the text will be logged. Every application would therefore use the logging service exactly the same way.
  Any application should be able to get away with only one instance of the logging service.
  Any object can be a logging requester, including reusable components, so that they should not be coupled to any particular application.
  


While others will argue it makes difficult to expand the log service once you eventually realize it actually shouldn't be only one instance.

Well, I say both arguments are valid. The problem here, again, isn't on the singleton pattern. It's on architectural decisions and weighting if refactoring is a viable risk. It's a further problem when, usually, refactoring is the last needed corrective measure.
AnswerKenntnis:
Singletons are just the projection of a serviced oriented architecture into a program.

An API is an example of a singleton at a protocol level. You access Twitter, Google etc through what are essentially singletons. So why do singletons become bad within a program? 

It depends on how you think of a program. If you think of a program as society of services rather than randomly bound cached instances then singletons make perfect sense.

Singletons are a service access point. The public interface to a tightly bound library of functionality that hides perhaps a very sophisticated internal architecture.

So I don't see a singleton as that different from a factory. The singleton can have constructor parameters passed in. It can be created by some context that knows how to resolve the default printer against all the possible selection mechanisms, for example. For testing you can insert your own mock. So it can be quite flexible.

The key is internally in a program when I execute and need a bit of functionality I can access the singleton with full confidence that the service is up and ready for use. This is key when there are different threads starting in a process that must go through a state machine to be considered ready.

Typically I would wrap a XxxService class that wraps a singleton around class Xxx. The singleton isn't in class Xxx at all, it's separated out into another class, XxxService. This is because Xxx can have multiple instances, allthough it's not likely, but we still wan't to have one Xxx instance globally accessible on each system. XxxService provides a nice separation of concerns. Xxx doesn't have to enforce a singleton policy, yet we can use Xxx as a singleton when we need to.

Something like:

XxxService.h:
/**
 * Provide singleton wrapper for Xxx object. This wrapper
 * can be autogenerated so is not made part of the object.
 */

include "Xxx/Xxx.h"

class XxxService
{
public:
   /**
    * Return a Xxx object as a singleton. The double check
    * singleton algorithm is used. A 0 return means there was
    * an error. Developers should use this as the access point to
    * get the Xxx object.
    *
    * 
    *@@ #include "Xxx/XxxService.h"
    @@ Xxx xxx= XxxService::Singleton();
    * 
    /
   static Xxx     Singleton();

private:
   static Mutex  mProtection;
};

XxxService.cpp:

include "Xxx/XxxService.h"                   // class implemented

include "LockGuard.h"

// CLASS SCOPE
//
Mutex        XxxService::mProtection;

Xxx*
XxxService::Singleton()
{
   static Xxx* singleton;  // the variable holding the singleton

// First check to see if the singleton has been created.
   //
   if (singleton == 0)
   {
      // Block all but the first creator.
      //
      LockGuard lock(mProtection);

  // Check again just in case someone had created it
  // while we were blocked.
  //
  if (singleton == 0)
  {
     // Create the singleton Xxx object. It's assigned
     // to a temporary so other accessors don't see
     // the singleton as created before it really is.
     //
     Xxx* inprocess_singleton= new Xxx;

     // Move the singleton to state online so we know that is has
     // been created and it ready for use.
     //
     if (inprocess_singleton->MoveOnline())
     {
        LOG(0, "XxxService:Service: FAIL MoveOnline");
        return 0;
     }

     // Wait until the module says it's in online state.
     //
     if (inprocess_singleton->WaitTil(Module::MODULE_STATE_ONLINE))
     {
        LOG(0, "XxxService:Service: FAIL move to online");
        return 0;
     }

     // The singleton is created successfully so assign it.
     //
     singleton= inprocess_singleton;


  }// still not created


}// not created

// Return the created singleton.
   //
   return singleton;

}// Singleton
AnswerKenntnis:
Have each screen take in the Manager in their constructor. 

When you start your app, you create one instance of the manager and pass it around.

This is called Inversion of Control, and allows you to swap out the controller when configuration changes and in tests. Additionally, you can run several instances of your application or parts of your application in parallell (good for testing!). Lastly your manager will die with its owning object (the startup class).

So structure your app like a tree, where things above owns everything used below them. Don't implement an app like a mesh, where everybody knows everybody and find each other through global methods.
QuestionKenntnis:
New Team Lead - How to deal with a resentful former peer
qn_description:
I've been told that I am to be a team lead of an upcoming project. I've not ever been team lead before but the responsibilities are what you would typically expect, with revolving door of 3 to 4 other developers through the 8 or 9 months it takes to complete the project.

My problem is this: one of the developers who will no doubt be working on this project will be a problem. He has more experience than me, has called me an idiot several times in the past, and has told me that he has taken this job because he is a natural leader. He has expected to be promoted to a leadership position with every new project (which has not happened to date), and even once told me that I was to report to him, even though the actual team lead was under no such illusion. Moreover, I've both observed and heard from others that he is extremely unprofessional (watches non-work videos at client sites - with no head phones, dresses unprofessionally, comes to work late, makes inappropriate jokes, etc.) He took or attempted to take credit for my work or insights multiple times while I worked with him as a peer. My current team lead told me that she threw out 1/2 of this guy's code once he left her project because the quality was no good. I could go on.

My fear is that this guy will actively work against me, because he will resent having to report to someone that he considers an inferior, especially since I have been given this opportunity before him. I've successfully dealt with this type of personality as a peer or even reporting to managers like this in the past. I've not dealt with, or ever thought about, having to deal with this type of character who is reporting to me. 

My question is: what kind of strategies can I use to effectively and professionally deal with this? Especially now, before it has become a problem, is there any way to cut it off before it gets out of hand etc. If anyone has a similar experience how did they deal with it?
AnswersKenntnis
AnswerKenntnis:
The other answers I'm seeing here ("short leash!", "document everything!", "be professional!") are nice and all, and they're superficially correct, but they fall short in that they fail to consider the human aspect of the situation. 

The thing is, everyone is a human, and humans are different, and their motivations and thought-processes and skills are all different. And while the somewhat algorithmic answers I'm seeing here are technically correct, they do not seem to take into account the nature of the way that human beings work and the nature of what true management can be.

You can't begin to solve someone's problem until you understand them. Which means, understanding their motivations. Which is why, in fact, good people management can be a lot more like psychoanalysis than you'd think.

Here's an example of how the thought process might go.


Joe thinks he should be leader because he has more experience than me, but he's delusional.


OK, what does that tell us? Is Joe unusually concerned about status? Is he bothered by the lack of respect? Does he chafe at being called Joe instead of DOCTOR Joe in recognition by his PhD? Where is this insecurity coming from? How can you address it?


Joe's code is not very good. Half of his code had to be thrown out.


Why? How? Is he sloppy? Rushed? Just not very smart? Is he cocky, thinking his code is awesome? Why? Is this that insecurity again?


Joe is unprofessional and watches videos at the client site.


Again... why? Is it because he thinks he's a rock star, and should be indulged for wearing hoodies and watching collegehumor videos because his work is sooooo amazing? Or is it because he is not very self-aware, and does not realize the image he is portraying? Does he genuinely not GET what it means to be professional, or is he actively rebelling against the idea of professionalism? I'm not sure. The answer is there to be found.

How do you manage this?

Again, you start looking at the human/emotional problems, and address them as such. One thing I'm sensing strongly from the description you've provided is insecurity. It seems like Joe may be genuinely insecure, and he's hiding behind a veneer of arrogance. Where does that come from? Maybe he didn't go to a highly selective university? Maybe he's a VB developer in a world of C++ developers? 

I don't know, you didn't tell me. I'm not even sure if insecurity is the issue here, because I don't know this guy. But bear with me for a bit--I just want you to see what it means to look at the human issues of management, not the superficial, "how do I enforce my will on this guy" issues of management.

So where would this lead us? Suppose you think about it, you have a few long conversations with Joe, and you decide that he is feeling insecure about something. How can you make him feel more comfortable? Will that make him happier and better adjusted? Maybe he needs some specific training to be great at something. Maybe he needs to pair program with someone who can encourage him. Maybe he needs to feel more loved. Actually, that last one is almost always true.

I've dealt with a lot of difficult management solutions, and the answer has always differed dramatically depending on the individual. A lot of times, they can't be fixed. But as a manager, you have to understand the individual as a human before you can start thinking about fixing them and fixing the situation and acting correctly in the situation in order to get the best outcomes.

So, here's what I recommend you actually do as a course of action.


Meet with his peers and former managers. Have a conversation that tries to get to the heart of what his human/emotional problems are. Is he immature? Just generally unintelligent? Unhappy? Depressed? Insecure? Arrogant? Emotionally unintelligent? All of those are different diagnoses for the real cause of the problem and all of them have different prescriptions.
Talk to him personally and privately, at length. Let him do most of the talking. Ask open questions. A good one is "How did that make you feel?" It uncovers a surprising amount of stuff. You would be shocked to learn how much better I got at managing human beings when I learned to ask people "How did that make you feel?"
Form a hypothesis of the root causes of his issues. Pick a course of action based on what you think the core problem is. 
It may work. It may not. If it doesn't, that's too bad, but life is too short, and you're not paid to solve his problems, only the company's problems, so follow whatever advice you see elsewhere in this thread to get rid of him.


Once again, and I apologize for going on soooo long here: the other answers I'm seeing here are mostly summarized as "You need to be very strict." Well, being strict works well in one situation and one situation only: an emotionally immature person. If, indeed, his problem is emotional immaturity, that's the way to go. If his problem is depression caused by a cycle of learned helplessness, strictness will have the exact OPPOSITE effect than the one you want. Life, and management, and people, is just not that simple. There are many "diseases" that make a person be a bad employee and each one has it's own medicines. Some of them have cures. Some don't.
AnswerKenntnis:
Your behavior will need to be, in this case, entirely professional. While team lead, you should refrain from any non-professional conduct involving this person so that they will only see you in an authoritative light. This means no discussions about sports, video games, etc. that are considered casual and will put you on a peer level with the others. This can be tough, especially if you've been peers with your new underlings up to this point, but it is essential for new leaders that have been thrust into such a position. You will be able to return to these intimate behaviors later, but early on it is important that you differentiate yourself.

Early in the project, you will need to establish your authority. Give very clear and direct orders for this person to fulfill, and in such a way that there is no ambiguity about your direction or intent - this person will either submit and all will be fine, or they will balk and you will have an open-and-shut case of insubordination to bring to your superiors as to why this worker is unfit for the position.

There can be no backing down. The moment you give in, you will have lost all credibility and authority with not only the problem employee, but the rest of the team as well.

Edit (incorporating Kevin's comment)

Be sure to keep impeccable documentation. Its tempting to give orders orally in meetings, but even if you do, you should follow those up with an email (which should be CC'd to relevant parties) or a task in your project management system that enumerates your expectations for that particular task. This is best practice anyway, but it has the side benefit of providing a complete and undeniable paper trail that you have been properly communicating with your underlings.

Edit (due to mass confusion)

My point about casual discussions (ie: sports, video games, etc.) needs to be taken with a grain of salt. I'm not suggesting that you shut down all non-work interaction with your office mates entirely, nor that you shun them when they try to discuss such things with you. Rather, such interactions need to shift into an appropriate supervisor-employee role, at the very least until your underlings have adjusted to you in that particular role.
AnswerKenntnis:
I've been in the same situation many times, and the most effective method I know is removing him from the team. No serious company can afford conflicts in teams. According your description, he is not a team player.

If you really need to keep him, here is what I suggest (this apply to any team member):


Don't consider him differently than the other team members. One of the biggest mistake is to consider him differently. This works particularly well with arrogant developers.
When he makes you feel uncomfortable, angry, anxious, excited, [put any emotion here], remember that every thought you will have at that time will be biased by the emotion. It's better to wait for the emotion to disappear before analyzing and making decisions.
When a conflict arises, try to take him in a face 2 face meeting. Don't do anything in front of the team.
Listen to him actively, by asking many questions. Seek to understand him before trying to be understood.
Ensure that decisions are made by the team with him included. His opinion should be as important as other team members opinion.
When you are wrong, accept it. Sometimes, he will be right, like anybody.
Be genuinely interested by him. This usually unlock difficult people. He must understand you are not a threat, but an ally.
AnswerKenntnis:
Let's see... some good answers have already covered a few on my list:


Be professional - a no brainer anyway, but avoid, at all costs, judgement calls about your potential problem employee.  If you really need to vent, ask your boss for the opportunity to talk through the issue, don't ever let his problem or your problem impact how you interact with your team.
Be direct - give work, expectations and deadlines.  Involve him as much as you would any other employee and stay willing to listen to ideas - but make sure you are impeccably direct.  I think one the tricks with problem people is that it's easy to say "do X task by Y date" but there's another set of elements of how you want the work to be done - how much bragging?  how much testing?  how much is due diligence vs. over engineering?  


Sadly, this will also have to include being direct about his behavior (dress code, watching non-work videos, off color humor) - as his day to day manager it is not just your right, it is your duty to help the rest of the team survive this guy, and that means setting the expectations point blank.  
Being direct also means calling him on bad behavior when it's happening.  You are actually in a good position as you already know some of his problems, so you can be on alert when he repeats previous problems.  The best time to halt a behavior is when it's happening.
And - being direct also means giving feedback on work - when you review his code (which you should do with some regularity) and find that you'll need to throw out 50% of it, then his job needs to change from writing new code to fixing code to meet your expectations.  Not your minimum expectations, either, the expectations you have for everyone.  And that doesn't mean a delay in deadlines - if this is something that should have been done right the first time, then he needs to do the work and keep on schedule.

Keep records - Personally, I like the giving tasking via in person conversation, since it (usually) reduces tension and improves shared understanding.  But there's no harm in leaving the meeting and following up with a friendly email reminder to everyone.  The best part is that once it goes through email, it goes through whatever corporate email scrutiny and backup processes you have, which means it's part of a more legal record.
Remember - you own team health as well as the technical solution - A lot of the behaviors you've pointed out are problems that can be counter to a healthy happy team.  One of the things to think about as you work on this problem is that you are not just the owner of the technical solution, but also the productivity of the team while they build the solution.  It can often be hard when you think about going head to head with someone - as if the situation is innately adversarial.  I've often made much better headway when I think of the problem behaviors as something that is hurting the whole team - then I can look for ways to change the context and isolate the problems so that I don't have to go head to head with someone - instead I can develop a team that is so healthy that it fights the "virus" in its midst.  Probably my biggest guidance here is work on overall team trust - if the rest of the team trusts you (and vice versa) it will be very hard for the bad behaviors to get much play or to do your group much harm.  
do have 1 on 1s - not just with your problem guy, but also with other people on the team.  IMO, team meetings are for status that everyone needs to hear, and problem solving.  1 on 1s are for the things no one wants to get into in public - what's going well, what's going bad?  What does a given employee want most of the job?  What's missing?  What's great?  This is a good way to build a healthy team, and also a good way to address problems 1 on 1.  


That's all the really good for all purposes advice.  Here's the way more political and harsher underside of management:

Know the blame lines 

What exactly is the direct chain of command that says whether this guy stays or goes in the company.  In other words - who gets the blame for a problem person remaining a problem that the company is paying for?  With the first line of technical leadership, you cannot automatically assume that the blame line involves you - for several of my first leadership situations, I was not the person who approved vacation time, raises, bonuses, employee evaluations or disciplinary action.  Often on short technical projects that responsibility can fall on a bigger section manager and not on the day to day technical lead.  Know this guy's blame line.  Get to know at least 2 tiers of those individuals - develop a collaborative relationship where you are checking in about the more trivial day to day stuff, so you already have a point of trust if you need to bring in problems.

Learn the company's employee evaluation process

Most importantly - is it you that will be the frontline reviewer of this process?  If not, who do you give feedback to, and why isn't it you?  But also - what ratings and guidance has this guy received in the past?  I'll bet ya that he's already gotten some pointed feedback about his behavior in the past, and most companies keep records on this.  Get the records, know what he's been told. 

One of the roughest parts of engineering management is that due to the speed of projects, there is sometimes no longevity of management, so a problem person can get bumped around between managers and each switch is a chance to revert to old behaviors.  You don't have to keep this a secret - if you end up leading him, you can sit him down and say point blank - "I checked up on previous evaluations, and I know you're working on improving X, Y, and Z - I expect you'll keep working on those skills and behaviors on this project and I'm ready and willing to help you improve here."  Obviously use your own words.  What I just wrote makes you sound like you're impersonating a human being. :)

Talk to HR and learn about the process of how someone gets fired

Yes, this is truly a horrible process.  There is no good way to fire someone for being incompetent.  No matter what the process is, it will trump any other process in terms of horribleness.

The only thing worse than knowing about the company's termination process is NOT knowing.  Not knowing will get you in plenty of trouble, knowing will let you prepare yourself in case you have to go there. 

A legally minded company will have a very formal process and your HR should be prepared to coach you.  This does NOT mean that you should step up ready to fire the guy - this means that you should know what it takes.  There are often some truly bizarre (to a sane person) nuances to how this works, and forewarned is forearmed.

Keep your own records

You do NOT owe your employee every thought that crosses your mind.  

Keeping records in general is a good idea, because it lets you remember the good (and bad) stuff that EVERYONE on the team does - so when it's time for the fun part of management - setting bonuses and other cool rewards - you can do more than give a generic gift.  One of the best things ever is having your management give you a bonus and a note that specifically calls out some of the awesome contributions you made to the team... I guarantee that employees remember the cool note much longer than they remember how much money the company forked over.

And, sadly, having records of issues for a problem employee is often part of the firing process.  It's also part of the sanity-keeping process.  The human mind likes to forget pain and suffering.  It's very easy at review time to conveniently forget just how many issues have come up, and when you do have to have a painful conversation, it's worse if you are not grounded in some details of specific issues and patterns.

Use your management chain

When I was an individual contributor, I was happy to stay as far away from management as possible.  Once I became a manager, I realized how invaluable a good working relationship with management can be.  Of course it totally depends upon the competence of your management - but often higher levels of management are composed of people with plenty of tricks in their tool kits.  They know the culture and they know the biggest red buttons in the company - so they are in a good position to give you help.  Also - if you are sharing your grief on a regular basis, they know where you stand in case of any political hijinks.

Do not assume that no sign of trouble means no trouble

I could be completely washed up, because this is just one snap shot in time.

But what you describe is a series of very defensive behaviors that seem to be coming from someone who has some serious problems with work behavior.  The fact that you have been promoted and he hasn't tells me that your management is smart enough to realize the issues on some level.

Most disciplinary action is taken in private one on one sessions.  Calling someone out in public rarely does any good - and when it comes to the very serious "shape up or ship out" discussions, they are ALWAYS in private.  Which means that no one who is NOT in the direct management chain should be privy to these. 

This is my biggest rational for most of the other advice -- it sounds to me like not only do you have a guy with some problems, but that you have decent enough management that problem-fixing may already be underway, and mostly what you need to do is establish how you fit in to the problem-fixing cycle and then continue the activities as needed.

The final hopeful finish

It's not unusual for people to change radically when dealing with a manager from dealing with a peer. All my cautionary advice may be pointless as it very well could be that this guy will be sweet as pie when you take the lead role as you have now suddenly become "The Boss".  Different people deal very differently with authority - for signs of how this guy is programmed, check out how his current boss deals with him.

In fact - one of the best tricks is to watch other managers.  I'd bet money that his current management does some stuff great and some stuff absolutely badly.  Look for the good tricks, and keep and eye open to blind spots in your current management hierarchy - be ready to try new things but also be aware of what works and therefore doesn't need to be fixed. :)
AnswerKenntnis:
You are in a tough spot, because if this guy wasn't fired yet then he either has political importance or you work for an organization that will never fire people for performance.

Pray it is the latter, that is much easier to deal with.  Politically important troublemakers like that can ruin you despite ALL of your best efforts to be professional, cover your *, and document EVERYTHING.  I have seen it happen.  The paper trail means nothing when politics are involved.

Be Professional - Don't try to be friends with anybody on the team.  Act as if you are on a mission the entire time and do not show favoritism.

Protect yourself/Document EVERYTHING - Don't make silly mistakes, you are being watched.  The sharks are attracted to blood in the water.  Also make sure that you fully document all decisions, email trails, discussions and team consensus.  When you get dinged on something you will want to pull up that email from 6 months ago.
AnswerKenntnis:
Just keep your humor. If you can't get rid of him, the worst thing you can do is let him bother you. He's not a big deal. He wants you to be nervous. He wants to be "that guy".  Let him be "that guy".  Just keep your good humor and always look on the bright side.

Handle him. Don't let him handle you.
AnswerKenntnis:
You have had good advice, my answer aims to complement what others have said.

Remember that you are in charge, so you set the parameters at all times. This may sound very strong, but with someone who is going to push on your boundaries you are going to need to make those boundaries clear and unambiguous, leaving no space for someone to undermine you. If everyone understands clearly what is expected of them, it is very easy for you and them to know if they are doing it right or not. This will apply to your whole team, obviously, but as a leadership approach it works.

This also means that ultimately as you will be considered responsible for the project as far as your managers are concerned, you have the final say in how things are done. That doesn't mean you don't consult the whole team or whichever members are your experts in a particular part of the project before making a call on something, but as it will appear to everyone you are reporting to that the decisions made are yours you had better make sure that they are.

There is a term we have which is "firm but fair" - that is a good way to treat everyone in your team - with clarity, consistency and transparency, but acting in a way that does not open the door to people questioning your leadership or their standards dropping without very good reason. 

Professionalism is typically unemotional, so stay calm as far as possible. Don't buy into other people's emotions either. Understand them, but don't allow yourself to get caught up in them.

Think about likely situations that might turn into a confrontation and see if you can find a way to head them off. If someone has a lot of experience in a field, they may well feel that their experience gives them the right to take some control over what you are doing, for example, but you can narrow it down - in what way does their experience fit them for control? Would it not work better if they shared the relevant parts of their experience with the rest of the team? Normally this will ultimately come down to a specific complaint and by narrowing things down to the exact issue they have, you can probably address it in a way that avoids bad feeling.

Where possible avoid one on one meetings.

When everything is going smoothly, make sure life is as good as you can for everyone on the team.
AnswerKenntnis:
I agree with Joel, all the answers are on the same frequency, everyone is giving suggestions as peers, not as managers.

Before dwelling in the human side of the problem, I think there's a meta-question you have to answer yourself.

As the leader of the team, your responsibility is for the team to deliver, on time, on budget.

The meta-question to be answered is, you wanna do it with this guy, let's name him Dick, or without him?

You wanna make it with Dick

This will make you sweat...


You need to understand what motivates him
Need to give him tasks suited to his skills
You will understand what skills are missing, and coach him on those
You will spend a lot of time arguing and positioning, which may damage your position as leader in the eyes of others.
If it doesn't work at the end, the team will lose.
If it does work at the end, your energy will be drained, but there's joy in helping someone be a little bit better.


If you have the time and the energy, take the path of the enlightenment, you are made from the battles you fight, not from the battles you win. Risk losing.

You wanna make it without Dick

This I call, damage control mode...


You won't put Dick on the critical path
Dick won't have sole ownership of any piece of the project
The priority is to keep the guy busy and the project moving, not to keep him happy, not to keep you safe.
You will need to prevent Dick from undermining the morale of the team:

Recognize loud and clear the good performers.
Make everyone to understand the deliverable of everybody.



Because life's too short to work with Dicks.

Only you know if it's worth it.

There is an alternative path if you have someone in your team who does get along with Dick, you may proxy through her.

My two cents.
AnswerKenntnis:
While I very much like both Pierre 303 and Jarrod's answers, I would also suggest keeping track of his code submissions. 

If you are a strong lead, you may be able to mentor him into being a successful teammate. Sometimes being straightforward with him and actually tell him what others have said behind his back (finding the most politically correct way of course, without mentioning anything along the lines of "well so and so said this about you") may open his eyes and make him either 1) look for help in correcting his ways, or 2) look for work elsewhere.

However, if all good intentions fail, then keeping track of his submissions (you are using version control I hope?) will give you clear indications of his intentions. If he's obviously trying to sabotage your project, approach him about it. If he continues or denies it, bring your proof to upper management. Sometimes bad apples just have to be thrown out to allow a project to thrive.
AnswerKenntnis:
Establish boundaries and expectations

First off before the project even starts you need to put this person in his place right from the start. You must clearly state in a private one-on-one setting your expectations. Start of by stating what he can expect from you. Then state what you expect from him. Do not let interruptions take place while you are speaking. If interruptions start to take place be firm in your tone and express that you will not tolerate that type of behavior. Follow this up with "Is that understood?"

Start With A Short Leash

You must keep this person on a short leash. You and only you decide when and if you extend the leash. By keeping him on a short leash from day-1 he will get used to it (eventually). He may not like it but he knows the boundary. If you give him a little more leash as his attitude improves he will adjust to that new boundary. If he gets out of line you just retract the leash back to the original short length. He may not like it but it is something he is already familiar with.

If you start on a long leash and then try an real the leash in, the resistance will be much worse.

Under No Circumstance Tollerate Insubordination

Make sure you make it known you have been put in charge and will not tolerate any insubordination. Make it clear that he can approach your supervisor to discuss what ever is bothering him but he must inform you and you will set up the meeting. 

You need to discuss this with your supervisor ahead of time and the two of you need to be in synch on this. If he approaches your supervisor without you setting up the meeting, the first question the supervisor should ask is... "What did aceinthehole say about this?" This is very important.

If he challenges you in a group setting pinch it off immediately. Say something like, "This is not the time to discuss that, we will talk about it after this meeting." If he continues to push the issue then you must... 


Ask he stop talking... "That is enough, I told you this is not the time."
Bring the meeting to a brief hault and excuse yourself.
Escort this person to a private setting and deal with the attitude. (keep this short)
If he cannot return to the meeting without being cooperative and cordial then send him away. Tell him you will discuss this after the meeting. Retun to the meeting by yourself.


Do not let him bully you in front of the other team members.

Ask Your Supervisor For Guidance

Don't be afraid to ask your supervisor for guidance on how to handle situations you are not comfortable with. Do this in a private one-on-one setting. Don't worry that you will appear weak in front of your supervisor, you will not. Your supervisor will more than likely appreciate that you asked for their assistance.

The same holds true for you. let your team members know that you are willing and open to discuss issues with them and that those issues will remain confidential.
AnswerKenntnis:
Some good answers here but for me there's a fundamental issue that nobody has addressed: you've decided what this guy is going to be like on your project before it's even started; you're judging him on something he hasn't done yet. While it's very much worth having a strategy for dealing with problems you might encounter with him in your back pocket, it's ESSENTIAL that you give him a fair chance and treat him the same way you do the rest of the team. Don't single him out, don't assume it's going to be a disaster, do think positive.

If you convince yourself that your project is doomed to failure, whether it's because this guy is on the team or for any other reason, then you can pretty much guarantee that it will indeed fail. For a project to succeed you have to believe it is going to, you have to believe in yourself and you have to believe, and trust, in your team. Above all you have to be fair.
AnswerKenntnis:
Could you ask your manager or the project manager to assist you in dealing with this problem individual?  That would be my suggestion as you may need backup for when he does jeopardize the project which could well happen.  If you don't make sure that these other managers know what is happening you may get blindsided as I'd imagine the problem individual may talk a good game to management which is how he has stayed where he is.
AnswerKenntnis:
The best approach is isolation. Assign most mission critical parts of the project to core team, leaving the rest for him. You can even come up with some useless feature idea, just to keep him ocupied. If he produces bad code, you will produce bug reports/cleanup requests. Just make sure that whatever he does has very little affect on the rest of the system. Yet somehow, you must convince him, that what he does is extremally important and you will fail if he won't deliver quality code.

This may seem like a total manpower waste, but surprisingly is quite contrary. The problem is that such person not only isn't helping. He is also producing turmoil/problems or killing productivity in other ways. If you won't isolate him, your team and your project will suffer.
AnswerKenntnis:
Most answers here are focused on accountability, which in my mind is setting the person up for failure, or at least there is an assumption that the person will fail. All may very well be the case in the end, but I do not think that entering into a situation presuming failure will create an atmosphere in which success is likely.

To Joel's point - remember first and foremost that the person is human. Do as he suggests: consider the person, their state of mind, and their needs before taking action one way or another. What Joel underscores is that management is never prescriptive - there is no single way to manage people, or even a person. So yes, consider the person you are managing, before you manage them. 

But also consider the situation they are in. You may find for example that a person who performs exceedingly well in one context, might perform poorly in another, and in this case vice versa. The fact that this person does not perform well (in a professional or social context) does not mean they can't perform well, just not well under the circumstances. 

So, consider their state of mind, and consider the circumstances they are in -- circumstances mind you that might have been around for a very long time. The key in my mind is how can you help this person be and feel successful? Success, no matter how small, will boost confidence and probably have other positive effects as well. 

To make them successful, perhaps what you can do is ask more directly what you can do to help. Perhaps it is running interference with other managers, perhaps it is getting the person out of meetings, perhaps they just need help finding and staying in their zone. Or perhaps they don't know what they need. In that case, it is your job to guide them. They might be resistant. Focus first on giving them small tasks that they can accomplish and for which you can give very specific direction. This might feel like micromanaging, but believe it or not, there is a time and a place. This might be it. When they succeed, praise them. Recognize them in front of their peers. Send an email to your manager and cc them. Make them feel valued. As their confidence picks up, slowly back off on the praise and direction you give them. 

Perhaps an analogy could be drawn to learning how to ride a bike, or how to snowboard for the first time. If you are left to teach yourself, it is quite likely that one bad fall, or a series of them will produce a feeling that you can't do it. That you are incapable. You might never say that out loud, but what you will do is direct a lot of anger at the equipment and the people around you. In order to learn how to obtain those skills however, you  eventually need to receive instruction in the form of very specific, very rudimentary advice. A good teacher will take you back to square one, help you center yourself, help you reset what you have learned thus far, and then take you through the process of learning again, but more slowly. They will praise you a lot. Eventually you will be able to do by yourself, and when that happens, they will instruct you less and praise you less, because your success in learning will be motivation enough. As you succeed you will be less angry, and so on and so on. You can see how this works right?

Interestingly, and not to abuse the metaphor, you can see how if after you have achieved mastery at snow boarding, someone comes to you and asks you to ski, the process of frustration and anger can start all over again. Why? Because you find yourself in a new situation that requires skills you don't have yet. In comes the instructor to help put you on the right path again. 

So my advice? Be patient, consider the person and their needs, and manage to the situation as much as you do to the person.
AnswerKenntnis:
My suggestion is to be very transparent with him. Be professional. As a team i understand that others will back you up if this guy tries to harm your project.
AnswerKenntnis:
Remember that human relationships trump just about everything when it comes to human interactions. People behave irrationally, but they respond to their own emotions. You can use that to your own advantage -- not by being manipulative, but by being responsive (almost the same thing, i know, but the difference is in how it makes people feel).

First of all, document everything -- all instructions, all responses, all interactions. Don't make a big deal about it, and don't include make the record look subjective. You want to be able to show it to anyone without making them feel defensive. 

Find out what people want, and use that help them take appropriate action. Find out what your uncooperative co-worker would like out out of the project; what the picture of an ideal environment is in his head. Once you know what he wants, you may be able to tailor the working environment to accommodate it without disturbing everyone else. Often just asking someone what they want is enough to change their attitude.

Don't invoke your authority if you don't want a fight. Don't give an ultimatum unless you want them to be resentful. Don't back someone into a corner unless you want them to snap at you. Sometimes you want someone to get angry and leave, but that's not usually the optimal outcome. You're "in charge", which means you're responsible for setting the team's direction. But don't use that as a club to beat others down and then expect them to help you. In the end, you have to make the decisions, but you're better off making them feel like it was their idea.

For most people, being in charge is a means to an end, and not the goal. They want to be in charge so that they can have what they want. But if they can get what they want without the responsiblity of being in charge, then they'll gladly work for someone else. Helping someone want what they have is just as good has helping them have what they want.

And finally, if someone is a poor fit for your team, then they shouldn't be on the team. If they don't have the skills for the task at hand, then get them off your team.
AnswerKenntnis:
There are some key points that I would address.

I consider it a failure on my part when a member of my team fails.  There will almost always be someone on your team that poses a challenge.  It is your job to make sure that your project succeeds and your team succeeds.  You appear to have an attitude of "This jerk is going to ruin my project".  This can be a self fulfilling prophecy.  Change your attitude towards him.  Find his strengths and leverage them.  

Treat him with the respect he desires.  Your attitude towards him will affect how your team views him.  You can poison your team against him or you can give them hope.  If you poison them you will get what you expect.  If you provide hope then there is hope.  Even if his work forces you to marginalise his efforts you can keep your team spirits high which will improve the entire experience.  And this does not mean you should treat him like he is above anyone else on the team.  Just treat him as an equally valuable member of the team (Even if he is not).

Use Build, Break, Build method of coaching Start by complementing on of his accomlishments(preferably something signifigant and recent), Then point out the problem, explain your expectation going forward, and get him to commit to making the change. Then build him back up with your belief in his ability to make the correction and continue to function as a valuable member of the team.

Set High Standards for the whole team Do not accept marginal from any of the members of your team.  Make sure your expecations are clear and unwavering.  Do not cut slack for anyone or set a higher bar for anyone.  This can be percieved as favoritism.  Better to the jerk with unrealistically high expectations of the team, than the guy that lets his favorites get away with murder (Even if you don't).  The former can help your team bond together to get the project to work.  The other will create dissent and demotivate.

Encourage Peer Mentoring Everyone on your team has a strength.  Leverage those strengths to help your team grow.  Have your team seek out the assistance of other members of the team to refine or improve their code.  But do not assign someone to look over someone elses work.  It will be more productive to have your team working together and asking each other for assistance in improving than critisizing each others work.

Talk to the teammember's manager He is still employed there so someone sees something in him.  Otherwise he would be in the datacenter writing migration scripts.  Find out what the manager has found to be effective for motivating, what (s)he feels his strengths are, and how they would like for you to help him as his lead.  

Throw out your old judgements and bias They serve no one least of all you.  You are in a different dynamic than you were previously.  Give him a chance to show you his value.
AnswerKenntnis:
I personally think the right course is probably the hardest one to take. Be honest with him and let him know what you're feeling. Most people who are jerks aren't trying to be jerks, they just don't know they are, and feel horrible when they discover that they are. If you approach him in all humility and respect and let him know that you think there might be feelings of unfairness on his side due to your new role, I'd bet his response will be of equal respect. Like Joel said, he's just a person. Think about how you'd want to be treated if you were in his shoes.
AnswerKenntnis:
You need a Mediator / Judge

(from a Soft. Dev. plus H.R. / psychology guy)

You cannot solve this problem by yourself. Whatever you do, "its already out of your hands", you require a person higher on your company with more authority.

I have seen several cases like that. That employee won't follow your guidelines, it doesn't consider you as "the boss".

Besides, even if he is right, and he should be given that job position, he must understand it wasn't your decision who is going to be the project manager.

In theory, it should be someone from Human Resources, but, in practice H.R. people try to be too much nice, so better find an admin. with negotiations skills, otherwise that guy will break your project.
AnswerKenntnis:
It seems that no one recommended the author to think about what it cost to be a team leader. Obviously being a team leader limits your ability to program, learn new programming stuff, hack a lot and become more and more professional developer. That is because a team leader receives a new responsibility - to help other developers, fix their bugs or just point them what to do / what to fix. Then all these endless meetings. I can not tell for an author, but for me this would be a perfect step(may be even with 15-20% bonus in salary) but in a wrong direction. Like getting extra 20% for converting my job from  passion to something where I never wanted to be.

-Dont take this responsibility. Someone should be a leader, someone should be a cleaner in toilets,others destiny are even worse, but you can be a programmer and just increase your hourly rate / skills day by day. Working with computer and working with people are skills which are rarely both on a high level for one person. 

-Other programmers should not affect you. They can go to job nude, watch their porn videos,do other crazy stuff - but that is their lives and your boss pain. Just demand such work conditions, like working from home or in an izolated room, where you feel yourself perfect and ready to code

-In case your work on a project and you are blocked by someone from your team - write them politely email with your boss in CC. This is bad - "Hey, I am waiting for your functions str2int for 2 weeks, so I just don't do anything at all!".       This is what I do - " Hi, xxx. It seems that you can not deliver the function str2int to me, so I just replacing it with my own implementation, which is slow but mostly works. If you are going to add it - just confirm to this interface ".

I hope that non standard answer may help an author, or someone else to do his job well and remain happy.
AnswerKenntnis:
A good solution requires an in depth understanding of the working environment. The fact that this guy has gotten away so far without being checked reflects badly on the situation. It is very easy to deal with such people provided you are supported by the management. Lousy management would be an even bigger concern than that guy.

One strategy I've used successfully in the past


Make him responsible for something, and hold him (and everyone) to his responsibilities. A weekly/daily status check meeting, with proper visibility to upper layer would make sure he does not fallback and enjoy trouble-making while leaving you to "handle" as a team-lead. I used to make co-leads and made sure they are the ones to send me a daily status check of all members. 
Do peer performance reviews, like every member of team answering basic question like how satisfied he/she is with peers. This is sort of a political tactic but if he's to feel pressure from everyone and not just you - he may just breakup or behave accordingly. Needless to mention, you can always pass on bad evaluation to upper layer.


I'll suffice with these :)
AnswerKenntnis:
most answers i see, i think, focus on the wrong problem.

i can't help but think that the OP is disillusioned, this is only half the story

leadership is hard, i can't help but wonder that if i was in the poster's shoes, the situation never would have evolved to the point that a peer disrespects me

developers don't "report" to the tech lead, they report to managers, "tech lead" (in the sense of the guy who knows the most on a team of 6 devs) doesn't get asigned on healthy team, a healthy team organically self-organizes and people naturally grow into ownership of functionality. that teams often don't self-organize is a culture problem at some bigcos for various off-topic reasons, see jurgen apello's writings (noop.nl) for further reading
AnswerKenntnis:
Give him the dirty job -- writing documentation.

You should soothe his ego at the same time by explaining why HE is the best person to be doing it:


Documentation needs to communicate the big picture, and naturally none of the other developers are as capable of even seeing the big picture, nevermind explaining it.  Only an "architect" like him can tackle it.
Reading code requires understanding any kind of code the other developers write, in all sorts of styles, and involving all the tricks of the other developers combined.  Only an "expert" like him can tackle it.
Documentation is very important to project success, so naturally you want your strongest team member -- him -- to take it on.


Or stick him with writing test cases; the same arguments can be made.

This has multiple upsides:


He won't be checking in any code.
He'll have to read and learn from the code of the other developers.
If he doesn't understand he's been had, great.
If he does understand he's been had, he can't really complain without also admitting he isn't the omniscient superhuman programmer he's been claiming to be.


Most likely, he'll be posting a topic here about how his team lead repeatedly ignores his suggestions about how to improve the code... at which point none of us will believe him (we've had those types before) and will tell him to focus on reading and understanding a wide variety of code because that's a core skill for someone writing documentation.
AnswerKenntnis:
I find a lot of the answers very cautious.

Remember you are responsible for your career no one else is. This means that by agreeing to work with this person you are saying that you are accepting this assignment and responsibility and that you will be able to manage the project (and this person) to its fruition.

If you don't think you can do it, then don't accept that responsibility: have him assigned to another project, or ask that they give you a leadership position on another project.

I know your question is about "how to deal with this person", but based on what you are writing it sounds to me that he is not worth any effort, you'll be busy enough proving yourself in this new leadership position.
AnswerKenntnis:
I have also seen this type of job. Some people have extra talent and obviously have risen to a high position, but you have to remember that he is older than you. 

So respect him. But never forget you are the team leader and you have to manage your team. You have to maintain a proper understanding between team .//// and go on.
QuestionKenntnis:
Best Java book you have read so far
qn_description:
Which Java book do you think is the must-have one for all Java developers?

Keep in mind:


One book per answer
Check for duplicates before adding new answers
AnswersKenntnis
AnswerKenntnis:
Effective Java, Second Edition by Joshua Bloch. No question.



If every Java developer read this book, there would be a lot less broken code in the world.

After that, I'd read Java Concurrency in Practice (see separate answer), and maybe Java Generics and Collections (see separate answer). Anyone that reads and puts into practice the information in these three books has come a long way toward mastering Java.



Comments from duplicate "Effective Java" answers:  

"I sure wish I had had this book ten years ago. Some might think that I don't need any Java books, but I need this one."
        - James Gosling, Fellow and Vice President, Sun Microsystems, Inc.

sammyo: It's a thin(!!) volume that focuses on real issues and how to think about the right approach to java problems. (as opposed to a listing of API methods) 

Hans Doggen: First edition of Effective Java and then the second edition, to see some of the ideas that changed over time.
AnswerKenntnis:
Thinking in Java by Bruce Eckel





Comments from duplicate answers:  

prash: Thinking in Java by Bruce Eckel is a great book for beginners and teaches you not only the "What"s and "How"s of Java but also the "Why"s. It is available from the above link.

Michael Easter: It is an introduction and yet discusses the background behind Generics, Swing, elementary threading, and a large metaphor for Java NIO. It is a massive work that covers the range from beginner to expert. There are other books that are better for experts but would be wasted on novices.
AnswerKenntnis:
When it comes to multithreading, Java Concurrency in Practice is the choice.
AnswerKenntnis:
Head First Design Patterns - not necessarily a pure Java book, but essential for every Java developers who designs his applications himself.
AnswerKenntnis:
Java Puzzlers is another great one by Joshua Bloch (with Neal Gafter). 



The entire content of the book is just small Java applications that are quirky enough that they don't necessarily behave how you might immediately think.
AnswerKenntnis:
Head First Java is great for beginners.



Effective Java will take you from journeyman to master.
AnswerKenntnis:
Refactoring by Martin Fowler



Especially the chapter about Bad Smells in Code should be understood by everyone.
AnswerKenntnis:
Java Programming Language is a good way to learn Java. I would highly recommend it.
AnswerKenntnis:
Core Java Vol 1 and Vol 2 by Cay S. Horstmann.

Hard to read but very informative and without excess words. These books cover every aspect in Java SE. But this book will be a little hard for beginners IMO.
AnswerKenntnis:
O'Reilly's Java in a Nutshell is a good book for both tutorials and reference.
AnswerKenntnis:
Java Language Specification (also freely available online) is great if you want to get deeper into the semantics of Java language.



(Links and comments above merged from a duplicate answer by folone.)
AnswerKenntnis:
Java Generics and Collections by Maurice Naftalin & Philip Wadler. Philip Wadler is one of Java generics grandfather. Java is close enough to C++ that it wasn't a big deal at all for me to switch over, that was until I started using Generics. This book is a gold mine of info.
AnswerKenntnis:
Better, Faster, Lighter Java by Bruce A. Tate and Justin Gehtland



It's a really good one.
AnswerKenntnis:
Filthy Rich Clients, by Chet Haase and Romain Guy.  Those guys are Swing ninjas.
AnswerKenntnis:
If you want to understand, how it all works, The Java Virtual Machine Specification (also freely available online) is the book for you.
AnswerKenntnis:
Data Structures and Algorithms in Java by Robert Lafore. Nice book.
QuestionKenntnis:
How can a code editor effectively hint at code nesting level - without using indentation?
qn_description:
I've written an XML text editor that provides 2 view options for the same XML text, one indented (virtually), the other left-justified. The motivation for the left-justified view is to help users 'see' the whitespace characters they're using for indentation of plain-text or XPath code without interference from indentation that is an automated side-effect of the XML context.

I want to provide visual clues (in the non-editable part of the editor) for the left-justified mode that will help the user, but without getting too elaborate.

I tried just using connecting lines, but that seemed too busy. The best I've come up with so far is shown in a mocked up screenshot of the editor below, but I'm seeking better/simpler alternatives (that don't require too much code).



[Edit]

Taking the heatmap idea (from: @jimp) I get this and 3 alternatives - labelled a, b and c:



The following section describes the accepted answer as a proposal, bringing together ideas from a number of other answers and comments. As this question is now community wiki, please feel free to update this.



NestView

The name for this idea which provides a visual method to improve the readability of nested code without using indentation.

Contour Lines

The name for the differently shaded lines within the NestView



The image above shows the NestView used to help visualise an XML snippet. Though XML is used for this illustration, any other code syntax that uses nesting could have been used for this illustration.

An Overview:


The contour lines are shaded (as in a heatmap) to convey nesting level
The contour lines are angled to show when a nesting level is being either opened or closed.
A contour line links the start of a nesting level to the corresponding end.
The combined width of contour lines give a visual impression of nesting level, in addition to the heatmap.
The width of the NestView may be manually resizable, but should not change as the code changes. Contour lines can either be compressed or truncated to keep acheive this.
Blank lines are sometimes used code to break up text into more digestable chunks. Such lines could trigger special behaviour in the NestView. For example the heatmap could be reset or a background color contour line used, or both.
One or more contour lines associated with the currently selected code can be highlighted. The contour line associated with the selected code level would be emphasized the most, but other contour lines could also 'light up' in addition to help highlight the containing nested group 
Different behaviors (such as code folding or code selection) can be associated with clicking/double-clicking on a Contour Line.
Different parts of a contour line (leading, middle or trailing edge) may have different dynamic behaviors associated.
Tooltips can be shown on a mouse hover event over a contour line
The NestView is updated continously as the code is edited. Where nesting is not well-balanced assumptions can be made where the nesting level should end, but the associated temporary contour lines must be highlighted in some way as a warning.
Drag and drop behaviors of Contour Lines can be supported. Behaviour may vary according to the part of the contour line being dragged.
Features commonly found in the left margin such as line numbering and colour highlighting for errors and change state could overlay the NestView.


Additional Functionality

The proposal addresses a range of additional issues - many are outside the scope of the original question, but a useful side-effect.

Visually linking the start and end of a nested region

The contour lines connect the start and end of each nested level

Highlighting the context of the currently selected line

As code is selected, the associated nest-level in the NestView can be highlighted

Differentiating between code regions at the same nesting level

In the case of XML different hues could be used for different namespaces. Programming languages (such as c#) support named regions that could be used in a similar way.

Dividing areas within a nesting area into different visual blocks

Extra lines are often inserted into code to aid readability. Such empty lines could be used to reset the saturation level of the NestView's contour lines.

Multi-Column Code View

Code without indentation makes the use of a multi-column view more effective because word-wrap or horizontal scrolling is less likely to be required. In this view, once code has reach the bottom of one column, it flows into the next one:



Usage beyond merely providing a visual aid

As proposed in the overview, the NestView could provide a range of editing and selection features which would be broadly in line with what is expected from a TreeView control. The key difference is that a typical TreeView node has 2 parts: an expander and the node icon. A NestView contour line can have as many as 3 parts: an opener (sloping), a connector (vertical) and a close (sloping).



On Indentation

The NestView presented alongside non-indented code complements, but is unlikely to replace, the conventional indented code view.

It's likely that any solutions adopting a NestView, will provide a method to switch seamlessly between indented and non-indented code views without affecting any of the code text itself - including whitespace characters. One technique for the indented view would be 'Virtual Formatting' - where a dynamic left-margin is used in lieu of tab or space characters. The same nesting-level data used to dynamically render the NestView could also used for the more conventional-looking indented view.

Printing

Indentation will be important for the readability of printed code. Here, the absence of tab/space characters and a dynamic left-margin means that the text can wrap at the right-margin and still maintain the integrity of the indented view. Line numbers can be used as visual markers that indicate where code is word-wrapped and also the exact position of indentation:



Screen Real-Estate: Flat Vs Indented

Addressing the question of whether the NestView uses up valuable screen real-estate:

Contour lines work well with a width the same as the code editor's character width. A NestView width of 12 character widths can therefore accommodate 12 levels of nesting before contour lines are truncated/compressed.

If an indented view uses 3 character-widths for each nesting level then space is saved until nesting reaches 4 levels of nesting, after this nesting level the flat view has a space-saving advantage that increases with each nesting level.

Note: A minimum indentation of 4 character widths is often recommended for code, however XML often manages with less. Also, Virtual Formatting permits less indentation to be used because there's no risk of alignment issues

A comparison of the 2 views is shown below:



Based on the above, its probably fair to conclude that view style choice will be based on factors other than screen real-estate. The one exception is where screen space is at a premium, for example on a Netbook/Tablet or when multiple code windows are open. In these cases, the resizable NestView would seem to be a clear winner.

Use Cases

Examples of real-world examples where NestView may be a useful option:


Where screen real-estate is at a premium

a. On devices such as tablets, notepads and smartphones

b. When showing code on websites

c. When multiple code windows need to be visible on the desktop simultaneously
Where consistent whitespace indentation of text within code is a priority
For reviewing deeply nested code. For example where sub-languages (e.g. Linq in C# or XPath in XSLT) might cause high levels of nesting.


Accessibility

Resizing and color options must be provided to aid those with visual impairments, and also to suit environmental conditions and personal preferences:



Compatability of edited code with other systems

A solution incorporating a NestView option should ideally be capable of stripping leading tab and space characters (identified as only having a formatting role) from imported code. Then, once stripped, the code could be rendered neatly in both the left-justified and indented views without change. For many users relying on systems such as merging and diff tools that are not whitespace-aware this will be a major concern (if not a complete show-stopper).



Other Works:

Visualisation of Overlapping Markup

Published research by Wendell Piez, dated from 2004, addresses the issue of the visualisation of overlapping markup, specifically LMNL. This includes SVG graphics with significant similarities to the NestView proposal, as such, they are acknowledged here.

The visual differences are clear in the images (below), the key functional distinction is that NestView is intended only for well-nested XML or code, whereas Wendell Piez's graphics are designed to represent overlapped nesting.



The graphics above were reproduced - with kind permission - from http://www.piez.org 

Sources:


Towards Hermenutic Markup
Half-steps toward LMNL
AnswersKenntnis
AnswerKenntnis:
I've attempted to answer my own question here, but this is incorporating the heatmap idea from @jimp and also the 'make it more XML-ish' idea from @Andrea:



Hopefully, the colors in the heat map along with the angular lines help draw the eye between the start and end tags; removing the horizontal line separators improves the 'flow' from start to end. As the user selects with an element the matching part in the heat map can be highlighted in some way - perhaps with a glowing border (as shown).

Edit
Have decided to go with this, there will probably have to be user options for the colours. A 'production ready' screenshot:



And for comparison...the alternate indented view:



Edit Now, for the more heavily nested case - testing my drawing skills...
AnswerKenntnis:
One idea might be to try and add 3D to the text. Increase/decrease the font size based on what level it's at.

For example, this code:



Would look like this:



That might be annoying to work with as it loses fixed text-size-alignment across different levels. Another idea; change the saturation of each level:



How well does that hold up for something really deep? Not sure...

I actually like your gutter visualization idea a lot; it's easy to group things together. Maybe combined with one of these idea's it'll look even better, or much crappier. ;)



A little while back I did a heat-map showing scope in C. Might be fun to look at for brainstorming:



Aligned-left:
AnswerKenntnis:
Just tweaking your original idea and switch from squares to capsules. I think these versions (including your original one) are easier to read because they are less complex then the one that shows nesting through nesting the display elements. I think tree elements convey the information in a simpler more intuitive manner.



I think the left is great for directly showing indentation, while the right is better at conveying a nested relationship.
AnswerKenntnis:
I love the idea.
My suggestion to keep the "busy" down would be to to use gradients instead of squares. It would cut down on lines. Perhaps different colors for extreme indentation. 

I would say everything you have have is great, though a little blocky for my tastes.

My comments: I have been constantly struggling with the way the Visual Studio IDE does indentation. I would love to use something like this or a variation. 



So imagine that link without the lines, and inline with your current xml / code.
AnswerKenntnis:
My idea:



The nesting looks more like nesting. The horizontal width of each layer doesn't need to be so wide.



What about just using the gradient?
AnswerKenntnis:
Since you said the visualization must exist in the non-editable (left?) margin, I believe that to mean the visualization cannot be intermingled or behind the code.

Perhaps a heat map in the left column, with brighter colors indicating deeper indentation? Make the margin a fixed size, with a visualization like what you have (expect going left to right like the indentation would) that dynamically uses all of the space given according to the maximum indentation as determined by the DOM depth.

If you were willing to branch into the editor region, I would suggest something very similar, but as a background of the document. The shaded area would be where whitespace would be if indentation were enabled. In this case, I would use a solid, light color that is contrasting to the text highlighting.
AnswerKenntnis:
jGRASP does this by using a visual marker in the margin: 



It even recognizes when you are using a loop and uses a different type of line to represent that inner loop. 

Just thought I would point out how an existing editor does it.
AnswerKenntnis:
Why not open and close parentheses?


Indentation means containment: ( and ) mean exactly that to programmers.
( and ) are each a single character: the left bar will stay very thin.
Empty elements are easily spotted: use () on the same line.
Content of an element does not need a visual clue: a blank is much better.
Cursor position on the right can be matched by the containing block on the left: dynamically add a color to the chars in the column with ( and )
You could make it more XML-ish using < and >, which look better from a distance.
AnswerKenntnis:
Vim can do something similar already, although not quite as pretty.

There are various ways of doing "code folding" in Vim.  One of them is based on a syntax folding rules.  When this is done the code can be folded using a nested outline structure, and the "FoldColumn" can be used to give a graphical (actually "character-based" with '|' and '-' chars) representation of the "foldlevel".

The foldcolumn can give the nesting representation regardless of foldmethod, but the syntax-based method is the one that would probably be appropriate for what you want.  I'm not sure if there are pre-made syntax-based folding rules out there for xml somewhere, I'd guess there may be.
AnswerKenntnis:
Not a bad idea but having to reference the left margin to clearly see my blocks might get a little annoying.  That's not even thinking about screen real-estate or what things might start to look like if the structure gets very deep.  

Since the motivation is to help users 'see' the whitespace characters they are using for indentation, you could just show them the white space characters. 

I'm not talking special visual characters like paragraph markers, just highlights.  Spaces in yellow, tabs in green(or whatever)

For the margin/nesting issue, you could just move the margin for each block in. There is nothing that says the margin has to be a straight line.

I'm sure this is not a new idea.

Something like this:
AnswerKenntnis:
I think you are on the right track with option B and C: include both width and heatmap coloring. I like option B more than C at the moment, because it is less intrusive (either being wide and diluted, or narrow and intense, rather than the very heavy block in the middle of C) One downside is that with that option you have to rebuild the whole graph if you insert a level somewhere.
I think you could make the blocks much smaller, 1 or 2 px would probably be enough. It doesn't have to be much, it only needs to be distinguishable. Especially when people are expected to use the editor many times, unobtrusive, more subtle effects are easier to work with because they don't distract as much.

One thing that is important when using an editor of sorts though is highlighting the current scope: when selecting a line in the editor, you need to see exactly what elements it contains, and where it stops. You could even highlight the tree up (what elements is it a child of). I think that is a separate issue that needs addressing and thinking out and will have more influence on how the users will rate their experience with the editor.
AnswerKenntnis:
Have a look at Xcode it does this by using darker shades of grey in the margin.
AnswerKenntnis:
One thing I haven't seen mentioned is what you can do with hue on top of the saturation effect you seem to have settled on. My suggestion is to change the color of the nest in which the pointer lies. This would make it easier for the user to distinguish which lines are part of the nest, versus siblings to it along the way.

When implementing hue-based stuff, please be conscious of color-blindness, and either select colors that are universally distinguishable, or offer a few options for people to pick from.
AnswerKenntnis:
One option, which could be used in conjunction with the other suggestions so far, would be to use a tooltip on the left margin which shows the path to the line using XPath notation. Browser "inspect element" tools (e.g. Firebug, the one built in to Chrome) often do something similar but in a status bar.
AnswerKenntnis:
Possibly you could have a collapsed view for the heatmap (from the original post) with only one column of colors and the depth numbers. This would allow them to know how deep they are and give them more screen real estate for the xml. Seems like a win win to me.

I am concerned whether there will be enough color differences to nest things deeply.
QuestionKenntnis:
Why is Mercurial considered to be easier than Git?
qn_description:
When looking at comparisons, it seems to me that there could be a 1:1 mapping between their feature sets. Yet, an often cited statement is that "Mercurial is easier". What is the basis of this statement? (if any)
AnswersKenntnis
AnswerKenntnis:
Case in point: Lets say that you want to change the username on all your previous commits. I've needed to do this several times for various reason.

Git Version

git filter-branch --commit-filter '
        if [ "$GIT_COMMITTER_NAME" = "<Old Name>" ];
        then
                GIT_COMMITTER_NAME="<New Name>";
                GIT_AUTHOR_NAME="<New Name>";
                GIT_COMMITTER_EMAIL="<New Email>";
                GIT_AUTHOR_EMAIL="<New Email>";
                git commit-tree "$@";
        else
                git commit-tree "$@";
        fi' HEAD


Mercurial Version:

authors.convert.list file:

<oldname>=<newname>


Command line:

hg convert --authors authors.convert.list SOURCE DEST


Now, which one looks easier to use?



Note: I spent 2 years working solely with Git, so this isn't a "I hate it, I didn't get it in 2 seconds" rant. 

For me, it's the usability. Git is very linux oriented with a linux way of doing things. That means command line, man pages, and figuring it out for yourself. It had a very poor GUI (note: I'm basing this off of msysGit from about a year ago), that seemed to just get in my way. I could barely use it

The command line was worse. Being a Linux oriented program, on Windows it was very difficult to use. Instead of a native port they simply wrapped git with MinGW (Think cygwin), which made working with it much more difficult. MinGW isn't Windows Command Prompt, and just acts different. It's crazy that this is the only way to work with Git. Even in Linux it seemed the only way was to work with straight command line. Projects like RabbitVCS helped some, but weren't very powerful. 

The command line oriented approach and being a linux program meant that almost all the howto guides, help documentation, and forum/QA questions relied on running monstrous commands like above. The basic SCM commands (commit, pull, push) aren't as complex, but any more and complexity grows exponentially. 

I also hate the one place that lots of OSS git users seem to hang around: Github. When you first go to a github page, it slams you with everything you can possibly do. To me, a projects git page looks chaotic, scary, and overly powerful. Even the explanation of what the project is, is pushed down to the bottom. Github really hurts people who don't have a full website already setup. Its issue tracker is also terrible and confusing. Feature overload. 

Git users also seemed to be very cult like. Git users seem to always be the ones starting "holy wars" over which DVCS is better, which then forces Mercurial users to defend themselves. Sites like http://whygitisbetterthanx.com/ show arrogance and an almost "Use my software or die" mentality. Many times I've gone into various places of help only to be flamed for not knowing X, using X beforehand, using Windows, etc. It's crazy.



Mercurial on the other hand seems to go towards the kinder approach. Their own home page seems much more friendly to new users than Git's. In a simple Google search the 5th result is TortoiseHg, a very nice GUI for Mercurial. Their entire approach seems to be simplicity first, power later. 

With Mercurial I don't have SSH nonsense (SSH is hell on Windows), I don't have stupidly complex commands, I don't have a cult user following, I don't have craziness. Mercurial just works.

TortoiseHg provides an actually usable interface (although lately it seems to be growing) that provides actually useful features. Options are limited to what you need, removing clutter and options that are rarely used. It also provides many decent defaults

Mercurial, being very friendly to new comers, was very easy to pick up. Even some of the more complex topics like the different branching model and history editing were very easy to follow. I picked up Mercurial quickly and painlessly.

Mercurial also just works the first time with little setup. On ANY OS I can just install TortoiseHg and get all the features I want (mainly context menu commands) without having to hunt for different Guis. Also missing is setting up SSH (half of the guides out there say to use Putty, Plink, and Pagent while the other half says to use ssh-keygen). For new users, TortoiseHg takes minutes to setup while Git takes 30 minutes to an hour with lots of googling. 

Lastly you have the online repos. Githubs equivalent is BitBucket, which has some of the issues I outlined above. However there's also Google Code. When I go to a Google Code project, I don't get feature overload, I get a nice clean interface. Google Code is more of a online repo/website combo, which really helps OSS projects who don't have an existing site setup. I would feel very comfortable using Google Code as my projects website for quite some time, only building a website when absolutely necessary. Its issue tracker is also powerful, fitting nicely in between Github's almost useless Issue Tracker and Bugzilla's monstrosity.     

Mercurial just works, first time, every time. Git gets in my way and only angers me the more I use it.
AnswerKenntnis:
Git versus Mercurial


  Mercurial is generally believed to be
  simpler and easier to learn than Git.
  In turn, there is often the perception
  that Git is more flexible and
  powerful. This is due, in part,
  because Git tends to provide more
  low-level commands, but also in part
  because the default Mercurial tends to
  hide advanced features, leaving it to
  users to edit the mercurial config
  file to activate the advanced features
  they like. This often leads to the
  perception that advanced features are
  not available in Mercurial.


Git Concepts


  Mercurial has always focused more on
  interface aspects, which made it
  originally easier to learn. In
  comparison to Git, a shallower
  understanding is required to operate
  with Mercurial in a useful manner. In
  the long run, such encapsulation has
  given Mercurial the false appearance
  of being less powerful and featureful
  than it really is.
AnswerKenntnis:
Context: I use both Mercurial (for work) and Git (for side projects and open source) on a daily basis.  I primarily use text-based tools with both (not IDEs) and I am on a Mac.  

In general, I find Mercurial easier to work with.  A few things that I find make Mercurial easier:


Lack of the index.  The index is a powerful tool enabling many of Git's features but it's also an extra layer that adds a step into many things I do regularly.  Mercurial's workflow is more similar to something like svn. 
Revision numbers instead of shas.  This is a small thing that I find makes working with every day commands a lot easier in Mercurial.  It's way easier to push a few revision numbers into your head during a rebase, merge, etc while writing a command than to do the same with even shortened shas.
Branches.  Git's approach to branches by naming commits is powerful and substantially different than other version control systems.  It makes certain things a lot easier.  I find that Mercurial's approach matches svn thinking a bit better and makes it easier to visually understand the branch history.  This may just be a preference.
AnswerKenntnis:
This is very subjective and depends from one person to another, but yes, I would go that to someone completely new to VCS or someone coming from one of the "old school" VCSs, Mercurial will seem easier.

For example, adding files, the non-existence of the index in Hg, the ease of going back to some old revision and branching from there (just update and commit) as some of the most "obvious" examples. Now most of features of one system can be emulated in another and vice versa, but that requires some knowledge in Git, while in Mercurial the defaults (if you'll allow me to call them that) are rather "user friendly". Those little things - the switch here and there, the non-obvious behaviour in one command and so on ... these things add up, and in the end, one system seems more easy to use than the other.

Just to make the answer complete; I use git, but when recommending a VCS for someone who's "new to them", I almost always recommend Mercurial. I remember, when it first came into my hands, it felt very intuitive. It is my experience that Mercurial produces less wtf/minute than Git.
AnswerKenntnis:
I think it's as simple as this: Mercurial has a more familiar syntax (particularly for SVN users) and is fairly well documented. Once you get used to the Git syntax, you'll find it as easy to use as anything else.
AnswerKenntnis:
Perceptions might be changing over time, on this.  Mercurial is very well designed, and so is Git.  Mercurial seems to be easier to learn (at least it was for me), and there have been difficulties that I encountered in Git, that I have no parallel for in Mercurial.  I tried to learn Python, and Ruby, and got farther, faster with Python.  That doesn't mean Python is always and everywhere better than Ruby, or even that it's better for me. It's just what I  learned and stuck with.  Programmers often make holy wars out of personal preference. Other human beings do that too.

I am a mercurial user who tries to keep an open mind about Git, and I freely admit that it hasn't "become my new favorite thing" to the same extent as Mercurial has. I think Git is really really nice though.

A counter example for GIT/mercurial complexity: Nice GIT support is built into XCode, on Mac.  Less easy to use XCode with Mercurial than GIT.  

My experience with GIT so far has been that I get confused and lost and need to go consult the documentation more while using it. I believe that a lot of documentation has been written, but nothing that has enabled me to "grok" it.   Secondly, I can modify and extend Mercurial easily in Python, and as I am adept in Python, and as anyone really could learn python quickly, it seems an advantage to me.  I also know C, and write Python extensions in C, so I suppose some day, if I needed one, I could easily write a Git extension in C.   

Ease-of-use is not something that is easy to quantify.  It's there, and I don't think it's entirely subjective, but we don't have good objective measurement techniques.  What would the units for ease-of-use be? Milli-iPods?

I am not so partisan as to be 100% pro-mercurial, and 100% anti-git. I'm more comfortable on Mercurial right now, on Windows and on Linux, and when I start doing more Mac work, I expect that I'll try to stick with XCode+GIT.

Update 2013: I have now used Mercurial AND GIT long enough to find some features that I wish it had that Git has, such as this question about merge strategies.  Really Git is amazing, if difficult to learn, and sometimes maddeningly complex.
AnswerKenntnis:
There are a few things that IMO are likely to put new users off Git:


The Git culture is more command-line centric. While both tools do tend to focus too much on the command line (as I've said several times, command line instructions may be powerful and more fluent, but they are not a good marketing strategy) this is much more the case with Git. Whereas Mercurial has a de facto standard GUI tool in TortoiseHg, which is even the default download option for Windows users on the Mercurial homepage, Git has several competing GUI front ends (TortoiseGit, Git Extensions, gitk, etc) which aren't well advertised on the Git website, and which are all a complete eyesore anyway. (Black text on red labels? C'mon, TortoiseGit, you can do better than that!) There's also a much more pervasive attitude in the Git community that people who use GUI tools are not proper software developers.
Git has several out-of-the-box defaults that make perfect sense to advanced users, but are likely to be surprising if not intimidating to new users. For instance, it is much more aggressive about automating tasks such as merging (for example, git pull automatically merges and commits where possible). There is a case for fully automated merges, but most inexperienced users are terrified of merging, and need to be given the opportunity to gain confidence in their tools before unleashing their full power on their source code.
Documentation and inherent complexity:

$ hg help log | wc -l
64
$ git help log | wc -l
912
AnswerKenntnis:
One thing I can think of is 

git add .
git commit -am "message"


vs.

hg ci -Am "message"


git commit -a doesn't add newly created files, hg ci -A does, which means that something that requires two commands with git can be done with one command in Mercurial. Then again, "less typing" doesn't necessarily mean "more user friendly".
AnswerKenntnis:
Because it is.

Git exposes far more of it's guts than mercurial. You can happily use mercurial within a few minutes of picking it up but I find git still very hard to grapple with after a couple of months of wrestling with it (I have done very little over the last couple of months other than try to learn git). I am using both from the command line and mostly on Linux so this is not just an aversion to the command line interface.

One simple example is the relatively few flags and command line arguments needed for mercurial compared to git.  The staging area and the behavior of the add command in git also adds more complexity than is necessary. The trio of reset, checkout and revert and their multiple permutations adds enormous complexity, which was quite unnecessary when you witness the straightforward nature of revert and update on mercurial.

I agree with the above comment also about Hginit, it definitely made mercurial much easier to comprehend. Well written and very easy to understand. None of the documentation written for git comes close. I for one, find most of what is written by Scott Chacone (who has singlehandedly written most of the documentation/books about git) particularly confusing.
QuestionKenntnis:
How do you know you're writing good code?
qn_description:
This question already has an answer here:
    
        
            How would you know if you've written readable and easily maintainable code?
                
                    19 answers
                
        
    

        Serious question here. I love programming. I've been messing around with code since I was a kid. I never went the professional route, but I have coded several in-house apps for various employers, including a project I got roped into where I built an internal transaction management/reporting system for a bank. I pick stuff up quickly, understand a lot of the concepts, and feel at ease with the entire process of coding.

That all being said, I feel like I never know if my programs are any good. Sure, they work - but is the code clean, tight, well-written stuff, or would another coder look at it and slap me in the head? I see some stuff here on SO that just blows my mind and makes my attempts at coding seem completely feeble. My employers have been happy with what I've done, but without peer review, I'm in the dark otherwise.

I've looked into peer code review, but a lot of stuff can't be posted because of NDAs or confidentiality issues. A lot of you pros might have teammates to look at stuff over your shoulder or bounce ideas around with, but what about the indies and solo guys out there like me? How do you learn best practices and make sure your code is up to snuff?

Or does it not matter if it's "the best" as long as it runs as expected and provides a good UX?

EDIT: Wow, I can't believe the great response and helpful answers this question generated!!! In the spirit of democracy I have selected the answer with the most upvotes as the correct one -- in reality, I wish I could accept many of these answers, since they all made some great points and helped me expand my mind a bit when it comes to my programming. Thank you to everyone who took the time to answer and comment!
AnswersKenntnis
AnswerKenntnis:
The biggest clue for me is:

When you have to go back and add/modify a feature, is it difficult?  Do you constantly break existing functionality when making changes?

If the answer to the above is "yes" then you probably have a poor overall design.

It is (for me at least) a bit difficult to judge a design until it is required to respond to change (within reason of course; some code is just bad and you can tell right away, but even that comes with experience.)
AnswerKenntnis:
This is certainly a pretty standard measure where I work.




  Which door represents your code? Which door represents your team or your company? Why are we in that room? Is this just a normal code review or have we found a stream of horrible problems shortly after going live? Are we debugging in a panic, poring over code that we thought worked? Are customers leaving in droves and managers breathing down our necks... 


(Robert C Martin, Clean Code - book that opens with above picture)
AnswerKenntnis:
You know you are writing good code when:


Things are clever, but not too clever
Algorithms are optimal, both in speed as well as in readability
Classes, variables and functions are well named and make sense without having to think too much
You come back to it after a weekend off, and you can jump straight in
Things that will be reused are reusable
Unit tests are easy to write
AnswerKenntnis:
Although you said you don't have other coders around to do this, I'll include this for the sake of others. 

The code quality test: Have another programmer who has never seen the code read it an explain what each module does to you while you look over their shoulder. The stronger your urge to jump in and explain something the worse the code likely is. If you can calmly sit there with your mouth shut and they don't need to ask a lot of questions, you are probably good.

For your benefit, here are some general guidelines for when you don't have a peer handy. They are not absolutes, just "smells".

Good signs


Methods tend to be very short that ideally perform a single task.   
You have enough information to call methods WITHOUT looking at the body of them.  
Unit tests are easy to write.


Bad signs


Long methods made up of 2-3 sub-tasks that are not broken out into other methods.  
Methods share data through some means other than their interface.  
If you change the implementation of one method (but not the interface) you need to change the implementation of other methods.  
Lots of comments, especially long winded comments.  
You have code that never runs in your application to provide "future flexibility"  
Large try/catch blocks  
You are having a hard time coming up with good method names or they contain the words "OR" and "AND" (e.g. GetInvoiceOrCustomer)
Identical or nearly identical blocks of code.


Here is a longer list of code smells that should also be helpful.
AnswerKenntnis:
For me personally, I think it's when I forget about the code. In other words:


Bugs rarely occur
When they do occur, other people resolve them without asking me anything
Even more important, no one ever asks me anything regarding my code
People don't have a high rate of WTF/min when reading it
A lot of new classes in the system start to use my class (high fan-in, as Steve McConnell would call it)
The code is easy to modify and/or refactor when/if needed, without cursing me (even if it's me - cursing myself!)
I also love it when I guess just the right amount of abstraction which seems to suit everyone on the team


It's a nice feeling when you open a file you've written a year ago and see all the nice additions to a class, but very few modifications, and - very high fan-in! :)

Of course, these are the things that make me feel like I'm writing good code, but in reality, it's really hard to know. My guess is, if people start making fun of your code more than they're making fun of you, it's time to worry.
AnswerKenntnis:
I have three golden rules: 


If I feel compelled to copy/paste blocks of code I'm doing something wrong
If I can't take the whole code in my head I'm doing something wrong
If someone jumps in and gets lost in my code I'm doing something wrong


Those rules has guided me to do some real architectural improvements, ending up with small, clean and maintainable classes/methods.
AnswerKenntnis:
This is an excellent question, and I applaud you for even asking.

First, it's good to get your mind blown every once and a while.  Keeps you humble, keeps you realizing that you don't know everything, that there are people out there better at you than this, and gives you something better to strive for.

Now, how do you know when you're writing good code?


When your classes each serve a single, very clearly defined purpose, separated from other classes with other clearly defined purposes.  
When your methods are short - ideally under 50 lines and certainly under 100 - and their names clearly define what they do exactly.  A method should not do anything but what it's name implies.  If you're going over 100 lines, or tabbed in very far, you can probably pull something out into it's own function. 
When your code provides one way to do something - when it does not provide the option to zig or zag, but instead provides a single linear path for each possible course of action the user may send it down.
When you've done everything you reasonably can to reduce coupling between classes; so that if Class A depends on Class B, and Class B is removed and Class C is put in it's place, little to no changes have to be made to Class A.  Class A should be as blind as possible to what's going on in the outside world.
When your class, method, and variable names can be read and immediately understood by anyone that comes across the code - there's no need for 'pktSize' when 'packetSize' reads much more easliy.


As others have said, essentially, if you're doing Object-Oriented Programming, and when the time comes to change something you find it like trying to untangle and rewind a ball of yarn, the code isn't following good Object-Oriented principles.

I highly recommend the book "Clean Code",  if you're interested in digging a little further into this.  It's a good read for novices and experts alike.
AnswerKenntnis:
Look around for a good open source project in your favorite language and see what they do.
For example, sendmail is someplace to look to see if you write spaghetti code. It's not sendmail's fault really, it's just 20 years old so it has a lot of cruft in it.
So if your code looks like sendmail code, you're probably on the wrong track.

postfix though I have not looked it it lately myself, is probably very well designed.
So if your stuff looks more like postfix you're on the right track.

When I started programming as a kid there was no internet and you had nothing to compare to. But now with a bazillion lines of code available for viewing for comparison, you should start to get an idea if you're doing it right or not.

And just because the linux kernel is the linux kernel doesn't mean it's well written. Keep that in mind.
AnswerKenntnis:
Here has been my experience since shifting from the college world of programming to the industry over the last 5 months:


Ease of use is so important that we pay people just to design User Interfaces. Coders often suck at UI design
You find that that getting it to work isn't enough
Optimizations become more critical in real-world scenarios.
There are a thousand ways to approach a problem. However, often times the approach does not account for slightly less well known factors that could adversely affect the performance of your approach such as database authentication, transactions, file io, reflection, just to name a random few. 
Maintainability is a very important aspect of coding. Just because your code is super optimized and super dense ... doesn't mean that it is sexy. Sometimes it is simply labeled as "Hero coding".
Design skills are learned, not inherent. I'm sure there are a few brain children out there but generally speaking, solid design with respect to real world problems, is wrought through time, investigation, and most importantly, the imparting of knowledge from your superiors =P 
Documentation is not an convenience, it is a necessity.
Same thing goes for Unit testing (this varies from company to company admittedly)


I would suggest that you take an opportunity with a open source project. You will get a chance to see how much you really know if you work along side other programmers bottom line. An open source project is probably the best way to find out based on your background.
AnswerKenntnis:
I'd say my main points would be:


Readability (for you and anyone else who has look into your code)
Maintainability (easy to modify)
Simplicity (not complicating things when there's no need for that)
Efficiency (of course your code should execute fast)
Clarity (if your code is self-explanatory, there's no need for comments most of the time, name your methods/properties etc. sensibly, break the long code down, never copy & paste a block of code)


I'm not putting Design in this list as I believe a good code can be written without sticking to a design pattern as long as it's consistent within your project.

Good MSDN article on this topic: What Makes Good Code Good?
AnswerKenntnis:
Ask someone else to take over your job for one day and check how stressed out he is at the end of the day ;-)

I'm bad at documenting and cleaning up stuff - so thats how I check it.
AnswerKenntnis:
Read good code and figure out why it's good. Read bad code and figure out why it's bad. Read mediocre code and figure out which parts are good, which are bad, and which are ok. Read your own code and do the same. Pick up a couple of (well-regarded) textbooks specifically for the purpose of looking at the examples and understanding why they wrote them the way they did. Mostly, read code until you can tell the difference between bad and good, and can do your own "Wtf?" tests.

You can't know whether you're writing good code until you can recognize good code in general. Just because something's over your head doesn't mean it's well-written ...

("Read other people's code" has popped up in a couple of comments on this thread, but I thought it deserved its own post)
AnswerKenntnis:
When you can read it like prose.
AnswerKenntnis:
From a code and design perspective, I like what others have already said about maintainability.

In addition, I also look at stability. Look at the production support statistics. If you are getting a high instance of support correspondence for things that seem like fundamental functionality but are finding that many people are unable to understand how to use the software or are finding it does not meet their needs, then there's something wrong.

Of course, there are some users that are truly clueless, but if over time you're still getting reports of breakage, confusion, or significant feature change requests, then this is an indication that one or all of the following apply:


The requirements were broken
The code is broken
The application is not intuitive
The developer did not understand the user need
Somebody pushed for delivery date over quality
Somebody didn't test well or know what to test for
AnswerKenntnis:
That all being said, I feel like I
  never know if my programs are any
  good. Sure, they work - but is the
  code clean, tight, well-written stuff,
  or would another coder look at it and
  slap me in the head?


Have you considered asking another coder what they think of your code?

Some places uses "peer review" where code must be acceptable to another coder before it is accepted to the code repository.
AnswerKenntnis:
You can use code analysis tool such as FindBugs, PMD. That will provide some information about your code's quality.
AnswerKenntnis:
In effort to code well, I aim to ensure that the program is highly functional and speedy about it. Make sure everything is in a place it should be so that the file, data, function and abstractions are fairly obvious.

After this point I put it to a test: Find a person fairly unversed in computers in general, try to explain some of the main code pattern. If they can sorta-kinda read it, wow. Good job.

Mostly just KISS and try to be innovative without crashing anything. I would also make more notes about coming back to the code and having a nice time improving and maintaining it, but that has been covered pretty well.
AnswerKenntnis:
Release your code, let some people mess with it to make sure that it does what it's always supposed to.  Or if you would like, design a spec and make sure it meets all of those specifications.  If you pass one or both of these tests, then your code is "good for right now."  For most people, your code will never be great, because you'll come back in a year and say "Why did I do that when it would've worked so much better this way?"

With more experience, you get better code.  But if you continually practice the same habits, you'll continually fall into the same pitfalls.  It's just a simple take on an old adage of "Practice makes permanent."  If you continually do things the right way (like testing your code to always make sure it works for what it's supposed to do), then you'll get better.  If you continually do things the wrong way (like never testing your code for certain situations, or failing to recognize where bugs could occur), you'll just get stubborn with your code.
AnswerKenntnis:
Nothing beats having someone experienced look at your code, but there are some resources to help you evaluate and improve your code on your own.  Take a look at Martin Fowler's Refactoring (or website).  Sutter and Alexandrescu's C++ Coding Standards is good if you're writing in C++.  A lot of the recommendations in there are language agnostic, but others may be able to recommend similar books for other languages.  Test driven development can be very useful to solo developers, as it provides a sort of quality check as you go, and you know when tests become difficult to write that it means your code could probably use restructuring.

Some other signs are more process-oriented.  They usually lead to better code, but not directly.  These include things like use of source control, an automated build process, not developing directly on the live server, use of bug tracking software, etc.
AnswerKenntnis:
It is a hard thing to say, there's always a question of taking longer to do it right, or taking less time and getting it done sooner so you can work on other things.  

http://xkcd.com/844/

I've gone back and looked at code I did a few years ago, and I think it looks awful. We are constantly learning in this field, the best way to code right is to look at other people's examples and learn from them. Once you learn something new, share it with others so that we all can benefit from it.
AnswerKenntnis:
IMHO, there's no "good code" per se, but there's "good piece of software".

When we are working on something, there are many constraint on our work, which many times would make us producing the code that falls out of "good code" standard of other fellow programmers. 

Some might say "good code" is the code that easy to maintain. The counterargument for this statement for me is, how easy? Do we need to put like 200% effort to the piece of code to make it so easy to maintain for the sake of "good code" standard, even we know we won't need to maintain it that much? I'm sorry but I don't think so.

In fact, I'm the one who really promote "good code" in my team that no one really cares for it. Every time I look at their codes, I never found that they write any perfect code. However, I must accept that they really got the job done, and also be able to maintain it adequately for our company needs. Of course the application is sometimes buggy, but we just made it in time, making our customers and users happy and securing our company in the position far ahead of our competitors.

So, I would say that "good code" is the code that produce what you need. You just have to think of what you really need, then use that to evaluate your code.
AnswerKenntnis:
For a particular piece of code : 

If it works and is maintainable ( pick your good practices ) it is good code.

For you as a developer over time :

Good is a relative and dynamic term, to the language domain, problem domain, current trends, and most importantly your experience. The "GOOD" acid test for this continuum might simply be looking back at your previous work and if you say "sigh did I really solve it like that?" then chances are you are still growing and are likely to keep writing good code. 

If you look back and see perfect code then either - you are perfect, and there is a risk you are stagnating and you may soon cease to write good code.
AnswerKenntnis:
Never.

"Good code" is only code for which no way of improvement has been found yet.
As Jeff Atwood said :


  There are a handful of programmers in
  the world capable of producing
  brilliant, perfect code. All the rest
  of us can do is keep making our
  software less shitty over time-- a
  process of continuous improvement


And by the way, you don't have to reach the perfection, because sometimes, "Sufficent Design means poor design"
AnswerKenntnis:
Good code is subjective to the person.  A professional coder that has read lots of books and been to seminars and used different coding techniques would probably tear your code to shreds...  However, I have found that code is really indicative of where the coders level of experience is.  To me it reads like a history book or an auto-biography.  It is what the coder knew at the time or what tools he/she was limited to.  

Ask yourself this...  Why does Microsoft take 3 versions of software to get something right?  Because they are constantly fixing the mistakes they made in the previous versions.  I know that my code always gets better and better after a revision.  Sure there will be people here that say, I write code perfect the first time.  If you believe that then I have some swamp land to sell you...

As you understand the concepts things get easier.  For me, the beginning of learning something new is "can I get it to work?", then the next step is "I wonder if I can do it this way...", then usually when I've nailed it I ask "how can I make it faster"...  

If you want to be a programmer then you have to dive into it and just do it.  It takes a lot of work and to be honest it is like a piece of art that never can be finished.  However, if you want to be just a casual hobbyist then if it works then don't worry about it.  You have to adapt to your surroundings.  If you don't have a way to do code reviews then ignorance is bliss =)  

But no matter what...  Everyone is going to have their own opinion.  Sure there are right ways and wrong ways of doing things...  But mostly I have found that there mostly better ways of doing things than just wrong ways...
AnswerKenntnis:
I use 5 points to know if code is good or not:


procedure, method, and class names are both usefully short and yet tell me what they do.
commenting out any include causes failure to run
looking at the code after a month away, I can follow the flow
code compiles and runs cleanly.
program performs the intended actions and only the intended actions.


For me, GetCustIDFromDB(var&ast; DB, char&ast; customer) is better than getcid(var&ast; DB, char&ast; c) because it tells me what I'm looking at. But *DBLookupCIDByName(var&ast; DB, char&ast; customer) works, too.

I often test to see if I actually still use all my #includes... If I can remove it, great. Sometimes, it's possible to examine your included headers, and see if the needed functions are actually in something that header includes via #include instead... but I've only done that rarely, and it saves a little bit of space, too, tho' it may make the process of adding new features much harder.
QuestionKenntnis:
My Dad is impatient with the pace of my learning to program. What do I do?
qn_description:
So my Dad bought me 5 books on programming (C++, Java, PHP, Javascript, Android) about a month ago. He's an architect and he knows NOTHING about programming. He bought me them because I told him programming was fun and I wanted to learn it. 

As you might know, being a kid (I'm 14) and being told to learn programming out of dull books isn't the easiest thing. I'm always getting distracted.. I told him before that I didn't need to buy books and I could just watch online tutorials.. but no, he's so old-fashioned. He's only letting me use the books.

Recently, he started asking me what I've done with it, and I showed him a C++ program I made that takes what you type in, then assigns values to each letter (A is the first letter in the alphabet so it gets the value of 1).. and so on. It then adds up all the values and tells you it. So the word "add" would have a value of 9.

^^ That wasn't very impressive to him. He yelled at me and told me all I've been doing is screwing around. That's not true. He is extremely traditional and stubborn and doesn't listen to anything I had to say. What should I tell him?

PS: If you have any tips on zoning in on a book, let me know

EDIT: Thank you so much everyone, you have no idea how much it means to know that there are some people that understand my situation. I've read every one and I'll consider everyone's opinion. ┬íGracias!
AnswersKenntnis
AnswerKenntnis:
I showed him a C++ program I made that takes what you type in, then assigns values to each letter (A is the first letter in the alphabet so it gets the value of 1).. and so on. It then adds up all the values and tells you it. So the word "add" would have a value of 9.


I don't know what you should do with your dad. But:

If you did this all by yourself, starting from scratch, learning from books, in a month, it's damn impressive. And you did it in C++, which is one of the scariest programming languages in existence.

There are quite a few people out there taking interviews, seriously trying to get programming jobs, who would struggle with that. See this story.

I can only suggest: keep doing what you enjoy. Ignore your dad in this context; he doesn't know what he's talking about. You have talent in programming and willingness to learn - the main ingredients in becoming a great programmer.
AnswerKenntnis:
Show him this post by Peter Norvig. Norvig is head of R&D at Google and teaches at Stanford, specifically Artificial Intelligence, he wrote the standard introductory book on AI. How long have you been working at it? I'd expect nothing more than that after a month of work by a novice with no additional instruction particularly with something as thorny as C++. Anything worth learning is worth learning well.
AnswerKenntnis:
Remember that your dad probably thinks that you're about the smartest kid in the world, and he's trying to help you learn about something that you've said you enjoy.
Know that even though your dad is unquestionably wrong (five completely different topics is a lot to throw at a kid all at once), he's also kinda right. A lot of people try to pick up programming by mimicking "tutorials" that they find on the web. That's not always a bad thing, but IMO it usually doesn't lead to solid understanding. The right book will teach you the fundamentals first. That might not be as much fun as following tutorials that get something flashy done, but your chance at long-term success will go way up, and the number of questions you have to ask on stackoverflow.com will go way down.
Forget about all but one of the books, at least for now. It sounds like you've already started in with C++, so go with that if you like it.
You're going to feel discouraged from time to time. Sometimes it feels like half of programming is getting stuck, and the other half is getting unstuck. Getting unstuck is a valuable skill, and the more you practice it the better off you'll be. (Avoiding getting stuck in the first place is also a valuable skill, so practice that too.)
Learning your first computer language is to software development what learning to draw is to designing a building. It might not be the most interesting part, but it's a required skill.
If you can get him to listen, try to break down all the things that you had to learn to write your little program. It takes some time just to learn to use the tools, and he surely doesn't understand how it all works.
Now that you've gotten to the point of having a simple running program, and you've apparently learned a little bit of C++, what interesting little programs can you write? How about a loan calculator that accepts an interest rate, loan amount, and loan term and prints out a payment schedule? Or a calculator that reads a molecular formula like "H2SO4" or "C6H12O6" and prints out the weight of the molecule? That's not too much more complicated than the program that you've already written.
AnswerKenntnis:
He bought me them because I told him programming was fun and I wanted
  to learn it.  ... What should I tell him?


"Dad, your approach to this is making learning to program absolutely no fun.  Knock it off."
AnswerKenntnis:
As an architect he must be surely pulling your leg or being plain rude.

Probe him about when he started out: Ask him if he drew skyscrapers with detailed plumbing plans, calculated the forces and stress on materials, and did disaster risk assessments (such as being hit by a plane with snakes in it) after a month spending with books when he was as young as you. Maybe that'll calm him down. 

Okay, maybe it's not the best advice I can give you, to talk back on your parent. I can understand that having a dad, that penalizes on progress instead of encourages, is a bit taxing as a kid but take it instead as an encouragement to do better. He may have an attitude problem, but that doesn't mean ill intent.
AnswerKenntnis:
I'd be curious which books you're using. Not all of them are actually good. Also, not every book is appropriate for everyone.

You didn't specify your age, so I'll assume you're well under 18. I started learning when I was about 8.

When I was a kid, I took advantage of lots of resources when learning to program. I had the manuals that came with my TI 99/4A, which contained lots of fun programs to draw images and animate figures. The internet wasn't available to me, but there were even computer magazines that targeted kids, with programs that I could type in and get immediate feedback on, and articles about how to solve other kinds of challenges. I was very fond of a spy novel series that let me type in and debug programs that were included as part of the story. Unfortunately, I don't think that this kind of resource is still around, but there are a few programming books that target younger people, like the Hello World book on Python, and Land of Lisp (though that's fun for adults too).

I don't know about you, but there was a time when I got quite a kick out of writing programs that do things very similar to what you just described. Eventually I moved on to more advanced things. I wrote a few mediocre games, some demos that played various sounds and animations in reaction to keystrokes, and some study aids. When I was around 10 or 11 I wrote a program that helped me memorize the periodic table of the elements by repeatedly quizzing me. (At the time, my memorization skills were better than they are now, so I got almost as much out of typing in the data the first time as I did playing the quiz, but the point was to make progress).

Your father may not realize it, but books are only part of the process when you're learning to program. Finding a little problem and trying to figure out how to solve it is the other half of the equation. Finding a book that teaches you a little bit at a time and lets you get something fun to happen on the machine is the other half. In my case, books that emphasized graphics and animation were the ones that won me over.

As a kid, my eyes glazed over when I read books about sorting algorithms and complex data structures, until I had learned enough to see how they applied to problems I actually cared about. Not every word in the books you'll read will be riveting. That's ok. You'll get to that stuff when you need it; some problem you'll want to solve will remind you of that technique you didn't think was interesting three weeks, three months or three years ago and you'll go back and review it and figure it out.

A month is not a very long time to learn programming. I've been writing code in one form or another for about 30 years, including during childhood, and I still learn something new every day. I'm pretty sure in the first month that I had my first computer, I spent a lot of time playing Munch Man and a much smaller number of hours trying to make sense of the sample programs in my reference book. Learn at the pace that works for you. There's no pressure right now, and that's great.

"Screwing around" is what you're supposed to do when you're first learning to program. Hackers (the Paul Graham kind) poke around, trying to understand how their system works, how their programming language works, how their tools work. You try something, you fail, you reason through the problem you're facing, and you try something else, until you get something working. Don't worry about it so much.

Unless your father's working through the same books, he probably doesn't understand how much you've learned so far. I wouldn't expect to be able to design a house or a skyscraper after reading a book on architecture for a month, especially as a teenager.

To put things into perspective, over the last four weeks or so I've been working in some esoteric corners of the Ruby on Rails framework's Engines feature. As of today, I finally have something to show for it from the user's point of view. I learned a ton in that time and developed a lot of critical foundational code that works pretty well, but is my professional equivalent of allowing users to type in some stuff and get something else back out: not that impressive at first glance to a casual user, but a whole lot of work went into it. If someone told me I'd been screwing around for four weeks, I'd be pretty disappointed, but I'd also know they have only the slightest understanding of what went into making things so "simple."
AnswerKenntnis:
Sounds like in this situation your dad is not a someone who gives you positive reinforcement and support in what you are doing.  Simple solution: don't use him for that. 

Do your own projects and do them because they are fun, not because he (or anyone else) wants you to do them. Pick something that's fun for you.  You do not need to learn entire language/technology from a book.  Instead just start tinkering with things.  Eventually as things become easy, you'll look for new concepts/challenges.  And you'll find youself wanting to read the books that today you find boring (or at least certain sections).

If you ever get stuck and need help with coding anything, stackexchange is great resource as you've already found out.
AnswerKenntnis:
It sounds to me that your dad has a penchant for unrealistic expectations, or perhaps you have a history of starting things and not following through, or both. The important thing to realize is none of that really matters here, all that matters is that you enjoy what you're doing and get better at doing it.

I have a very difficult time with books that don't entertain me, I always have. I learn best by watching other people do things, asking questions when I don't understand the purpose of something and then struggling until I figure it out. Struggling makes us stronger and gives us ownership of our eventual accomplishments, perhaps that's why it's so darn difficult to climb out of a uterus.

That being said, one of the sharpest tools in your toolbox is going to be the ability to remain open, work productively amidst criticism and not rule out advice based on the source. Good programming books tend to grab you, inspire you and expect you to return to them when you hit a wall. Without them, you'll be doing lots of amazing things without initially understanding precisely what you're doing. Programming is a very deliberate art, so I do encourage you to seek out books that are written in a manner that is easy to digest.

I've been programming professionally for quite a while now and I can tell you that my satisfaction is truly my own. My boss doesn't quite understand why things I have written are so awesome, my wife falls asleep when I tell her about my day and my friends go out of their way to avoid asking me about my job. I relish my victories and the occasional opportunity to share them with my peers. Your dad, in this case, isn't one of your peers.

Keep going and keep improving. Get used to the fact that non-programmers need to see something visually impressive before being impressed. Why not try your luck at writing something like a Mandelbrot / Julia set generator? That might show him the kind of progress he's looking for, getting him off your case for a while. In ten years, you'll do something very similar to get a non-technical manager off your back so you can get real work done.

Just remember, you're doing it because you enjoy it.
AnswerKenntnis:
It is hard to tell whether this question is best for the Parenting forum, or the Programming forum. I fear that my advice my not be on target, because I sense that tangled up in this question is potentially a lot of father-son relationship "issues."

That being said, what I would focus on is the great opportunity that exists for you and your father to connect on a subject the two of you may share a passion for in some way. Being a father myself, and knowing my own father, I know how desperately we want our children to listen to us, follow our instruction, and learn from us. Our children are often driven to do the opposite. So the fact that the two of you share this common interest, is wonderful. 

Fathers can be stubborn for sure, but I still think there is an opportunity for you to flex your own individuality and choice through this exercise. Not out of a sense of defiance, but stemming from your own drive and ambition to know the subject of programming well. Personally, it sounds like your father threw you into the deep end when it comes to programming. Granted it may not be MIPS Assembly Language or LISP, but still, C++? Java? Yikes. I love programming, but reading those early on might have turned me off all together. Kidding. :)

So what I would ask is this: what inspires you? What kind of things do you want to build? Answer that, and then seek out your own books and tutorials on that subject. Come here and ask questions. Build something. Then show your dad what you built. Tell him what you learned. Thank him for being so engaged with you and tell him you appreciate him. I say that because in the end, that is what this is all about. He wants to share something with you because he loves you. Sometimes fathers have a hard time expressing that in traditional ways, so we seek these indirect ways to say the same thing. Sometimes we suck at doing even that. But don't forget that in all of this is a desire to be closer to you.
AnswerKenntnis:
It doesn't seem like anybody has suggested this yet:


  Recently, he started asking me what I've done with it, and I showed him a C++ program I made that takes what you type in, then assigns values to each letter (A is the first letter in the alphabet so it gets the value of 1).. and so on. It then adds up all the values and tells you it. So the word "add" would have a value of 9.
  
  ^^ That wasn't very impressive to him. He yelled at me and told me all I've been doing is screwing around. That's not true. He is extremely traditional and stubborn and doesn't listen to anything I had to say. What should I tell him?


If you think about it, that program is not very impressive to somebody who uses computers and doesn't know anything about programming.

Show your dad how you made it. Walk him through the source code. He probably (definitely) won't understand it, but it will convey how much work you have put into it, and how much you understand, and maybe he will come away with a more positive impression of how much you have actually accomplished.
AnswerKenntnis:
Do whatever you want

It's your life. You can do whatever you want. Don't let anybody (even your dad) to control your life.

I assume that you enjoy programming more or less and want to become a successful specialist (and your dad wants, everybody wants). But the thing is, you will never become successful unless you know what you're doing and unless you like it.

Mastering any field is hard. You can't do this only under someone's influence. And nobody knows what you should do better than you. You want to rest today and feel like playing games all day? That's your decision and you are responsible for it. You should learn to be responsible for your decisions and your life. Being responsible for your decisions is mandatory for every successful specialist.

So what do you do in that situation? Stand your ground, don't listen to anybody and do whatever feels worthwhile to you.
AnswerKenntnis:
You requested books. Honestly, you will do better solving problems than reading books. Find a puzzle or a problem you are passionate about. Project Euler is a great source, and CodeGolf can offer interesting questions on occasion. My first programming project was a C++ app which found prime numbers. The textbook I was using at the time offered a simple implementation like this:

bool isPrime(int x){
    int c = 1;
    while(c < x){
        if(x%c == 0){
            return False;
        }
        c=c+1
        /*c+=1 if compiler is set to '98 or more recent*/
    }
    return True;
}


which is massively inefficient. I found about the first 500 primes with that function before giving up in disgust at its slowness. Dad and I had a great time trying to optimize above, and even though it was a simple app that didn't even write its output to a file I had fun and the resulting product was cool. 

Will you learn by reading books? yeah, but I cannot emphasize the value of just mucking around with a programming language and learning by trial and error.

If you are learning programming for your dad, stop and find your own reason do do it. Mine still hounds me to build him a high-volume automated trading system. I'm deadly serious when I say that you need to find something you want to work on because otherwise you'll probably abandon programming altogether because you will come to see it as a chore not a hobby or a sport.

Try reading Linus  Torvalds's book "Just For Fun" in which he recounts the origins of the Linux operating system. The title says it all.

Seriously. Just go hack on some problem. When stuck, get the old man involved. 

If your dad doesn't appreciate your work, its not because he's disappointed. He's an architect, he doesn't know how complex some things are. Try to walk him through your code, if only the logic and make him appreciate what it is about that logic that you learned by writing it.



TL;DR




Just mess with something. Solve a problem. If you can't solve it, find someone else's solution and try to understand it.
Have fun with #1
Whenever you learn something or make headway, remember to tell your old man.
Pace yourself. Everyone learns in their own way and at their own rate. Just keep at it and eventually you'll have the skills to do whatever it is you want.
AnswerKenntnis:
Try finding a book that's specifically an introduction to programming for novices.  Maybe something like Hello World! Computer Programming for Kids and Other Beginners by Warren Sande.

You should also see what your library has.  Most libraries should have a few beginner books.
AnswerKenntnis:
In my mind you have two problems : 


You want to learn programming and keep it fun (the fun is essential, it makes everything easier)
You have a customer that really doesn't understand a thing about programming but has an idea what he wants. (Your dad in your case)


IMHO:  


For your first problem, you just go to sites like this and ask questions like you did. Find resources like these How to become a Professional Programmer? . Think of something you want to create (a game, to-do list, movie collection management system, the next best social platform) and just start coding. Or start with solving puzzles Programming Puzzles 
Your second problem is harder, you need to educate your dad (while he doesn't seem very willing) While you learn you will get better in explaining him what is hard about programming. You could try to use metaphors like explained her What's a good Programming Metaphor? . Another tip is that non-programmers in general don't get the complexitys of a great algorithm but are easily imprest by nice looking interfaces. Depending o nthe platform you choose you can generate pretty looking interfaces pretty easily to impress your dad. Use for example:


http://rubyonrails.org/
http://www.microsoft.com/visualstudio/en-us/products/2010-editions/visual-web-developer-express 
other developers here at programmers exchange probably also have more recommendations. 



Hope this helps.
AnswerKenntnis:
I'd just like to add that I was in a very similar position at one time in my life, my dad didn't really understand what positive reinforcement meant. But seeing as I was persistent and resilient towards his sometimes harsh disapproval I managed to keep at it. Some twenty years later the relationship with my dad had changed a lot, it improved but the subtle nuances of back then, are still there. 

This is just a difficult time in your life and the important thing here is that you have fun doing this more than anything else.
AnswerKenntnis:
Regarding the programming language, I agree that you should focus on just one.

Actually, I recommend one that you don't have yet: Python. Python is a language that is quite easy to learn, but also quite powerful. C++, Java, PHP, and Javascript are all much more complex.

There's several books from Manning that teach programming using Python. I'd strongly consider "Hello, World!", even if it is aimed at a slightly younger demographic.


Hello, World!
The Quick Python Book
Quick and Easy Python (not published yet)


If you do want to continue with C++, then I recommend this book:


The C++ Standard Library: A Tutorial and Reference


Though it's more of a "reference" and less of a "tutorial". I'm not aware of a good C++ tutorial - there's a ton of "teach yourself C++ fast" kind of books out there, but I don't think they're very useful. (C++ is a pretty difficult language to learn first; most C++ programmers started out on an easier language).
AnswerKenntnis:
About C++, Java, PHP, JavaScript and Android (so that you might be able to choose between them, because trying to master all at once is likely to fail):


C++: It is an extremely powerful language. But too powerful, too unforgiving, too cryptic to start with. You have to understand far to many things to get going. I think, this in a poor choice of language for starting too program. 
Java: A popular choice for beginners. In a sense, it is the opposite of C++: C++ offers you about any imaginable way to shoot yourself in the foot, while Java attempts to not allow anything that could be beyond your control, which is in fact quite paralyzing. It is a little too simplistic, too trivial, too restrictive to show you much of programming. And you can't get very much done in Java without knowing a lot of the standard API and several frameworks. Java as a technology has a lot to offer, but has its shortcomings as a language.
PHP: A very popular language, mostly because of its low entrance barrier. PHP as a language has matured and is now rich with the features one expects from a modern language. However PHP carries around a lot of baggage for historic reasons. So while it actually allows writing good programs, few people do and you will not find so much information about how to do it. And the standard library is a mess. Should you decide to write PHP, my advise is to start working with a framework right from the start, as they usually promote robust solutions to common problems. Personally, I recommend symfony, flow3 and CakePHP. However, my advice is: don't start with it.
JavaScript: A surprisingly powerful language, once you get to know it. It has a "few" quirks, but in fact you should be able to live with that. Although initially used to add interactivity to HTML pages, JavaScript can now be used in a number of fields. Apart from classic use, it can be used for Desktop and Mobile app development with platforms as Appcelerator, PhoneGap and AIR and to create servers using node.js.
There are many JavaScript libraries and frameworks out there. I suggest you check out knockout and jQuery as well as qooxdoo and ext.js if you're looking for something full-blown. Also, for serverside development, you should check out express.js.
Also, I'd like to point out CoffeeScript, a language that compiles to JavaScript, but has quite a few extras, that come in handy.
Android: Unlike the other four, this is a platform. Platforms should be chosen depending on what you want to do. If it is mobile app development for Android devices, then go for it. Not sure it's the best thing to start with, but ultimately you need to create things you think are cool.


In any case, what's really important is, that you find this enjoyable. That you create things, you think are cool. That solve some of your needs, or that are fun to play with. Programming is for those who enjoy it. You need a toolset, that allows you to build apps with few lines of code. JavaScript/CoffeeScript might be a good starting point.
Personally, I would like to point you to Ruby. It has taught me a lot about programming and I feel unfortunate for not having known of it when I started programming. Basically, there are two formidable books (both available for free):


Why's Poignant Guide - Personally, it was a little too much distraction (jokes, cartoons, etc.) in that book for me, but you might enjoy just that.
Pragmatic Ruby - Worked perfectly for me. It is a little dry, but it simply deals with the essentials.


Along with that I suggest you check out shoes. It is a great tool with an awesome integrated help, including reference, tutorials and demos. You'll have your first things up and running within days.

And, probably for later, I would like to point you to haXe. I think it is a great language (my language of choice), and there's a brand new beginner's guide, that has been issued just recently. However haXe does not have tools available, that make it equally simple to create applications as with JavaScript and Ruby. Therefore you might find it tedious or even frustrating to start with, which defeats the whole purpose.
AnswerKenntnis:
That's quite a bit to try to expect in one month. Rather you should try to concentrate on learning one language or paradigm at a time or it could get overwhelming. Getting too much shoved at once is also a good way to get turned off on it.  You may learn better by working on a specific program that you're interested.  If Android phones is where your interest lies you're probably better off going with Java rather than C++.  He may have been expecting some whiz-bang UI thing as a typical customer would so don't be discouraged by his disappointment :)

Some of the Head First books can help make the learning a little less dry if you're constrained to stick with books.
AnswerKenntnis:
I think you should put the more formal stuff away for now, and have a look at Scratch - http://scratch.mit.edu/ - it allows you to deal with most programming constructs in an easier way while still learning you the stuff you need like loops etc.

It also allows for flawless multithreading which is perhaps the hardest part to do by hand, and which is needed to give interesting results in todays world.

Do not underestimate it because it uses colours and a lego brick like approach to programming.  You can do a lot with it, without getting lost in technical details.
AnswerKenntnis:
Ask your dad if he could design a high rise building at your age. That is what programming is. It takes time to learn, because there are so much to learn. It's like riding a bike, only you have a thousand pedals, gears, handles, knobs, and you can't pick and chose which of them to use at any given time. It takes practice.

I don't doubt you. You have dabbled with C++ early on in your goal to become a programmer. That is impressive to me.

Hang in there, and good luck with your career as a programmer!
AnswerKenntnis:
I know there is a lot of answers already.
But I haven't see this advice: try to get your father to help you. Try to pick something hairy in the C++ book, that you can understand and ask him to help.
Give him the book and let him crawl in C++.
I'm pretty sure this will make him realize how tough it is to code.
AnswerKenntnis:
Try to turn this situation for your own good. Just accept that what you accomplished until now is not enough and try to do better... You'll have enough hard times like these when you will be working for a boss if you don't start getting over it and improving from now on. 

If only I worked twice as hard when I was your age...
AnswerKenntnis:
Your father's input isn't conducive to your learning process. Yes, there are a number of ways to go through learning programming as others have mentioned. However, you have shown phenomenal progress in C++ in my opinion. Your adding program would've been something I could barely handle after my first semester in java programming.

There are two things you have to tackle if you want to continue:

1) Handle your father.

Look, every dad has extravagant dreams for their children. However, their expectations can be very high and will eventually lead to something not even possible in some fairy tales. I'd recommend bringing in someone external in the situation to better evaluate your progress and bring your father down to earth. You can try talking to family/friends you know with programming experience or even people in this community can email him on behalf of our own experiences and knowledge.
This is the most important step of all, because if your dad doesn't cool out, you will lose interest all together.

2) Find a focus.

It is not conducive to learn everything at once. Pick a language and stick with it. As you read a book on a specific language. Following the book's examples verbatim isn't the way to go exactly. Regurgitation is a learning process for some, but not all (especially myself). It is important to attack it from as many angles that you can and are angles that you are interested in.
Programming books by different authors on the same language but different uses can grow your curiosity on how you would like to use the language and insight to your own desire from programming (make games? design applications? testing? ect, ect). You can also go on a personal adventure into creating a program. Figure out what you want to do regardless of what you know at the moment, then research it as you program.
As time goes on, you'll eventually want to try out other programming languages that are related or not. You may also realize that you don't want to do programming all together.

Above all else, remember that it isn't the end result that matters, it is the journey. Don't forget to have fun!
AnswerKenntnis:
You have been given some good feedback and advice about the programming aspects of your situation. But I want to add something from a different perspective...

I have the impression that you experiencing some negative thoughts about your Dad - he has put pressure on you and has made some comments that have caused you some grief and worry - and that have taken the fun out of programming. That is a shame and I think most people here would agree that programming is fun - I've been programming for over thirty years and I still find it fun.

But, I think your Dad is on your side really. He did what he thought was right, he was trying to help, he bought you the books to give you a good start. But he got it wrong (and I speak as a Dad, we do get things wrong sometimes!). To him, books are probably where he started in his architectural career (I assume this started before the internet and the www were commonly available, if at all). So to his mindset, books are where you start.

So, assuming that your Dad is on your side (and I am certain that he is) then the problem is that he just doesn't understand how difficult it can be when you first start programming - so show him all the answers that he been posted here - I'm sure your Dad is a good guy and he'll understand and he will support you.

And good luck with your programming - looks like you are making a great start.
AnswerKenntnis:
One thing not mentioned in the other answers:


  As you might know, being a kid (I'm 14) and being told to learn programming out of dull books isn't the easiest thing. I'm always getting distracted.. I told him before that I didn't need to buy books and I could just watch online tutorials.. but no, he's so old-fashioned. He's only letting me use the books.


You certainly need to work on that part. The Internet is every programmer's crucial tool and you have to work with it.

Firstly, programming is about problem solving. When you don't know how to do X in language XYZ, you google it and look for solutions. *Finding solutions quickly i*s as important to a programmer as using a keyboard.

Secondly, another crucial skill is using documentation. Books are usually like tutorials- they offer guidance, but they don't offer you complete knowledge - and this is where documentation comes in handy. For example: you're programming in C++, you have a month of experience. Sooner or later (I'd say quite soon) you're going to need to use the standard library... or maybe you have already used it? If so, that's a good sign, your book is probably not a bad one in that case. Anyway- it will be useful to know what is already present in C++ standard libraries, and what is not. For that, it's useful to keep a reference like http://cplusplus.com/ open all the time during programming.



The internet is crucial for coding, and even more crucial for learning to code. If your Dad doesn't understand it and expects you to learn programming using the books only, then his approach is counter-productive and makes you progress slower (and possibly get bad habits).
AnswerKenntnis:
He has no idea about the process of leaning to program. You can read all of those books and still not be able to write anything decent, because what takes the longest is the genuine understanding of what you can do, and how to approach it. 

You've told your dad that you enjoy programming - don't let him ruin it for you. And certainly don't try and learn 5 programming languages, at least not now. Stick with a relatively simple on like Java, that can be transferred to the others once you've mastered it.

With regards to the books, by the way, it's the right way to do it, because you learn about the language the right way. Following tutorials often leads to picking up bad habits etc. 

Best of luck; and let him know that you're doing it properly, progress is slow but you're learning so so much while you're doing it.
AnswerKenntnis:
I think alot of programmers aged 25-35 and up grew up playing Nintendo and Sega. A good part of them/us made the mental transition of why play a game when you can make your own. It's a very self-serving motivation that can drive you to be a better programmer. It's a start. Later in life you might transition from writing video games (virtual problems) to games with higher stakes (real world problems). Like is this prescription for this patient not going to adversely interact with another prescribed drug, can these trucks make the most amount of deliveries with the least amount of gas in order to reduce pollution, or how can I ensure the purchase of this stock will buy at the price I want when there 1000's of other buying it at the same time. I think your dad would be impressed by you solving these real world problems but he needs to understand that you gotta take baby steps to get there.
AnswerKenntnis:
Architecture is rooted in the physical world, which you have been learning about since you were born. To an architecture university student, playing with legos would be a waste of time. Your dad assumes that since you know math, you are familiar with the fundamentals, and you should be able to start producing things. Well that's not true. He doesn't know the first thing about programming, because he doesn't even realize that it's not a branch of mathematics.

Programming is a new world. The best way to become adept at it is to learn it just like you learned the physical world: Immerse yourself and experiment. In a word, play. It's lego time.

For comparison, consider astronauts. They too have a new world for  which they are totally unprepared. They have to start with the basics like how to move across the room.

As a next step, I'd suggest making a game where the computer picks a random number, and you try to guess it, and it tells you if you're guessing too high or too low.

Try lots of different languages, too! Not all at once, but maybe for a week at a time.

When you've gained some experience, making a full-fledged video game is one of the biggest challenges you can take on as a journeyman programmer. Video games touch almost every corner of our discipline, and building one is a great way to build your skills. It's also a perfect place to experiment and learn at higher and higher levels of skill.
AnswerKenntnis:
Take your own time in learning programming languages the more you practice the more you will get into programming, this time you have created a very simple program which is quite cute for a beginner but as you progress make sure that you make sure that you create a good program which could be actually used by your dad, well if you ask me this is how i helped my dad in his work.

Just have a word with your dad and explain him the things and i am sure that he will truly understand and i am sure that one day you will be an excellent developer.
AnswerKenntnis:
I study as a software engineer and the progress of learning programming at my education has been more than just reading book. 

First of all I would recommend you to learn C, because it is an ease language and many other languages (such as C++, objective-c) are built upon C. The way we did it was that we had to buy a micro chip (in our case an Atmel Mega16 with an STK 500 kit, which is just a board with LEDs and buttons on it), and then we just played around with it, programmed programs to make it bip and bop. That's more fun than just reading and writing hello world programs IMO.

When C becomes a walk in the park for you, move on to C++ and object oriented programming (OOP). OOP is the key concept in many languages and ways to think of programming and is therefore a must if you are serious with your programming. Make sure to understand the theory behind OOP before diving into it - otherwise it might be a hard proces :-)

Last but not least, I will recommend you to study different data types, such as stacks, queues, heaps etc., which are very great to understand when you are programming and reading about new languages.

Good luck my friend!
QuestionKenntnis:
What's the canonical retort to "it's open source, submit a patch"?
qn_description:
The danger of ever suggesting some feature on a product, especially open source, is that you'll get the response, "why don't you do it?".

That's valid, and it's cool that you can make the change yourself. But we know practically that products do often improve as programmers listen to the voice of users ΓÇö even if those users are other programmers. And, the efficient way to make those changes can include someone who's already working on the project taking up the idea and implementing it.

There are some common terms used to refer to software development problems. e.g. Bikeshedding. Is there a common term used that essentially replies, "Yes, I know that I can change just about anything in the world ΓÇö even closed source. I could get hired, and go write that code. But in this case I'm just making an observation that may in fact be useful for another coder already well suited to easily make that change ΓÇö or just generally discussing possibilities."

[p.s. (a few days in) - I should have pointed out that "submit a patch" is often said with wry humor, and I'm seeking an appropriate witty response. ;)]
AnswersKenntnis
AnswerKenntnis:
It's a difficult point: since the user doesn't directly or indirectly pay for a product, she cannot ask for a feature to be implemented. It's not as if you were a stakeholder or a direct customer who ordered the product, and not even an end user of a commercial product.

This being said, "submit a patch" is not a valid answer. It's not polite. It's not correct. Even for an open source product. "Submit a patch" is the short version of:


  "we don't care if you like our product or not. Go and modify it if you want, but don't bother us with your customer requests."


What about submitting a patch?

Well, it's not so easy. To do it:


You must know the language(s) used in the open source project.
You must be able to load the source code from the version control to be able to modify it.
You must have all the correct versions of any build dependencies installed (including both runtime libraries and build tools).
You must be able to compile this source code, which is not so obvious in some cases. Especially, when a huge project takes a few hours to compile and displays 482 errors and thousands of warnings, you may be courageous to go and search for the source of those errors.
You should understand very well how the project is done, what are the coding style to use, if any, how to run unit tests, etc. If the project doesn't have a decent documentation (which is often the case for open source projects), it may be really hard.
You must adapt yourself to the project and to the habits of the developers who are participating actively to the project. For example, if you use .NET Framework 4 daily, but the project uses .NET Framework 2.0, you can't use LINQ, nor Code Contracts, nor other thousands of new features of the latest versions of the framework.
Your patch must be accepted (unless you do the change only for yourself, without the intent to share it with the community).


If your intention is to actively participate to the project, then you can do all those things and invest your time for it. If, on the other hand, there is just an annoying minor bug or a simple feature which is missing, spending days, weeks or months studying the project, then doing the work itself in a few minutes is just unreasonable, unless you like it.

So is there a canonical retort to "it's open source, submit a patch"? I don't think so. Either you explain to the person that she's impolite, or you just stop talking to her.
AnswerKenntnis:
The canonical retort is to submit a patch.
AnswerKenntnis:
This is the standard answer when developers don't think they will get around to doing something in any reasonable timeframe, but it's been repeatedly brought up.

It's most unfair when it's been repeatedly brought up, but the person who's most recently mentioned it doesn't know that, and just gets "we are taking patches for that" right away. In this case the maintainer is fed up with the discussion but the user thinks it's a new topic. Anyhow, most likely if you get "taking patches" right away, you shouldn't take it personally but might want to read over the archives and bug tracker for more details on the issue.

If you are repeatedly bringing up a request yourself, "taking patches" is potentially intended to be a relatively polite brush-off, vs. some less polite alternatives...

And then of course there are rude maintainers who will say "taking patches" with no explanation ever to anyone, but I'd say that's a minority.

If you've ever maintained an open source project with a lot of users, you'll know that there are 100x more requests than the maintainers could ever get to, and many of those requests are important to the requester but would be outrageously difficult, or would disrupt a lot of other users, or have some other flaw that's only visible with a global understanding of the project and codebase. Or sometimes there are just judgment calls, and it takes too much time to argue every one over and over.

Most non-open-source companies will not give you access to the developers at all, and you'll just get the silent treatment or a polite but bogus story from customer support. So, in open source at least you have some options (pay someone to code the feature, etc.) and while developers might be rude, at least they give straight answers. I'd rather have "no" than the usual "it's on our roadmap... [2 years later] ... it's still on our roadmap" kind of thing I've gotten from a number of vendors...

So I don't think there's a retort. Maybe the open source maintainer is just really busy, maybe they're a jerk, but either way, they likely have a tough job and getting into a who-has-the-last-word debate isn't going anywhere. The best you can do is contribute in some way and try to be constructive.

Maybe it isn't code, but possibly there's a lot of analysis and documenting user scenarios you could do. When I was maintaining the GNOME window manager, lots of times it would have been helpful for people to go analyze a problem globally considering all users, and really write down the issues and pros and cons and what should happen from a global perspective.

(Instead, the usual thing was to start flaming as if they were the only user that mattered and there were no tradeoffs. And while that's great, and was a datapoint, and often I managed to stay polite or even solve their problem eventually... flaming does not make anything happen more quickly. It just confuses emotions into the issue and wastes everyone's time.)
AnswerKenntnis:
The reason you get this response is not that the maintainers are jerks, it's that you haven't adequately convinced them of the value proposition of them working on your feature for you.

The best response is to start a dialogue about the value of your feature to their community as a whole, to see if you can convince them to change their minds. Maybe they're right and they know more about their own community's needs than you do -- but, then again, maybe not.

If the feature is only valuable to you and of little to no value to the community, I find that money is an excellent motivator, while complaining about their attitude is not.
AnswerKenntnis:
What's the canonical retort to ΓÇ£it's open source, submit a patchΓÇ¥?


There is no reasonable retort that is likely to make any difference.  Attempting to persuade volunteers to do something that they have no intention of doing is a waste of your time ... or worse.

Your options are:


Do what the response suggests; i.e. implement the feature and submit it as a patch.  It is called "giving something back".
Find someone who would be willing to implement the feature for you for real money.  It could be the project itself (e.g. in return for sponsorship), someone associated with the project, or some random "coder for hire".
Find an alternative product.




If you received this response when you made a "helpful" suggestion, consider how you might have responded if you were in his shoes.  For instance, how would YOU respond if you thought that the suggestion wasn't worthwhile / well-thought-out / intelligible / etc, but didn't have the time or patience to engage in a protracted debate?



I've been involved in a long running open source OS project, and one of the most annoying things is people who sit in the "peanut gallery" and pepper you with a stream of suggestions about doing things "better" that:


are incomplete, unintelligible or downright nonsensical,
are untried ideas with an objectively low chance of success,
would require a huge amount of effort to implement, and / or
are counter to the stated goals of the project.


Often the best response is to pointedly challenge the person to get involved in the project ... and hope that they take the hint ... to "put up or shut up".  Unfortunately, the most annoying ones don't even take a hint.

Of course, the other response to such people is to not respond at all, or completely ignore them.
AnswerKenntnis:
The response would be reasonable if you and the programmer in question were equals, and knew just about the same about the code base and the language and all the other things relevant to this particular thing you are pointing out.

You aren't equals (or you probably would just have done it) so I would suggest a proper retort to be:

"There is no way I can possible do it as fast and good as you can, which is why I asked you to help me in the first place. Please!"

I believe that it goes against fundamental human nature to say then "oh, yes, this thing I have spent a long time on and is really good at, is so simple that anybody can come in from the street and do as good a job as I can".
AnswerKenntnis:
The canonical retort is to fork the project.
AnswerKenntnis:
Submit a comprehensive test case.
AnswerKenntnis:
The canonical answer to "submit a patch" is:


  "I don't have the skills, experience
  or time required so can you please
  tell me where to ship the cases of
  beer to the guy who can do it for me"
AnswerKenntnis:
"If you do it, I will include it" is much better than "no."

If you are unable to do the work for one reason or another, explain the circumstance to the project maintainer in private.

If you are unwilling to contribute in some way to an open-source project that you would like to use, then you should be looking for commercial support or another commercial product instead.
AnswerKenntnis:
A good open source project will have a bug/feature request system where users can submit bugs/features and others can vote on them so the maintainers can identify what's important to the community as a whole. The quickest way to get your feature in place however is to submit a patch for it. Period...no ways around that.
AnswerKenntnis:
Personally, I would rather get a response of "This is a known issue, but unfortunately  it is not an issue being addressed any time soon.  Developers are working on other issues.  There is no ETA at the moment."  

The "submit a patch" response is very rude, as it assumes a number of things:


All users of the program are programmers or all bug reporters are programmers.
All programmers know the language the program is in.
All programmers know about every kind of problem a program of any kind might have.
All programmers have free time to work on an open source project.


Even if we assume the "submit a patch" response maker knows all the above, that simply makes the statement sound like "X hours of my time is worth more than the orders of magnitude more of hours of your time you'd to get up to speed and fix the issue".

Generally, when I get a rude response from a developer when I ask about a problem I have or submit a bug, I stop using that program.  I no longer use uTorrent (not open source, but the point remains) for example, because the responses I got on their "support" forum were so rude.  I submitted a problem I had in the Bug Reports forum.  The thread was immediately locked with a link to another thread about a similar, but different issue in a thread (which was also locked, of course).  In the meantime, I opened a thread in the General Discussion forum asking if anyone had found a workaround to the problem.  In the time it took to save that thread and go back and see that my first thread had been locked, my thread in General was locked and my forum account banned for disruptive behavior.  I uninstalled uTorrent and haven't been back since.
AnswerKenntnis:
You don't have to say anything. The very fact that the developers have responded is indication enough that they already know the problem exists and that is causes pain for (at least some) users.

At the end of the day, nothing you say is going to convince the developer to work for you if they don't want to.
AnswerKenntnis:
"Thanks for the response."

Because:


At zero price, demand (requests for features) exceeds supply (available coders to implement said features).
Ragging on anything that's provided free lacks class IMHO.
This is the whole point of FOSS: people bringing vegetables and meat of their own to add nutrition to the stone soup.  If I can't contribute something, then I should be thankful that I can eat at all, and not complain that I'm not eating better.
AnswerKenntnis:
Just replying "submit a patch" is rude IMO, but still...if you use open source software for anything serious, you must be prepared to take care of it should the need arise.

The following is based on a post by Jeremias Maerki (of Apache FOP fame):


  If something doesn't work for you, you have several options:
  
  
  This is open source: you can fix it yourself.
  If you cannot fix it yourself, you can wait until someone has free time and thinks it is fun to implement.
  If that doesn't happen, you can find or hire someone to do it for you.
  


I think it's a very valid full version of the "submit a patch" answer.
AnswerKenntnis:
Every time I see this I immediately start looking for an alternative product. To me this is a dangerous sign that the maintainers either don't care about their users (bad if your project is used everywhere) or have lost interest in the project. Both of these usually mean that the project will die soon or will be plagued by stagnation as developers refuse to move the project forward

(Note that I'm not saying that the very first bug report you see with this kind of response you run. You have to look at a general trend. If most bug reports end with this kind of response, then follow this advice. If its just a few, then those are most likely feature requests that don't fit a the projects goals or are extremely use specific)

As @MainMa said starting to contribute to a brand new project is very difficult. Most developers don't understand this as they've been working on the project for months/years and it makes sense to them. This can sometimes be an honest mistake.
AnswerKenntnis:
"I can work on only one thing at a time, but I can complain about many things at once. I think both functions are useful." - akkartik on ycombinator.
AnswerKenntnis:
There is nothing you can say that will make him do it. After all, why should he? Because of one user's wishes? It's not good enough a reason.

But, if you can collect a reasonable number of users and give rational reasons ("I want it" is not a rational reason.) why that feature could be useful, in general and to you and number of others he just might change his mind.

Although, there is also a special case that must be considered. That a dev. is tired of developing the app, slowly wishing of abandoning it (has other things to do), and so he is slowly abadoning feature requests. Apart frm trying to convince him to release the code, in this case there is practically nothing you can do, even with respect to the above.
AnswerKenntnis:
Open source projects in particular are friendly to bounties or funding of the development of a particular feature, even if the new feature doesn't make it to to the official releases.

Plus, yes, one of the ideas behind publishing open source is for anyone and everyone to have the right and the responsibility to make their own contributions.

With closed source, your best resource is to gather an statistically important group from the user base that wants solutions like the ones you want.
AnswerKenntnis:
In my experience this response is usually given if the user request doesn't fit the project's aim. It happens when people think that you're going to implement everything that they propose to you, and a bit more - for free, open source and a great and happy future.

'Submit a patch' is a relatively rude response (and it should be avoided, of course. Especially in this concise and sharp form. There are many ways to express roughly the same message - for example 'invite' the users to help because you don't have the time to implement it on your own). But as is, it's a clear 'stop wasting my time'-indicator. As such, there's not much you could do about it, nor is there are 'canonical' response.

The best response I can think of is to actually present a patch. Assuming your patch works, you have at least proven that the idea is not totally unrealistic. Usually, this means that the people in charge of the project will re-consider the proposal.
AnswerKenntnis:
"submit a patch" is a legitimate brush off for ideas that don't fit into the goals of the project. It's probably better in the long run to just tell you the idea sucks or you're trying to use the project for something far outside it's intended scope, but "hey, if you think what you're asking is so trivial, why don't you try to fit it into our existing code base" is sometime appropriate.

If it's minor and really of use to the intended users of the project, then just submit the bug report, clearly describe the issue, give steps to reproduce, indicate that you're using the current nightly build, and leave it at that.

What may seem like a minor simple change that would help tons of users may actually be a huge pain in the ass that no one would use besides you. That's the best case for "submit a patch".

It's also possible that you have run into a case like the notorious glibc maintainer who seems to have a one-track mind that his system is the universe, his interpretation of specs is the word of god, and that's all there is to it, regardless of how many people would prefer otherwise.
AnswerKenntnis:
The best I can think of is "you suck".

Sorry this obviously isn't very helpful, but I think this is just one of the unfortunate situations where the user is completely screwed. A brutally honest appeal to the developer's conscience is a last ditch effort.

You could try offering (cough) "donations" to have your problem addressed, but I fear that such a practice if made common would lead to some really bad loss of integrity in the industry, as bug fixes should never be made profitable, either for "Free" or commercial software.
AnswerKenntnis:
I consider that when one is working on a project, providing releases and support, an unspoken, implied, contract of support between dev and user comes into being. The dev has taken on the implied responsibility of supporting the codebase for his users, including adding features at request. 

"Submit a patch" is basically giving the finger to the users, in my opinion. This is contextual - sometimes it's just too much effort to implement, sometimes it would wreck the existing project or incur feeping creaturitis, or any of a host of other reasons. But, ultimately, it is saying, "screw you, not doing it". Which, in my mind, is, in some level, a breach of that unspoken contract.
AnswerKenntnis:
There are several ways it should be done.


Feature proposal and vote. but this takes time.
Get hired by a company who need it to make the patch. Obviously this is the best solution, but get ready to collaborate with the guy who makes the open source software you want to upgrade.
Finding out why the feature is not implemented in the first place is important too. Often the feature is out of the line of the software project: the team doesn't want this feature, don't feel necessary or they just think it's not the good way to do something. In this case you should just fork the project and make it yourself.
Use proprietary software that does what you want.
Remember that OOP software often eases the process of integrating a feature.
Whining on a mailing list, on irc or in a forum will just piss out programmers, and will give ammo to OSS proponents.
AnswerKenntnis:
Switch to well maintained alternative. 

From my experience with well maintained open-source projects is, that if you create well defined bug report or feature request, then it has very high chance of being implemented.
AnswerKenntnis:
I actually signed up just to answer this question. 

Is there is need for a retort? 
this response is usually used when the developer knows about the issue but doesn't think it as important.

I will give you a live example. ubuntu dropped the systray support(but can be worked around by white listing a app) and added new app indicators. some apps like jupiter relied on systray support so the developer told about the workaournd instead of adding app-indicator support so i asked the developer to add the app-indicator suppory  the reply was "Send us patches." on asking the reason they chose not to implement this . there was this


  I have no interest in spending my time building support for a libra1ry that I will never use just because someone with a lot of money demands it by blacklisting my applications ability to function properly on his Linux distribution simply because he can.
  
  If it was a real technical problem, I would probably take action but this is purely a political maneuver, so no I don't think so.
  
  No, I'll just whitelist it


Fair enough. the developer has reason not to implement a feature but is willing to accept patches. this is not really rude and offensive so there was no need of a retort.

bottom line : the canonical retort would be to submit the patch but if you can't there is no need for a retort
AnswerKenntnis:
Start a bounty for the feature you want.

Or go out and buy the product that claims to do exactly what you want and abuse their support personnel when you discover that the marketing doesn't match your expectations.
AnswerKenntnis:
I would suggest creating a project for implementing the feature on sites like RentACoder/ELance/etc, and posting about it on the original open source project's forum. Any of the programmers on the open source projects, including the author, now has a financial incentive to consider your request.
AnswerKenntnis:
I occasionally joke that free software can be free as in beer, free as in speech or free as in you get what you pay for.

While I say it jokingly (I work for a company who use a lot of OSS) but I think there is a truth there - if you want commercial level support then you need to either use commercial software with a suitable support deal, or find an open source software solution that allows you that level of support (usually through someone being paid to provide it but potentially through your organisation employing or assigning development resource to work on it).

"Submit a patch" is infuriating but it highlights something about OSS and perhaps it should be a reminder that OSS isn't right for everyone in every situation, at least not without making sure you've got a solid support framework for it (either in-house, paid for or through the community).

We often think about software which is free as in beer but not as in speech (that is non-open freeware). Perhaps this is a case where we should think about the software as free as in speech but not as in beer.
QuestionKenntnis:
Why was Tanenbaum wrong in the Tanenbaum-Torvalds debates?
qn_description:
I was recently assigned reading from the Tanenbaum-Torvalds debates in my OS class. In the debates, Tanenbaum makes some predictions:


Microkernels are the future
x86 will die out and RISC architectures will dominate the market
(5 years from then) everyone will be running a free GNU OS


I was a one year old when the debates happened, so I lack historical intuition. Why have these predictions not panned out? It seems to me, that from Tanenbaum's perspective, they're pretty reasonable predictions of the future. What happened so that they didn't come to pass?
AnswersKenntnis
AnswerKenntnis:
Microkernels are the future

I think Linus hit the points on monolithic kernels in his debate. Certainly some lessons learned from microkernel research was applied to monolithic kernels. Microsoft sometimes used to claim that the Win32 kernel was a microkernel architecture. It's a bit of a stretch when you look at some textbook microkernels, but the claims had some technical justification.

x86 will die out and RISC architectures will dominate the market

If you back up from desktops and servers, RISC dominates the processor market by any measure. ARM (R stands for RISC) outsells x86 in number of processors, there are more ARM processors than x86 processors in use, and there is more total ARM computing capacity than x86 computing capacity. This year, a single ARM vendor (yeah, Apple) may outsell all x86 vendors combined. Only in the desktop and server space does x86 dominate. So long as Windows is the dominant platform for desktop computers and Linux for servers, this is likely to continue to be true for a while.

There's a part b to this as well. Intel engineers did some amazing work to squeeze life out of their instruction set, even to the point of making a RISC core with an opcode translator that sits on top. Compare to one of the dominant RISC desktop chip makers, IBM, who could not get a power-efficient and high performance G5 for Apple laptops in a reasonable timeframe.

(5 years from then) everyone will be running a free GNU OS

I think the various OS vendors still offer compelling value propositions on their OSes. GNU isn't even necessarily the most important player in the Open Source community, so even a more widespread adoption of open source software didn't necessarily translate into GNU OSes. Yet, there's a whole lot of GNU stuff out there (all Macs ship with GNU's Bash, for example. There's probably some GNU system tools on Android phones). I think the computer ecosystem is a lot more diverse than Tannenbaum foresaw, even when you restrict your view to desktop computers.
AnswerKenntnis:
Software Experts Ignored the Economics Of Hardware

...or "Moore was right and they were both wrong"

The biggest thing that was overlooked in this debate was the impact of CPU manufacturing technology and economics, driven by shrinking transistor sizes as expressed in Moore's Law (not surprising as though they knew a lot about CPU hardware, these guys studied and debated software, not CPU manufacturing or economics). Fixed manufacturing costs which are amortized over CPU's (e.g. ISA design, CPU design and CPU production facilities) have grown rapidly, thereby increasing the value of economies of scale; with per unit CPU costs (in terms of "bang for the buck" & "bang for the watt") plummeting, the cost of a CPU needn't be amortized over such a broad selection of functions to provide value, so computing in products with fixed function has exploded; CPU transistor budgets have grown exponentially, therefore wasting a fixed number of transistors due inefficiencies in ISA design is of no consequence.

1. CPU Scale Wins Over CPU Diversity

The importance of economies of scale has made the benefits of a ISA/CPU targeting a larger (therefore broader) market outweighs the potential benefits from design choices which narrow the market for an ISA/CPU. OS's can address larger and larger portions of the market per supported ISA/CPU, so there is little need (or even no need) for porting exercises to allow an OS ecosystem to thrive. The problem domains ISA's & CPU's target tend to be so broad that they mostly overlap, so for any software beyond a compiler, the size of porting exercises has also diminished. Arguably, both Torvalds & Tanenbaum overestimated the portion of kernel design and implementation that now needs to be ISA or even CPU specific. As Tanenbaum described, modern OS kernels do abstract out the distinctions between CPU's & ISA's. However, the CPU/ISA specific code in modern OS's is much smaller than a microkernel. Rather than implementing interrupt handling/scheduling, memory management, communication & I/O, these non-portable bits address only a tiny fraction of the implementation of those services, with the vast majority of the architecture of even these core OS functions being portable.

2. Open Source Won the Battle, But Lost the War

More bang for the buck means that a larger share of computing is performed by fixed function products, where the ability to modify the product is not part of the value proposition for the customer. Now ironically, open source has flourished in these fixed function devices, but more often than not, the benefits of those freedoms being realized more by those making the products rather than end users (which actually was true of the software market even back then: Microsoft was a big consumer of open source software, but their customers were not). Similarly, one could argue that open source has struggled more in the general purpose desktop space than anywhere else, but as the web and cloud computing has grown, desktop computing has increasingly been used for a narrower purpose (primarily running a browser), with the remaining functions running in the cloud (ironically, primarily on open source platforms). In short: open source does really own the general purpose computing space, but the market has become more sophisticated; computing product packaging less often stops at general purpose function, but continues along to product intended for fixed functions, where much of the advantage of open source computing is in conflict with the product goals.

3. 2n Growth Means Fixed k Savings Are Not Important

The exponential growth of transistor budgets has brought with it the realization that the transistor budget cost of a CISC architecture is almost completely fixed. RISC's strategic advantage was that it moved complexity out of the CPU's instruction set and in to the compiler (no doubt partly motivated by the fact that compiler writers benefited far less from complex ISA's than human developers coding in assembly, but compilers could much more easily reason mathematically about, and therefore exploit, a simpler ISA); the resulting transistor savings could then be applied to improving CPU performance. The caveat was that the transistor budget savings from a simpler ISA was mostly fixed (and overhead in compiler design was mostly fixed too). While this fixed impact was a huge chunk of the budget back in the day, as one can imagine it only takes a few rounds of exponential growth for the impact to become trivial. This rapidly declining impact combined with the afore mentioned rapidly increasing importance of the CPU monoculture meant a very small window of opportunity for any new ISA to establish itself. Even where new ISA's did succeed, modern "RISC" ISA's are are not the orthogonal ISA's described by the RISC strategy, as continued growth in transistor budgets and broader applicability of SIMD processing in particular has encouraged the adoption of new instructions tuned for specific functions.

4. Simple: Separation Of Concerns. Complex: Separation Of Address Space.

The modern Linux kernel (along with most other kernels) fits the rather loose definition of a macrokernel and not the rather than the narrow definition of a microkernel. That said, with its driver architecture, dynamically loaded modules and multiprocessing optimizations which make kernel space communications increasingly resemble a microkernel's message passing, it's structure more closely resembles a microkernel design (as embodied by Minix) than the macrokernel design (as embodied by Linux's design at the time of the discussion). Like a microkernel design, the Linux kernel does provide generalized communication, scheduling, interrupt handling, and memory management for all other OS components; its components do tend to have distinct code and data structures. While modules are dynamically loaded, loosely coupled pieces of portable code, that communicate through fixed interfaces, the don't employ one remaining property of microkernels: they aren't user space processes. In the end, Moore's Law ensured that issues motivated by hardware concerns like portability (a concern of Tanenbaum's) & performance (a concern of Torvalds') diminished, but software development issues became of paramount importance. The remaining unrealized advantages that a separation of address spaces could provide are outweighed by the additional baggage imposed on the OS software due to design limitations and increased complexity of component interfaces.

Interestingly, what has been a strong trend is the emergence of the hypervisor, which much like microkernels, abstracts out the hardware. Some claim that hypervisors are microkernels. Hypervisor architecture is different though, as responsibilities meant to be owned by microkernels are handled by the "guest" kernels sitting atop, with hypervisors multiplex between them, and the hypervisor abstraction is not generic messaging and memory address space, but predominantly actual hardware emulation.

In Conclusion: The Future Favours Those Who Adopt Least Strict Semantics

*..or "nitpickers suck at predicting the future"

In practice a lot of the rightness/wrongness in the debate is a matter of semantics (and that was part of what Torvalds was arguing and IMHO Tanenbaum failed to fully appreciate). It's hard to make precise definitions about the future because there are so many factors outside of the argument that can come in to play; looser semantics means your predictions are a larger target on the dartboard than the other guy's, giving you way better odds. If you ignore semantics, the arguments advanced by both Torvalds and Tanenbaum were right about a lot of things and wrong about very little.

tl;dr

Most ISA's don't fit the semantic definition of RISC, but harness most of the design advantages which were distinctive of RISC CPU's at the time; the amount of OS which is CPU specific is less than Tanenbaum expected, let alone Torvalds; open source does dominate general purpose computing, but the consumers of that market are now primarily those who package computing in to more fixed function products where much of the benefit of open source software isn't realized; separating out OS function across address spaces didn't prove to be beneficial, but separating out OS function across "virtual" hardware has. If you want to claim your predictions proved right, leave yourself as much semantic maneuvering room as possible, just like Mr. Torvalds.

P.S. A final ironic observation: Linus Torvalds is one of the strongest proponents of keeping as much new functionality as possible up in user space and out of the Linux kernel.
AnswerKenntnis:
Microkernels are the future
  


He got that wrong, seems everything is converging into using hybrid kernels. Linux formally still is monolithic, but adding stuff like FUSE etc. make it look bit hybrid too. 


  
  x86 will die out and RISC architectures will dominate the market
  


Ok, so x86 didn't die out. But doesn't RISC dominate the market? Billions of smartphones using ARM, all gaming consoles using RISC processors, most network hardware using MIPS processors.

Besides, by early-2000s RISC and CISC have converged so much, that there were no clear cut differences in internal design. Modern x86 processors basically are internally RISC with CISC interface. 


  
  (5 years from then) everyone will be running a free GNU OS
  


If by GNU OS he meant GNU Hurd, then indeed a totally failed prediction.  What people do massively use is Android. Android is Linux, however it is not GNU, as it doesn't use GNU libc. Instead it uses Google's own Bionic. And the market share of standard desktop Linux is still below 2%. But did he really mean consumer PCs? On server market Linux absolutely dominates with 70-90% of share depending on the segment.
AnswerKenntnis:
Not sure.
Half-right.  Today's "x86" chips are RISC under the hood, with basically a "CISC interface".  x86 didn't die out because Intel had enough market share and enough revenue to make that transition and get it right before other RISC solutions captured significant market share from them.
Two main reasons: compatibility and usability.

Again, the existing systems (Windows and, to a lesser extent Mac OS) have a very large installed base.  That means lots of users using lots of programs.  Free GNU OSes can't duplicate that.  The WINE project has done a lot of work in that direction, but it's still no substitute for an actual Windows system, and the WINE developers don't even try to claim that it is.  People don't want to use an OS that won't run their favorite programs, no matter how theoretically awesome it is.  (And without an installed user base, no one wants to develop for the system. It's a chicken-and-egg problem.)

And then we get to usability.  My mom has a Windows system.  She can use it just fine for her purposes.  Everything she needs to work with her computer is available from the Windows interface, and if I said the words "command line" to her, she wouldn't even know what I'm talking about.  As far as I know, it's still not possible to do that on any free GNU OS.  In fact, Linux is so hard to work with that even the greatest of the community demigods have serious troubles with simple tasks sometimes.  And they never seem to "get it" and work to fix the issues, which is why they never gain market share.  (The linked article should be required reading for anyone who ever tries to produce any mass-market program!)
AnswerKenntnis:
I think there are a couple of reasons that are pretty serious, but haven't been mentioned.

The first is Tanenbaum's rather blind assumption that technical superiority would lead to market dominance. People have argued for years about whether microkernels (nanokernels, picokernels, etc.) are technically superior, but for the moment, let's just assume they are. We're still left with a question of whether that technical superiority is likely to translate to market dominance. I will posit that it doesn't. For most people, Windows, Mac OS, etc., are  good enough. Worse,the improvements that would make a significant difference to most users would be in the user interface, not the kernel.

The second is drastically over-estimating the rate of change in a (reasonably) mature market. It's pretty easy to change things in a hurry when you have a new market. To get essentially everybody changed over in five years, the migration would had to have been happening at full speed even as he predicted it.

I'd also note another way in which he was right that I haven't seen mentioned. People have already noted the ubiquity of RISC (e.g., in cell phones). What they haven't mentioned is the ubiquity of what I'd call "microkernel 2.0". This is now more often known as a "virtual machine" or "hypervisor". They really are pretty much microkernels though.

The big difference is that Tanenbaum thought in terms of a microkernel and a user-mode OS emulation both designed specifically for each other. Instead, we've kept the OS essentially unchanged, and tailored the microkernel to run it as-is. This isn't as nice technically, but from a market perspective, it's dramatically superior -- instead of a whole new system top to bottom, the user can continue to use most of his existing code as-is, and just add a cool new "utility" that happens to really be a microkernel OS.
AnswerKenntnis:
One big reason was Windows, especially Windows 3.1 and a little later, Windows 95 and NT 3.51. Consumers particularly loved the GUI interfaces as opposed to the old text based systems of Unix and DOS. This meant more average people would purchase computers for home use. Also, the explosion of the Internet in the mid-90's increased sales.

Prices for PC's also dropped throughout the 90's until they reached the point where they are today. This was due to the economies of scale presented by an increased consumer and business demand. For example, all five of my current computers cost less combined than the one 486 desktop I purchased in 1992.

Now, in a way he might be right but in an unexpected direction. The rise of mobile devices, smartphones and tablets, have partially brought about simplified operating systems and may reduce the prominence of x86. However, they go far beyond what was predicted in 1992.
AnswerKenntnis:
All are true if you don't think that a computer is something on your desktop.


True - microkernels never worked because they were never micro enough. If the whole of your stripped down embedded linux is smaller than the x86 specific bit of the MACH kernel is the question of micro-kernels relevent?
RISC is dominating the market. More ARM cpus are sold each year than X86 cpus ever. You are probably never more than 6ft from an ARM cpu.
Almost everybody is running linux, it's in their router, their TV setop box, their Tivo and their Android phone - they just don't know that these have an OS
AnswerKenntnis:
Ultimately it all comes down to the fact that things don't really like to change. 

We didn't migrate to a better-architected microkernel because monolithic ones were easier to create, ran faster, and everyone knew how to build them. Also because Linux was developed as a monolithic kernel and became popular, there were no microkernels that achieved enough success to take off. (its a bit like the same reason we all run Windows, we run Windows because everybody runs Windows)

RISC, others have pointed out that x86 is pretty much a RISC architecture nowadays, with a CISC wrapper on top for backwards compatibility.

A lot of people are running a free GNU OS - on the server. The web is pretty much driven by it. Nobody notices because all the clients are Windows. Back in those days, you had a choice: Linux that was still very much a hobby OS; a flavour of Unix but you couldn't afford to buy it; or cheap n cheerful Windows. Linux took too long to replace Unix, and still doesn't quite have enough of a compelling solution to running on the desktop (partly because of ideological problems with different Window systems, binary graphics drivers, and the lack of a stable ABI). It's doing rather well in other non-desktop markets like embedded and mobile however.
AnswerKenntnis:
1)  He was wrong on the microkernels.  To my understanding the need for speed requirement trumps the separation of concerns enforced in microkernels, at least in the Linux kernel.

2) The predominant architecture in tablets and mobile phones is ARM which is a RISC instruction set.  Even Windows has been ported.

3) Everyone does not run a free GNU OS.  This is primarily because of patents and backward compatibility. Those not wanting Windows frequently choose OS X.
AnswerKenntnis:
Microkernels replace method calls with inter-process messaging, which adds development complexity for some hypothetical benefits. As it turns out for the most part you can get just about the same advantages from good componentization even if everything is living in one big process.
The question is no longer relevant. CISC architectures no longer exist, all modern processors are RISC in their hearts, but that didn't kill the x86 instruction set. x86 processors since the Pentium-Pro era (17 years ago) use op-code translation to allow an essentially RISC core to look like an x86 CPU from the outside.
Classic Worse is Better. Iteration, pragmatism, network and ecosystem effects beat purity every time.
AnswerKenntnis:
Production became cheaper, x86 came so close to the price of RISC, that it wasn't feasible anymore to use it. There was also a small vendor lock-in.
AnswerKenntnis:
For 2:
The CISIC instruction set has a big advantage: It's more compact, like compressed machine code. Nowadays it's very cheap to decode CISC instructions into micro-ops and it's very expensive to access the RAM. So CISC has the advantage of pushing more code in the L1/L2/L3 caches
AnswerKenntnis:
Microkernels are the future
  x86 will die out and RISC architectures will dominate the market
  (5 years from then) everyone will be running a free GNU OS


It depends how you view and define future, in the traditional sense his predictions have failed.

However the time has not yet ended (another deeper discussion aside).

Thus, things still could change:


Microkernels or some variant could make a comeback 
RISC/ARM may well dominate -> tablets / mobiles
10 or 15 years from now. Who knows, open source is changing the world slowly..
AnswerKenntnis:
1. Microkernels failed 

For the reasons that Linus Torvalds stated, that on paper it looks theoretically attractive but in implementation on modern systems - which are very complex systems - the complexity becomes exponentially unmanageable.  Case study is GNU Hurd, a fully microkernel system which failed to even achieve basic functions.  Mac OS X is similar to Hurd in structure and it's the least stable and most constrained OS out there.

2. CPU architecture 

This has become diversified for various use cases.  One CPU architecture did not dominate because embedded, mobile, desktop, server and so forth use cases are different and required different approaches.  Tannenbaum failed to see this diversification.

3. GNU vs World?

GNU did not dominate, but Linux did on server, embedded and mobile.  Apple tablets and phones run iOS which is just plain old Unix.  Accurate statistics are difficult to acquire for Linux deployments on desktop because there is no real core mechanism - sale of units - that can surely give an accurate value.  Most Linux deployments on the desktop are sometimes recorded as Windows deployments because users buy a Windows system and then write over it with Linux.  However, if you segment OS's then Linux has around 5-6% on the desktop according to http://www.w3schools.com/browsers/browsers_os.asp and this is equal to the number of Windows Vista users worldwide which is highly significant.  

Based on my own estimations from various sources it seems that Linux on the desktop could actually be equal to the number of users on Windows XP - roughly 25% - if you count non-Western nations like China and India where Linux is more popular than in the USA or EU but who might not be counted in Western statistics because they only count traffic to English speaking websites aimed at Westerners. 

In India most college students use Ubuntu or Fedora because this is the default OS of the Indian education systems and at the famous IIT's. Most Indian government offices use Linux also.  In China Red Flag Linux is the offical OS of the Chinese government and school systems - Academies of Arts and Sciences - and is the recommended OS in China by the state run media as an effort to stop young impoverished Chinese from using pirated copies of Windows.  If you counted the useage of Linux in India and China it would shock most Western tech experts and radically change the perceptions of the true dominance of Linux desktop in non-Western developing nations where it is dominant.
AnswerKenntnis:
I remember the time -- and the time that preceded it. Dunno about microkernals, but

2) The idea of RISC had two legs: that software optimizations could be done in software better than in the hardware, and that RISC chips could economically be made that were faster than CISC chips. 

Both ideas turned out to be false in the short term. Intel could, and did, make CISC chips that clocked instructions faster than the competing RISC chips, at a competitive price. Intel could, and did, make CISC chips that did program optimization better in hardware than could be done in the compiler, or in the software runtime supervisor --and any software optimization could be added on top of that, just as it would with a RISC chip.

3) Computer Science, Programming, and Operations, have been completely re-invented 4 times in my career. From main frame to PC. From command line to GUI. From GUI to internet. From Internet to iPad. Revolution seems normal now, but WE DID NOT PREDICT THAT. Like all older programmers at the time, he was predicting the 'end of history'.

Very few people were running a GNU OS in five years, because the count restarted. 

Perhaps it is still happening. 5 years ago you would have predicted that our Windows Server would be replaced by *nix servers (As I write, SAMBA has released an AD Domain Server, which was the missing piece of the puzzle). It's not going to happen where I work: we won't have any local servers.
QuestionKenntnis:
Why do programmers write closed source applications and then make them free?
qn_description:
As an entrepreneur/programmer who makes a good living from writing and selling software, I'm dumbfounded as to why developers write applications and then put them up on the Internet for free.  You've found yourself in one of the most lucrative fields in the world.  A business with 99% profit margin, where you have no physical product but can name your price; a business where you can ship a buggy product and the customer will still buy it.

Occasionally some of our software will get a free competitor, and I think, this guy is crazy.  He could be making a good living off of this but instead chose to make it free.  


Do you not like giant piles of money?
Are you not confident that people would pay for it?  
Are you afraid of having to support it?


It's bad for the business of programming because now customers expect to be able to find a free solution to every problem. (I see tweets like "is there any good FREE software for XYZ? or do I need to pay $20 for that".) It's also bad for customers because the free solutions eventually break (because of a new OS or what have you) and since it's free, the developer has no reason to fix it. Customers end up with free but stale software that no longer works and never gets updated. Customer cries. Developer still working day job cries in their cubicle. What gives?

PS: I'm not looking to start an open-source/software should be free kind of debate. I'm talking about when developers make a closed source application and make it free.
AnswersKenntnis
AnswerKenntnis:
Because I don't want to feel obligated to provide technical support or offer refunds.
AnswerKenntnis:
Sharing

Most of us make use of software that has been provided to use free of charge. As a result, it makes sense to share our own software free of charge as well. Basically, we are exchanging our software for the other free software but without the overhead of actually going through a transaction. There will be leaches who do not contribute, but since distribution is so cheap that does not matter.

Selling is Hard

Actually trying to sell software makes the process much more difficult as you have to market, collect money, and worry about the legal ramifications of selling to people. For a lone programmer this takes them away from what they really want to be doing. As a result they may release their program simply so that other people can have benefit even if they cannot.

A New Model

It might be argued that a new model of software development is arriving. The model of selling software is an attempt to take physical-world selling and apply it to software. However, software is not like the physical world. Because distribution is so cheap a couple of issues arise. 


Letting someone use your software is basically free for you.
Attempting to prevent people who haven't paid for the software from using it is really expensive.


Under this view, attempting to charge per copy of the software is a losing game. Thus you should attempt to make money on software-related services, not software itself. Thus you might charge for a support contract, hosting services, etc. rather than the right to use the software itself.

Incidentally, this model is used by webcomics, web series, etc. which give the primary product away for free and sell related merchandise.
AnswerKenntnis:
Releasing free apps and working on open source programs are great advertisements for selling a product, namely you. (Alternatively phrased: free apps are a loss leader for selling your time.)

There's also the concept of the "gift economy", where the more you give away the wealthier you 
 are. Why would I not donate back to my peers/society at large when I have received so much from so many people?

Lastly, what other field allows you to directly affect the lives of millions of people by writing something that makes their lives that little bit easier?
AnswerKenntnis:
I suggest that you watch this fantastic video to learn why money is often not the motivation for doing things: RSA Animate - Drive: The surprising truth about what motivates us 

I recommend that you watch the whole thing, but it also directly answers your question around the 6:40 mark.
AnswerKenntnis:
Some people write programs for the fun of it—selling it turns it into work.
Some people rank the number of people who use their programs above how much cash they get for it—selling it pushes down the first where they don't care much about the second.
AnswerKenntnis:
I release my software for free because I have spent time and energy on it but have neither the time or inclination to market it, someone might-as-well benefit.

By personal philosophy is (and I do sell software too), "Competition makes you better".

If you can't create a product that blows the competition (free or not) out of the water you're going to be in trouble.
AnswerKenntnis:
A lot of free apps are created by someone who is fully employed and has come up with an idea for an application that they produce in their spare time.  That person doesn't "need" the money to survive.  

A lot of times finding the mechanisms to market, sell and collect payment are just not worth the effort and sometimes individuals just enjoy offering something they thought as useful to the general public.

If you are competing with a free application then the best strategy is to make a better product.  I've often purchased an application over using a free version just because it offered more features or was better implemented in some way.
AnswerKenntnis:
There does come a point where enough is enough, and then there is the fact that it does take more effort to sell something even though it may be a small effort. I still need to come up with a way to collect money for example.

I think the reason I post free apps that are closed source is simply because I love full featured freeware myself, so I like sending it out to the world with the same idea in mind. When I can get a significant task done with a completely free software package it feels great, so I like to share that.

Really if the answer of 'why not make it free?' comes down to 'because you can get piles of money' then it all is about what your motivation for releasing some software is. Not everyone is motivated by more and more cash.
AnswerKenntnis:
I see two main reasons:


An individual programmer may just want to be known and loved.
There is an alternate economic model behind the scene. Some famous examples: iTunes, Acrobat reader, Firefox, Ubuntu are all free but their promoters all make money with these products (selling entertainment, paid features, audience for search engines, support).
AnswerKenntnis:
Why does anyone offer free advice here on Stack Exchange when some people make money answering technical questions? I think this points to a basic psychological need to be generous. Jorge Moll and Jordan Grafman, neuroscientists at NIH, have found that charity is hard-wired in the brain. See the Washington Post article ``If It Feels Good to Be Good, It Might Be Only Natural'' at http://www.washingtonpost.com/wp-dyn/content/article/2007/05/27/AR2007052701056.html

Both Kohlberg's theory of cognitive development and Gilligan's ethics of caring view people as interdependent and developing towards increased empathy and altruism. This behavior is necessary for humanity to survive and thrive.

Lewis Hyde says there are two types of economy: (1) The exchange economy (economy of scarcity), where status is accorded to those who have the most and (2) the gift economy (economy of abundance) where status is accorded to those who give the most. Examples of gift economies include marriage, family, friendship, traditional scientific research, social networks (like Wikipedia and Stack Exchange), and, of course, F/OSS.

IMHO, Eric S. Raymond and Linus Torvalds performed a miracle: transforming selfish programmers into generous programmers.  This is very similar to how Elisha transformed 2,200 selfish students into generous people with the miracle of ``the feeding of the multitude.'' In II Melachim 4:42-48 Elisha must support 2,200 students. There's a famine. His students are hungry and selfish. Each of them has some food, but they refuse to share with each other. After Elisha distributed a mere 22 loaves of bread to them, they began to share with one another. Soon, not only are they all fed, but there's food left over. The true miracle is not that bread materialized out of thin air, but that those who were once selfish became generous, inspired by the example of one person's generosity. Something similar has happened over the last couple decades, as a result of the release of Linux and other free software.
AnswerKenntnis:
I get paid enough at my day job as a programmer. I mostly code on my own little projects for fun. I release almost all of what I write on my own time for free and under a free/open source license because:


These are fun projects (e.g. an interpreter for a simple language, a tool to clean up JavaScript code, various small scripts, etc.). These are not "enterprise" applications. Not even small applications home users need to get some job done or for entertainment. Okay, there might be a few people who might actually pay a very small amount for some of the tools I write. But really, it would be a trifling sum, and I really don't need the money badly enough for me to consider the effort involved in marketing and selling them.
As someone growing up in the 1980s and early 1990s, and that too in a developing country, I understand how it feels not to have enough money for or access to the tools I need. Payment is a big hassle for a lot of people not living in the west, and even if it is possible, a few dollars can translate to a lot of money for a student on the other side of the world. If most of the people who might actually use these tools wouldn't be able to pay for them anyway, what's the use of charging for them?
As other answers have already pointed out, my own projects, as well as the effort I put into any larger projects that are not owned by me, pays off for me as advertisement for my skills. Apart from such things as making me more liked by other people, it also helps me getting noticed by potential employers and thus helps me career-wise. A freely available software is bound to be better known and more widely used to something of equal quality but not free of cost.


As other answers already point out, if the efforts of a single or a small group of people who are coding in their spare time are threatening the commercial prospects of software written by people doing it to make a living - I think it is up to the latter to work harder to make their product worth spending money on rather than the other way around. If anything, it just sets the bar higher for quality software which is good for all concerned.

It's like saying giving away your old clothes to charity hurts people in the textile industry.
AnswerKenntnis:
I've come across quite a few app where I ask my self "You are asking for $20.00 for this crap?" I know I can do it better and in order to "stick it to the man" I release it for free. 

I understand that there is lots of time and money going into those apps but I also believe that if you are going to put out a product for sale, it should be top notch or just give it away.
AnswerKenntnis:
Quality

Having the source code open, quality can improve drastically. Think other programmers improving the code, think automated source code analyzers. 

Durability

Closed source tends to get lost when there is some better/more competitive product. Open Source can be shared forever.

Sharing...

is caring. Now everyone in the world is enabled to use the functionality in your app, including third-world countries. 

Self improvement
Feedback from fellow programmers is now more possible, is free and is offered by fellow programmers who actually care. 

Freedom

I hate getting locked in by companies. Likewise, I don't want to produce software aiming for the same.

CV building

Instead of emailing a CV, you can now email a bunch of links referring to projects/patches I contributed. Cut the crap, no more bullshit bingo on the CV. Just a list of contributions.

Bible mindset


  A greedy man brings trouble to his family, but he who hates bribes will live.
  (Proverbs 15:27, New International Version)


A business model based on selling apps is usually greedy, a business model based on free software and providing services with them less so.

Viral

Open source software is (depending on the license) more likely to get included in other software packages.

Decrease business risk

Basing software on open source components, decreases dependence on third party businesses. When a business goes down, your business is still able to gain support for the code/software. Android is a great example of how disruptive open source can be, and how current businesses carry higher risk when using certain non-open source software.

Fun

I have a project which is just fun to do. No need to require a business around it, with all the hassle coming with it.

Recognition

You can hardly be recognized by closed source. Open source opens up lots of possibilities to become recognized.

Create services market

Change the market from a per-copy based revenue model to a services-based revenue model. Example: Lots of software around the Google app engine stack is free as in beer. Google makes money from providing the infrastructure.

For the children

Piles of money disappear, but your shared source code never disappears. Future generations will be thankfull for your contribution.

Reinventing the wheel sucks

We stand on the shoulders of giants. What if Alan Turing kept his design proprietary? Would we have a software ecosystem like we have today?

Customization model

Give the software away for free, charge for customizations. For example, offer free CMS software but charge for specialized modules appropriate for custom business requirements.

Winning

Charge less for your product, and you gain customers. Going lower then asking no monetary compensation is hard. You increase chances to outcompete others.

Independence

Charging for software means becoming dependent on paying clients or paying advertisers. You might not want to need money from businesses with unethical practices.
AnswerKenntnis:
Programming can also be a hobby

Many people treat programming as a hobby, writing programs for fun when they get home, and sharing them on the net, or participate in open source projects.

This is just like photographers like to take pictures and share them with the world on sites like picasa or flickr, and musicians that like to create music and share it with the world on sites like myspace.com or mp3.com, then some programmers also like to share their work with the world.
AnswerKenntnis:
Software is free, because it's information. The expression is that "Information wants to be free."

Why is that? Why does information want to be free? Consider Stack Exchange. Do you see how Stack Exchange crushed ExpertSexChange? Why? Because the user interface is superior. What's the biggest way in which the user interface is superior? You can ask a question and get an answer without a credit card.

Money adds friction to the flow of information. Everything about charging money requires you to try to get a monopoly on information and then erect some sort of obstacle or barrier to the smooth flow of that information. It's the same with downloading a movie from the Internet. The movie is worth something, but charging for the movie adds friction to the flow of the movie's bits, and frictionless always beats friction.

Free software isn't about cheap bastards trying to ruin your business. It's about a fundamental law of information flow inexorably crushing the payware software business model. You can try to ascribe motivations to people, we can talk about joy and pain and morality if it amuses us, but the deep reason is that we have a system where information that is frictionless beats information that has friction, and inexorably the frictionless information wins.

Frictionless software beats software with friction. Sure it may be deficient in other terms, but the power of frictionless is so great that entire markets will reorganize around frictionless. If they don't, they shrink and frictionless markets beat the markets with friction.

All is not lost for you. These things take a long time to happen. Windows is still with us, Linux hasn't driven it from the face of the Earth, and iOS is very successful even though it is fighting a difficult battle against Android. But if you want slow down your losses such that you can enjoy a good living or get rich in your lifetime, I advise you to think of yourself as being in the business of information, and see friction as being an obstacle to your success. If you must charge for software, try to think of ways to do it with the least friction possible.

p.s. http://github.com/raganwald
AnswerKenntnis:
As an entrepreneur/programmer who makes a good living from writing and selling software, 


You are not a programmer, at least not one sharing the scientific and engineering that makes most programmers choose their field. You are an entrepreneur who uses programming to make a living (not a bad thing by the way.)


  I'm dumbfounded as to why developers
  write applications and then put them
  up on the Internet for free.


Sense of charity? Sharing? Common good will? Scientific and engineering desire to advance technology and knowledge?


  You've found yourself in one of the most lucrative fields in the world. 


Inconsequential, even for industries outside of software. How many companies, profitable in other fields, actively engage in charity and community support? 


  A business with 99% profit margin, 


Only if you are working on the small, playing tax games or doing something under the table. The idea of a business that is that profitable, continuously and in a manner that is sustainable is not supported by the laws of economics. 


  where you have no physical product but can name your price; 


You can only name your price when you 


are dealing with a very desperate (and uninformed) client,
you are a technical ace (say a MSEE specialized in RF and MW circuit design or FPGA programming or a very experienced software architect.)


Otherwise, no, you don't get to name your price because there are a lot of very capable people competing with you for contracts.


  a business where you can ship a buggy product and the customer will still buy it.


And that's why you will never understand why programmers, scientists and engineering alikes (as opposed to money whores), do contribute to open source.

I would actually state that I doubt what it entails to have a successful, sustainable company, independently of the industry.

You ask Nike and they'll tell you they are in the business of making good shoes. They are not in the business of shipping a shitty product.

You ask Apple and they'll tell you they are in the business of combining the best technology with the most exquisite of user-experience aesthetics. They are not in the business of shipping shitty products.

You ask AstraZeneca and they'll tell you they are in the business of medical advancement, not on shipping a shitty product.

And those are not examples of empty rhetoric.

And so on and so on. And though it is always possible for defective products to be put on the market, all successful companies define themselves by a particular goal of excellence. Profit is a side-effect of it, and certainly the primary objective. But it is certainly not their primary drive that get things moving.

There is nothing greater than working in an environment like that. And there is nothing shittier than working with people who see profit as their main drive. Quality takes a dive completely.

You should do some reading on Warren Buffet's work or on Henry Ford's drive for quality and work ideology. Then you'll understand not only what open source is all about, but you might learn a bit or two about sustainable, successful businesses.

Entrepreneurs that don't understand that aren't really entrepreneurs. They are just peddlers riding a for-the-moment speculative wave.
AnswerKenntnis:
One of the main reasons why I'd consider releasing an app for free is because it's a surefire addition to my portfolio for future endeavors (potential job opportunities, promoting your name in the programming world). That's more than enough payment if you ask me.
AnswerKenntnis:
Toolmakers

Personally, I release the tools I use. My assumption is that the things that I build with these tools should be where I make my money. Programmers hate hassle, and most of use who live by the Unix Philosophy know that there is no need to reinvent the wheel over and over again. So, we develop tools that help us in our day to day chores, release them to the public hoping that others will find them useful, and, if we're lucky contribute to making them better. Most programmers don't want to be involved in doing mundane things over and over, we want to write NEW things that use our skills to their full potential, we don't want to write editors, parsers, databases, etc etc, and most of the time the community created versions of these tools are better anyway (ie, Linux vs M$). So when the community rises up and allows people with specializations in specific areas to do what they're best at we come up with some really cool projects that make all of our lives better. 

Responsible Citizens

If you use enough free software you eventually begin to feel indebted to the community, and if you have the ability you WANT to contribute. Also, there's value in doing. I've learned more by writing software for free than I ever have being paid to do so. It's a great way to learn, and I love to program. I love to solve problems and I love being able to do it however I want. When I'm releasing the product for free there's no expectation as to what it has to do, that's completely up to me.

Nobody Wants to Pay Me

I'm still in school, so when I work on open source projects, or create my own free projects it's experience I can put on my resume. It's how I taught myself several languages, and it's what makes me a better programmer than my peer who have only worked on coursework during the duration of their education.
AnswerKenntnis:
Start them off with a free version.

Then by version 4 start charging.

If the product is any good, people will continue to buy it.

Alternatively, go the Google route and offer a cut-down version for free, with a pro version costing a small amount extra.
AnswerKenntnis:
One reason is, that many software developers hate to reinvent the wheel. If all software were closed, there would be a lot more of that going on.

Open source gravitates much to infrastructure level software, like system and tools, that enable the developers to focus on the actual problem solution rather than reimplementing simple library functions a zillionth of time.
AnswerKenntnis:
You might find a lot of insight in Chris Anderson's Wired article Free! Why $0.00 Is the Future of Business.

You will however find many examples where the developers accept donations, and maybe Flattr will succeed where micropayments have failed.

There is also other transactions being made here, although it does not involve cash:


Labor: Debugging and testing effort on platforms and in usage scenarios never envisioned by the original developers.  By automatically tracking usage the developers get valuable information.
Reputation: For many programmers, programming is ever so much about the positive feedback from making the software in the first place and people cherishing the result.
Altruism: Making software products is relatively easy these days because of the availability of free and good developer tools and libraries.  Releasing software back for free is one way of paying back to the community.
AnswerKenntnis:
If you're writing a platform instead of a product, making it open source assures that people can build on it with confidence. So that's one reason.
AnswerKenntnis:
Because obscurity is far more damaging than not making money on one idea. Because programmers may not be living in a vacuum of living in a coding box, their own source of income may be covering their needs. Because free from price allows you to be free from support and free from obligations. Because payments mean you accept a certain liability as a provider of a service or product. There are more arguments in favour of not charging for software if your primary motivation is not to be rich. 

Finally, because money, whilst a great incentive, is also a poor motivator.
AnswerKenntnis:
I've "released" (well uploaded to my website) a couple of desktop applications for free because I didn't think anyone would be prepared to pay for them.

They're very small applications and I couldn't justify charging more than ┬ú10 or so for them anyway. I didn't expect to get many users (I know I have at least one) so it didn't seem worth setting up the PayPal integration on my website to collect payments.

If I ever write something larger that I think will have a market then I will look harder and longer at getting payment for it.
AnswerKenntnis:
I shared my application for free. In fact, it helped my potential customers to see how it is working and they contacted me with a proposal of buying and with some additional features to implement. Free distribution of software helped my customers to see how much beneficial it is for them.
AnswerKenntnis:
Because good software tools need some time to develop.

So you start your project and are aware that no one would pay for it, as it is.

But if you give it away for free people might start using it, provide feedback and free testing, development ideas, etc...

Finally, if all goes well you can create a non-free version and sell it.
AnswerKenntnis:
The free software movement insures, basically, innovation on it's most competitive scale.

Things change every day in the programming world and there needs to be a checks and balances system to make sure that everyone is up to par. Otherwise, we would be stuck with a lot of crappy programs just because people made a "Standard"(Microsoft Anyone?).

The fact of the matter is that YOU don't feel like you have the time or the resources to keep up with a free competitor. You have this complaint because it actually forces you to work to MAKE YOUR PROGRAM WORTH THE MONEY. You have to innovate and improve your program(Insert Takei "OH MY!").

Sorry, your vanilla version you planned on riding on for the next five years just isn't going to cut it. You have to constantly develop. That is what it takes.

Don't be upset because you are too lazy to work to make your product decent while people who work harder than you give it up for free.
AnswerKenntnis:
I write code because I enjoy writing code. Not because I want to be rich, or because I want to change the world, or anything like that. I enjoy writing code, and I like it when people get to benefit from this fact. Why should I charge them lots of money for that?

I also get to benefit from lots of people who feel the same way, and it's a way of giving back to them. I get to use Linux, and Firefox, and .... for free every day, so if I can do something that somehow benefits others then why not?
AnswerKenntnis:
Because I have the feeling that my knowledge can help others in improving their daily work. I also think that public projects increase your visibility across the globe and companies will be interested in you and possibly want to hire you. The latter of course requires that your code base is good and the project becomes popular.
AnswerKenntnis:
People are less willing to pay for virtual stuff like programs, plus, there are many other free programs, so your commercial program, even for 1 cent, won't sell.
Also, programs can be copied easily.

"money is the human word for quatloos", that's why some programmers avoid it.
QuestionKenntnis:
Should you keep a copy of all the code you write?
qn_description:
I know the company you work for owns the code and obviously you will get arrested if you try to sell it. But is it uncommon for developers to keep a personal copy of the code they wrote (for future reference)?

Apparently this guy was sent to prison for copying source code.
AnswersKenntnis
AnswerKenntnis:
But is it uncommon for developers to keep a personal copy of the code
  they wrote (for future reference)?


I don't know how common it is, but common or not, it's still a bad idea.

Programmers often operate in the mindset that solving the same problem twice is a waste of time. We try to design our code to be reusable (sometimes). We build libraries of classes and functions to reuse at some point in the future. We sometimes even give our code away so that nobody else will ever have to write code to solve the same problem that we just did. So it may be understandable to want to take "your" code with you when you move from one job to another. But you still shouldn't do it, for the following reasons:


It's not your code to take.
The code you wrote for your former employer is part of the business they built. Their code is part of their competitive advantage. Sure, competitors could write their own code to solve the same problem, but they shouldn't get the advantage of building on work that your employer paid for, owns, and didn't authorize you to take.
If they have any sense at all, your new employer doesn't want any part of the code that you took from your former employer. The more you "refer" to work you did for some previous employer, the more you put your new employer in legal jeopardy.
If you ever accidentally let it slip at New Employer that you've still got a copy of the stuff you did for Old Employer, your boss at New will probably realize that you'll take a copy of their code when you leave for some other job. That might not sit well with him or her.
Even if you're not cribbing actual lines or just vague ideas from your old stuff, just having your old stuff in your possession could raise suspicions that you might be using it for something. Imagine that Old Employer sues New Employer, and as one of a small handful of employees that moved from Old to New, you suddenly find yourself giving a deposition. None of you actually copied Old's code into New's product, but the lawyer in front of you asks: "Mr. SuperFoo, do you now or have you at any time since leaving Old Employer had in your possession a copy of any code that you or anyone else wrote while working at Old Employer?"
You don't need the code that you wrote last month, last year, or longer ago. You solved the problem once, and now you know how to solve the problem again. Or, you may know how not to solve the problem -- your new implementation will be better because you have experience.
There are better ways. It's hard to go back and learn anything useful by reading old code out of context. A diary or journal that describes what you learn, ideas that you have, etc. is far more useful later on.
Even if Old Employer knows that you've got their code and is OK with that, you still don't want it! The only thing that can come of having it is a 3am phone call: "Hey there, SuperFoo? How are you doing? Listen, you've got a copy of our stuff, right? Look, we've got a problem with the system, and we've narrowed it down to a couple files that you wrote that our new guy just doesn't understand. I know it's late, but could you walk him through SuperDuper.pl?"


Let it go. You don't need it.
AnswerKenntnis:
I always keep a copy of the code I write and take it between jobs. Subsequent employers never get to see/run the code, but I use it as a reference at home: 'Ah yes, didn't I do something similar to that on Project X?'.

Is this legal? Depends on jurisdiction and circumstances, but it is fairly common. Morally, I have no problem with it, providing you aren't simply giving code to new employers... It's a reminder and a demonstration of what you have done, rather than a free resource for your employer.

[The flip side of this is the inevitable shame that comes along when you look at older code: 'What was I thinking?? Why on earth did I do it that way??']
AnswerKenntnis:
This is a very bad idea. That code doesn't belong to you (legally speaking) and having possession of it can get you into a lot of trouble. This becomes even more true when you move to a new job and still keep that source code around. Even worse if it's a competitor. Your company would not be happy if you had access to their source code when you don't work for them anymore.

It's all about managing your risk. It's obviously expected that you retain things from a previous employer that you can use somewhere else. This is why they make your sign non-compete clauses that last X months/years after you leave them, however having possession of the code makes you more vulnerable to someone accusing you of blatantly copying the company's code (even if you didn't, and just used the same ideas). Is having the code worth managing this risk?

Surely the useful stuff you got from writing the code is not the exact syntax; it's the knowledge you gained. It's probably not worth dealing with all this legal stuff.
AnswerKenntnis:
It's not uncommon. 

I have a copy of almost1 every piece of code I've written professionally, and certainly all the code from my current projects, regardless of who wrote it2. Along with the code I have a huge pile of legal paperwork clearly defining what I can and cannot do with it. Just having the code is not the same as trying to profit from the code. 

That said, it's a legal issue, and legal issues tend to be extremely complicated and localized. If in doubt, you really need to talk to a lawyer. I may keep my code around, but I'm 99% certain I won't get in trouble for doing so.

1 What's missing is mostly what I didn't care to archive. Only one small project's code is missing because of legal reasons.

2 Nature of the projects and my role in them, I'm one of the guys that need to have at least an idea of what goes where, even if I wasn't involved in building a specific module.
AnswerKenntnis:
I see your arrested Chinese man and raise you with a "code is not property, therefore cannot be stolen".

Ref.: Code 'not physical property', court rules in Goldman Sachs espionage case

With that said.


Do I keep code I write? Absolutely. 
Do I keep full projects? Absolutely.
Do I make sure I thumb drive my code from my work PC onto a home machine? You betcha!
Do I ever re-use that code at another company or in a personal project. Nope.
Do I often look at old code and go WtF!? All the time.
AnswerKenntnis:
Here's a simple question for you. Go to your boss and tell them: "I have a copy of all code I wrote while I worked here. Only code I wrote, not other peoples. This is for my own education, and I will never give it out".

Their next actions will dictate (yes, that word does exist in North America/World) if you are in the wrong or right in their eyes.

Regardless of your own "ethics", you work for an employer. If they deem that what you are doing is wrong, then it is wrong based on their ethics. As they pay you, and you are employed by them, then it is in your court to either agree with them, or disagree, which could get you fired.

Now it's a question of integrity. I've let a previous employee's take some of our code, but I have vetted all of it first.

Just because you believe you are in the right, does not make you in the right. Usually software developers sign contracts when they are employed. If you signed one, you need to live by your word.
AnswerKenntnis:
is it uncommon for developers to keep a personal copy of the code they wrote


Answering the question directly, I'll say in my experiences this is uncommon. The exception to this that I have seen is people who do a lot of freelancing and they keep the code on hand for their client's future maintenance and enhancement projects, and I would imagine that this is clearly stated in the contract (though I am not in the habit of reviewing my friends' freelancing contracts, so who knows). People I know who work in larger companies have never admitted to keeping code from former employers.

I know I wouldn't because I can't think of a single situation where it would be useful (not to mention that I'm pretty sure my current employer prohibits such things - I would just have to look up the documents to be certain). The code I've written/fixed is usually so specific to a particular business requirement that I can't imagine it ever coming up again in the future in such a way that it would be easier to re-use the old code than write new code.
AnswerKenntnis:
Back in the day it was common for developers to have their personal library of routines that they would use to solve problems at the current job.  The source would stay behind when the developer left, but any enhancements also went with him.

This resulted in a win-win situation.  It was also just a subset of all the code written.   

Of course most of what was in personal libraries would be in standard libraries today.
AnswerKenntnis:
Under an employer to employee relationship in most parts of North America it is illegal to transfer or transmit digital material (i.e. source code) from your employers equipment without prior legal authorization from the employer.

Part of the legal laws relating to the definition of an employee in the work place, is the description that the employee does not provide his/her own working equipment unless otherwise stated in the employee contract, with exception to trades which require the employee to purchase their own equipment (i.e. construction worker).

Most employment laws in North America define the employer as the principal risk taker in an employee to employer relationship. The employee is paid for his/her time while the employer provides materials, equipment and controls the work related activities of the employee.

At what time during this relationship does it make it OK to steal valuable material from the employer, who has paid for and taken risk in having said material created?

The key problem with this was the question "source code that you wrote?". No sir, it is not you who wrote it. Under guidance of your employer it is them who wrote it. You are just the hired hand who typed it out. There is no court in North America that will side with you should your employer take legal actions to secure their property. Just copying the source code to a USB thumb drive can get you into hot water.

With that said, if an employer allowed you to use your own equipment (i.e. laptop) or transmit the material, then it's a different matter. The employer must notify you upon termination that any materials should be returned/destroyed.

Just thought I would post this answer, because it seems like some people thought this was a gray area. I really don't think if you're a developer, that you should be going around the Internet posting that you keep copies of employer's materials. I mean, you clearly already knew the answer to this question because you created a new member account just to ask this question. ;)
AnswerKenntnis:
I've done this in the past a few jobs ago.

However I've never once gone back and looked at it. I've occasionally reused ideas and things I learned but I've not once found a reason to go back and look at the code.

So I wouldn't bother any more. It's legally dubious, and I've never actually found it useful in practice.
AnswerKenntnis:
I have just recently deleted all the old code I kept from my previous employer. I only kept pieces of code that I thought were good for future reference. I've actually found that I've moved on considerably since I left and I've never referred to the old code. I've found/discovered/learnt far better ways of solving the same problems.

It was a nice little trip down memory lane though :)
AnswerKenntnis:
Sure. I like to keep a copy of all the work I do - whether it's writing code or otherwise. Call it a scrapbook, if you like. Breaking the rules? Perhaps. 

The comments about competitive advantage are irrelevant unless your next employer is a direct competitor. If you move from a phone company to a software house, or from a game developer to a database developer, it doesn't matter. If you're actually planning to reuse code - well, that's a different story.

Interestingly, you often hear about web developers bringing a "box of tools" with them, with a standard set of JavaScript libraries and CSS stylesheets. But I haven't seen that mentioned here.
AnswerKenntnis:
I think there needs to be a distinction between a design-pattern and actual code (copying line-by-line)

Writing down some psuedocode something like - this is a great way to lazy load X in Y is one thing. Writing down all the code is something else.
AnswerKenntnis:
For a different number of reasons, like working from home, you might as well already have a copy, and I for one wouldn't go deleting these after a job, why do that? It doesn't go against my beliefs or something.

But, regarding using it, it is not so useful as a blog post!

Bottom line: write your code, blog about the problems you've faced and how you solved them (especially when it's generic and broad stuff), and don't mind to keep a (possibly encrypted) souvenir from the last job.
AnswerKenntnis:
In finance the CFA addresses this issue. You are not allowed to take information related to clients or your work for the company (in this case code). But nothing prevents you from memorizing what you can and then writing it down later.

Not sure how legitimate that is, but I think your best bet is to leave your source code but write down your ideas and how you did it as soon as you get home. Stealing is stealing and in court all they are trying to find out to decide is if you copied the code or not.
AnswerKenntnis:
If you are saving your code with the goal of reusing it later, I have two issues with that:


If the code is domain specific, it is probably also proprietary. Anyway, no two businesses or problems are exactly alike and trying to fix one problem with the solution for another is not a good pattern.
If the code you are saving solves a common problem, you should
question your approach. Why are you spending a lot of effort to solve a
common problem when they are likely existing (and better) open source
solutions?

If you believe that you have the best solution to a common problem,
you should really try to share you code publicly on a public repository
and/or code blog when you write it, not keep it to yourself. Your
boss should not object to sharing a generic library or force you to
reinvent the wheel (if he does, find a new job).


If you are saving your code because you want to show it to a potential employer or to improve your skills, I suggest you contribute to an open source project instead.
AnswerKenntnis:
I haven't yet kept any code that I've done for employers. Freelance stuff I've kept (it can be hard to support a client if I lose my code).

However, I'm considering keeping the code that I have made for this employer. The reason is that I'm a web developer, and I've developed a lot of client-side things that look rather pretty. I didn't design them, but I implemented them and often came up with the idea.
I would like to keep a copy of this stuff to build an online portfolio. As we all know, sites don't last forever, so I can't realistically count on my work staying online. Having a backup copy would allow me to have a portfolio online.

I know that designers often keep copies of their work (even if it was work for hire) for portfolio purposes.

I'm not sure what the legality of this is (it is not explicitly mentioned in my contract).
AnswerKenntnis:
With your employer's permission publish the neat re-usable code as open source projects for other people to use. Then your employer can also benefit from contributions by other people to this code. 

This way you can keep the code legally, build a public portfolio of code you've written and the code can benefit other people.
AnswerKenntnis:
Well, for developing the business logic for my company, I can't keep the code because it's illegal & personal property of the company. As the developer, I know how to develop that logic, so I can keep that logic in my mind. Basically by default it is stored in your mind, and if you need it next time, then automatically you should implement the logic/better logic than the previous one. It's the human nature & intelligence. :)

But the issue creates when you develops some utility logic, they are re-usable & always important, you may need it frequently in different projects. So you should want to keep it with you.

I have an alternate solution to that. Just create a without source/documentation JAR for thos utilitiess and add it as an external third-party JAR in your project. Probably by doing this you can confirm your responsibility as well as your self satisfaction that you have the code ;)
AnswerKenntnis:
I don't keep code for a simple reason: The company paid me to write the code for them. I give them my code, and they give me my paycheck. The guy who made my lunch, does not get to keep part of it for themselves, why would code be different?

Also, the point made before, I walk out of job, all those headaches stay there, new job, new start, but with a bit more experience and understanding of the types of problems in that old job's domain.
AnswerKenntnis:
It is piracy, in other words theft, isn't it?

I think most will agree that copying a compiled program is piracy and theft. I think copying the source code is even worse. You would be able to easily make modifications to the program and re-sell it under a different name.

If someone got their hands on the code they could also discover security vulnerabilities and exploit those in the software potentially affecting your ex-employers customers. Do you really want to risk getting pulled into a situation like this?

I say just rewrite the code; better.
AnswerKenntnis:
In the US it is more common that only things such as specific algorithms that accomplish a task that gives a company a strategic advantage over another can really be copyrighted as anything else can be replicated. Though when I quit I would still use the old code to come up with a different way to accomplish the same task. Then open source it. I see it as they pay me to come up with the idea not to give them my ideas that I can come up with in hour. These can and do become invaluable to me as I reuse them every day for the most part. Encouraging your employer to open source their code is one of the best things you can do as it lets them have others over look the code and fix errors. 

You could suggest a license that would allow you to retain your code to do as you may with it. Though this seems like to much to ask for (to them or you), you may be surprised how many employers will let you keep at least personal rights to your code.

Shoot for Mars and settle on the Moon! Hell you may even get to Mars ;)

Another good idea is that you can write the code on your own time and then only answer questions while on the clock. This would make the code 100% yours if it ever came to a legal battle, and they would not have to know till they try and get rid of you or you quit. REMEMBER TO KEEP PROOF! (logs save time, etc). Make sure there is no notion that you are submitting your code to them either and this can be an effective method. This would mean they have been paying for you to answer questions :).
AnswerKenntnis:
A far as I am concerned, employee skill sets belong to them and they have every right to sell their ideas. But it also necessary to mention the contract agreement between the particular employer and his employee. That will enable one to be more specific on the issue.

If for example I write an algorithm for company x during my contract, selling the same algorithm to another company put me at a risk, if only if my contract with the previous employer mentions intellectual property rights.
QuestionKenntnis:
How can programming ability be used to help people in poverty?
qn_description:
As a student studying Computer Science in college, I often hear from friends working on various humanitarian projects, and I want to do something myself. But it seems that programmers don't have as many obvious avenues to help out as, say, doctors or teachers. What are some ways in which programmers can put their talent to use for people in poverty?
AnswersKenntnis
AnswerKenntnis:
When I was in just out of college, there was a Guy who would stop by my house on recycling day and pick out all our cans and bottles that had a deposit. I became kind of friends with the Guy, IΓÇÖd ask him howΓÇÖs business, heΓÇÖd ask me how I liked my cube and weΓÇÖd have good laugh.

One day we got to talking about what I do and I told himΓÇ¥I made things to help people do their job better with less workΓÇ¥. He tells me he could use one of those thingsΓÇª.

So, I had an old Palm3, I wrote an app where he could enter his Cans collected at each address and after 2 month he would know which Houses drink the most soda and what where his best blocks. (No route generator :-( ) I presented it to him the next week on recycling day.

Increased his productivity by 30%!

So there you go, Programming to help people in poverty!
AnswerKenntnis:
Use your talent to earn lots of money, and donate a good part of it. As programmers, we are in the lucky situation to be able to earn more money than we need for our personal needs.
AnswerKenntnis:
Contribute to open source software

By making a contribution to open source software (or creating your own) you can have a direct effect on the overall cost of a computer system. This in turn lowers the cost poor or needy people (or charitable organizations) incur to provide them with computers. 

I know it's not very fancy but it could help!
AnswerKenntnis:
I've had the same question, and I've thought of two things: doing web dev work for a nonprofit, or assisting with computer courses, like an adult education course. There could be poor people there trying to catch up with technology, depending on where the class is offerred, the area, etc. Maybe you could volunteer to manage computers at your local library- they have useful resources for people who can't afford their own computers.

If you just want to help people, you can always just go serve meals at the soup kitchen. Among the poor, I think general manpower is in greater demand than algorithms.
AnswerKenntnis:
I live in India but I want to make this answer more general, probably the answer will go to some political or on a non IT way, so apologies in advance.

Poor people are poor because they can not enjoy some basic things like food, water, home, jobs and transportation. The lack of these basic things are the main cause of poverty. 

Our job is to make programs, and our target audience is most probably those who have some extra bucks to spend on a server and a website. Mostly in India IT is not seen as necessity, many firms and govt organization avoid this if they don't have enough money. 

Here the govt. spending the major part of money and manpower to make above mentioned resource proper and after that more better. 

They have to build and keep better roads, better house and constant electricity. They have to watch and forecast weather and keep some proper storage if weather goes out of control so everyone can have proper food no matter how good or bad the whether is. They have to create and manage a good transportation system to keep business alive anywhere. They have to provide better medical and emergency services.

That's where I think a better programming helps, to provide a good and reliable programs to people and (Government and Non Government) organizations who handle them. And It's way better than programming for some freak clients who are like Dogs chasing cars. Atleast it makes me (And probably you too) feel proud by giving back the society.

P.S. As a mobile developer I felt proud once by developing an application for Blackberry to help people to cope with some medical emergencies.

P.S. 2 There is also one easier way, earn more by programming for these clients and spend more earned money to charity.
AnswerKenntnis:
Unfortunately most causes of poverty don't respond well to programming solutions. Google made a valiant effort with the Google.org project, but after 6 years of trying, they've largely found that only donating money to lower-level "on the ground" organizations has much effect. A recent article in the New York Times highlights the problem: Google Finds It Hard to Reinvent Philanthropy

Using your skills to earn more money and donate it to causes that help relieve poverty is likely your best bet. When you donate to at an organization like Charity Water, where $20 provides clean water for one person for 20 years, you can do a whole lot of good just working an extra hour a day and donating the results. The Developers Against Poverty campaign is an example of programmers promoting doing good this way.
AnswerKenntnis:
You can also participate in hackathons organized by Random Hacks of Kindness.

From their website:


  Random Hacks of Kindness (RHoK) is all
  about using technology to make the
  world a better place by building a
  community of innovation. RHoK brings
  software engineers together with
  disaster risk management experts to
  identify critical global challenges,
  and develop software to respond to
  them. A RHoK Hackathon event brings
  together the best and the brightest
  hackers from around the world, who
  volunteer their time to solve
  real-world problems.
AnswerKenntnis:
I have a suggestion, 

We can develop a website, which will have the details of hospitals, free medical checkups, government free medical services.
In case of emergency, people can search for the blood donor near by their state/region/town. 

Not only poor, you can help every single individual who really need someone in extreme emergency. 

+1 for this nobel step.
AnswerKenntnis:
Schools are always looking for people to come into the classroom and talk about (or demo) their jobs. I guarantee you the kids will love it.

-Ralph Winters
AnswerKenntnis:
I live in Brazil and here, the software costs are prohibitive. This situation has been changed in last years, for the economic development. But it still is unconfortable. Almost every commercial software application imported is too expensive due to the differences in people's income and the minimum wage in relation to a rich nation. All the imported technology stuff costs, effectively, more than it would be cost in a full developed nation. An iPhone 4, for example, in Brazil costs more than a thousand dollars. If you compare the minimum wage of Brazil with US, UK, Germany, Japan, etc, the cost increases too much. I think that with India, China and Russia the situation is no different.

In my opinion, a good way to help poor people to be technologically included is provide free and open-source software for education and small businesses (that creates jobs). Imagine the development of better OSes, Office Suites, CAD tools and other software products that would help the development of a nation... I believe that we, programmers, analysts, software engineers and related, can do much to reduce social inequality.
AnswerKenntnis:
Help the people doing the humanitarian aid and/or help alleviate the issues causing poverty.

For example, you could:


Develop software which enables doctors/teachers to help/work with remote villages more effectively.
Help farmers increase the efficiency/yield per acre in poverty stricken areas
This list goes on...
AnswerKenntnis:
Bill Gates has been pushing to help eliminate Polio world wide, you can check the Bill and Melinda Gates foundation web site and see it there is anything you can do to help that effort.

Maybe organize a local fundraiser or something. Its not specifically programming related but it will definitely help people in poor countries! Or how about a hack-a-thon fundraiser? 

And for the record I have been a Linux geek since about 1994, so not a Microsoft fan. But I am a fan of good public health policy, so for this I will give Bill a big round of applause.
AnswerKenntnis:
I'm no econ expert, but I've heard a lot of people say that lack of education and access to shared knowledge is what helps to keep poor people poor. 

If that's the case, then one could make a case for saying that FOSS could help lower the bar economically and allow the lesser privileged folks access to those realms. I know for a fact that a lot of schools that are on tight budgets have to spend massive sums on windows and office licenses, which if replaced by ubuntu and open office, could have been spent on more computers/professors. 

Those projects are always looking for help.
AnswerKenntnis:
"Deliver Good" would be a site built by a programmer to help match charities and donors that I believe is in the same ball park of what you mean.  The site was started by someone in Calgary, Alberta, within the past year so it may still be gaining adoption to some extent.

"Talking Shop: Beef up your resume by doing volunteer IT work" is an old article about how your could volunteer your IT skills.
AnswerKenntnis:
Something as simple as showing people in charities and non-profit organisations how to use computers and various platforms effectively can be a big help.  Small groups particularly often lack people with technical skills and IT tasks often go undone.  Sometimes basic things like setting up a website or Facebook page can seem daunting to those outside our field.  It can be as simple as letting them know they can call you up with technical questions or helping out with setting up data security and backups.
AnswerKenntnis:
This can be a top town approach and things can be done at all levels.
You can help the world by participating in developer challenges like the one from the World Bank Apps for Development that tackles the Millennium Development Goals. Those guys have a lot of data that can be processed in a lot of useful ways. Also you can search Google with "PROBLEM-NAME developer challenge". That gives some interesting results.

At a lower level you can offer your services to one of your national non governmental organizations. Surely they have some ideas and are in need of some skilled programmers.
And of course there is the 1 to 1 approach. Volunteer your time to teach people how to use a computers/the internet in ways that help them do stuff with less money. Teach children computer programming.
AnswerKenntnis:
Hire programmers in developing markets.
AnswerKenntnis:
"If you've come here to help me, you're wasting your time. But if you've come because your liberation is bound up with mine, then let us work together."

ΓÇö  Australian Aboriginal Elder Lilla Watson.
AnswerKenntnis:
As a service learning project in my senior year of college, we built a system for the local Salvation Army to help manage their annual Christmas gift donation drive for needy families. While it may be rare for such an organization to need the help of a programmer, it doesn't hurt to ask around in your area.
AnswerKenntnis:
I am a (computers) student in India, and I have had the fortune to know and work with several people who are passionate about using technology to help people in need. Check out these webpages - Microsoft Research India, act4d and Gramvaani.

Personally, I feel that you are looking at in the wrong way. If you think that you can help people with programming ability, you are probably thinking of building some kind of website/software. However, the fact of the matter is poor people largely (I am talking about the developing world here) don't have access to computers etc. and don't usually speak English. If you think that technology alone will solve the world's problems, take a look at this. This is the text of a talk by a person who was involved quite heavily in this field. 

In my view, if you really want to make a difference, you have to combine technical engineering with social engineering. Try to realize what obstacles are holding people in poverty - it could be lack of education, lack of access to opportunities, lack of money etc. Then try to think of some way that these can be overcome, and then you can perhaps be ready to help them. If you are really interested in helping out, remember that technology is typically geared towards people with money, and you have to remember that technology is only useful if it actually meets the needs of people. Think less about new technology, and more about better ways of using existing technology.
AnswerKenntnis:
Contribute your time and efforts to improving Open Source projects such as Ubuntu linux. The philosophy of Ubuntu speaks directly to your cause. Humanity to others...

By improving this excellent free operating system and software you contribute directly to assisting people in poverty. Since this Linux operating system is free and very easy to use, it can assist people in all walks of life gain the tools they need to manage their information needs.
AnswerKenntnis:
I am living in the Philippines and I shifted my career to IT because I want to help other people... and I plan to give my service for FREE... For example, I want to develop a program for our public CITY library because currently they are still using a manual catalogue system.  Another thing is that if I became an expert in programming, maybe I could teach children who cannot afford to have a proper education some computer skills.. :)
AnswerKenntnis:
One thing that programmers are often exceptionally good at, in comparison to the rest of the population, is collecting and organizing information.  I live in a relatively poor neighbourhood compared to the rest of my city and there are literally dozens of disparate programs designed to help people.  The problem is, is that nobody knows about them.  You ask anyone working for these programs, and one of their biggest issues is connecting people with problems to people with solutions.

In our community we've started a extremely local web site (for roughly the 4x8 block area in our municipal neighbourhood).  We've actively sought out the various mailing lists and organizations that offer programs, and we filter and organize them, and add them to the website. Nearly everyone in the area has access to the internet in some form or another.  But we're also supplementing the site with a monthly newsletter.  The intention is to centralize the information that is already out there.  We've even given some organizations the ability to post directly to our site.  So far the response has been quite positive.
AnswerKenntnis:
There are tons of charitable open source projects out there--it's finding them that's the rub. Sahana comes to mind; can't think of any others offhand...

If you don't find anything that resonates with you, then start something! Think of how automation and information transfer (the core of software's strength) can improve the lives of those you wish to help. I find Dean Kamen a great source of inspiration here. When you have your great idea developed--make it happen. It's that simple.
AnswerKenntnis:
Programming is for the well off and well connected.
There's only two things a programmer can do:
1. Donate his time.
2. Donate his money.
Since every human being has only a relatively limited amount of time to donate, I would prefer the second option.
Earn enough money to make a significant impact and then set out to change the world.
PS I am not advocating against giving time, just that given the scale of the poverty issue, especially here in India, money is much more valuable as a resource.   

Additions:  

Disregard the previous answer. I later remembered a fantastic TED talk I had seen earlier and went to find it again.
Now I agree with Mr. Anil Gupta's answer. It's a must watch.
A year earlier, I had also seen another TED talk related to technology and poverty. Though I couldn't find it now, the idea was to give a computer with internet connection in a slum or in remote villages. Even without being taught how to, children learnt how use them very quickly.

Edit: Found it: Sugata Mitra: The Child Driven Education..
It's doesn't directly relate to poverty but since education is the key to break the vicious circle, this idea is brilliant.
AnswerKenntnis:
Whilst not programming, I have done quite a bit of voluntary work within IT drop-in centres.  Primarily teaching the elderly and those with learning disabilities on how to use popular software.

A lot of people in society don't realise the benefits that things we may take for granted, can give them, such as the internet.  This is prevalent with the groups I mentioned above and it can be extremely rewarding when you see the progress they make and the benefits that they inevitably gain.

Maybe this is an area you could look into, as I think most major conurbations would have such set-ups.  If not, ask your University would they be interested in giving something back to the community and volunteering their resources for a few hours a week.
AnswerKenntnis:
Have you heard of the Venus and boinc projects? search for them and see what we can all do.



http://www.thezeitgeistmovement.com/wiki/index.php?title=BOINC

http://www.thevenusproject.com/



get informed mate ;)

sincerely,

LSonic

...

The reference to the BOINC project didn't work for me.  However, I have found a BOINC project that serves a a good introduction to BOINC:

World Community Grid

http://www.worldcommunitygrid.org/index.jsp

Click on each of the pictures under the big picture to see all their current subprojects, some of which should help mostly people in poverty.  I've given them years of computer time already.

A few more BOINC projects of interest:

http://www.malariacontrol.net/

for malaria

http://dnahome.cs.rpi.edu/dna/

for tuberculosis, but early in development so I recommend it only for those with experience in testing new software

http://www.rnaworld.de/rnaworld/

likely to help with research on RNA-based viruses, but not giving much detail yet; not quite so early in development but still not recommended for beginners

I'd recommend selecting at least two BOINC projects so your computer can still do useful work if one of the projects goes down for a while.

BOINC is used mainly when some research project needs much more computer time than the computers they can afford can deliver within a reasonable time.

Robert Miles
AnswerKenntnis:
I don't think you can contribute a lot in that field: what you do on computer often stays on computer, while poverty is material thing.

What you can do is teach kid how to use a computer or how o program, but again, they need a computer to continue learning, and most kids tend to learn alone.

I've had this idea for a long time now, but obviously the thing that would improve our society from the bottom, is some kind of clean, location based, goods/service exchange for everyone website.

I'm not thinking about you regular "selling" announcement website, but something which makes a lot of emphasis on locality.

Some sort of craigslist + google maps, but better and not necessarily based on emails.
There would be people who search, and people who propose...

I don't think successful companies like google or facebook put much emphasis on how the economy has to organises itself, while it is like the biggest feature that internet allows: transportation of information.

Try to think that the current economy makes people pay for meeting each other and delivering services, while what you really need might be much closer of where you live.

Delivering information organised in this manner would greatly diminish poverty, which to my sense is just a big lack of organisation money can not always solve effectively.

SOrry for the political post :)
AnswerKenntnis:
You can work on Mifos: http://mifos.org/ - the software used by Grameen bank.

"Participate in an open source project that's building technology to end poverty faster" - quote from the website.
AnswerKenntnis:
You can always help indirectly - eg. make some software, that would help some humanitarian project. Our you can make free ( or symbolic price ) web for em. if you are web programmer - this can help to get more other people know about project.
QuestionKenntnis:
How to respond when you are asked for an estimate?
qn_description:
We, as programmers, are constantly being asked 'How long will it take'?

And you know, the situation is almost always like this:


The requirements are unclear. Nobody has done an in depth analysis of all the implications.
The new feature will probably break some assumptions you made in your code and you start thinking immediately of all the things you might have to refactor. 
You have other things to do from past assignments and you will have to come up with an estimate that takes that other work into account.
The 'done' definition is probably unclear: When will it be done? 'Done' as in just finished coding it, or 'done' as in "the users are using it"?
No matter how conscious you are of all these things, sometimes your "programmer's pride" makes you give/accept shorter times than you originally suppose it might take. Specially when you feel the pressure of deadlines and management expectations.


Many of these are organizational or cultural issues that are not simple and easy to solve, but in the end the reality is that you are being asked for an estimate and they expect you to give a reasonable answer. It's part of your job. You cannot simply say: I don't know. 

As a result, I always end up giving estimates that I later realize I cannot fulfill. It has happened countless of times, and I always promise it won't happen again. But it does.

What is your personal process for deciding and delivering an estimate? What techniques have you found useful?
AnswersKenntnis
AnswerKenntnis:
From The Pragmatic Programmer: From Journeyman to Master:


  What to Say When Asked for an Estimate
  
  You say "I'll get back to you."
  
  You almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.


In the section, the authors recommend the following process:


Determine the accuracy that you need. Based on the duration, you can quote the estimate in different precision. Saying "5 to 6 months" is different than saying "150 days". If you slip a little into the 7th month, you're still pretty accurate. But if you slip into the 180th or 210th day, not so much.
Make sure you understand what is being asked. Determine the scope of the problem.
Model the system. A model might be a mental model, diagrams, or existing data records. Decompose this model and build estimates from the components. Assign values and error ranges (+/-) to each value.
Calculate the estimate based on your model.
Track your estimates. Record information about the problem you are estimating, your estimate, and the actual values.
Other things to track include requirements, risk, and validation.
AnswerKenntnis:
Software estimation is the most difficult single task in software engineering- a close second being requirements elicitation.

There are a lot of tactics for creating them, all based on getting good requirements first.  But when your back's against the wall and they refuse to give you better details, Fake It:


Take a good look at the requirements you have.
Make assumptions to fill in the gaps based on your best guess of what they want
Write down all your assumptions
Make them sit down, read, and agree to your assumptions (or, if you're lucky, get them to give in and give you real requirements).  
Now you have detailed requirements that you can estimate from.


It's like my mother used to threaten when I was a kid "Hurry up and pick out some clothes, or I'll pick them out for you!"
AnswerKenntnis:
I did development for a guy who was very adamant about wanting accurate estimates.  What we settled on, which worked very well, was this:


I billed for all the time I spent estimating.  It came to around 20-25% of what I billed.
I did extremely detailed examination of the tasks.  No shooting from the hip.  I went into the code, figured out what lines needed to be changed, what other parts of the program it would affect, how much testing I'd have to do to ensure that things still worked.  I'd estimate each piece in units of .1 hours (6 minutes).
I sent him my estimate for each task along with that detailed breakdown.


20-25% of billing sounds like a lot.

But he'd ask me to make change XYZ, thinking it'd take about 2 hours.  In 1 hour of detailed estimating, I'd determine it'd take 8.5 hours.  So he'd decide whether it was worth 8.5 hours of pay.  If not, then he saved 7.5 hours over what it would've cost him if I'd done it without an estimate.

And if he did want to invest the 8.5 hours, the detail work I did for the estimate was work I'd have had to do anyway.

I found that with this method I was able to bring most tasks in on time or even early, without having to heavily overestimate.  Because the time was broken down so minutely, I could tell early on if I was slipping.  If I hit roadblocks so that after 3 hours I could tell that my 8.5-hour task was going to take 12, I could talk to him about it before more time passed so he could reevaluate and yank the feature if he was concerned about the cost.

Was he nickel-and-diming?  No, I looked at it as letting him apply his money where he saw the most benefit.  And I was glad to get experience in estimating, which I'd always been terrible at.
AnswerKenntnis:
It depends on what the estimate is for.

For an initial, high-level estimate for a business case then the key things are:


Speed. Whatever method you use it needs to be quick. The whole point is the stakeholders aren't sure if it is even worth doing the project - which is why they need the numbers for the business case. If the business case was solid they wouldn't need your estimates. The bulk of these projects won't go ahead so it is important that too much effort isn't expended providing the estimate.
Give a range. Make it broad. You have had no time to analyse requirements, workshop with stakeholders, validate assumptions. A wide range tells the recipient of the estimate "Software projects are naturally complex and risky - if you want a proper estimate you need to give me more details and more time". The problem with giving a single number or a narrow range is that it paints you into a corner by setting expectations before any real analysis is done.


I find the best technique to pick a comparable project that "feels" the same. "Feel" is completely subjective - but with this kind of estimate my experience tells me you won't find objective measurements. Then provide a wide range. I've read some books that say a range of -50% to +100% is good but it depends on many factors.

For a detailed, low-level estimate:


You need a baseline. If the baseline isn't stable the estimate is meaningless.
Bottom up is best. Get a detailed work breakdown, estimate each component then roll it up into a larger number. I find planning poker to be a great technique here.
Document contingency. Make it clear where any contingency (if any) is added. Is it added to each line item? Or to the whole estimate? Or to specific risks? Or is there none?
State your assumptions. Validate as many as possible given the time frame.
State explicitly what is included and excluded in the estimate. For example, is review included? Are technical delays included?
AnswerKenntnis:
We're often asked for an "ballpark estimate" during meetings where we're given very broad and vauge ideas of what they'd like to do.  I always say, "if you want an answer today it's a year and a million dollars.  If you'd like to give me a lot more details and some time to review them then I can refine those numbers for you."

They almost always get the point.
AnswerKenntnis:
Here is a blog that I was sent towards of estimation:

http://www.joelonsoftware.com/items/2007/10/26.html

This blog outlines how to keep a record of how accurate your previous estimations have gone, and then next time you say to someone "it'll be 2 weeks", you can look at your previous history and see how long it actually took last time you said "it'll be 2 weeks". 

I haven't tried it myself, but I'd like to, to see how accurate my estimations are.
AnswerKenntnis:
"Two weeks!"

Seriously.  My first estimate is always two weeks.  Because I have some sort of bizarre mental block that makes me think everything sounds like it'll two weeks.

I try to work around it, try to really think about how long I think something will take, trying to identify all the potential trouble spots and bits that look too black-box-y for me to be accurately estimating.  And try to recognize that if my answer is "Two weeks!", I've likely failed to do so.

Pretty much every good manager I've had has learned to recognize "Two weeks!" as an answer that requires a mild verbal pimp-slap in response.
AnswerKenntnis:
It depends on the organization and how the estimates are used.

If the estimate is just to provide a general idea on when it will be ready, I can generally do a quick estimate based on my experience. Often times I will include any uncertainty or possible variations with the estimate along with how the changes may impact other areas of the system and the extent of regression testing required.

If the estimate is used for anything contractual or in a scenario where more precise timing is required, I do a full work break down. This is more work and requires more in depth thinking about the design and changes to the system, but is much more accurate, especially for larger pieces of work.

In either case, on-going communication is key. If you do run into something unexpected, make it known at the time instead of waiting until the deadline. If the requirements are not-clear, make sure you document your understanding of them and the functionality that you plan to deliver. This is also helpful with any assumptions you make. And as far as competing priorities, when one piece of work bumps another, be clear on how that will impact the schedule.
AnswerKenntnis:
Present a range based on what you know today. Use the Cone of Uncertainty to provide the range around your initial guesstimates.

Every week calculate how much is left to do, re-estimate based on what you know. Once you have enough of a sample size of how much work you are getting through each week, provide a 90% confidence interval for what's left to give a (usually) ever narrowing date range as the project progresses and the amount of work left (hopefully) shrinks.
AnswerKenntnis:
Estimates for what? Small tasks or complete solutions.

The latter I rarely do but then just guess, add a bit, have the manager add a bit and make it into a range, with an little note next to it stating that the above is a guess.

Small tasks - Planning poker I've found to work really well (not perfect, some 1pt tasks have taken much longer and some 5pt tasks took minutes, but it all evens out in the end).
AnswerKenntnis:
Confidently. I can't tell you how many times I botched up an initial meeting with a client by not putting on professionalism when giving an estimate. Even if you're blowing numbers out of thin air - make sure you always keep some estimate around. That said, be careful not to estimate yourself into a hole. Different things take different amount amounts of time, effort and resources to put together. Here's a good way to do it:


  Them: How much will it cost?
  
  Me: It depends on what you want me to do. Generally, I start this sort of project at around $X.
AnswerKenntnis:
Sometimes estimating becomes an enormous challenge for you and your team, especially when we are talking about software project estimation.

Once we had decided to share our experience and our knowledge about software estimation process and defined four distinct types of estimations:


ballpark figure
service estimate
feature estimate
componential estimate


Of course, those types are distinct. Ballpark is what is often called a ΓÇ£guesstimateΓÇ¥. So it's an approximate number or range that gives a general idea of cost and that may help a prospect decide whether they would like to take the discussion further. 

As a rule, clients need a ballpark figure at the beginning of the project. And our advise is: discussion of the project and providing ballpark figures should just be steps well towards receiving componential estimate (which is flexible, one can make use of componential type estimate for the whole development process. No need to re-estimate from scratch when you want to add, remove or replace features, services etc).

Everyone should keep in mind the risks that come with software development estimating: underestimating, overestimating, total epic fail scenario etc. 

You can read more on our blog!

http://blog.lemberg.co.uk/project-management/software-estimation-process/

Hope this information will help you!
AnswerKenntnis:
First, if some task was assigned to me I would break it down into subtasks.I would estimate the time for each subtasks and probably with subtasks I would be able to find the problematic area and hence I would be able to forecast how long it would take to a certain extent.

But still all the planning would help only to a certain extent. Only when you start coding you can find the exact issues
